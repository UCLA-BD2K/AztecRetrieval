{
  "doi": "10.1093/bioinformatics/btv035", 
  "name": "3USS a web server for detecting alternative 3UTRs from RNAseq experiments", 
  "links": [
    "http://www.biocomputing.it/3uss", 
    "http://genome.ucsc.edu)expression", 
    "http://cufflinks.cbcb.umd.edu/igenomes.html", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "3USS: a web server for detecting alternative 3 0 UTRs from RNA-seq experiments", 
  "toolName": "3USS: a web server for detecting alternative 3 0 UTRs from RNA-seq experiments", 
  "abstract": "Protein-coding genes with multiple alternative polyadenylation sites can generate mRNA 3 0 UTR sequences of different lengths, thereby causing the loss or gain of regulatory elements , which can affect stability, localization and translation efficiency. 3USS is a web-server developed with the aim of giving experimentalists the possibility to automatically identify alternative 3 0 UTRs (shorter or longer with respect to a reference transcriptome), an option that is not available in standard RNA-seq data analysis procedures. The tool reports as putative novel the 3 0 UTRs not annotated in available databases. Furthermore, if data from two related samples are uploaded, common and specific alternative 3 0 UTRs are identified and reported by the server.", 
  "summary": "The alternative 30UTRs are then compared with already annotated ones in other databases [the latest available data from iGenomes (http://cufflinks.cbcb.umd.edu/igenomes.html) and Gencode (Harrow et al., 2012)] to identify putative novel (not yet annotated) 30UTRs. The most informative output files that the 3USS server produces are: a fasta file with the alternative 30UTR nucleotide sequences and other information in their headers (gene Id, coding strand, 30UTR genomic coordinates and length); a list of the transcripts with their 30UTR length difference and a list of the alternative not yet annotated 30UTRs (putative novel) in other databases as well as the annotated ones.", 
  "affiliations": [], 
  "grants": [
    "Funding for open access charge: KAUST.", 
    "Funding\nThis project was supported by KAUST [Grant Number KUK-I1-012-43], PRIN 20108XYHJS and Epigenomics Flagship Project--EPIGEN."
  ], 
  "acks": " The authors thank the Biocomputing Unit for interesting discussions. This project was supported by KAUST ", 
  "authors": [
    " Loredana Le Pera", 
    " Mariagiovanna Mazzapioda", 
    " Anna Tramontano", 
    " Istituto Pasteur", 
    " \u2013 Fondazione", 
    " Cenci Bolognetti"
  ], 
  "keyWords": [
    [
      "genomics", 
      "genes", 
      "differences", 
      "utrs", 
      "alternative", 
      "transcripts"
    ]
  ], 
  "sourcelinks": [
    "http://cufflinks.cbcb.umd.edu/igenomes.html"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-24T04:27:59Z"
}{
  "doi": "10.1093/bioinformatics/btu829", 
  "name": "3Dmoljs molecular visualization with WebGL", 
  "links": [
    "http://www.jmol.org", 
    "http://biasmv.github.io/pv", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://threejs.org", 
    "http://3dmol.csb.pitt.edu.does", 
    "http://3Dmol.csb.pitt.edu"
  ], 
  "title": "Structural bioinformatics 3Dmol.js: molecular visualization with WebGL", 
  "toolName": "pv", 
  "abstract": "3Dmol.js is a modern, object-oriented JavaScript library that uses the latest web technologies to provide interactive, hardware-accelerated three-dimensional representations of molecular data without the need to install browser plugins or Java. 3Dmol.js provides a full featured API for developers as well as a straightforward declarative interface that lets users easily share and embed molecular data in websites. Availability and implementation: 3Dmol.js is distributed under the permissive BSD open source license. Source code and documentation can be found at http://3Dmol.csb.", 
  "summary": "Summary: 3Dmol.js is a modern, object-oriented JavaScript library that uses the latest web technologies to provide interactive, hardware-accelerated three-dimensional representations of molecular data without the need to install browser plugins or Java.\n3Dmol.js is a pure JavaScript, hardware-accelerated, object-oriented molecular visualization library that enables web developers and casual users to visualize and interact with molecular data in any modern desktop or mobile web browser with near native performance.\n3Dmol.js can be used to view molecular data by web application developers, HTML authors and end users.\n3Dmol.js is an high-performance interactive viewer for 3D molecular data that requires no plugins to work in modern desktop and mobile web browsers.", 
  "affiliations": [
    " Department of Computational and Systems Biology University of Pittsburgh "
  ], 
  "grants": [
    "Funding\nThis work was supported by the National Institute of Health [R01GM108340]."
  ], 
  "sourcelinks": [
    "http://biasmv.github.io/pv", 
    "http://3dmol.csb.pitt.edu.does", 
    "http://3Dmol.csb.pitt.edu"
  ], 
  "acks": " We are grateful to Takanori Nakane for creating GLmol. We would also like to thank Caleb Martin for his contributions. This work was supported by the National Institute of Health. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. Conflict of Interest: none declared. ", 
  "authors": [
    " Nicholas Rego", 
    " David Koes"
  ], 
  "keyWords": [
    "dmol molecular", 
    [
      "data", 
      "viewer_", 
      "javascript", 
      "http"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/biasmv/pv/master/LICENSE.txt"
      }
    ], 
    "name": "pv", 
    "contributors": [
      {
        "contributions": 764, 
        "html_url": "https://github.com/biasmv"
      }, 
      {
        "contributions": 11, 
        "html_url": "https://github.com/ajfarkas"
      }, 
      {
        "contributions": 4, 
        "html_url": "https://github.com/wilzbach"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/julianheinrich"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/aschwanb"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/hainm"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/aebrahim"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/andreasprlic"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/vavrusa"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/smsaladi"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/tomck"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/kaulkie"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.8.1", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.8.1", 
        "name": "v1.8.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.8.0", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.8.0", 
        "name": "v1.8.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.7.2", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.7.2", 
        "name": "v1.7.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.7.1", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.7.1", 
        "name": "v1.7.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.7.0", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.7.0", 
        "name": "v1.7.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.5.0", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.5.0", 
        "name": "v1.5.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.4.0", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.4.0", 
        "name": "v1.4.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.3.1", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.3.1", 
        "name": "v1.3.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.3", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.3", 
        "name": "v1.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.2", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.2", 
        "name": "v1.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.1", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.1", 
        "name": "v1.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/v1.0", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/v1.0", 
        "name": "v1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biasmv/pv/zipball/1.7.0", 
        "tarball_url": "https://api.github.com/repos/biasmv/pv/tarball/1.7.0", 
        "name": "1.7.0"
      }
    ], 
    "created_at": "2013-02-16T11:32:05Z", 
    "updated_at": "2016-08-06T06:27:18Z", 
    "languages": [
      "Python", 
      "Shell", 
      "JavaScript", 
      "HTML", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/biasmv"
      }, 
      {
        "html_url": "https://github.com/mizuchi3"
      }, 
      {
        "html_url": "https://github.com/Traksewt"
      }, 
      {
        "html_url": "https://github.com/sheeep"
      }, 
      {
        "html_url": "https://github.com/biochem-fan"
      }, 
      {
        "html_url": "https://github.com/andreasprlic"
      }, 
      {
        "html_url": "https://github.com/cccsander"
      }, 
      {
        "html_url": "https://github.com/javierbq"
      }, 
      {
        "html_url": "https://github.com/dsimm"
      }, 
      {
        "html_url": "https://github.com/josemduarte"
      }, 
      {
        "html_url": "https://github.com/williamratcliff"
      }, 
      {
        "html_url": "https://github.com/JoaoRodrigues"
      }, 
      {
        "html_url": "https://github.com/mcanthony"
      }, 
      {
        "html_url": "https://github.com/mriffle"
      }, 
      {
        "html_url": "https://github.com/platinhom"
      }, 
      {
        "html_url": "https://github.com/kaulkie"
      }, 
      {
        "html_url": "https://github.com/rbyirdaw"
      }, 
      {
        "html_url": "https://github.com/logishkas"
      }, 
      {
        "html_url": "https://github.com/viktortarm"
      }, 
      {
        "html_url": "https://github.com/tmgriffiths"
      }, 
      {
        "html_url": "https://github.com/pennnnny"
      }, 
      {
        "html_url": "https://github.com/hainm"
      }, 
      {
        "html_url": "https://github.com/kozo2"
      }, 
      {
        "html_url": "https://github.com/RasmusFonseca"
      }, 
      {
        "html_url": "https://github.com/ajfarkas"
      }, 
      {
        "html_url": "https://github.com/gomoto"
      }, 
      {
        "html_url": "https://github.com/curtpw"
      }, 
      {
        "html_url": "https://github.com/zhongduowang"
      }, 
      {
        "html_url": "https://github.com/RafalUrniaz"
      }, 
      {
        "html_url": "https://github.com/dgosting"
      }
    ], 
    "owner": "https://github.com/biasmv", 
    "homepage": "https://pv.readthedocs.org"
  }, 
  "technologies": [
    "HTML", 
    "JavaScript"
  ], 
  "dateCreated": "2014-12-14T01:33:54Z"
}{
  "doi": "10.1093/bioinformatics/btu710", 
  "name": "A coalescentbased method for population tree inference with haplotypes", 
  "links": [
    "http://www.engr.uconn.edu/*ywu/STELLS.html", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "A coalescent-based method for population tree inference with haplotypes", 
  "toolName": "A coalescent-based method for population tree inference with haplotypes", 
  "abstract": "Motivation: Population trees represent past population divergence histories. The inference of population trees can be useful for the study of population evolution. With the size of data increases in large-scale population genetic projects, such as the 1000 Genomes Project, there are new computational challenges for ancestral population inference, including population tree inference. Existing methods for population tree inference are mainly designed for unlinked genetic variants (e.g. single nucleotide polymorphisms or SNPs). There is a potential loss of information by not considering the haplotypes. Results: In this article, we propose a new population tree inference method (called STELLS H) based on coalescent likelihood. The likelihood is for haplotypes over multiple SNPs within a non-recom-bining region, not unlinked variants. Unlike many existing ancestral inference methods, STELLS H does not use Monte Carlo approaches when computing the likelihood. For efficient computation, the likelihood model is approximated but still retains much information about population divergence history. STELLS H can find the maximum likelihood population tree based on the approximate likelihood. We show through simulation data and the 1000 Genomes Project data that STELLS H gives reasonably accurate inference results. STELLS H is reasonably efficient for data of current interest and can scale to handle whole-genome data. Availability and implementation: The population tree inference method STELLS H has been implemented as part of the STELLS program", 
  "summary": "A coalescent-based method for population tree inference with haplotypes\nResults: In this article, we propose a new population tree inference method (called STELLSH) based on coalescent likelihood.\nThe main feature of STELLSH is its use of haplotypes rather than individual genetic variations as used in existing population tree inference methods, e.g. Bryant et al (2012) and Pickrell and Pritchard (2012).\nThe main focus of this article is demonstrating that population trees can be inferred relatively accurately and efficiently from haplotypes, and haplotypes may provide more useful information for population tree inference than individual SNPs. There are several features of our new method STELLSH that need more discussion.", 
  "affiliations": [], 
  "grants": [
    "Funding\nResearch partly supported by NSF Grants IIS-0953563 and CCF-1116175.", 
    "Parts of simulations are performed on a computer cluster that is supported under Grant S10-RR027140 from NIH."
  ], 
  "acks": " ", 
  "authors": [], 
  "keyWords": [
    "population tree inference", 
    [
      "stellsh", 
      "genetics", 
      "populations", 
      "modeled", 
      "trees", 
      "haplotypes", 
      "data", 
      "inferring"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-25T03:54:31Z"
}{
  "doi": "10.1093/bioinformatics/btu860", 
  "name": "A haplotypebased framework for groupwise transmissiondisequilibrium tests for rare variant association analysis", 
  "links": [
    "https://medschool.vanderbilt.edu/cgg", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetic and population analysis A haplotype-based framework for group-wise transmission/disequilibrium tests for rare variant association analysis Downloaded from", 
  "toolName": "Genetic and population analysis A haplotype-based framework for group-wise transmission/disequilibrium tests for rare variant association analysis Downloaded from", 
  "abstract": "Motivation: A major focus of current sequencing studies for human genetics is to identify rare variants associated with complex diseases. Aside from reduced power of detecting associated rare variants, controlling for population stratification is particularly challenging for rare variants. Transmission/disequilibrium tests (TDT) based on family designs are robust to population stratifi-cation and admixture, and therefore provide an effective approach to rare variant association studies to eliminate spurious associations. To increase power of rare variant association analysis, gene-based collapsing methods become standard approaches for analyzing rare variants. Existing methods that extend this strategy to rare variants in families usually combine TDT statistics at individual variants and therefore lack the flexibility of incorporating other genetic models. Results: In this study, we describe a haplotype-based framework for group-wise TDT (gTDT) that is flexible to encompass a variety of genetic models such as additive, dominant and compound heterozygous (CH) (i.e. recessive) models as well as other complex interactions. Unlike existing methods, gTDT constructs haplotypes by transmission when possible and inherently takes into account the linkage disequilibrium among variants. Through extensive simulations we showed that type I error was correctly controlled for rare variants under all models investigated, and this remained true in the presence of population stratification. Under a variety of genetic models, gTDT showed increased power compared with the single marker TDT. Application of gTDT to an autism exome sequencing data of 118 trios identified potentially interesting candidate genes with CH rare variants. Availability and implementation: We implemented gTDT in C\u00fe\u00fe and the source code and the detailed usage are available on the authors' website (https://medschool.", 
  "summary": "The collapsing approaches of rare variants originally developed for unrelated samples have been extended to gene-based TDT and show improved power over single marker TDT while controlling for population stratification/admixture (De et al., 2013; He et al., 2014; Jiang et al., 2014), providing a robust approach to rare variant analysis.\nIn this study, we describe a general framework for group-wise TDT (gTDT) for rare variants and implement a few classical genetic models in a user-friendly and computationally efficient tool.\nVariants that are heterozygous in all individuals of a trio are non-informative in single marker tests, but have impact on the type I error of haplotype-based gTDT if not phased correctly.", 
  "affiliations": [
    " Department of Medicine University of Chicago ", 
    " Quantitative Biomedical Research Center University of Texas Southwestern Medical Center ", 
    " Department of Molecular Physiology and Biophysics Vanderbilt University ", 
    " Department of Psychiatry University of Illinois at Chicago ", 
    " Department of Pediatrics University of Pittsburgh ", 
    " Department of Epidemiology and Biostatistics Case Western Reserve University ", 
    " Center for Quantitative Sciences Vanderbilt University "
  ], 
  "grants": [
    "Funding\nThis study was supported by the National Institute of Health [grant R01HG006857 to R.C., Q.W."
  ], 
  "acks": " The authors thank Goncalo Abecasis in the Department of Biostatistics at the University of Michigan for sharing the C\u00fe\u00fe code for processing pedigrees. This study was supported by the National Institute of Health ", 
  "authors": [
    " Rui Chen", 
    " Qiang Wei", 
    " Xiaowei Zhan", 
    " Xue Zhong", 
    " James S Sutcliffe", 
    " Nancy J Cox", 
    " Edwin H Cook", 
    " Chun Li", 
    " Wei Chen", 
    " Bingshan Li"
  ], 
  "keyWords": [
    "rare variant", 
    [
      "modeling", 
      "haplotyping", 
      "genetics", 
      "variants"
    ]
  ], 
  "sourcelinks": [
    "https://medschool.vanderbilt.edu/cgg"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-08T02:10:15Z"
}{
  "doi": "10.1093/bioinformatics/btu695", 
  "name": "A model for the expression of gap genes based on the Jeffreystype equation", 
  "links": [
    "http://urchin.spbcas.ru/flyex", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/wswgG/solver-for-the-Jeffreys-type-equations-system", 
    "http://flyex.ams.sunysb.edu/FlyEx"
  ], 
  "title": "Gene expression A model for the expression of gap genes based on the Jeffreys-type equation", 
  "toolName": "solver-for-the-Jeffreys-type-equations-system", 
  "abstract": "Motivation: We propose the third-order model equation of the Jeffreys type for concentrations of gap gene proteins in order to take into account particle inertia. Gap genes are responsible for formation of body segments in Drosophila melanogaster embryo during its early development. Usually the expression of the genes is described by the model of protein transport based on conventional diffusion equation. However, the model is known to govern the Brownian (non-inertial) motion of particles; hence, it is hardly applicable to the description of protein transport. Results: Analysis of the Jeffreys-type equation results in the necessary condition for the problem to be well-posed. Application of the Jeffreys-type equation with non-linear terms to description of the dynamics of gap gene network demonstrates better fitting to experimental data than the conventional model. Availability and implementation: Implementation of solver algorithms and the software are freely available from: https://github.com/wswgG/solver-for-the-Jeffreys-type-equations-system Contact:", 
  "summary": "Application of the Jeffreys-type equation with non-linear terms to description of the dynamics of gap gene network demonstrates better fitting to experimental data than the conventional model.\nThe results of the first set of 10 calculations demonstrate slightly better quantitative fitting to the experimental data on gap gene expression in Drosophila blastoderm as compared with the conventional model (1.1) with lowest RMS  8.502 value for the model with the Jeffreys-type equation and the Neumann boundary conditions, while for the conventional model the lowest value is RMS  8.532.", 
  "affiliations": [
    " The Ioffe Physical Technical Institute St. Petersburg "
  ], 
  "grants": [
    "Funding\nThis work was supported by the Russian Foundation for Basic Researches (grant number 14-01-00034) and by the Ministry of Education and Science of Russia (programme `5-100-2020')."
  ], 
  "sourcelinks": [
    "https://github.com/wswgG/solver-for-the-Jeffreys-type-equations-system"
  ], 
  "acks": " We thank V. Gursky, S. Rukolaine, K. Kozlov and M. Samsonova for helpful comments. ", 
  "authors": [
    " Igor A Gula", 
    " Alexander M Samsonov"
  ], 
  "keyWords": [
    [
      "equations", 
      "differences", 
      "genes", 
      "proteins", 
      "modelling"
    ]
  ], 
  "github_data": {
    "name": "solver-for-the-Jeffreys-type-equations-system", 
    "contributors": [
      {
        "contributions": 5, 
        "html_url": "https://github.com/wswgG"
      }
    ], 
    "versions": [], 
    "created_at": "2014-04-30T12:27:06Z", 
    "updated_at": "2014-04-30T14:09:32Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/wswgG"
      }
    ], 
    "owner": "https://github.com/wswgG", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-23T00:08:46Z"
}{
  "doi": "10.1093/bioinformatics/btu515", 
  "name": "A method for de novo nucleic acid diagnostic target discovery", 
  "links": [
    "http://ucr.syn", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://ucr.synblex.com"
  ], 
  "title": "A method for de novo nucleic acid diagnostic target discovery", 
  "toolName": "A method for de novo nucleic acid diagnostic target discovery", 
  "abstract": "Motivation: A proper target or marker is essential in any diagnosis (e.g. an infection or cancer). An ideal diagnostic target should be both conserved in and unique to the pathogen. Currently, these targets can only be identified manually, which is time-consuming and usually error-prone. Because of the increasingly frequent occurrences of emerging epidemics and multidrug-resistant 'superbugs', a rapid diagnostic target identification process is needed. Results: A new method that can identify uniquely conserved regions (UCRs) as candidate diagnostic targets for a selected group of organisms solely from their genomic sequences has been developed and successfully tested. Using a sequence-indexing algorithm to identify UCRs and a k-mer integer-mapping model for computational efficiency , this method has successfully identified UCRs within the bacteria domain for 15 test groups, including pathogenic, probiotic, commensal and extremophilic bacterial species or strains. Based on the identified UCRs, new diagnostic primer sets were designed, and their specificity and efficiency were tested by polymerase chain reaction amplifications from both pure isolates and samples containing mixed cultures. Availability and implementation: The UCRs identified for the 15 bacterial species are now freely available at http://ucr.synblex.com. The source code of the programs used in this study is accessible at", 
  "summary": "Results: A new method that can identify uniquely conserved regions (UCRs) as candidate diagnostic targets for a selected group of organisms solely from their genomic sequences has been developed and successfully tested.\nUsing a sequence-indexing algorithm to identify UCRs and a k-mer integer-mapping model for computational efficiency, this method has successfully identified UCRs within the bacteria domain for 15 test groups, including pathogenic, probiotic, commensal and extremophilic bacterial species or strains.\nIn contrast, if the background index has already been built with completed genomes, and multiple samples of a new group were sequenced, UCR k-mers could be identified directly from the raw sequencing reads with this method.", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was supported by the Synblex LLC research fund."
  ], 
  "acks": " The authors sincerely thank Prof. Qi Ji (Fudan University) for discussions related to the k-mer index method. And they also thank the three anonymous reviewers for their critical and constructive comments. ", 
  "authors": [
    " Yeting Zhang", 
    " Yazhou Sun", 
    " John Hancock"
  ], 
  "keyWords": [
    "genomic sequences", 
    [
      "primers", 
      "ucrs", 
      "proteins", 
      "genomes", 
      "groups", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [
    "http://ucr.syn", 
    "http://ucr.synblex.com"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-08T05:05:20Z"
}{
  "doi": "10.1093/bioinformatics/btu398", 
  "name": "A multiobjective method for robust identification of bacterial small noncoding RNAs", 
  "links": [
    "http://m4m.ugr.es/SupInfo/sRNAOS", 
    "http://newbio.cs.wisc.edu/sRNA", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://blast.ncbi.nlm.nih.gov"
  ], 
  "title": "Sequence analysis A multiobjective method for robust identification of bacterial small non-coding RNAs", 
  "toolName": "Sequence analysis A multiobjective method for robust identification of bacterial small non-coding RNAs", 
  "abstract": "Motivation: Small non-coding RNAs (sRNAs) have major roles in the post-transcriptional regulation in prokaryotes. The experimental validation of a relatively small number of sRNAs in few species requires developing computational algorithms capable of robustly encoding the available knowledge and using this knowledge to predict sRNAs within and across species. Results: We present a novel methodology designed to identify bacterial sRNAs by incorporating the knowledge encoded by different sRNA prediction methods and optimally aggregating them as potential pre-dictors. Because some of these methods emphasize specificity, whereas others emphasize sensitivity while detecting sRNAs, their optimal aggregation constitutes trade-off solutions between these two contradictory objectives that enhance their individual merits. Many non-redundant optimal aggregations uncovered by using multiobjec-tive optimization techniques are then combined into a multiclassifier, which ensures robustness during detection and prediction even in gen-omes with distinct nucleotide composition. By training with sRNAs in Salmonella enterica Typhimurium, we were able to successfully predict sRNAs in Sinorhizobium meliloti, as well as in multiple and poorly annotated species. The proposed methodology, like a meta-analysis approach , may begin to lay a possible foundation for developing robust predictive methods across a wide spectrum of genomic variability. Availability and implementation: Scripts created for the experimentation are available at", 
  "summary": "Although several methods have been developed and represent a step forward in the computational detection of sRNAs, their success is limited because of (i) the excessive emphasis on specificity, which generates a high number of false-negative predictions by targeting few well-known sRNA families corresponding only to a small percentage of the available sRNAs, (ii) the use of genomic features (e.g. motifs of terminators, RNA polymerase) that are only conserved in closely related organisms [e.g. E.coli and Salmonella (Argaman et al., 2001; Vogel, 2009)] and thus cause", 
  "affiliations": [
    " Department of Computer Science and Artificial Intelligence Universidad de Granada "
  ], 
  "grants": [
    "Funding: All authors were funded in part by FEDER fonds; the Spanish Ministry of Science and Technology under projects TIN2009-13950 and TIN2012-38805; the Consejeria de Innovacio n, Investigacio n y Ciencia, Junta de Andalucia, under project TIC-02788."
  ], 
  "acks": " ", 
  "authors": [
    " Javier Arnedo", 
    " Roc Io Romero-Zaliz", 
    " Igor Zwir", 
    " Coral Del Val", 
    " John Hancock"
  ], 
  "keyWords": [
    "prediction methods", 
    [
      "sequencing", 
      "genomics", 
      "srnas", 
      "predictions", 
      "method"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-06-24T06:29:49Z"
}{
  "doi": "10.1093/bioinformatics/btu630", 
  "name": "A Python package for parsing validating mapping and formatting sequence variants using HGVS nomenclature", 
  "links": [
    "https://github.com/counsyl/hgvs", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://pypi.python.org/pypi", 
    "http://bitbucket.org/hgvs/hgvs"
  ], 
  "title": "Sequence analysis A Python package for parsing, validating, mapping and formatting sequence variants using HGVS nomenclature", 
  "toolName": "hgvs", 
  "abstract": "Biological sequence variants are commonly represented in scientific literature, clinical reports and databases of variation using the mutation nomenclature guidelines endorsed by the Human Genome Variation Society (HGVS). Despite the widespread use of the standard, no freely available and comprehensive programming libraries are available. Here we report an open-source and easy-to-use Python library that facilitates the parsing, manipulation, formatting and validation of variants according to the HGVS specification. The current implementation focuses on the subset of the HGVS recommendations that precisely describe sequence-level variation relevant to the application of high-throughput sequencing to clinical diagnostics. Availability and implementation: The package is released under the Apache 2.0 open-source license. Source code, documentation and issue tracking are available at", 
  "summary": "The hgvs Python package comprises six key components: (i) object models for representing components of HGVS-formatted variants; (ii) a parser that generates an object representation from an HGVS-formatted string; (iii) formatting tools that generate an HGVS-formatted string from an object representation; (iv) mapping classes that transform variants between genomic, CDS and protein representations; (v) validation tools that ensure conformance to HGVS guidelines; (vi) an interface for defining external data sources required for validation and mapping.\nThe hgvs package requires sequence data and exon structures to map variants between the genome and transcript coordinates, to infer protein sequence changes from transcripts and to validate variants.", 
  "affiliations": [
    " Invitae Inc", 
    " 23andMe Inc"
  ], 
  "grants": [
    "Conflict of interest: All authors are employed by and have equity in the company that sponsored this work."
  ], 
  "sourcelinks": [
    "https://github.com/counsyl/hgvs", 
    "http://bitbucket.org/hgvs/hgvs"
  ], 
  "acks": " ", 
  "authors": [
    " Reece K Hart", 
    " Rudolph Rico", 
    " Emily Hare", 
    " John Garcia", 
    " Jody Westbrook", 
    " Vincent A Fusaro", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "validation", 
      "hgvs", 
      "bioinformatics", 
      "sequencing", 
      "variants", 
      "packages", 
      "transcripts"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/counsyl/hgvs/master/LICENSE"
      }
    ], 
    "name": "hgvs", 
    "contributors": [
      {
        "contributions": 4, 
        "html_url": "https://github.com/pkaleta"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/ChrisBeaumont"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/mdrasmus"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/counsyl/hgvs/zipball/v0.9.2", 
        "tarball_url": "https://api.github.com/repos/counsyl/hgvs/tarball/v0.9.2", 
        "name": "v0.9.2"
      }
    ], 
    "created_at": "2013-10-22T05:18:32Z", 
    "updated_at": "2016-08-08T18:27:23Z", 
    "languages": [
      "Python", 
      "Shell", 
      "Makefile"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/balajis"
      }, 
      {
        "html_url": "https://github.com/abhik"
      }, 
      {
        "html_url": "https://github.com/jdavisp3"
      }, 
      {
        "html_url": "https://github.com/pkaleta"
      }, 
      {
        "html_url": "https://github.com/dandavison"
      }, 
      {
        "html_url": "https://github.com/ck0"
      }, 
      {
        "html_url": "https://github.com/rkacker"
      }, 
      {
        "html_url": "https://github.com/jbronn"
      }, 
      {
        "html_url": "https://github.com/mdrasmus"
      }, 
      {
        "html_url": "https://github.com/ascottp"
      }, 
      {
        "html_url": "https://github.com/jwan"
      }, 
      {
        "html_url": "https://github.com/ericevans"
      }, 
      {
        "html_url": "https://github.com/hrichards"
      }, 
      {
        "html_url": "https://github.com/jfalkner"
      }, 
      {
        "html_url": "https://github.com/richardxia"
      }, 
      {
        "html_url": "https://github.com/cyc"
      }, 
      {
        "html_url": "https://github.com/sakoht"
      }, 
      {
        "html_url": "https://github.com/AndrewUzilov"
      }, 
      {
        "html_url": "https://github.com/jared-maguire"
      }, 
      {
        "html_url": "https://github.com/lucaswiman"
      }, 
      {
        "html_url": "https://github.com/mattmdedek"
      }, 
      {
        "html_url": "https://github.com/aristretto"
      }, 
      {
        "html_url": "https://github.com/jonemo"
      }, 
      {
        "html_url": "https://github.com/boboppie"
      }, 
      {
        "html_url": "https://github.com/MichaelPhantom"
      }, 
      {
        "html_url": "https://github.com/nlwashington"
      }, 
      {
        "html_url": "https://github.com/cmarkello"
      }, 
      {
        "html_url": "https://github.com/nisanharamati"
      }, 
      {
        "html_url": "https://github.com/myyk"
      }, 
      {
        "html_url": "https://github.com/hobbeswalsh"
      }
    ], 
    "owner": "https://github.com/counsyl", 
    "homepage": null
  }, 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu479", 
  "name": "A new approach for detecting riboswitches in DNA sequences", 
  "links": [
    "http://rfam.xfam.org", 
    "http://drd.denison.edu", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Sequence analysis A new approach for detecting riboswitches in DNA sequences", 
  "toolName": "Sequence analysis A new approach for detecting riboswitches in DNA sequences", 
  "abstract": "Motivation: Riboswitches are short sequences of messenger RNA that can change their structural conformation to regulate the expression of adjacent genes. Computational prediction of putative ribos-witches can provide direction to molecular biologists studying riboswitch-mediated gene expression. Results: The Denison Riboswitch Detector (DRD) is a new computational tool with a Web interface that can quickly identify putative ribos-witches in DNA sequences on the scale of bacterial genomes. Riboswitch descriptions are easily modifiable and new ones are easily created. The underlying algorithm converts the problem to a 'heaviest path' problem on a multipartite graph, which is then solved using efficient dynamic programming. We show that DRD can achieve $88\u201399% sensitivity and 499.99% specificity on 13 riboswitch families.", 
  "summary": "Results: The Denison Riboswitch Detector (DRD) is a new computational tool with a Web interface that can quickly identify putative riboswitches in DNA sequences on the scale of bacterial genomes.\nSecond, to measure the corresponding false-positive rate (FPR) for each family in genome-scale sequences, we randomly selected a small number of Rfam-predicted riboswitch sequences and tested DRD on the set of GenBank sequence records in which they are contained.\nThe results, displayed in Figure 3 and Table 1, show that, with the exception of the cobalamin riboswitch, we get a sensitivity of $90% or better for each riboswitch family when S1 is set to be $95% of the maximum possible motif identify score.", 
  "affiliations": [
    " Department of Biology Denison University ", 
    " Department of Computer Science Wake Forest University ", 
    " Department of Mathematics and Computer Science", 
    " College of Human Medicine Michigan State University "
  ], 
  "grants": [
    "Funding: This work was performed by C.B., S.M.J."
  ], 
  "acks": " ", 
  "authors": [
    " Jessen T Havill", 
    " Chinmoy Bhatiya", 
    " Steven M Johnson", 
    " Joseph D Sheets", 
    " Jeffrey S Thompson", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "sequences", 
      "structures", 
      "riboswitches", 
      "motifs", 
      "specificity"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "PHP", 
    "C++"
  ], 
  "dateCreated": "2014-07-12T03:16:15Z"
}{
  "doi": "10.1093/bioinformatics/btu661", 
  "name": "A5miseq an updated pipeline to assemble microbial genomes from Illumina MiSeq data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://sourceforge.net/projects/ngopt"
  ], 
  "title": "Genome analysis A5-miseq: an updated pipeline to assemble microbial genomes from Illumina MiSeq data", 
  "toolName": "ngopt", 
  "abstract": "Motivation: Open-source bacterial genome assembly remains inaccessible to many biologists because of its complexity. Few software solutions exist that are capable of automating all steps in the process of de novo genome assembly from Illumina data. Results: A5-miseq can produce high-quality microbial genome assemblies on a laptop computer without any parameter tuning. A5-miseq does this by automating the process of adapter trimming, quality filtering, error correction, contig and scaffold generation and detection of misassemblies. Unlike the original A5 pipeline, A5-miseq can use long reads from the Illumina MiSeq, use read pairing information during contig generation and includes several improvements to read trimming. Together, these changes result in substantially improved assemblies that recover a more complete set of reference genes than previous methods. Availability: A5-miseq is licensed under the GPL open-source license. Source code and precompiled binaries for Mac OS X 10.6+ and Linux 2.6.15+ are available from http://sourceforge.net/projects/ngopt", 
  "summary": "Results: A5-miseq can produce high-quality microbial genome assemblies on a laptop computer without any parameter tuning.\nWe previously published A5, a pipeline that automated all the steps to generate bacterial genome assemblies from raw Illumina data (Tritt et al., 2012).\nWe ran A5-miseq and A5 assemblies and obtained results for other assemblers from the GAGE-B publication (Magoc et al., 2013).\nAssembly accuracy for the A5-miseq and A5 pipelines measured on raw 100 coverage MiSeq PE250 GAGE-B data.\nA5-miseq produces results that are competitive with the best achieved by other assemblers on the GAGE-B data, but requires only minimal user-effort.", 
  "affiliations": [
    " Genome Center University of California Davis ", 
    " ithree institute University of Technology Sydney "
  ], 
  "grants": [
    "Funding: This work was supported in part by a collaborative agreement with the NSW Department of Primary Industries."
  ], 
  "acks": " The authors would like to thank users of the A5 pipeline for reporting bugs and usability problems with the software, in particular Piklu Roy Chowdhury and the manuscript peer reviewers. Funding: This work was supported in part by a collaborative agreement with the NSW Department of Primary Industries. Conflict of interest: none declared. ", 
  "authors": [
    " David Coil", 
    " Guillaume Jospin", 
    " Aaron E Darling"
  ], 
  "keyWords": [
    "miseq data", 
    [
      "assemblies", 
      "genes", 
      "genomes", 
      "results", 
      "reads", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net/projects/ngopt"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-23T00:08:46Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/ngopt/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/gjospin/", 
        "name": "Guillaume Jospin"
      }, 
      {
        "url": "https://sourceforge.net/u/atritt/", 
        "name": "Andrew Tritt"
      }, 
      {
        "url": "https://sourceforge.net/u/koadman/", 
        "name": "Aaron Darling"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu557", 
  "name": "A novel featurebased approach to extract drugdrug interactions from biomedical text", 
  "links": [
    "http://micromedex.com", 
    "http://www.biosemantics.org/uploads/DDI.zip", 
    "http://alias-i.com/lingpipe", 
    "http://www.drugbank.ca", 
    "https://opennlp.apache.org", 
    "http://www.csie.ntu.edu.tw", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.cs.waikato.ac.nz/ml/weka/).3"
  ], 
  "title": "Data and text mining A novel feature-based approach to extract drug\u2013drug interactions from biomedical text", 
  "toolName": "Data and text mining A novel feature-based approach to extract drug\u2013drug interactions from biomedical text", 
  "abstract": "Motivation: Knowledge of drug\u2013drug interactions (DDIs) is crucial for health-care professionals to avoid adverse effects when co-administering drugs to patients. As most newly discovered DDIs are made available through scientific publications, automatic DDI extraction is highly relevant. Results: We propose a novel feature-based approach to extract DDIs from text. Our approach consists of three steps. First, we apply text preprocessing to convert input sentences from a given dataset into structured representations. Second, we map each candidate DDI pair from that dataset into a suitable syntactic structure. Based on that, a novel set of features is used to generate feature vectors for these candidate DDI pairs. Third, the obtained feature vectors are used to train a support vector machine (SVM) classifier. When evaluated on two DDI extraction challenge test datasets from 2011 and 2013, our system achieves F-scores of 71.1% and 83.5%, respectively, outper-forming any state-of-the-art DDI extraction system. Availability and implementation: The source code is available for academic use at", 
  "summary": "In fact, the winning teams of the DDI extraction 2011 and 2013 challenges both incorporate feature-based kernels proposed by Giuliano et al.\nTo generate features for each candidate DDI pair, we find the smallest syntactic container (e.g. a phrase, a clause or clauses) from the structured representation containing that pair.\nBased on the obtained case, corresponding features are generated to represent the position between the trigger and the candidate DDI pair (i.e. left, middle or right) and to indicate which prepositions are used to connect the trigger and the target pair as well as the chunks between the drugs of the target pair.", 
  "affiliations": [
    " Department of Medical Informatics Erasmus University Medical Center Rotterdam "
  ], 
  "grants": [
    "Funding: PMAS is partially supported by Russian Scientific Foundation, proposal #14-21-0037."
  ], 
  "acks": " ", 
  "authors": [
    " Quoc-Chinh Bui", 
    " Peter M A Sloot", 
    " Erik M Van Mulligen", 
    " Jan A Kors"
  ], 
  "keyWords": [
    [
      "drugs", 
      "performances", 
      "datasets", 
      "features", 
      "syntactically"
    ]
  ], 
  "sourcelinks": [
    "http://www.biosemantics.org/uploads/DDI.zip"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-21T04:05:53Z"
}{
  "doi": "10.1093/bioinformatics/btu376", 
  "name": "ABRA improved coding indel detection via assemblybased realignment", 
  "links": [
    "https://github.com/mozack/abra", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/3.0"
  ], 
  "title": "Sequence analysis ABRA: improved coding indel detection via assembly-based realignment", 
  "toolName": "abra", 
  "abstract": "Motivation: Variant detection from next-generation sequencing (NGS) data is an increasingly vital aspect of disease diagnosis, treatment and research. Commonly used NGS-variant analysis tools generally rely on accurately mapped short reads to identify somatic variants and germ-line genotypes. Existing NGS read mappers have difficulty accurately mapping short reads containing complex variation (i.e. more than a single base change), thus making identification of such variants difficult or impossible. Insertions and deletions (indels) in particular have been an area of great difficulty. Indels are frequent and can have substantial impact on function, which makes their detection all the more imperative. Results: We present ABRA, an assembly-based realigner, which uses an efficient and flexible localized de novo assembly followed by global realignment to more accurately remap reads. This results in enhanced performance for indel detection as well as improved accuracy in variant allele frequency estimation.", 
  "summary": "Short read micro aligner locally realigns reads to regionally assembled variant graphs (Homer and Nelson, 2010).\nlocalized assembly and calling on regions containing reads where only one half of a paired read is mapped (Li et al., 2012).\nLarger insertions of sequence from another location in the genome are likely to be aligned elsewhere and not included in local assembly, thus limiting detection of insertions as the size approaches read length.\nABRA improves on next-generation sequencing read alignments, providing enhanced performance in detection of indels as well as greater accuracy in variant allele frequency estimation.", 
  "affiliations": [
    " Lineberger Comprehensive Cancer Center"
  ], 
  "grants": [
    "Funding: This work was supported in part by the National Cancer Institute Breast SPORE program (P50-CA58223-09A1) and The Cancer Genome Atlas (U24-CA143848-05)."
  ], 
  "sourcelinks": [
    "https://github.com/mozack/abra"
  ], 
  "acks": " ", 
  "authors": [
    " Lisle E Mose", 
    " Matthew D Wilkerson", 
    " D Neil Hayes", 
    " Charles M Perou", 
    " Joel S Parker"
  ], 
  "keyWords": [
    [
      "genomics", 
      "assembler", 
      "reads", 
      "abra", 
      "sequencing", 
      "variants", 
      "alignments"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/mozack/abra/master/LICENSE.txt"
      }
    ], 
    "name": "abra", 
    "contributors": [
      {
        "contributions": 720, 
        "html_url": "https://github.com/mozack"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/lmose"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/NoSeatbelts"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.97", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.97", 
        "name": "v0.97"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.97c", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.97c", 
        "name": "v0.97c"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.97b", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.97b", 
        "name": "v0.97b"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.96", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.96", 
        "name": "v0.96"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.95", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.95", 
        "name": "v0.95"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.94", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.94", 
        "name": "v0.94"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.93", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.93", 
        "name": "v0.93"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.92", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.92", 
        "name": "v0.92"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.91", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.91", 
        "name": "v0.91"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.90", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.90", 
        "name": "v0.90"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.89", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.89", 
        "name": "v0.89"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.88", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.88", 
        "name": "v0.88"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.87", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.87", 
        "name": "v0.87"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.86", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.86", 
        "name": "v0.86"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.85", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.85", 
        "name": "v0.85"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.84", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.84", 
        "name": "v0.84"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.83", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.83", 
        "name": "v0.83"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.82", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.82", 
        "name": "v0.82"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.81", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.81", 
        "name": "v0.81"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.80", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.80", 
        "name": "v0.80"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.79", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.79", 
        "name": "v0.79"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.78", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.78", 
        "name": "v0.78"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.77", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.77", 
        "name": "v0.77"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.76", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.76", 
        "name": "v0.76"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.75", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.75", 
        "name": "v0.75"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.74", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.74", 
        "name": "v0.74"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.73", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.73", 
        "name": "v0.73"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.72", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.72", 
        "name": "v0.72"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.71", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.71", 
        "name": "v0.71"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mozack/abra/zipball/v0.69", 
        "tarball_url": "https://api.github.com/repos/mozack/abra/tarball/v0.69", 
        "name": "v0.69"
      }
    ], 
    "created_at": "2012-11-07T20:43:17Z", 
    "updated_at": "2016-06-08T16:53:52Z", 
    "languages": [
      "Makefile", 
      "C", 
      "Shell", 
      "Java", 
      "C++"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/mozack"
      }, 
      {
        "html_url": "https://github.com/yanlinlin82"
      }, 
      {
        "html_url": "https://github.com/dh10"
      }, 
      {
        "html_url": "https://github.com/rhshah"
      }, 
      {
        "html_url": "https://github.com/etwatson"
      }, 
      {
        "html_url": "https://github.com/jasper1918"
      }, 
      {
        "html_url": "https://github.com/ryan-williams"
      }, 
      {
        "html_url": "https://github.com/meissnert"
      }, 
      {
        "html_url": "https://github.com/simexin"
      }
    ], 
    "owner": "https://github.com/mozack", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-06-07T06:24:55Z"
}{
  "doi": "10.1093/bioinformatics/btu553", 
  "name": "Acceleration of short and long DNA read mapping without loss of accuracy using suffix array", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://source", 
    "https://github.com", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://www.hpc4g.org", 
    "http://wiki.opencb", 
    "https://github.com/opencb/hpgaligner"
  ], 
  "title": "Sequence analysis Acceleration of short and long DNA read mapping without loss of accuracy using suffix array", 
  "toolName": "github.com", 
  "abstract": "HPG Aligner applies suffix arrays for DNA read mapping. This implementation produces a highly sensitive and extremely fast mapping of DNA reads that scales up almost linearly with read length. The approach presented here is faster (over 20\u00c2 for long reads) and more sensitive (over 98% in a wide range of read lengths) than the current state-of-the-art mappers. HPG Aligner is not only an optimal alternative for current sequencers but also the only solution available to cope with longer reads and growing throughputs produced by forthcoming sequencing technologies. Availability and implementation: https://github.com/opencb/hpg-aligner.", 
  "summary": "ABSTRACT HPG Aligner applies suffix arrays for DNA read mapping.\nSuffix array (SA) has recently started to be applied to accelerate DNA (Bussotti et al., 2011; Chen et al., 2013) or RNA (Dobin et al., 2013) read mapping.\nApplying SA in DNA mapping, the large text is the reference genome, and the query text is the read sequence.\nHPG Aligner uses an uncompressed SA to map each of these seeds into the reference genome (Supplementary Fig. S1B).\nWhile percentages of correctly mapped reads were quite similar, HPG Aligner runtimes were significantly lower than BWA MEM ones, especially when read length increases, arriving to 18 for long reads (5000 bp).", 
  "affiliations": [
    " Functional Genomics Node (INB) at CIPF 46012 ", 
    " Department of Computational Genomics Centro de Investigaci on Pr \u0131ncipe Felipe (CIPF) ", 
    " Departamento de Inform atica Universidad de Valencia ", 
    " Bioinformatics of Rare Diseases (BIER) CIBER de Enfermedades Raras (CIBERER) Associate Editor: Inanc Birol "
  ], 
  "grants": [
    "Funding: This work is supported by BIO2011-27069 and PRIPIBIN-2011-1289 (Spanish Ministry of Economy and Competitiveness), the HPC4G initiative (http://www.hpc4g.org) and the Bull-CIPF Chair for Computational Genomics."
  ], 
  "sourcelinks": [
    "http://wiki.opencb", 
    "https://github.com/opencb/hpgaligner", 
    "http://source", 
    "https://github.com"
  ], 
  "acks": " ", 
  "authors": [
    " Joaqu In T Arraga", 
    " Vicente Arnau", 
    " Mart Inez", 
    " Raul Moreno", 
    " Diego Cazorla", 
    " Jos ", 
    " Salavert-Torres ", 
    " Ignacio Blanquer-Espert", 
    " Joaqu In Dopazo", 
    " Ignacio Medina", 
    " "
  ], 
  "keyWords": [
    "read mapping", 
    "genomic sequences", 
    [
      "genomics", 
      "sequencers", 
      "mappings", 
      "reads", 
      "bioinformatics", 
      "alignments"
    ]
  ], 
  "github_data": {
    "name": "node-v0.x-archive", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/orangemocha"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/works", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/works", 
        "name": "works"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.7", 
        "name": "v0.12.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.6", 
        "name": "v0.12.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.5", 
        "name": "v0.12.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.4", 
        "name": "v0.12.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.3", 
        "name": "v0.12.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.2", 
        "name": "v0.12.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.1", 
        "name": "v0.12.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.0", 
        "name": "v0.12.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.16", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.16", 
        "name": "v0.11.16"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.15", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.15", 
        "name": "v0.11.15"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.14", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.14", 
        "name": "v0.11.14"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.13", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.13", 
        "name": "v0.11.13"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.12", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.12", 
        "name": "v0.11.12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.11", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.11", 
        "name": "v0.11.11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.10", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.10", 
        "name": "v0.11.10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.9", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.9", 
        "name": "v0.11.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.8", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.8", 
        "name": "v0.11.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.7", 
        "name": "v0.11.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.6", 
        "name": "v0.11.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.5", 
        "name": "v0.11.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.4", 
        "name": "v0.11.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.3", 
        "name": "v0.11.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.40", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.40", 
        "name": "v0.10.40"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.39", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.39", 
        "name": "v0.10.39"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.38", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.38", 
        "name": "v0.10.38"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.37", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.37", 
        "name": "v0.10.37"
      }
    ], 
    "created_at": "2009-05-27T16:29:46Z", 
    "updated_at": "2016-08-09T16:10:45Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/isaacs"
      }, 
      {
        "html_url": "https://github.com/koichik"
      }, 
      {
        "html_url": "https://github.com/igorzi"
      }, 
      {
        "html_url": "https://github.com/kunoichi-co"
      }, 
      {
        "html_url": "https://github.com/carloslemes"
      }, 
      {
        "html_url": "https://github.com/aboyon"
      }, 
      {
        "html_url": "https://github.com/arden"
      }, 
      {
        "html_url": "https://github.com/guyoun"
      }, 
      {
        "html_url": "https://github.com/vissul"
      }, 
      {
        "html_url": "https://github.com/bdtgzj"
      }, 
      {
        "html_url": "https://github.com/santigimeno"
      }, 
      {
        "html_url": "https://github.com/chrisbateskeegan"
      }, 
      {
        "html_url": "https://github.com/FlashSoft"
      }, 
      {
        "html_url": "https://github.com/charlesgan"
      }, 
      {
        "html_url": "https://github.com/BlaneCordes"
      }, 
      {
        "html_url": "https://github.com/mdgoody"
      }, 
      {
        "html_url": "https://github.com/gindis"
      }, 
      {
        "html_url": "https://github.com/chrisdickinson"
      }, 
      {
        "html_url": "https://github.com/jorgenio"
      }, 
      {
        "html_url": "https://github.com/joityui"
      }, 
      {
        "html_url": "https://github.com/wxnet2013"
      }, 
      {
        "html_url": "https://github.com/be5invis"
      }, 
      {
        "html_url": "https://github.com/ashleymcfarland"
      }, 
      {
        "html_url": "https://github.com/jimxl"
      }, 
      {
        "html_url": "https://github.com/bluefisher80"
      }, 
      {
        "html_url": "https://github.com/subhaze"
      }, 
      {
        "html_url": "https://github.com/yaoming6046"
      }, 
      {
        "html_url": "https://github.com/hirozhang"
      }, 
      {
        "html_url": "https://github.com/rhyw"
      }, 
      {
        "html_url": "https://github.com/rhaldkhein"
      }
    ], 
    "owner": "https://github.com/nodejs", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-08-21T04:05:53Z"
}{
  "doi": "10.1093/bioinformatics/btu635", 
  "name": "A twostage statistical procedure for feature selection and comparison in functional analysis of metagenomes", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis A two-stage statistical procedure for feature selection and comparison in functional analysis of metagenomes", 
  "toolName": "Genome analysis A two-stage statistical procedure for feature selection and comparison in functional analysis of metagenomes", 
  "abstract": "Motivation: With the advance of new sequencing technologies producing massive short reads data, metagenomics is rapidly growing, especially in the fields of environmental biology and medical science. The metagenomic data are not only high dimensional with large number of features and limited number of samples but also complex with a large number of zeros and skewed distribution. Efficient computational and statistical tools are needed to deal with these unique characteristics of metagenomic sequencing data. In metagenomic studies, one main objective is to assess whether and how multiple microbial communities differ under various environmental conditions. Results: We propose a two-stage statistical procedure for selecting informative features and identifying differentially abundant features between two or more groups of microbial communities. In the functional analysis of metagenomes, the features may refer to the pathways, subsystems, functional roles and so on. In the first stage of the proposed procedure, the informative features are selected using elastic net as reducing the dimension of metagenomic data. In the second stage, the differentially abundant features are detected using generalized linear models with a negative binomial distribution. Compared with other available methods, the proposed approach demonstrates better performance for most of the comprehensive simulation studies. The new method is also applied to two real metagenomic datasets related to human health. Our findings are consistent with those in previous reports. Availability: R code and two example datasets are available at", 
  "summary": "Several statistical methods or tools have been developed to compare various microbial communities in terms of detecting differentially abundant features, e.g. SONs (Schloss and Handelsman, 2006), XIPE-TOTEC (Rodriguez-Brito et al., 2006), Metastats (White et al., 2009) and MEGAN (Huson et al., 2009, 2011).\nOur approach requires (i) a metagenomic dataset corresponding to two or more conditions/phenotypes (e.g. diseased and healthy human guts, or different locations of sea water); each condition/phenotype consists of multiple individuals (or samples), and (ii) each sample/individual consists of count data representing the relative abundance of features, or number of shotgun reads mapped to a specific biological pathway or subsystem.", 
  "affiliations": [
    " Department of Agricultural & Biosystems Engineering", 
    " Interdisciplinary Program in Statistics University of Arizona "
  ], 
  "grants": [
    "Funding: This work was supported by National Science Foundation [DMS-1043080 and DMS-1222592 to L.A. and H.J."
  ], 
  "acks": " ", 
  "authors": [
    " Naruekamol Pookhao", 
    " Michael B Sohn", 
    " Qike Li", 
    " Isaac Jenkins", 
    " Ruofei Du", 
    " Hongmei Jiang", 
    " Lingling An", 
    " John Hancock"
  ], 
  "keyWords": [
    "data metagenomics", 
    [
      "metagenomic", 
      "methods", 
      "functional"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-26T00:20:55Z"
}{
  "doi": "10.1093/bioinformatics/btv010", 
  "name": "ADME SARfari comparative genomics of drug metabolizing systems", 
  "links": [
    "https://www.ebi.ac.uk/chembl/admesarfari", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://www.chemaxon.com/products/marvin/marvin-js", 
    "http://www.pharmaadme.VC", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Databases and ontologies ADME SARfari: comparative genomics of drug metabolizing systems", 
  "toolName": "Databases and ontologies ADME SARfari: comparative genomics of drug metabolizing systems", 
  "abstract": "Motivation: ADME SARfari is a freely available web resource that enables comparative analyses of drug-disposition genes. It does so by integrating a number of publicly available data sources, which have subsequently been used to build data mining services, predictive tools and visualizations for drug metabolism researchers. The data include the interactions of small molecules with ADME (absorption , distribution, metabolism and excretion) proteins responsible for the metabolism and transport of molecules; available pharmacokinetic (PK) data; protein sequences of ADME-related molecular targets for pre-clinical model species and human; alignments of the orthologues including information on known SNPs (Single Nucleotide Polymorphism) and information on the tissue distribution of these proteins. In addition, in silico models have been developed, which enable users to predict which ADME relevant protein targets a novel compound is likely to interact with.", 
  "summary": "The data include the interactions of small molecules with ADME (absorption, distribution, metabolism and excretion) proteins responsible for the metabolism and transport of molecules; available pharmacokinetic (PK) data; protein sequences of ADME-related molecular targets for pre-clinical model species and human; alignments of the orthologues including information on known SNPs (Single Nucleotide Polymorphism) and information on the tissue distribution of these proteins.\nTo help enhance all actual (e.g. orthologue mappings, tissue expression levels), predicted (e.g. ADME protein interaction models) and inferred (e.g. cross-species PK comparison) results produced by the system, the subsequent direction is to identify additional sources of ADME data.", 
  "affiliations": [
    " European Molecular Biology Laboratory European Bioinformatics Institute (EMBL-EBI) Wellcome Trust Genome Campus ", 
    " Pfizer Ltd", 
    " GlaxoSmithKline R&D"
  ], 
  "grants": [
    "Funding\nGSK provided the funding to develop the ADME SARfari system.", 
    "Bioinformatics, 31(10), 2015, 16951697 doi: 10.1093/bioinformatics/btv010\nAdvance Access Publication Date: 8 January 2015 Applications Note\n\nDownloaded from http://bioinformatics.oxfordjournals.org/ at :: on August 8, 2016\n\nDatabases and ontologies\nADME SARfari: comparative genomics of drug metabolizing systems\nMark Davies1, Nathan Dedman1, Anne Hersey1, George Papadatos1, Matthew D. Hall2, Lourdes Cucurull-Sanchez2, Phil Jeffrey3, Samiul Hasan2, Peter J. Eddershaw2 and John P. Overington1,*\n1European Molecular Biology Laboratory, European Bioinformatics Institute (EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, 2GlaxoSmithKline R&D, Gunnels Wood Road, Stevenage SG1 2NY and 3Pfizer Ltd., Granta Park, Great Abington, Cambridge CB21 6GP, UK\n*To whom correspondence should be addressed."
  ], 
  "acks": " We thank Edmund Duesbury for preliminary data preparation and target model building work carried out during an internship with the ChEMBL group. ", 
  "authors": [
    " Mark Davies", 
    " Nathan Dedman", 
    " Anne Hersey", 
    " George Papadatos", 
    " Matthew D Hall", 
    " Lourdes Cucurull-Sanchez", 
    " Phil Jeffrey", 
    " Samiul Hasan", 
    " Peter J Eddershaw ", 
    " John P Overington"
  ], 
  "keyWords": [
    "model species", 
    [
      "models", 
      "adme", 
      "proteins", 
      "users", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://www.pharmaadme.VC"
  ], 
  "technologies": [
    " Oracle "
  ], 
  "dateCreated": "2015-01-10T04:10:59Z"
}{
  "doi": "10.1093/bioinformatics/btu670", 
  "name": "Accurate estimation of haplotype frequency from pooled sequencing data and costeffective identification of rare haplotype carriers by overlapping pool sequencing", 
  "links": [
    "http://bioinfo.seu.edu.cn/Ehapp/).2", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinfo.seu.edu.cn/Ehapp"
  ], 
  "title": "Sequence analysis Accurate estimation of haplotype frequency from pooled sequencing data and cost-effective identification of rare haplotype carriers by overlapping pool sequencing", 
  "toolName": "Sequence analysis Accurate estimation of haplotype frequency from pooled sequencing data and cost-effective identification of rare haplotype carriers by overlapping pool sequencing", 
  "abstract": "Motivation: A variety of hypotheses have been proposed for finding the missing heritability of complex diseases in genome-wide association studies. Studies have focused on the value of haplotype to improve the power of detecting associations with disease. To facilitate haplotype-based association analysis, it is necessary to accurately estimate haplotype frequencies of pooled samples. Results: Taking advantage of databases that contain prior haplotypes, we present Ehapp based on the algorithm for solving the system of linear equations to estimate the frequencies of haplotypes from pooled sequencing data. Effects of various factors in sequencing on the performance are evaluated using simulated data. Our method could estimate the frequencies of haplotypes with only about 3% average relative difference for pooled sequencing of the mixture of 10 haplotypes with total coverage of 50\u00c2. When unknown haplotypes exist, our method maintains excellent performance for haplotypes with actual frequencies 40.05. Comparisons with present method on simulated data in conjunction with publicly available Illumina sequencing data indicate that our method is state of the art for many sequencing study designs. We also demonstrate the feasibility of applying overlapping pool sequencing to identify rare haplotype carriers cost-effectively. Availability and implementation: Ehapp (in Perl) for the Linux platforms is available online", 
  "summary": "Since CS was proposed to recover sparse signals from incomplete and inaccurate measurements (Candes et al., 2006; Donoho, 2006), suppose the haplotypes for the samples in pooled sequencing are included in the database which is large enough, it is viable to infer the frequencies of haplotypes contained in the prior database by utilizing techniques from CS.\nHere, by means of prior haplotype information contained in the database, we present a method to estimate the haplotype frequencies from pooled sequencing data.\nTherefore, we compared only the performance of Harp with that of our method in inferring haplotype frequencies from pooled sequencing data.", 
  "affiliations": [
    " School of Biological Science and Medical Engineering State Key Laboratory of Bioelectronics Southeast University "
  ], 
  "grants": [
    "Funding: This work was supported by the National Basic Research Program of China [grant number 2012CB316501] and the National Natural Science Foundation of China [grant numbers 61472078, 61073141]."
  ], 
  "acks": " The authors thank Darren Kessner for providing simreads to generate simulated sequencing reads. ", 
  "authors": [
    " Chang-Chang Cao", 
    " Xiao Sun", 
    " John Hancock"
  ], 
  "keyWords": [
    "haplotype frequency", 
    "pooled sequencing data", 
    [
      "haplotypes", 
      "frequencies", 
      "reads", 
      "sequences", 
      "pooling"
    ]
  ], 
  "sourcelinks": [
    "http://bioinfo.seu.edu.cn/Ehapp/).2", 
    "http://bioinfo.seu.edu.cn/Ehapp"
  ], 
  "technologies": [
    "C", 
    "Perl"
  ], 
  "dateCreated": "2014-10-11T03:33:58Z"
}{
  "doi": "10.1093/bioinformatics/btv012", 
  "name": "Acquire an opensource comprehensive cancer biobanking system", 
  "links": [
    "https://tcrbacquire-stg.research.bcm.edu", 
    "https://github.com/BCM-DLDCC/Acquire.2", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://1.usa.gov/1vRKzWO", 
    "http://txcrb.org", 
    "http://bit.ly/1znXuEU", 
    "http://1.usa.gov/1GHE9Qi", 
    "https://github.com/BCM-DLDCC/Acquire", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/BCM-DLDCC/Acquire/tree/master/supplementaryMaterials"
  ], 
  "title": "Acquire: an open-source comprehensive cancer biobanking system", 
  "toolName": "Acquire", 
  "abstract": "Motivation: The probability of effective treatment of cancer with a targeted therapeutic can be improved for patients with defined genotypes containing actionable mutations. To this end, many human cancer biobanks are integrating more tightly with genomic sequencing facilities and with those creating and maintaining patient-derived xenografts (PDX) and cell lines to provide renewable resources for translational research. Results: To support the complex data management needs and workflows of several such biobanks, we developed Acquire. It is a robust, secure, web-based, database-backed open-source system that supports all major needs of a modern cancer biobank. Its modules allow for i) up-to-the-minute 'scoreboard' and graphical reporting of collections; ii) end user roles and permissions; iii) specimen inventory through caTissue Suite; iv) shipping forms for distribution of specimens to pathology, genomic analysis and PDX/cell line creation facilities; v) robust ad hoc querying; vi) molecular and cellular quality control metrics to track specimens' progress and quality; vii) public researcher request ; viii) resource allocation committee distribution request review and oversight and ix) linkage to available derivatives of specimen. Availability and Implementation: Acquire implements standard controlled vocabularies, ontolo-gies and objects from the NCI, CDISC and others. Here we describe the functionality of the system, its technological stack and the processes it supports.", 
  "summary": "Its modules allow for i) up-to-the-minute `scoreboard' and graphical reporting of collections; ii) end user roles and permissions; iii) specimen inventory through caTissue Suite; iv) shipping forms for distribution of specimens to pathology, genomic analysis and PDX/cell line creation facilities; v) robust ad hoc querying; vi) molecular and cellular quality control metrics to track specimens' progress and quality; vii) public researcher request; viii) resource allocation committee distribution request review and oversight and ix) linkage to available derivatives of specimen.\nSupports biobank programs and virtual banks Freezer and container management Participant consent and study registration Flexible creation of work flows based on IRB protocols Cancer molecular and pathological QC metrics Comprehensive reporting and dashboards Billing module Participant annotation Comprehensive clinical annotations Specimen annotations Molecular annotations Public researcher requests for specimens Tracking of RAC reviews Clinical trials management system interoperation", 
  "affiliations": [
    " Dan L. Duncan Cancer Center"
  ], 
  "grants": [
    "Funding\nAcquire is supported by the Cancer Prevention Research in Texas (CPRIT) through a multi-investigator research award [RP101353], by the NCI/NIH as part of its Cancer Center Support Grant program [P30 CA125123] and through philanthropy via the Dan L. Duncan Cancer Center.", 
    "If the requester elects to submit an online application, the form records their contact information; detailed project information including funding agency, award number and Institutional Review Board (IRB) approval; detailed specimen criteria; requested material type criteria such as frozen tissues, formalin-fixed paraffinembedded slides, DNA, etc.", 
    "3.2.1 Graphical reports Frequently investigators require graphs for presentations or grants."
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/BCM-DLDCC/Acquire.2", 
    "https://github.com/BCM-DLDCC/Acquire", 
    "https://github.com/BCM-DLDCC/Acquire/tree/master/supplementaryMaterials"
  ], 
  "acks": " We gratefully acknowledge the members of the TCRB for their feedback on requirements and business needs, in particular Dr. Richard Gibbs, Dr. Amy McGuire, Dr. Acquire is supported by the Cancer Prevention Research in Texas (CPRIT) through a multi-investigator research award, by the NCI/NIH as part of its Cancer Center Support Grant program ", 
  "authors": [
    " Heidi Dowst", 
    " Benjamin Pew", 
    " Chris Watkins", 
    " Apollo Mcowiti", 
    " Jonathan Barney", 
    " Shijing Qu", 
    " Lauren B Becnel"
  ], 
  "keyWords": [
    [
      "supporting", 
      "biobanking", 
      "reporting", 
      "data", 
      "acquire", 
      "specimens", 
      "researchers"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "Apache License 2.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/lehoff/acquirex/master/LICENSE"
      }
    ], 
    "name": "acquirex", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/lehoff"
      }
    ], 
    "versions": [], 
    "created_at": "2015-06-10T12:11:43Z", 
    "updated_at": "2016-08-05T12:28:05Z", 
    "languages": [
      "Elixir"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/lehoff"
      }, 
      {
        "html_url": "https://github.com/Papipo"
      }, 
      {
        "html_url": "https://github.com/jmerriweather"
      }, 
      {
        "html_url": "https://github.com/elasticdog"
      }, 
      {
        "html_url": "https://github.com/isensible"
      }, 
      {
        "html_url": "https://github.com/mdohring"
      }, 
      {
        "html_url": "https://github.com/gabrielelana"
      }, 
      {
        "html_url": "https://github.com/mrluc"
      }, 
      {
        "html_url": "https://github.com/davidberglund"
      }
    ], 
    "owner": "https://github.com/lehoff", 
    "homepage": "https://vimeo.com/131757761"
  }, 
  "technologies": [
    " Oracle "
  ], 
  "dateCreated": "2015-01-09T02:26:44Z"
}{
  "doi": "10.1093/bioinformatics/btu486", 
  "name": "AffyPipe an opensource pipeline for Affymetrix Axiom genotyping workflow", 
  "links": [
    "https://github.com/nicolazzie", 
    "http://www.affymetrix.com", 
    "http://www.ncbi.nlm.nih.gov/geo/query", 
    "http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi", 
    "http://www.affymetrix.com/support", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/nicolazzie/AffyPipe", 
    "http://www.Rproject.org"
  ], 
  "title": "Genome analysis AffyPipe: an open-source pipeline for Affymetrix Axiom genotyping workflow", 
  "toolName": "AffyPipe", 
  "abstract": "The Affymetrix Axiom genotyping standard and 'best practice' work-flow for Linux and Mac users consists of three stand-alone executable programs (Affymetrix Power Tools) and an R package (SNPolisher). Currently, SNP analysis has to be performed in a step-by-step procedure. Manual intervention and/or programming skills by the user is required at each intermediate point, as Affymetrix Power Tools programs do not produce input files for the program next-in-line. An additional problem is that the output format of genotypes is not compatible with most analysis software currently available. AffyPipe solves all the above problems, by automating both standard and 'best practice' workflows for any species genotyped with the Axiom technology. AffyPipe does not require programming skills and performs all the steps necessary to obtain a final genotype file. Furthermore, users can directly edit SNP probes and export genotypes in PLINK format. Availability and implementation: https://github.com/nicolazzie/", 
  "summary": "ABSTRACT The Affymetrix Axiom genotyping standard and `best practice' workflow for Linux and Mac users consists of three stand-alone executable programs (Affymetrix Power Tools) and an R package (SNPolisher).\nThere are two types of workflow: the standard (http://www.affymetrix.com/support/ downloads/manuals/axiom_genotyping_solution_analysis_ guide.pdf) and the `best practice' (http://www.affymetrix.com/ support/downloads/manuals/axiom_best_practice_supplement_ user_guide.pdf) workflow.\nAffyPipe allows both standard and `best practice' workflows to be run, and produces edited final files in PLINK format (Purcell et al., 2007), a standard and widely used format for genetic analyses.", 
  "affiliations": [], 
  "grants": [
    "Funding: Italian Ministry of Education, University and Research, project GenHome (D.M."
  ], 
  "sourcelinks": [
    "https://github.com/nicolazzie", 
    "http://www.affymetrix.com/support", 
    "http://www.ncbi.nlm.nih.gov/geo/query", 
    "http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi", 
    "http://www.affymetrix.com", 
    "https://github.com/nicolazzie/AffyPipe", 
    "http://www.Rproject.org"
  ], 
  "acks": " ", 
  "authors": [
    " Ezequiel L Nicolazzi", 
    " Daniela Iamartino", 
    " John L Williams", 
    " Fondazione Parco", 
    " Tecnologico Padano", 
    " Lodi"
  ], 
  "keyWords": [
    "genotyping workflow", 
    [
      "files", 
      "users", 
      "affymetrix", 
      "genotyped", 
      "steps", 
      "workflows", 
      "software"
    ]
  ], 
  "github_data": {
    "name": "AffyPipe", 
    "contributors": [], 
    "versions": [], 
    "created_at": "2014-05-21T09:52:29Z", 
    "updated_at": "2016-05-05T21:02:55Z", 
    "languages": [
      "Python", 
      "Shell"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/nicolazzie"
      }
    ], 
    "owner": "https://github.com/nicolazzie", 
    "homepage": null
  }, 
  "technologies": [
    "R", 
    "C++"
  ], 
  "dateCreated": "2014-07-16T00:23:13Z"
}{
  "doi": "10.1093/bioinformatics/btu531", 
  "name": "AliView a fast and lightweight alignment viewer and editor for large datasets", 
  "links": [
    "http://ormbunkar.se/aliview", 
    "http://creativecommons.org/licenses/by-nc/4.0", 
    "http://code.google.com/p/jebl2", 
    "http://tree.bio.ed.ac.uk/software/figtree", 
    "http://mesquiteproject.org", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "AliView: a fast and lightweight alignment viewer and editor for large datasets", 
  "toolName": "AliView: a fast and lightweight alignment viewer and editor for large datasets", 
  "abstract": "AliView is an alignment viewer and editor designed to meet the requirements of next-generation sequencing era phylogenetic datasets. AliView handles alignments of unlimited size in the formats most commonly used, i.e. FASTA, Phylip, Nexus, Clustal and MSF. The intuitive graphical interface makes it easy to inspect, sort, delete, merge and realign sequences as part of the manual filtering process of large datasets. AliView also works as an easy-to-use alignment editor for small as well as large datasets. Availability and implementation: AliView is released as open-source software under the GNU General Public License, version 3.0 (GPLv3), and is available at GitHub (www.github.com/AliView). The program is cross-platform and extensively tested on Linux, Mac OS X and Windows systems. Downloads and help are available at", 
  "summary": "Here, AliView is introduced as an alignment viewer and editor with a unique combination of features that allows the user to work with large datasets.\nThe key features of AliView include the ability to swiftly handle large alignments with low memory impact (see Table 1 for comparison with other popular free cross-platform alignment viewers).\nFor comparison of the key features of AliView with other free cross-platform editors such as Jalview 2 (Waterhouse et al., 2009), SeaView (Gouy et al., 2010), ClustalX (Larkin et al., 2007) and Mesquite (Maddison, and Maddison, 2011) see Table 2.", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was supported by grants from the Swedish Research Council for Environment, Agricultural Sciences\n\nand Spatial Planning (Formas) to Petra Korall (2006-429 and 2010-585)."
  ], 
  "acks": " Thanks to my colleagues at the Systematic Biology department for immense beta-testing, bug-reporting and mostly good suggestions . Thanks to Allison Perrigo, John Petterson and Martin Ryberg for comments on the manuscript. I also would like to thank the three reviewers and the associate editor David Posada, who gave me very valuable feedback on the previous versions of the manuscript. ", 
  "authors": [], 
  "keyWords": [
    [
      "aliview", 
      "features", 
      "editors", 
      "functionality", 
      "bioinformatics", 
      "sequencing", 
      "alignments"
    ]
  ], 
  "sourcelinks": [
    "http://code.google.com/p/jebl2", 
    "http://tree.bio.ed.ac.uk/software/figtree"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-06T05:13:41Z"
}{
  "doi": "10.1093/bioinformatics/btu793", 
  "name": "Adaptive settings for the nearestneighbor particle tracking algorithm", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://sourceforge.net/p/adaptivespt"
  ], 
  "title": "Adaptive settings for the nearest-neighbor particle tracking algorithm", 
  "toolName": "adaptivespt", 
  "abstract": "Background: The performance of the single particle tracking (SPT) nearest-neighbor algorithm is determined by parameters that need to be set according to the characteristics of the time series under study. Inhomogeneous systems, where these characteristics fluctuate spatially, are poorly tracked when parameters are set globally. Results: We present a novel SPT approach that adapts the well-known nearest-neighbor tracking algorithm to the local density of particles to overcome the problems of inhomogeneity. Conclusions: We demonstrate the performance improvement provided by the proposed method using numerical simulations and experimental data and compare its performance with state of the art SPT algorithms.", 
  "summary": "(C) Descriptors of tracking performance on trajectories generated with D  24 pixels versus mD used by C&G algorithm (top panel), and pdf of d (bottom panel).\nWe performed particle tracking on each time series, computing mD with the algorithm described in Section 2.2 by setting D/(x,y) constant and choosing the minimal a among all frames (Fig. 3D, bottom panel).\n(E) Performance descriptors for the standard C&G (full lines) and Adaptive (dashed lines) algorithms as a function of c (top), and values of mD used for the C&G algorithm computed with the automatic algorithm by setting D/(x,y) constant and choosing the minimal a among all frames (bottom).", 
  "affiliations": [
    " Centre de Recherche de l'H\u00f4 pital Maisonneuve-Rosemont"
  ], 
  "grants": [
    "Funding\nThis work was funded by grants from the Fondation de l'Ho^ pital Maisonneuve Rosemont (La nephrologie et son Impact) to S.L., and the National Sciences and Engineering Research Council (NSERC) to S.C. S.C. and S.L.", 
    "J.R. was supported by funds from the NSERC-CREATE Training Program in Neuro-Engineering and an FRQS doctoral scholarship."
  ], 
  "acks": " ", 
  "authors": [
    " Javier Mazzaferri", 
    " Joannie Roy", 
    " Stephane Lefrancois", 
    " Santiago Costantino"
  ], 
  "keyWords": [
    "particle tracking algorithm", 
    "d et", 
    [
      "particles", 
      "trajectories", 
      "algorithms", 
      "tracked"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net/p/adaptivespt"
  ], 
  "technologies": [
    "IDL"
  ], 
  "dateCreated": "2014-12-06T02:51:31Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/adaptivespt/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/javimazzaf/", 
        "name": "Javier Mazzaferri"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu849", 
  "name": "An automatic tool to analyze and cluster macromolecular conformations based on selforganizing maps", 
  "links": [
    "http://nbviewer.ipython.org/gist/bougui505", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/bougui505/SOM"
  ], 
  "title": "An automatic tool to analyze and cluster macromolecular conformations based on self-organizing maps", 
  "toolName": "SOM", 
  "abstract": "Motivation: Sampling the conformational space of biological macromolecules generates large sets of data with considerable complexity. Data-mining techniques, such as clustering, can extract meaningful information. Among them, the self-organizing maps (SOMs) algorithm has shown great promise; in particular since its computation time rises only linearly with the size of the data set. Whereas SOMs are generally used with few neurons, we investigate here their behavior with large numbers of neurons. Results: We present here a python library implementing the full SOM analysis workflow. Large SOMs can readily be applied on heavy data sets. Coupled with visualization tools they have very interesting properties. Descriptors for each conformation of a trajectory are calculated and mapped onto a 3D landscape, the U-matrix, reporting the distance between neighboring neurons. To delin-eate clusters, we developed the flooding algorithm, which hierarchically identifies local basins of the U-matrix from the global minimum to the maximum. Availability and implementation: The python implementation of the SOM library is freely available on github: https://github.com/", 
  "summary": "Descriptors for each conformation of a trajectory are calculated and mapped onto a 3D landscape, the U-matrix, reporting the distance between neighboring neurons.\nexploit the property of large self-organizing maps (SOMs) to spontaneously and clearly cluster similar conformations in basins, and to separate dissimilar ones by barriers, by projecting the highdimensional exploration onto 3D landscape maps.\nThe SOM algorithm was applied to the analysis of 15 ms molecular dynamics at 330 K of a simplified sequence of a 56-residue a=b subdomain of the protein G (Guarnera et al., 2009) (Fig. 1).", 
  "affiliations": [
    " UMR 3528 Institut Pasteur Unit\u00e9 de Bioinformatique Structurale CNRS "
  ], 
  "grants": [
    "Funding\nThis work was funded by the European Union (FP7-IDEAS- ERC 294809 to M.N.)."
  ], 
  "sourcelinks": [
    "https://github.com/bougui505/SOM"
  ], 
  "acks": " ", 
  "authors": [
    " Guillaume Bouvier", 
    " Nathan Desdouits", 
    " Mathias Ferber", 
    " Arnaud Blondel", 
    " Michael Nilges"
  ], 
  "keyWords": [
    [
      "clustering", 
      "soms", 
      "conformational", 
      "matrix", 
      "largely"
    ]
  ], 
  "github_data": {
    "name": "SOM", 
    "contributors": [
      {
        "contributions": 10, 
        "html_url": "https://github.com/DanKoloff"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/TsvetanUsunov"
      }
    ], 
    "versions": [], 
    "created_at": "2014-06-20T16:01:13Z", 
    "updated_at": "2016-07-16T12:38:21Z", 
    "languages": [
      "Eagle"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/TsvetanUsunov"
      }, 
      {
        "html_url": "https://github.com/urjaman"
      }, 
      {
        "html_url": "https://github.com/otavio"
      }, 
      {
        "html_url": "https://github.com/trygvis"
      }, 
      {
        "html_url": "https://github.com/radolin"
      }, 
      {
        "html_url": "https://github.com/DanKoloff"
      }, 
      {
        "html_url": "https://github.com/ByteSyze"
      }, 
      {
        "html_url": "https://github.com/longcongduoi"
      }, 
      {
        "html_url": "https://github.com/hehopmajieh"
      }, 
      {
        "html_url": "https://github.com/SelfDestroyer"
      }, 
      {
        "html_url": "https://github.com/juancamicro"
      }, 
      {
        "html_url": "https://github.com/peter-valkov"
      }, 
      {
        "html_url": "https://github.com/Stanimir-Petev"
      }, 
      {
        "html_url": "https://github.com/mathieurouppert"
      }, 
      {
        "html_url": "https://github.com/fpavanati"
      }, 
      {
        "html_url": "https://github.com/giklette"
      }
    ], 
    "owner": "https://github.com/OLIMEX", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-12-28T01:07:50Z"
}{
  "doi": "10.1093/bioinformatics/btu516", 
  "name": "Amplicon identification using SparsE representation of multiplex PYROsequencing signal AdvISERMPYRO application to bacterial resistance genotyping", 
  "links": [
    "http://sites", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://cran.rproject.org/web/packages/penalized/index.html.).Hawkey,P.M"
  ], 
  "title": "Gene expression Amplicon identification using SparsE representation of multiplex PYROsequencing signal (AdvISER-M-PYRO): application to bacterial resistance genotyping", 
  "toolName": "Gene expression Amplicon identification using SparsE representation of multiplex PYROsequencing signal (AdvISER-M-PYRO): application to bacterial resistance genotyping", 
  "abstract": "Motivation: Pyrosequencing is a cost-effective DNA sequencing technology that has many applications, including rapid genotyping of a broad spectrum of bacteria. When molecular typing requires to genotype multiple DNA stretches, several pyrosequencing primers could be used simultaneously but this would create overlapping primer-specific signals, which are visually uninterpretable. Accordingly, the objective was to develop a new method for signal processing (AdvISER-M-PYRO) to automatically analyze and interpret multiplex pyrosequen-cing signals. In parallel, the nucleotide dispensation order was improved by developing the SENATOR ('SElecting the Nucleotide dispensATion Order') algorithm. Results: Inthis proof-of-concept study,quintuplex pyrosequencingwas applied on eight bacterial DNA and targeted genetic alterations underlying resistance to-lactam antibiotics. Using SENATOR-driven dispen-sation order, all genetic variants (31 of 31; 100%) were correctly identified with AdvISER-M-PYRO. Among nine expected negative results, there was only one false positive that was tagged with an 'unsafe' label. Availability and implementation: SENATOR and AdvISER-M-PYRO are implemented in the AdvISER-M-PYRO R package (http://sites. uclouvain.be/md-ctma/index.php/softwares) and can be used to improve the dispensation order and to analyze multiplex pyrosequencing signals generated in a broad range of typing applications.", 
  "summary": "The SENATOR function considers all unique nucleotide sequences (UNS) expected to be found within each genomic region of interest, hence improving the selection of a dispensation order that produces uncorrelated uniplex pyrosequencing signals.\nThis novel multiplex pyrosequencing approach, which integrates the selection of the nucleotide dispensation order with SENATOR and the signals interpretation with AdvISER-M-PYRO reading software, enables therefore to lower the global turnaround time of genotyping and to decrease substantially analytical reagent costs while providing reliable target-specific results.", 
  "affiliations": [
    " Center for Applied Molecular Technologies (CTMA) Institut de Recherche Exp erimentale et Clinique (IREC) Universit e catholique de Louvain ", 
    " Defence Laboratories Department Belgian Armed Forces ", 
    " Epidemiology and Biostatistics Department (EPID) Institut de Recherche Exp erimentale et Clinique (IREC) Universit e catholique de Louvain ", 
    " Department of Epidemiology and Hygiene Military Medical Academy of Sofia "
  ], 
  "grants": [
    "Funding: This project was funded by the Department Management of Scientific and Technological Research of Defence (IRSD-RSTD; Royal High Institute for Defence) supporting research and development (grants MED-20).", 
    "is funded by a grant from Institut de Recherche Experimentale et Clinique (IREC)."
  ], 
  "acks": " The authors gratefully acknowledge Dan Marinescu from Bucharest Clinical Emergency Hospital, Romania for providing some of the strains used in this study. ", 
  "authors": [
    " J Er ^ Ome Ambroise", 
    " Yann Deccache", 
    " Leonid Irenge", 
    " Encho Savov", 
    " Annie Robert", 
    " Jean-Luc Gala", 
    " John Hancock"
  ], 
  "keyWords": [
    "pyrosequencing signal", 
    [
      "dictionary", 
      "pyrosequencer", 
      "highly", 
      "signals", 
      "resulting", 
      "multiplex"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-31T00:26:27Z"
}{
  "doi": "10.1093/bioinformatics/btu654", 
  "name": "An alternative approach to multiple testing for methylation QTL mapping reduces the proportion of falsely identified CpGs", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetics and population analysis An alternative approach to multiple testing for methylation QTL mapping reduces the proportion of falsely identified CpGs", 
  "toolName": "Genetics and population analysis An alternative approach to multiple testing for methylation QTL mapping reduces the proportion of falsely identified CpGs", 
  "abstract": "Introduction: An increasing number of studies investigates the influence of local genetic variation on DNA methylation levels, so-called in cis methylation quantitative trait loci (meQTLs). A common multiple testing approach in genome-wide cis meQTL studies limits the false discovery rate (FDR) among all CpG\u2013SNP pairs to 0.05 and reports on CpGs from the significant CpG\u2013SNP pairs. However, a statistical test for each CpG is not performed, potentially increasing the proportion of CpGs falsely reported on. Here, we presented an alternative approach that properly control for multiple testing at the CpG level. Results: We performed cis meQTL mapping for varying window sizes using publicly available single-nucleotide polymorphism (SNP) and 450 kb data, extracting the CpGs from the significant CpG\u2013SNP pairs (FDR50:05). Using a new bait-and-switch simulation approach, we show that up to 50% of the CpGs found in the simulated data may be false-positive results. We present an alternative two-step multiple testing approach using the Simes and Benjamini\u2013Hochberg procedures that does control the FDR among the CpGs, as confirmed by the bait-and-switch simulation. This approach indicates the use of window sizes in cis meQTL mapping studies that are significantly smaller than commonly adopted.", 
  "summary": "To test for associations of methylation at CpGs with genetic variants in cis, that is locally, studies have been considering SNPs anywhere between 5 (Gutierrez-Arcelus et al., 2013) to 1000 kb (Gibbs et al., 2010) from measured CpGs. Particularly large window sizes will result in hundreds of millions statistical tests, and thus brings about a huge multiple testing problem.\nTo obtain a valid list of CpGs that are significantly associated with genetic variation in cis in the original data, we calculated one P-value per CpG, testing the global hypothesis of no association between variation in methylation and any of the SNPs in its window.", 
  "affiliations": [
    " Department of Medical Statistics and Bioinformatics Leiden University Medical Center ", 
    " Biostatistics Department for Health Evidence Radboud University Nijmegen Medical Center ", 
    " Department of Molecular Epidemiology"
  ], 
  "grants": [
    "Funding: This work was done within the framework of the Biobank-Based Integrative Omics Studies (BIOS) Consortium funded by BBMRI-NL, a research infrastructure financed by the Dutch government (NWO 184.021.007) and financially supported by the European Unions Seventh Framework Program IDEAL (FP8/2007-2011) under grant agreement No."
  ], 
  "acks": " ", 
  "authors": [
    " Ren E Luijk", 
    " Jelle J Goeman", 
    " Eline P Slagboom", 
    " Bastiaan T Heijmans", 
    " Erik W Van Zwet"
  ], 
  "keyWords": [
    [
      "associations", 
      "values", 
      "approaches", 
      "genetics", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-05T00:35:52Z"
}{
  "doi": "10.1093/bioinformatics/btu521", 
  "name": "APPEX analysis platform for the identification of prognostic gene expression signatures in cancer", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Gene expression APPEX: analysis platform for the identification of prognostic gene expression signatures in cancer", 
  "toolName": "Gene expression APPEX: analysis platform for the identification of prognostic gene expression signatures in cancer", 
  "abstract": "Because cancer has heterogeneous clinical behaviors due to the progressive accumulation of multiple genetic and epigenetic alterations, the identification of robust molecular signatures for predicting cancer outcome is profoundly important. Here, we introduce the APPEX Web-based analysis platform as a versatile tool for identifying prognostic molecular signatures that predict cancer diversity. We incorporated most of statistical methods for survival analysis and implemented seven survival analysis workflows, including CoxSingle, CoxMulti, IntransSingle, IntransMulti, SuperPC, TimeRoc and multivariate. A total of 236 publicly available datasets were collected, processed and stored to support easy independent validation of prognostic signatures. Two case studies including disease recurrence and bladder cancer progression were described using different combinations of the seven workflows. Availability and implementation: APPEX is freely available at http:// www.appex.kr.", 
  "summary": "The APPEX system currently contains four independent statistical approaches for identifying and estimating a signature associated with cancer outcome, i.e. the Cox proportional hazard model (Cox, 1972), an in-trans correlation approach (Lee et al., 2010), SuperPC (Bair and Tibshirani, 2004) and time-dependent ROC curves (Heagerty et al., 2000).\nAPPEX consists of two parts: the APPEX analyzer, which determines signatures associated with cancer outcome, and the public dataset explorer, in which a user explores previously published cancer patient cohorts and directly applies them to the APPEX analyzer (Supplementary Fig. S2A).", 
  "affiliations": [
    " Department of Urology Chungbuk National University College of Medicine ", 
    " Department of Functional Genomics University of Science and Technology ", 
    " Medical Genomics Research Center"
  ], 
  "grants": [
    "Funding: This work was supported by grants from the stem cell (2012M3A9B4027954) and genomics (2012M3A9D1054670) program of the National Research Foundation of Korea funded by the Ministry of Science, ICT and Future Planning and a KRIBB Research Initiative grant."
  ], 
  "acks": " ", 
  "authors": [
    " Seon-Kyu Kim", 
    " Jong Hwan Kim", 
    " Seok-Joong Yun", 
    " Wun-Jae Kim", 
    " Seon-Young Kim", 
    " Janet Kelso"
  ], 
  "keyWords": [
    "appex analysis", 
    [
      "signatures", 
      "cancers", 
      "researchers", 
      "genes"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-05T01:47:54Z"
}{
  "doi": "10.1093/bioinformatics/btu432", 
  "name": "An improved method for computing qvalues when the distribution of effect sizes is asymmetric", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Gene expression An improved method for computing q-values when the distribution of effect sizes is asymmetric", 
  "toolName": "Gene expression An improved method for computing q-values when the distribution of effect sizes is asymmetric", 
  "abstract": "Motivation: Asymmetry is frequently observed in the empirical distribution of test statistics that results from the analysis of gene expression experiments. This asymmetry indicates an asymmetry in the distribution of effect sizes. A common method for identifying differentially expressed (DE) genes in a gene expression experiment while controlling false discovery rate (FDR) is Storey's q-value method. This method ranks genes based solely on the P-values from each gene in the experiment. Results: We propose a method that alters and improves upon the q-value method by taking the sign of the test statistics, in addition to the P-values, into account. Through two simulation studies (one involving independent normal data and one involving microarray data), we show that the proposed method, when compared with the traditional q-value method, generally provides a better ranking for genes as well as a higher number of truly DE genes declared to be DE, while still adequately controlling FDR. We illustrate the proposed method by analyzing two microarray datasets, one from an experiment of thale cress seedlings and the other from an experiment of maize leaves. Availability and implementation: The R code and data files for the proposed method and examples are available at Bioinformatics online.", 
  "summary": "We propose a new method for FDR estimation that alters the traditional q-value method by separating the two-sided P-values from an experiment into two subsets of P-values based on the sign of the test statistics, and then computing the q-values separately for each subset to create a better ranking of the genes with respect to differential expression.\nFigure 4 plots the log ratio of the q-values versus the test statistics from the proposed and traditional method for the gene expression experiment in thale cress seedlings.", 
  "affiliations": [
    " Department of Statistics North Dakota State University ", 
    " Department of Statistics Iowa State University "
  ], 
  "grants": [
    "2008-3560418805) and the National Science Foundation (NSF) Plant Genome Research Program (grant no.", 
    "Funding: This research was partially supported by the USDA NRICGP Microbial Functional Genomics program (grant no."
  ], 
  "acks": " ", 
  "authors": [
    " Megan Orr", 
    " Peng Liu", 
    " Dan Nettleton"
  ], 
  "keyWords": [
    [
      "data", 
      "testing", 
      "values", 
      "methods", 
      "genes"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    " Oracle "
  ], 
  "dateCreated": "2014-07-15T03:51:45Z"
}{
  "doi": "10.1093/bioinformatics/btv009", 
  "name": "An integrative approach to predicting the functional effects of noncoding and coding sequence variation", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://fathmm", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "An integrative approach to predicting the functional effects of non-coding and coding sequence variation", 
  "toolName": "An integrative approach to predicting the functional effects of non-coding and coding sequence variation", 
  "abstract": "Motivation: Technological advances have enabled the identification of an increasingly large spectrum of single nucleotide variants within the human genome, many of which may be associated with monogenic disease or complex traits. Here, we propose an integrative approach, named FATHMM-MKL, to predict the functional consequences of both coding and non-coding sequence variants. Our method utilizes various genomic annotations, which have recently become available, and learns to weight the significance of each component annotation source. Results: We show that our method outperforms current state-of-the-art algorithms, CADD and GWAVA, when predicting the functional consequences of non-coding variants. In addition, FATHMM-MKL is comparable to the best of these algorithms when predicting the impact of coding variants. The method includes a confidence measure to rank order predictions. Availability and implementation: The FATHMM-MKL webserver is available at: http://fathmm.", 
  "summary": "When assessing the functional consequences of non-coding variants, we observe improved performance when compared with two recently proposed variant prediction algorithms: GWAVA (Ritchie et al., 2014) and CADD (Kircher et al., 2014), which as far as we are aware, are the only other proposed methods that can predict the functional consequences of non-coding variants.\nEvaluation of accuracy and false-positive counts for FATHMM-MKL on the full set of known examples from the 1000 Genomes and HGMD (evaluated on both coding and non-coding variants).\nFor some non-coding examples, we had no data in any of the feature groups and hence FATHMM-MKL could not make a prediction.", 
  "affiliations": [
    " Institute of Medical Genetics Cardiff University ", 
    " Intelligent Systems Laboratory University of Bristol ", 
    " Bristol Centre for Systems Biomedicine University of Bristol ", 
    " Department of Computer Science University of Bristol "
  ], 
  "grants": [
    "For example, provided the number of feature groups remains small, we could learn sets of kernel weights specific\n\nFunding\nThis work was supported by the Medical Research Council [MC_UU_12013/ 8 and G1000427/1].", 
    "was supported by an Engineering and Physical Sciences Research Council grant [EP/K008250/1]."
  ], 
  "acks": " ", 
  "authors": [
    " Hashem A Shihab", 
    " Mark F Rogers", 
    " Julian Gough", 
    " Matthew Mort", 
    " David N Cooper", 
    " Ian N M Day", 
    " Tom R Gaunt", 
    " Colin Campbell"
  ], 
  "keyWords": [
    [
      "genomics", 
      "based", 
      "predictions", 
      "examples", 
      "performance", 
      "variants", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://fathmm"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-13T01:15:54Z"
}{
  "doi": "10.1093/bioinformatics/btu815", 
  "name": "andi Fast and accurate estimation of evolutionary distances between closely related genomes", 
  "links": [
    "http://github.com/evolbioinf/andi", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://homepage3.nifty.com/wpage/software", 
    "http://www.vmatch.de).For"
  ], 
  "title": "Genome analysis andi: Fast and accurate estimation of evolutionary distances between closely related genomes", 
  "toolName": "andi", 
  "abstract": "Motivation: A standard approach to classifying sets of genomes is to calculate their pairwise distances. This is difficult for large samples. We have therefore developed an algorithm for rapidly computing the evolutionary distances between closely related genomes. Results: Our distance measure is based on ungapped local alignments that we anchor through pairs of maximal unique matches of a minimum length. These exact matches can be looked up efficiently using enhanced suffix arrays and our implementation requires approximately only 1 s and 45 MB RAM/Mbase analysed. The pairing of matches distinguishes non-homologous from homologous regions leading to accurate distance estimation. We show this by analysing simulated data and genome samples ranging from 29 Escherichia coli/Shigella genomes to 3085 genomes of Streptococcus pneumoniae. Availability and implementation: We have implemented the computation of anchor distances in the multithreaded UNIX command-line program andi for ANchor DIstances. C sources and documentation are posted at", 
  "summary": "It is based on the MUMmer software (Kurtz et al., 2004), which makes mugsy one of the fastest multiple genome aligners available: it took only 19 h to align 57 complete E.coli genomes.\nMoreover, we apply andi to three sets of bacterial genomes: the 29 genomes of E.coli/Shigella Yi and Jin (2013) used for benchmarking co-phylog, the 109 genomes of E.coli ST131 studied by Petty et al.\nFigure 2A shows our new distance measure da as a function of the number of substitutions per site, K, for simulated pairs of 100 kbase sequences, which implies a minimum anchor length of 8.\nThe phylogeny of 109 E.coli ST131 strains based on da computed from complete genome sequences.", 
  "affiliations": [
    " Mathematical Stochastics Mathematical Institute Freiburg University ", 
    " Department of Evolutionary Genetics Max-Planck-Institute for Evolutionary Biology "
  ], 
  "grants": [
    "Funding\nPP is supported by the Deutsche Forschungsgemeinschaft through grant Pf672/3-1."
  ], 
  "sourcelinks": [
    "http://github.com/evolbioinf/andi", 
    "http://www.vmatch.de).For", 
    "http://homepage3.nifty.com/wpage/software"
  ], 
  "acks": " We thank Angelika B\u00f6 rsch-Haubold and Frederic Bertels for comments on the manuscript. ", 
  "authors": [
    " Bernhard Haubold", 
    " Fabian Kl\u00f6 Tzl", 
    " Peter Pfaffelhuber", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "sequence alignment", 
    [
      "genomics", 
      "andi", 
      "distances", 
      "lengths", 
      "times", 
      "sequencing", 
      "alignments"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v3.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/EvolBioInf/andi/master/COPYING"
      }
    ], 
    "name": "andi", 
    "contributors": [
      {
        "contributions": 700, 
        "html_url": "https://github.com/kloetzl"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/haubold"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.10", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.10", 
        "name": "v0.10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.6.2", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.6.2", 
        "name": "v0.9.6.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.6.1", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.6.1", 
        "name": "v0.9.6.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.6", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.6", 
        "name": "v0.9.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.5", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.5", 
        "name": "v0.9.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.4.1", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.4.1", 
        "name": "v0.9.4.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.4", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.4", 
        "name": "v0.9.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.3", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.3", 
        "name": "v0.9.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.2", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.2", 
        "name": "v0.9.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9.1", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9.1", 
        "name": "v0.9.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.9", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.9", 
        "name": "v0.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.8.1", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.8.1", 
        "name": "v0.8.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.8", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.8", 
        "name": "v0.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.7.3", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.7.3", 
        "name": "v0.7.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v.0.7.2", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v.0.7.2", 
        "name": "v.0.7.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.7.2", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.7.2", 
        "name": "v0.7.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.7.1", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.7.1", 
        "name": "v0.7.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.7", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.7", 
        "name": "v0.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.6", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.6", 
        "name": "v0.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.5", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.5", 
        "name": "v0.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.4", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.4", 
        "name": "v0.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.3", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.3", 
        "name": "v0.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.2", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.2", 
        "name": "v0.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/EvolBioInf/andi/zipball/v0.1", 
        "tarball_url": "https://api.github.com/repos/EvolBioInf/andi/tarball/v0.1", 
        "name": "v0.1"
      }
    ], 
    "created_at": "2014-06-02T08:24:51Z", 
    "updated_at": "2016-04-13T08:07:13Z", 
    "languages": [
      "C", 
      "Shell", 
      "Makefile", 
      "C++", 
      "M4", 
      "Awk"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/kloetzl"
      }
    ], 
    "owner": "https://github.com/EvolBioInf", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-12-12T02:23:08Z"
}{
  "doi": "10.1093/bioinformatics/btu481", 
  "name": "ASPG an ASPbased method for finding attractors in genetic regulatory networks", 
  "links": [
    "http://bioinformatics.intec.ugent.be/kmarchal/Supplementary", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "ASP-G: an ASP-based method for finding attractors in genetic regulatory networks", 
  "toolName": "ASP-G: an ASP-based method for finding attractors in genetic regulatory networks", 
  "abstract": "Motivation: Boolean network models are suitable to simulate GRNs in the absence of detailed kinetic information. However, reducing the biological reality implies making assumptions on how genes interact (interaction rules) and how their state is updated during the simulation (update scheme). The exact choice of the assumptions largely determines the outcome of the simulations. In most cases, however, the biologically correct assumptions are unknown. An ideal simulation thus implies testing different rules and schemes to determine those that best capture an observed biological phenomenon. This is not trivial because most current methods to simulate Boolean network models of GRNs and to compute their attractors impose specific assumptions that cannot be easily altered, as they are built into the system. Results: To allow for a more flexible simulation framework, we developed ASP-G. We show the correctness of ASP-G in simulating Boolean network models and obtaining attractors under different assumptions by successfully recapitulating the detection of attractors of previously published studies. We also provide an example of how performing simulation of network models under different settings help determine the assumptions under which a certain conclusion holds. The main added value of ASP-G is in its modularity and declarativity, making it more flexible and less error-prone than traditional approaches. The declarative nature of ASP-G comes at the expense of being slower than the more dedicated systems but still achieves a good efficiency with respect to computational time. Availability and implementation: The source code of ASP-G is available at", 
  "summary": "The declarative nature of ASP allows one to specify and modify the domain-specific logic (here the definition of the network interactions, activation rules and update schemes) required to represent and solve the computational problem at hand (here dynamical modeling and attractor calculation) in an intuitive and modular way (Eiter et al., 2009).\nConclusively, ASP-G is tailored to simulate Boolean network models of GRNs and to compute attractors in a diagnostic mode, where one wants to test different update schemes and activation rules to find the setting that best matches experimental data or to correctly delineate the boundary conditions under which the biological conclusions based on these simulations are valid.", 
  "affiliations": [
    " Department of Applied Mathematics, Computer Science and Statistics Ghent University "
  ], 
  "grants": [
    "Funding: This work was supported by the Ghent University Multidisciplinary Research Partnership `Bioinformatics: from nucleotides to networks' and the Interuniversity Attraction Poles Programme [IUAP P6/25], initiated by the Belgian State, Science Policy Office (BioMaGNet) and by the IWT: SBONEMOA; FWO: G.0428.13N fund."
  ], 
  "acks": " ", 
  "authors": [
    " Mushthofa Mushthofa", 
    " Gustavo Torres", 
    " Yves Van De Peer", 
    " Kathleen Marchal", 
    " Martine De Cock", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "network models", 
    [
      "genes", 
      "attractor_is_found", 
      "networks", 
      "modelling"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.intec.ugent.be/kmarchal/Supplementary"
  ], 
  "technologies": [
    "ASP"
  ], 
  "dateCreated": "2014-07-16T00:23:13Z"
}{
  "doi": "10.1093/bioinformatics/btu723", 
  "name": "Automated structural classification of lipids by machine learning", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://www.github.com/princelab/lipid"
  ], 
  "title": "Databases and Ontologies Automated structural classification of lipids by machine learning", 
  "toolName": "lipid", 
  "abstract": "Motivation: Modern lipidomics is largely dependent upon structural ontologies because of the great diversity exhibited in the lipidome, but no automated lipid classification exists to facilitate this partitioning. The size of the putative lipidome far exceeds the number currently classified, despite a decade of work. Automated classification would benefit ongoing classification efforts by decreasing the time needed and increasing the accuracy of classification while providing classifications for mass spectral identification algorithms. Results: We introduce a tool that automates classification into the LIPID MAPS ontology of known lipids with >95% accuracy and novel lipids with 63% accuracy. The classification is based upon simple chemical characteristics and modern machine learning algorithms. The decision trees produced are intelligible and can be used to clarify implicit assumptions about the current LIPID MAPS classification scheme. These characteristics and decision trees are made available to facilitate alternative implementations. We also discovered many hundreds of lipids that are currently misclassi-fied in the LIPID MAPS database, strongly underscoring the need for automated classification. Availability and implementation: Source code and chemical characteristic lists as SMARTS search strings are available under an open-source license at https://www.github.com/princelab/lipid_ classifier.", 
  "summary": "And, although automatic classification tools have been alluded to (Fahy et al., 2009), currently there is no publicly available software for the automated classification of lipids.\n(B) A novel lipid is analyzed structurally and then compared with the WEKA produced decision trees at each hierarchal level to generate a complete classification.\nThis generated hierarchy of classification trees were loaded into a programmatic classification system (see Fig. 1, panel B) implemented in Ruby, which classified each given lipid structure by (i) generation of a structural feature list and (ii) application of each hierarchical decision tree.", 
  "affiliations": [
    " Department of Chemistry and Biochemistry"
  ], 
  "grants": [
    "Funding\nThis work was supported by BYU Institutional Funds, BYU Undergraduate Research Awards and BYU CHIRP Grant."
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://www.github.com/princelab/lipid"
  ], 
  "acks": " ", 
  "authors": [
    " Ryan Taylor", 
    " Ryan H Miller", 
    " Ryan D Miller", 
    " Michael Porter", 
    " James Dalgleish", 
    " John T Prince"
  ], 
  "keyWords": [
    "structural classification", 
    [
      "structurally", 
      "classifications", 
      "ontologies", 
      "lipidomics"
    ]
  ], 
  "github_data": {
    "name": "Lipid", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/bahatanner"
      }
    ], 
    "versions": [], 
    "created_at": "2014-06-02T22:57:39Z", 
    "updated_at": "2014-12-27T13:44:04Z", 
    "languages": [
      "R"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/bahatanner"
      }
    ], 
    "owner": "https://github.com/bahatanner", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-31T00:44:56Z"
}{
  "doi": "10.1093/bioinformatics/btu599", 
  "name": "BalestraWeb efficient online evaluation of drugtarget interactions", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0"
  ], 
  "title": "BalestraWeb: efficient online evaluation of drug\u2013target interactions", 
  "toolName": "BalestraWeb: efficient online evaluation of drug\u2013target interactions", 
  "abstract": "BalestraWeb is an online server that allows users to instantly make predictions about the potential occurrence of interactions between any given drug\u2013target pair, or predict the most likely interaction partners of any drug or target listed in the DrugBank. It also permits users to identify most similar drugs or most similar targets based on their interaction patterns. Outputs help to develop hypotheses about drug repurposing as well as potential side effects. Availability and implementation: BalestraWeb is accessible at http:// balestra.csb.pitt.edu/. The tool is built using a probabilistic matrix fac-torization method and DrugBank v3, and the latent variable models are trained using the GraphLab collaborative filtering toolkit. The server is implemented using Python, Flask, NumPy and SciPy.", 
  "summary": "One of the common suggestions brought forth to explain and remedy this trend is a paradigm shift in drug discovery efforts from high-affinity binding on a single target toward modulation of cellular network states through multiple interactions (Csermely et al., 2005, 2013; Hopkins, 2008; Keskin et al., 2007; Korcsmaros et al., 2007; Mencher and Wang, 2005; Zimmermann et al., 2007).\nBalestraWeb is built by training a latent factor model, as described in our previous work (Cobanoglu et al., 2013), on approved drugs and their interactions data from DrugBank v3 (Knox et al., 2011).", 
  "affiliations": [
    " Department of Computational and Systems Biology School of Medicine University of Pittsburgh "
  ], 
  "grants": [
    "Funding: Support from the NIH (U19 AI068021 and PO1 DK096990) is gratefully acknowledged by I.B."
  ], 
  "acks": " ", 
  "authors": [
    " Murat Can Cobanoglu", 
    " Zolt", 
    " N Oltvai", 
    " D Lansing Taylor", 
    " Ivet Bahar"
  ], 
  "keyWords": [
    "similar drugs", 
    [
      "targeting", 
      "interactions", 
      "similarities", 
      "drug", 
      "networks", 
      "balestraweb"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-06T04:48:38Z"
}{
  "doi": "10.1093/bioinformatics/btv031", 
  "name": "ASSIGN contextspecific genomic profiling of multiple heterogeneous biological pathways", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bioconductor.org/packages/release/bioc/html/ASSIGN.html", 
    "https://github.com/wevan"
  ], 
  "title": "Gene expression ASSIGN: context-specific genomic profiling of multiple heterogeneous biological pathways", 
  "toolName": "wevan", 
  "abstract": "Motivation: Although gene-expression signature-based biomarkers are often developed for clinical diagnosis, many promising signatures fail to replicate during validation. One major challenge is that biological samples used to generate and validate the signature are often from heterogeneous biological contexts\u2014controlled or in vitro samples may be used to generate the signature, but patient samples may be used for validation. In addition, systematic technical biases from multiple genome-profiling platforms often mask true biological variation. Addressing such challenges will enable us to better elucidate disease mechanisms and provide improved guidance for personalized therapeutics. Results: Here, we present a pathway profiling toolkit, Adaptive Signature Selection and InteGratioN (ASSIGN), which enables robust and context-specific pathway analyses by efficiently capturing pathway activity in heterogeneous sets of samples and across profiling technologies. The ASSIGN framework is based on a flexible Bayesian factor analysis approach that allows for simultaneous profiling of multiple correlated pathways and for the adaptation of pathway signatures into specific disease. We demonstrate the robustness and versatility of ASSIGN in estimating pathway activity in simulated data, cell lines perturbed pathways and in primary tissues samples including The Cancer Genome Atlas breast carcinoma samples and liver samples exposed to geno-toxic carcinogens. Availability and implementation: Software for our approach is available for download at:", 
  "summary": "An alternative way to infer pathway activity is by experimentally perturbing the pathway of interest in controlled settings and projecting the associated molecular signature (e.g. changes in gene expression) onto patient or other target samples to estimate pathway activity levels (Bild et al., 2006; Gustafson et al., 2010; Sweet-Cordero et al., 2005).\nASSIGN can adaptively estimate background gene expression levels across a set of samples, giving it the unique ability to estimate absolute pathway activity levels or drug efficacy in clinical samples before the samples have received a treatment, even when the signature was generated using a different profiling platform.", 
  "affiliations": [
    " Department of Pharmacology and Toxicology University of Utah ", 
    " Department of Biomedical Informatics", 
    " Division of Computational Biomedicine Boston University School of Medicine "
  ], 
  "grants": [
    "Funding\nThis research was supported by funds from the NIH (U01CA164720) and (T15LM007124)."
  ], 
  "sourcelinks": [
    "http://www.bioconductor.org/packages/release/bioc/html/ASSIGN.html", 
    "https://github.com/wevan"
  ], 
  "acks": " The authors thank the Linux Clusters for Genetic Analysis and the Shared Computing Cluster at Boston University for computational support for this project. The authors thank Marc E. Lenburg and Paola Sebastiani for critical reading of their manuscript. This research was supported by funds from the NIH (U01CA164720) and (T15LM007124). ", 
  "authors": [
    " Ying Shen", 
    " Mumtahena Rahman", 
    " Stephen R Piccolo", 
    " Daniel Gusenleitner", 
    " Nader N El-Chaar", 
    " Luis Cheng", 
    " Stefano Monti", 
    " Andrea H Bild", 
    " W Evan Johnson"
  ], 
  "keyWords": [
    [
      "signatures", 
      "genes", 
      "sampling", 
      "assigned", 
      "profiling", 
      "pathways"
    ]
  ], 
  "github_data": {
    "name": "WeVan", 
    "contributors": [
      {
        "contributions": 46, 
        "html_url": "https://github.com/daniellavoie"
      }, 
      {
        "contributions": 40, 
        "html_url": "https://github.com/invalid-email-address"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/CSPInformatique/WeVan/zipball/0.1.0", 
        "tarball_url": "https://api.github.com/repos/CSPInformatique/WeVan/tarball/0.1.0", 
        "name": "0.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/CSPInformatique/WeVan/zipball/0.0.1", 
        "tarball_url": "https://api.github.com/repos/CSPInformatique/WeVan/tarball/0.0.1", 
        "name": "0.0.1"
      }
    ], 
    "created_at": "2013-08-10T15:55:02Z", 
    "updated_at": "2014-12-18T23:12:30Z", 
    "languages": [
      "Shell", 
      "JavaScript", 
      "Java", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/daniellavoie"
      }
    ], 
    "owner": "https://github.com/CSPInformatique", 
    "homepage": null
  }, 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2015-01-24T04:27:59Z"
}{
  "doi": "10.1093/bioinformatics/btu574", 
  "name": "ballaxy web services for structural bioinformatics", 
  "links": [
    "http://ball-trac.bioinf.uni-sb.de", 
    "http://www.ball-project.org/ballaxy", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://registry.hub.docker"
  ], 
  "title": "Structural bioinformatics ballaxy: web services for structural bioinformatics", 
  "toolName": "Structural bioinformatics ballaxy: web services for structural bioinformatics", 
  "abstract": "Motivation: Web-based workflow systems have gained considerable momentum in sequence-oriented bioinformatics. In structural bio-informatics, however, such systems are still relatively rare; while commercial stand-alone workflow applications are common in the pharmaceutical industry, academic researchers often still rely on command line scripting to glue individual tools together. Results: In this work, we address the problem of building a web-based system for workflows in structural bioinformatics. For the underlying molecular modelling engine, we opted for the BALL framework because of its extensive and well-tested functionality in the field of structural bioinformatics. The large number of molecular data structures and algorithms implemented in BALL allows for elegant and sophisticated development of new approaches in the field. We hence connected the versatile BALL library and its visualization and editing front end BALLView with the Galaxy workflow framework. The result, which we call ballaxy, enables the user to simply and intuitively create sophisticated pipelines for applications in structure-based computational biology, integrated into a standard tool for molecular modelling. Availability and implementation: ballaxy consists of three parts: some minor modifications to the Galaxy system, a collection of tools and an integration into the BALL framework and the BALLView application for molecular modelling. Modifications to Galaxy will be submitted to the Galaxy project, and the BALL and BALLView integrations will be integrated in the next major BALL release. After acceptance of the modifications into the Galaxy project, we will publish all ballaxy tools via the Galaxy toolshed. In the meantime, all three components are available from http://www.ball-project.org/ballaxy. Also, docker images for ballaxy are available at https://registry.hub.docker.", 
  "summary": "The result, which we call ballaxy, enables the user to simply and intuitively create sophisticated pipelines for applications in structure-based computational biology, integrated into a standard tool for molecular modelling.\nAvailability and implementation: ballaxy consists of three parts: some minor modifications to the Galaxy system, a collection of tools and an integration into the BALL framework and the BALLView application for molecular modelling.\nHence, a web-based workflow system for structural computational biology would, at present, require constant switching between tools: from the commonly locally installed molecular modelling application to the web service, and vice versa.", 
  "affiliations": [
    " Center for Bioinformatics Saarland University ", 
    " Applied Bioinformatics Center for Bioinformatics Quantitative Biology Center T \u20ac ubingen University of T \u20ac ubingen ", 
    " Chair for Software-Engineering and Bioinformatics Institute for Informatics Johannes-Gutenberg-University Mainz "
  ], 
  "grants": [
    "They gratefully acknowledge the use of software from OpenEye Scientific Software, Inc.\nFunding: A.H. acknowledges financial support from the Intel Visual Computing Institute (IVCI) of Saarland University and the `Schwerpunkt Rechnergestu tzte Forschungsmethoden in den Naturwissenschaften' of Johannes-Gutenberg University Mainz, A.H. and H.P.L.", 
    "acknowledges financial support from DFG core facilities (grant KO 2313/6-1), the European Commissions Seventh Framework Programme (FP7/2007-2013) under grant agreement no."
  ], 
  "acks": " The authors wish to thank the BALL development team. They gratefully acknowledge the use of software from OpenEye Scientific Software, Inc. ", 
  "authors": [
    " Anna Katharina Hildebrandt", 
    " Daniel St", 
    " Nina M Fischer", 
    " Luis De La Garza", 
    " Jens Kr \u20ac Uger", 
    " Stefan Nickels", 
    " Marc R \u20ac Ottig", 
    " Charlotta Sch", 
    " \u20ac Arfe", 
    " Marcel Schumann", 
    " Philipp Thiel", 
    " Hans-Peter Lenhof", 
    " Oliver Kohlbacher", 
    " Andreas Hildebrandt", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "molecular modelling", 
    [
      "computational", 
      "based", 
      "bioinformatics", 
      "ball", 
      "workflows", 
      "modeling", 
      "tools"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-03T06:17:49Z"
}{
  "doi": "10.1093/bioinformatics/btu716", 
  "name": "Automatic prediction of polysaccharide utilization loci in Bacteroidetes species", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Automatic prediction of polysaccharide utilization loci in Bacteroidetes species", 
  "toolName": "Automatic prediction of polysaccharide utilization loci in Bacteroidetes species", 
  "abstract": "Motivation: A bacterial polysaccharide utilization locus (PUL) is a set of physically linked genes that orchestrate the breakdown of a specific glycan. PULs are prevalent in the Bacteroidetes phy-lum and are key to the digestion of complex carbohydrates, notably by the human gut microbiota. A given Bacteroidetes genome can encode dozens of different PULs whose boundaries and precise gene content are difficult to predict. Results: Here, we present a fully automated approach for PUL prediction using genomic context and domain annotation alone. By combining the detection of a pair of marker genes with operon prediction using intergenic distances, and queries to the carbohydrate-active enzymes database (www.cazy.org), our predictor achieved above 86% accuracy in two Bacteroides species with extensive experimental PUL characterization. Availability and implementation: PUL predictions in 67 Bacteroidetes genomes from the human gut microbiota and two additional species, from the canine oral sphere and from the environment, are presented in our database accessible at www.cazy.org/PULDB/index.php.", 
  "summary": "SusC-like proteins are TonB-dependent transporters characterized by a specific linear sequence of domains: PF13715 (unknown function), PF07715 (TonB-dependent receptor's plug) and PF00593 (TonB-dependent receptor), as illustrated in Figure 1.\nThe user can visualize the predicted PUL with confidence levels (CAZy1 to CAZy3 colored from green to red) as well as published PULs. A contextual menu associated to the genes and PULs allows the user to open new pages with detailed information about (i) the encoded proteins (following IMG/M-HMP annotation or Pfam domains), (ii) the function of CAZymes if known (EC numbers) and (iii) the experimentally verified substrates.\nSci., 23, 324328.\n(2008) Starch catabolism by a prominent human gut symbiont is directed by the recognition of amylose helices.\nSci. USA, 97, 66526657.\nSci., 33, 330338.\nSci., 39, 156158.", 
  "affiliations": [
    " Institute for Cell and Molecular Biosciences The Medical School Newcastle University ", 
    " UMR 7257 Centre National de la Recherche Scientifique CNRS "
  ], 
  "grants": [
    "Funding\nEuropean Research Council under the European Union's Seventh Framework Program (FP/2007-2013)/ERC Grant Agreement no 322820."
  ], 
  "acks": " ", 
  "authors": [
    " Nicolas Terrapon", 
    " Vincent Lombard", 
    " Harry J Gilbert", 
    " Bernard Henrissat"
  ], 
  "keyWords": [
    [
      "genomics", 
      "pairs", 
      "glycans", 
      "genes", 
      "predictions", 
      "puls"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-30T05:01:31Z"
}{
  "doi": "10.1093/bioinformatics/btu497", 
  "name": "Basic4Cseq an RBioconductor package for analyzing 4Cseq data", 
  "links": [
    "http://www.bioconductor.org", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Sequence analysis Basic4Cseq: an R/Bioconductor package for analyzing 4C-seq data", 
  "toolName": "Sequence analysis Basic4Cseq: an R/Bioconductor package for analyzing 4C-seq data", 
  "abstract": "Basic4Cseq is an R/Bioconductor package for basic filtering , analysis and subsequent near-cis visualization of 4C-seq data. The package processes aligned 4C-seq raw data stored in binary alignment/map (BAM) format and maps the short reads to a corresponding virtual fragment library. Functions are included to create virtual fragment libraries providing chromosome position and further information on 4C-seq fragments (length and uniqueness of the fragment ends, and blindness of a fragment) for any BSGenome package. An optional filter is included for BAM files to remove invalid 4C-seq reads, and further filter functions are offered for 4C-seq fragments. Additionally, basic quality controls based on the read distribution are included. Fragment data in the vicinity of the experiment's viewpoint are visualized as coverage plot based on a running median approach and a multi-scale contact profile. Wig files or csv files of the fragment data can be exported for further analyses and visualizations of interactions with other programs. Availability and implementation: Basic4Cseq is implemented in R and available at http://www.bioconductor.org/. A vignette with detailed descriptions of the functions is included in the package. Contact: Carolin.Walter@uni-muenster.de", 
  "summary": "The recently published fourSig method (Williams et al., 2014) includes fragment filtering options and detects different types of interactions, but does not provide more complex visualization routines for the data profile in the viewpoint region.\nThe distribution of reads throughout 4C-seq fragment ends can provide information on the quality of the experiment data (van de Werken et al., 2012b).\nThe package allows users to import fragment data for visualization, and to export filtered 4C-seq reads as csv or wig files for visualization or further analysis of significant interactions.", 
  "affiliations": [
    " Institute of Medical Informatics University of M \u20ac unster ", 
    " Institute of Molecular Tumorbiology University of M \u20ac unster "
  ], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " Carolin Walter", 
    " Daniel Schuetzmann", 
    " Frank Rosenbauer", 
    " Martin Dugas"
  ], 
  "keyWords": [
    "c data", 
    [
      "interactions", 
      "bioinformatics", 
      "reads", 
      "basic", 
      "genomic", 
      "fragments"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-31T03:29:43Z"
}{
  "doi": "10.1093/bioinformatics/btu410", 
  "name": "bammds a tool for assessing the ancestry of lowdepth wholegenome data using multidimensional scaling MDS", 
  "links": [
    "https://savannah.nongnu.org/projects/bammds", 
    "http://www.r-project.org", 
    "https://github.com/samtools/samtools", 
    "http://www.gnu.org/software/parallel", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Genome analysis bammds: a tool for assessing the ancestry of low-depth whole-genome data using multidimensional scaling (MDS)", 
  "toolName": "samtools", 
  "abstract": "We present bammds, a practical tool that allows visualiza-tion of samples sequenced by second-generation sequencing when compared with a reference panel of individuals (usually genotypes) using a multidimensional scaling algorithm. Our tool is aimed at determining the ancestry of unknown samples\u2014typical of ancient DNA data\u2014particularly when only low amounts of data are available for those samples. Availability and implementation: The software package is available under GNU General Public License v3 and is freely available together with test datasets https://savannah.nongnu.org/projects/bammds/. It is using R (http://www.r-project.org/), parallel (http://www.gnu.org/-software/parallel/), samtools (https://", 
  "summary": "Here, we describe a tool that allows to assign an ancestry to low-depth mapped WGS data when compared with an existing reference panel of genotype data using multidimensional scaling (MDS) based on genetic distances, a related method that provides results similar to those of PCA (Cox and Cox, 2000).\npublic reference panel that we make available in the Supplementary data, i.e. HGDP (Li et al., 2008), which includes 4600 000 SNPs and $950 individuals subdivided into 53 populations and 7 geographic regions (Africa, Eastern-, Western-, Central- and South Asia, Europe, Oceania and Native America).", 
  "affiliations": [
    " Centre for GeoGenetics Natural History Museum of Denmark University of Copenhagen "
  ], 
  "grants": [
    "was supported by a Swiss NSF, J.V.M.-M. by the `Consejo Nacional de Ciencia y Tecnologia' (Mexico) and M.D.", 
    "by the US NSF (DBI-1103639).", 
    "Funding: A.-S.M."
  ], 
  "sourcelinks": [
    "http://www.r-project.org", 
    "https://github.com/samtools/samtools", 
    "https://savannah.nongnu.org/projects/bammds", 
    "http://www.gnu.org/software/parallel"
  ], 
  "acks": " The authors thank Mar \u0131a C. Avila-Arcos, Amhed Missael Vargas Vel azquez, Morten E. Allentoft, Hannes Schroeder, Kerttu Majander, Maanasa Raghavan and Johannes Krause for helpful discussions and testing, and the National high-throughput DNA Sequencing Center for assistance with the sequencing. ", 
  "authors": [
    " Anna-Sapfo Malaspinas", 
    " Ole Tange", 
    " Jos ", 
    " V Ictor Moreno-Mayar", 
    " Morten Rasmussen", 
    " Michael Degiorgio", 
    " Yong Wang", 
    " Cristina E Valdiosera", 
    " Gustavo Politis", 
    " Eske Willerslev", 
    " Rasmus Nielsen"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "genomics", 
      "data", 
      "populations"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "Other"
      }, 
      {
        "link": "https://raw.githubusercontent.com/samtools/samtools/develop/LICENSE"
      }
    ], 
    "name": "samtools", 
    "contributors": [
      {
        "contributions": 376, 
        "html_url": "https://github.com/jmarshall"
      }, 
      {
        "contributions": 243, 
        "html_url": "https://github.com/pd3"
      }, 
      {
        "contributions": 160, 
        "html_url": "https://github.com/mp15"
      }, 
      {
        "contributions": 113, 
        "html_url": "https://github.com/jkbonfield"
      }, 
      {
        "contributions": 65, 
        "html_url": "https://github.com/lh3"
      }, 
      {
        "contributions": 53, 
        "html_url": "https://github.com/peterjc"
      }, 
      {
        "contributions": 49, 
        "html_url": "https://github.com/daviesrob"
      }, 
      {
        "contributions": 24, 
        "html_url": "https://github.com/SamStudio8"
      }, 
      {
        "contributions": 16, 
        "html_url": "https://github.com/mcshane"
      }, 
      {
        "contributions": 11, 
        "html_url": "https://github.com/jrandall"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/lindenb"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/leecbaker"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/nc6"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/smowton"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/wookietreiber"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/dkj"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/sb10"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/vsbuffalo"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/pruzanov"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/bewt85"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/antonkratz"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/brentp"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/charles-plessy"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/cbrueffer"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/gdv"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/nieder"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/karel-brinda"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/kdmurray91"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/holtgrewe"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/MariadeAnton"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/st-final", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/st-final", 
        "name": "st-final"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/1.3.1", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/1.3.1", 
        "name": "1.3.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/1.3", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/1.3", 
        "name": "1.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/1.2", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/1.2", 
        "name": "1.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/1.1", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/1.1", 
        "name": "1.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/1.0", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/1.0", 
        "name": "1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc12", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc12", 
        "name": "0.2.0-rc12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc11", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc11", 
        "name": "0.2.0-rc11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc10", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc10", 
        "name": "0.2.0-rc10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc9", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc9", 
        "name": "0.2.0-rc9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc8", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc8", 
        "name": "0.2.0-rc8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc7", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc7", 
        "name": "0.2.0-rc7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc6", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc6", 
        "name": "0.2.0-rc6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc5", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc5", 
        "name": "0.2.0-rc5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc4", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc4", 
        "name": "0.2.0-rc4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc3", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc3", 
        "name": "0.2.0-rc3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc2", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc2", 
        "name": "0.2.0-rc2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.2.0-rc1", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.2.0-rc1", 
        "name": "0.2.0-rc1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.20", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.20", 
        "name": "0.1.20"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.19", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.19", 
        "name": "0.1.19"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.18", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.18", 
        "name": "0.1.18"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.17", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.17", 
        "name": "0.1.17"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.16", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.16", 
        "name": "0.1.16"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.15", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.15", 
        "name": "0.1.15"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.14", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.14", 
        "name": "0.1.14"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.13", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.13", 
        "name": "0.1.13"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.12", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.12", 
        "name": "0.1.12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.12a", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.12a", 
        "name": "0.1.12a"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.11", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.11", 
        "name": "0.1.11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/samtools/samtools/zipball/0.1.10", 
        "tarball_url": "https://api.github.com/repos/samtools/samtools/tarball/0.1.10", 
        "name": "0.1.10"
      }
    ], 
    "created_at": "2012-03-09T02:49:58Z", 
    "updated_at": "2016-08-09T05:22:17Z", 
    "languages": [
      "C", 
      "Shell", 
      "Java", 
      "M4", 
      "Python", 
      "Makefile", 
      "C++", 
      "Perl", 
      "Lua", 
      "Groff"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/lh3"
      }, 
      {
        "html_url": "https://github.com/mp15"
      }, 
      {
        "html_url": "https://github.com/tonatiuhpenacenteno"
      }, 
      {
        "html_url": "https://github.com/bnbowman"
      }, 
      {
        "html_url": "https://github.com/jmarshall"
      }, 
      {
        "html_url": "https://github.com/joelmartin"
      }, 
      {
        "html_url": "https://github.com/yaomin"
      }, 
      {
        "html_url": "https://github.com/ltnetcase"
      }, 
      {
        "html_url": "https://github.com/sauloal"
      }, 
      {
        "html_url": "https://github.com/stsmall"
      }, 
      {
        "html_url": "https://github.com/caddymob"
      }, 
      {
        "html_url": "https://github.com/mcshane"
      }, 
      {
        "html_url": "https://github.com/zym1905"
      }, 
      {
        "html_url": "https://github.com/rfinkers"
      }, 
      {
        "html_url": "https://github.com/boboppie"
      }, 
      {
        "html_url": "https://github.com/tseemann"
      }, 
      {
        "html_url": "https://github.com/pchines"
      }, 
      {
        "html_url": "https://github.com/agarwal"
      }, 
      {
        "html_url": "https://github.com/TarjinderSingh"
      }, 
      {
        "html_url": "https://github.com/tk2"
      }, 
      {
        "html_url": "https://github.com/mattmdedek"
      }, 
      {
        "html_url": "https://github.com/dbolser-ebi"
      }, 
      {
        "html_url": "https://github.com/iniguh"
      }, 
      {
        "html_url": "https://github.com/apetkau"
      }, 
      {
        "html_url": "https://github.com/fidelram"
      }, 
      {
        "html_url": "https://github.com/svm-zhang"
      }, 
      {
        "html_url": "https://github.com/mteague"
      }, 
      {
        "html_url": "https://github.com/idworkin"
      }, 
      {
        "html_url": "https://github.com/presentone"
      }, 
      {
        "html_url": "https://github.com/spvensko"
      }
    ], 
    "owner": "https://github.com/samtools", 
    "homepage": "http://htslib.org/"
  }, 
  "technologies": [], 
  "dateCreated": "2014-06-29T00:12:32Z"
}{
  "doi": "10.1093/bioinformatics/btu492", 
  "name": "Big data and other challenges in the quest for orthologs", 
  "links": [
    "http://questfor", 
    "http://www.w3.org/TR/rdf-sparql-query", 
    "http://www.w3.org/RDF", 
    "http://swisstree", 
    "http://questfororthologs", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://orthoxml.org", 
    "http://questfororthologs.org", 
    "http://www.ebi.ac.uk/reference_proteomes", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis Big data and other challenges in the quest for orthologs", 
  "toolName": "Genome analysis Big data and other challenges in the quest for orthologs", 
  "abstract": "Given the rapid increase of species with a sequenced genome, the need to identify orthologous genes between them has emerged as a central bioinformatics task. Many different methods exist for orthology detection, which makes it difficult to decide which one to choose for a particular application. Here, we review the latest developments and issues in the orthology field, and summarize the most recent results reported at the third 'Quest for Orthologs' meeting. We focus on community efforts such as the adoption of reference proteomes, standard file formats and benchmarking. Progress in these areas is good, and they are already beneficial to both orthology consumers and providers. However, a major current issue is that the massive increase in complete prote-omes poses computational challenges to many of the ortholog database providers, as most orthology inference algorithms scale at least quadratically with the number of proteomes. The Quest for Orthologs consortium is an open community with a number of working groups that join efforts to enhance various aspects of orthology analysis, such as defining standard formats and datasets, documenting community resources and benchmarking. Availability and implementation: All such materials are available at", 
  "summary": "Hierarchical orthologous groups are defined with respect to specific species clades and-- barring inference errors--contain all the sequences that have evolved from a single ancestral gene in the last common ancestor of that clade (Jothi et al., 2006; Kriventseva et al., 2008; Merkeev et al., 2006; Powell et al., 2014; reviewed in Boeckmann et al., 2011).\nAdrian Altenhoff (ETH Zurich, Switzerland) introduced a new method to compute hierarchical orthologous groups from pairs of orthologous genes (Altenhoff et al., 2013), available in the OMA database and the OMA stand-alone software (http:// omabrowser.org/standalone).", 
  "affiliations": [
    " EMBL-European Bioinformatics Institute", 
    " Division of Bioinformatics Department of Preventive Medicine University of Southern California "
  ], 
  "grants": [
    "Funding: Open access charges were covered by the UCL Library."
  ], 
  "acks": " Members of the Quest for Orthologs Consortium: Adrian Altenhoff, Judith Blake, Brigitte Boeckmann, Mike Cherry, Hirokazu Chiba, Vincent Daubin, Todd DeLuca, Christophe Dessimoz, Jean-Francois Dufayard, Dannie Durand, Ingo Ebersberger, Jesualdo Tomas Fernandez Breis, Kristoffer Forslund, Toni Gabaldon, Manuel Gil, - Dord -e Grbic\u00b4,Grbic\u00b4, Javier Herrero, Ingrid Keseler, Evgenia Kriventseva, Gilles Lasalle, Suzanna Lewis, Lucy Mengqi Li, Benjamin Linard, Ari L \u20ac oytynoja, Rachel Lyne, Maria Martin, Raja Mazumder, Vincent Miele, Sebastien Moretti, Matthieu Muffato, Christopher Mungall, Mateus Patricio, Simon Penel, Cecile Pereira, Malte Petersen, R emi Planel, Marc Robinson-Rechavi, David Roos, Thomas Schmitt, Fabian Schreiber, Kimmen Sj\u20ac olander, Nives Skunca, Erik Sonnhammer, Alan Wilter Sousa da Silva, Radek Szklarczyk, Paul Thomas, Ikuo Uchiyama, Asier Ullate, Michiel Van Bel, Klaas Vandepoele, ", 
  "authors": [
    " Erik L L Sonnhammer", 
    " Toni Gabald On", 
    " Alan W Sousa Da Silva", 
    " Maria Martin", 
    " Marc Robinson-Rechavi", 
    " Brigitte Boeckmann", 
    " Paul D Thomas", 
    " Christophe Dessimoz", 
    " ", 
    " John Hancock"
  ], 
  "keyWords": [
    "orthologous genes", 
    "ortholog database", 
    [
      "genomics", 
      "orthologs", 
      "databases", 
      "bioinformatics", 
      "gene", 
      "species"
    ]
  ], 
  "sourcelinks": [
    "http://questfor", 
    "http://questfororthologs", 
    "http://www.w3.org/RDF", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.w3.org/TR/rdf-sparql-query"
  ], 
  "technologies": [
    " SQL "
  ], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btu520", 
  "name": "Bias correction for selecting the minimalerror classifier from many machine learning models", 
  "links": [
    "http://epub.ub.uni-muenchen.de/12231", 
    "http://tcga-data.nci.nih.gov/tcga", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://www.synapse.org"
  ], 
  "title": "Genome analysis Bias correction for selecting the minimal-error classifier from many machine learning models", 
  "toolName": "Genome analysis Bias correction for selecting the minimal-error classifier from many machine learning models", 
  "abstract": "Motivation: Supervised machine learning is commonly applied in gen-omic research to construct a classifier from the training data that is generalizable to predict independent testing data. When test datasets are not available, cross-validation is commonly used to estimate the error rate. Many machine learning methods are available, and it is well known that no universally best method exists in general. It has been a common practice to apply many machine learning methods and report the method that produces the smallest cross-validation error rate. Theoretically, such a procedure produces a selection bias. Consequently, many clinical studies with moderate sample sizes (e.g. n = 30\u201360) risk reporting a falsely small cross-validation error rate that could not be validated later in independent cohorts. Results: In this article, we illustrated the probabilistic framework of the problem and explored the statistical and asymptotic properties. We proposed a new bias correction method based on learning curve fitting by inverse power law (IPL) and compared it with three existing methods: nested cross-validation, weighted mean correction and Tibshirani-Tibshirani procedure. All methods were compared in simulation datasets, five moderate size real datasets and two large breast cancer datasets. The result showed that IPL outperforms the other methods in bias correction with smaller variance, and it has an additional advantage to extrapolate error estimates for larger sample sizes, a practical feature to recommend whether more samples should be recruited to improve the classifier and accuracy. An R package 'MLbias' and all source files are publicly available.", 
  "summary": "Figure 2, left, shows the averaged error rate for classification method m at sample size n [i.e. P^ n;mB] estimated from B = 1000\nIn this section, we propose a new resampling-based IPL method to correct the MEC error rate bias and estimate the true optimal classification error rate Pn;M.\nWe calculated both bias-corrected estimates with M = 2 classifiers (DLDA and QDA) and all M = 10 classifiers and compared them with the true best classification error rate at sample size n = 20, 40, 80, 160, 640, 1280 with B = 100 simulated datasets.", 
  "affiliations": [
    " Magee-Womens Research Institute", 
    " Department of Biostatistics Graduate School of Public Health University of Pittsburgh "
  ], 
  "grants": [
    "Funding: (NIH R21MH094862)."
  ], 
  "acks": " The authors would like to thank suggestions from the reviewers that have significantly improved this article. ", 
  "authors": [
    " Ying Ding", 
    " Shaowu Tang", 
    " Serena G Liao", 
    " Jia Jia", 
    " Steffi Oesterreich", 
    " Yan Lin", 
    " George C Tseng", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "classifying", 
      "biased", 
      "methods", 
      "estimating", 
      "error", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://tcga-data.nci.nih.gov/tcga"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-02T07:42:41Z"
}{
  "doi": "10.1093/bioinformatics/btv023", 
  "name": "Bias in microRNA functional enrichment analysis", 
  "links": [
    "http://sgjlab.org/empirical-go", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Gene expression Bias in microRNA functional enrichment analysis", 
  "toolName": "Gene expression Bias in microRNA functional enrichment analysis", 
  "abstract": "Motivation: Many studies have investigated the differential expression of microRNAs (miRNAs) in disease states and between different treatments, tissues and developmental stages. Given a list of perturbed miRNAs, it is common to predict the shared pathways on which they act. The standard test for functional enrichment typically yields dozens of significantly enriched functional categories, many of which appear frequently in the analysis of apparently unrelated diseases and conditions. Results: We show that the most commonly used functional enrichment test is inappropriate for the analysis of sets of genes targeted by miRNAs. The hypergeometric distribution used by the standard method consistently results in significant P-values for functional enrichment for targets of randomly selected miRNAs, reflecting an underlying bias in the predicted gene targets of miRNAs as a whole. We developed an algorithm to measure enrichment using an empirical sampling approach, and applied this in a reanalysis of the gene ontology classes of targets of miRNA lists from 44 published studies. The vast majority of the miRNA target sets were not significantly enriched in any functional category after correction for bias. We therefore argue against continued use of the standard functional enrichment method for miRNA targets. Availability and implementation: A Python script implementing the empirical algorithm is freely available at http://sgjlab.org/empirical-go/.", 
  "summary": "We used our algorithm to investigate whether the null hypothesis used by the standard method was appropriate by comparing the hypergeometric distribution with an empirical distribution for the number of predicted target genes belonging to a GO term for randomly sampled miRNAs. As an illustration, we use the GO term `ion transport' (GO:0006811), which is often reported as significantly enriched in the literature (Liu et al., 2010; Sokolov et al., 2012; Yunta et al., 2012).", 
  "affiliations": [
    " Faculty of Medical and Human Sciences", 
    " Faculty of Life Sciences University of Manchester "
  ], 
  "grants": [
    "Funding\nThis work was supported by a Medical Research Council studentship (MR/ K501311/1) and University of Manchester President's Doctoral Scholar award [to T.B.].", 
    "Funding for this project was provided by Autistica grant [7248 to J.L.]."
  ], 
  "acks": " The authors thank Antonio Marco for the canonical seed target prediction script and Crispin Miller, David Gerrard, Ryan Ames and David Talavera for useful comments on the manuscript. ", 
  "authors": [
    " Thomas Bleazard", 
    " Janine A Lamb", 
    " Sam Griffiths-Jones"
  ], 
  "keyWords": [
    "genes targeted", 
    [
      "targeting", 
      "humans", 
      "mirnas", 
      "terms", 
      "gene", 
      "micrornas"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-22T03:50:58Z"
}{
  "doi": "10.1093/bioinformatics/btu595", 
  "name": "BigDataScript a scripting language for data pipelines", 
  "links": [
    "http://pcingola.github.io/BigDataScript", 
    "https://github.com/pcingola", 
    "http://pcingola", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://www.illumina.com/platinumgenomes).The", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://pcingola.github.io/BigDataScript.Unlike"
  ], 
  "title": "Genome analysis BigDataScript: a scripting language for data pipelines", 
  "toolName": "BigDataScript", 
  "abstract": "Motivation: The analysis of large biological datasets often requires complex processing pipelines that run for a long time on large computational infrastructures. We designed and implemented a simple script-like programming language with a clean and minimalist syntax to develop and manage pipeline execution and provide robustness to various types of software and hardware failures as well as portability. Results: We introduce the BigDataScript (BDS) programming language for data processing pipelines, which improves abstraction from hardware resources and assists with robustness. Hardware abstraction allows BDS pipelines to run without modification on a wide range of computer architectures, from a small laptop to multi-core servers, server farms, clusters and clouds. BDS achieves robustness by incorporating the concepts of absolute serialization and lazy processing , thus allowing pipelines to recover from errors. By abstracting pipeline concepts at programming language level, BDS simplifies implementation , execution and management of complex bioinformatics pipelines, resulting in reduced development and debugging cycles as well as cleaner code. Availability and implementation: BigDataScript is available under open-source license at http://pcingola.github.io/BigDataScript.", 
  "summary": "As BDS is intended to solve or simplify the main challenges in implementing, testing and programming data processing pipelines without introducing a steep learning curve, our main design goals are (i) simple programming language; (ii) abstraction from system's architecture; and (iii) robustness to hardware and software failure during computationally intensive data analysis tasks.\nIn the same way that high-level programming languages such as C or Java allow abstraction of the CPU type and other hardware features, BDS supports system-level abstraction, including the number and the type of computing-nodes or CPU-cores that are available to the pipeline and its component tasks, whether firing another process may saturate the server's memory or whether a process is executed immediately or queued.", 
  "affiliations": [
    " McGill University and G enome Qu ebec Innovation Centre", 
    " School of Computer Science McGill University "
  ], 
  "grants": [
    "Funding: This work was supported by NIH grants to R.S.", 
    "(T2DGENES, U01 DK085545-01) and by an NSERC Discovery grant to M.B."
  ], 
  "sourcelinks": [
    "http://pcingola.github.io/BigDataScript.Unlike", 
    "http://pcingola.github.io/BigDataScript", 
    "http://www.illumina.com/platinumgenomes).The", 
    "http://pcingola", 
    "https://github.com/pcingola"
  ], 
  "acks": " The authors would like to thank Hernan Gonzalez for implementing bug fixes; Fernando Garcia Sanz for testing and contributing to the documentation; and Louis Letourneau his feedback on useful features in pipeline design. ", 
  "authors": [
    " Pablo Cingolani", 
    " Rob Sladek", 
    " Mathieu Blanchette", 
    " John Hancock"
  ], 
  "keyWords": [
    "pipeline execution", 
    [
      "executable", 
      "tasks", 
      "pipelines", 
      "processing", 
      "programming", 
      "languages", 
      "bioinformatics"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "Apache License 2.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/pcingola/BigDataScript/master/LICENSE.txt"
      }
    ], 
    "name": "BigDataScript", 
    "contributors": [
      {
        "contributions": 1211, 
        "html_url": "https://github.com/pcingola"
      }, 
      {
        "contributions": 16, 
        "html_url": "https://github.com/leonbloy"
      }, 
      {
        "contributions": 4, 
        "html_url": "https://github.com/babass83"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/golharam"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/jpjais"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/karthik-rp"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.9999", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.9999", 
        "name": "v0.9999"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.9999c", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.9999c", 
        "name": "v0.9999c"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999", 
        "name": "v0.999"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999l", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999l", 
        "name": "v0.999l"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999k", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999k", 
        "name": "v0.999k"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999h", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999h", 
        "name": "v0.999h"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999g", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999g", 
        "name": "v0.999g"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999f", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999f", 
        "name": "v0.999f"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999d", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999d", 
        "name": "v0.999d"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999b", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999b", 
        "name": "v0.999b"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.999a", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.999a", 
        "name": "v0.999a"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.99h", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.99h", 
        "name": "v0.99h"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.99g", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.99g", 
        "name": "v0.99g"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.99f", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.99f", 
        "name": "v0.99f"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.99e", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.99e", 
        "name": "v0.99e"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/v0.99d", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/v0.99d", 
        "name": "v0.99d"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/0.999", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/0.999", 
        "name": "0.999"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/0.99", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/0.99", 
        "name": "0.99"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pcingola/BigDataScript/zipball/0.98", 
        "tarball_url": "https://api.github.com/repos/pcingola/BigDataScript/tarball/0.98", 
        "name": "0.98"
      }
    ], 
    "created_at": "2013-03-03T18:00:45Z", 
    "updated_at": "2016-06-06T08:35:58Z", 
    "languages": [
      "Shell", 
      "Java", 
      "ANTLR", 
      "JavaScript", 
      "Perl", 
      "Python", 
      "HTML", 
      "Go", 
      "GAP"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/pcingola"
      }, 
      {
        "html_url": "https://github.com/keithchambers"
      }, 
      {
        "html_url": "https://github.com/SuriyaaKudoIsc"
      }, 
      {
        "html_url": "https://github.com/apastore"
      }, 
      {
        "html_url": "https://github.com/jpjais"
      }, 
      {
        "html_url": "https://github.com/khl0798"
      }
    ], 
    "owner": "https://github.com/pcingola", 
    "homepage": null
  }, 
  "technologies": [
    "Python", 
    "Java"
  ], 
  "dateCreated": "2014-09-05T05:34:33Z"
}{
  "doi": "10.1093/bioinformatics/btu558", 
  "name": "BioBloom tools fast accurate and memoryefficient host species sequence screening using bloom filters", 
  "links": [
    "http://creativecommons.org/licenses/by-nc/4.0", 
    "https://github.com/SciLifeLab/facs", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/nh13/DWGSIM", 
    "https://basespace.illumina.com/run/716717/2x150-HiSeq-2500demo-NA12878"
  ], 
  "title": "Sequence analysis BioBloom tools: fast, accurate and memory-efficient host species sequence screening using bloom filters", 
  "toolName": "facs", 
  "abstract": "Large datasets can be screened for sequences from a specific organism , quickly and with low memory requirements, by a data structure that supports time-and memory-efficient set membership queries. Bloom filters offer such queries but require that false positives be controlled. We present BioBloom Tools, a Bloom filter-based sequence screening tool that is faster than BWA, Bowtie 2 (popular alignment algorithms) and FACS (a membership query algorithm). It delivers accuracies comparable with these tools, controls false posi-tives and has low memory requirements.", 
  "summary": "Receiver operator characteristic curves of BBT and FACS using simulated 100 bp SE reads from Homo sapiens mixed with (A) E.coli and (B) Mus musculus filtered against an H.sapiens Bloom filter using a k-mer size of 25 bp; (C) CPU time benchmark comparing BT2 (for a range of built-in settings), BWA (using aln and mem settings), FACS and BBT, on one lane of human 2  150 bp PE Illumina HiSeq 2500 reads\nFor categorization, using the human reference and simulated reads, the peak memory usage (GB) for each tool was 3.8 (BBT), 4.8 (FACS), 3.1 (BWA aln), 5.2 (BWA mem) and 3.4 (BT2).", 
  "affiliations": [
    " Michael Smith Genome Sciences Centre British Columbia Cancer Agency "
  ], 
  "grants": [
    "Funding: The work was funded by Genome Canada, British Columbia Cancer Foundation and Genome British Columbia."
  ], 
  "sourcelinks": [
    "https://github.com/SciLifeLab/facs", 
    "https://github.com/nh13/DWGSIM"
  ], 
  "acks": " ", 
  "authors": [
    " Justin Chu", 
    " Sara Sadeghi", 
    " Anthony Raymond", 
    " Shaun D Jackman", 
    " Ka Ming Nip", 
    " Richard Mar", 
    " Hamid Mohamadi", 
    " Yaron S Butterfield", 
    " A Gordon Robertson", 
    " Inanc\u00b8birolinanc\u00b8birol ", 
    " ", 
    " "
  ], 
  "keyWords": [
    [
      "filtered", 
      "bioinformatics", 
      "mers", 
      "memory", 
      "sequencing", 
      "facs"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "Other"
      }, 
      {
        "link": "https://raw.githubusercontent.com/SciLifeLab/facs/master/LICENSE"
      }
    ], 
    "name": "facs", 
    "contributors": [
      {
        "contributions": 405, 
        "html_url": "https://github.com/tzcoolman"
      }, 
      {
        "contributions": 339, 
        "html_url": "https://github.com/brainstorm"
      }, 
      {
        "contributions": 46, 
        "html_url": "https://github.com/guillermo-carrasco"
      }, 
      {
        "contributions": 5, 
        "html_url": "https://github.com/DaveMessina"
      }, 
      {
        "contributions": 4, 
        "html_url": "https://github.com/arvestad"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/henrikstranneheim"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/okulev"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/wmarquesr"
      }
    ], 
    "versions": [], 
    "created_at": "2011-01-26T14:09:57Z", 
    "updated_at": "2016-05-15T09:05:07Z", 
    "languages": [
      "Python", 
      "Jupyter Notebook", 
      "JavaScript", 
      "Makefile", 
      "C"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/luminita"
      }, 
      {
        "html_url": "https://github.com/farahani"
      }, 
      {
        "html_url": "https://github.com/paniccodewriter"
      }, 
      {
        "html_url": "https://github.com/hussius"
      }, 
      {
        "html_url": "https://github.com/ellensherwood"
      }, 
      {
        "html_url": "https://github.com/henrikstranneheim"
      }, 
      {
        "html_url": "https://github.com/jfnavarro"
      }, 
      {
        "html_url": "https://github.com/percyfal"
      }, 
      {
        "html_url": "https://github.com/alekut"
      }, 
      {
        "html_url": "https://github.com/tzcoolman"
      }, 
      {
        "html_url": "https://github.com/mayabrandi"
      }, 
      {
        "html_url": "https://github.com/pelinakan"
      }, 
      {
        "html_url": "https://github.com/acsonnhammer"
      }, 
      {
        "html_url": "https://github.com/arvestad"
      }, 
      {
        "html_url": "https://github.com/brainstorm"
      }, 
      {
        "html_url": "https://github.com/inodb"
      }, 
      {
        "html_url": "https://github.com/alneberg"
      }, 
      {
        "html_url": "https://github.com/mariogiov"
      }, 
      {
        "html_url": "https://github.com/remiolsen"
      }, 
      {
        "html_url": "https://github.com/ewels"
      }, 
      {
        "html_url": "https://github.com/azanov"
      }
    ], 
    "owner": "https://github.com/SciLifeLab", 
    "homepage": "http://facs.scilifelab.se/"
  }, 
  "technologies": [
    "C++"
  ], 
  "dateCreated": "2014-08-21T04:05:53Z"
}{
  "doi": "10.1093/bioinformatics/btu767", 
  "name": "Biological Dynamics Markup Language BDML an open format for representing quantitative biological dynamics data", 
  "links": [
    "http://ssbd.qbic.riken.jp/bdml/).The", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://tools.ietf.org/html", 
    "http://ssbd.qbic.riken.jp", 
    "http://ssbd.qbic.riken.jp/bdml/.Table", 
    "http://www.w3.org", 
    "http://ssbd", 
    "http://lw3.hdfgroup.org/projects/nara/XML_and", 
    "http://ssbd.qbic.riken.jp/bdml", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://ssbd.qbic.riken.jp/phenochar", 
    "http://creativecommons.org/licenses", 
    "http://ssbd.qbic.riken.jp/BDML4DViewer/.4", 
    "http://ssbd.qbic.riken.jp/pdpml/.Fig"
  ], 
  "title": "Biological Dynamics Markup Language (BDML): an open format for representing quantitative biological dynamics data", 
  "toolName": "Biological Dynamics Markup Language (BDML): an open format for representing quantitative biological dynamics data", 
  "abstract": "Motivation: Recent progress in live-cell imaging and modeling techniques has resulted in generation of a large amount of quantitative data (from experimental measurements and computer simulations) on spatiotemporal dynamics of biological objects such as molecules, cells and organisms. Although many research groups have independently dedicated their efforts to developing software tools for visualizing and analyzing these data, these tools are often not compatible with each other because of different data formats. Results: We developed an open unified format, Biological Dynamics Markup Language (BDML; current version: 0.2), which provides a basic framework for representing quantitative biological dynamics data for objects ranging from molecules to cells to organisms. BDML is based on Extensible Markup Language (XML). Its advantages are machine and human readability and exten-sibility. BDML will improve the efficiency of development and evaluation of software tools for data visualization and analysis. Availability and implementation: A specification and a schema file for BDML are freely available online at", 
  "summary": "Results: We developed an open unified format, Biological Dynamics Markup Language (BDML; current version: 0.2), which provides a basic framework for representing quantitative biological dynamics data for objects ranging from molecules to cells to organisms.\nIn this study, we developed an open format for representing quantitative biological dynamics data, Biological Dynamics Markup Language (BDML).\nThe info element provides information about the BDML file, whereas the ontology, summary, contact and methods elements represent meta-information of the quantitative data.\nThese examples demonstrate that BDML can represent quantitative biological dynamics data from molecules to cells to organisms.\nBDML is an open XML-based format for representing quantitative biological dynamics data.", 
  "affiliations": [
    " RIKEN Quantitative Biology Center Laboratory for Developmental Dynamics "
  ], 
  "grants": [
    "Funding\nThis work was supported by the National Bioscience Database Center (NBDC) of the Japan Science and Technology Agency (JST).", 
    "(2012) NIH Image to ImageJ: 25 years of image analysis."
  ], 
  "acks": " We are grateful to Drs. Taiji Adachi, Satya N.V. Arjunan, Kazunari Iwamoto and Satoru Okuda for providing their original data and to the members of the Onami laboratory for feedback and discussions. This work was supported by the National Bioscience Database Center (NBDC) of the Japan Science and Technology Agency (JST). Conflict of interest: none declared. ", 
  "authors": [
    " Koji Kyoda", 
    " Yukako Tohsato", 
    " Kenneth H L Ho", 
    " Shuichi Onami"
  ], 
  "keyWords": [
    [
      "elements", 
      "bdml", 
      "formatted", 
      "figs", 
      "biological", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://tools.ietf.org/html", 
    "http://ssbd.qbic.riken.jp", 
    "http://ssbd.qbic.riken.jp/BDML4DViewer/.4", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses", 
    "http://ssbd"
  ], 
  "technologies": [
    "XML"
  ], 
  "dateCreated": "2014-11-21T01:18:58Z"
}{
  "doi": "10.1093/bioinformatics/btu498", 
  "name": "Biocellion accelerating computer simulation of multicellular biological system models", 
  "links": [
    "http://biocellion.com", 
    "http://paraview.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://hpc.pnl"
  ], 
  "title": "Systems biology Biocellion: accelerating computer simulation of multicellular biological system models", 
  "toolName": "Systems biology Biocellion: accelerating computer simulation of multicellular biological system models", 
  "abstract": "Motivation: Biological system behaviors are often the outcome of complex interactions among a large number of cells and their biotic and abiotic environment. Computational biologists attempt to understand , predict and manipulate biological system behavior through mathematical modeling and computer simulation. Discrete agent-based modeling (in combination with high-resolution grids to model the extracellular environment) is a popular approach for building biological system models. However, the computational complexity of this approach forces computational biologists to resort to coarser resolution approaches to simulate large biological systems. High-performance parallel computers have the potential to address the computing challenge, but writing efficient software for parallel computers is difficult and time-consuming. Results: We have developed Biocellion, a high-performance software framework, to solve this computing challenge using parallel computers. To support a wide range of multicellular biological system models, Biocellion asks users to provide their model specifics by filling the function body of pre-defined model routines. Using Biocellion, modelers without parallel computing expertise can efficiently exploit parallel computers with less effort than writing sequential programs from scratch. We simulate cell sorting, microbial patterning and a bacterial system in soil aggregate as case studies. Availability and implementation: Biocellion runs on x86 compatible systems with the 64 bit Linux operating system and is freely available for academic use. Visit http://biocellion.com for additional information.", 
  "summary": "We designed Biocellion to allow computational biologists to use discrete agent-based modeling--in combination with highresolution grids to model the extracellular environment--from simple small-scale models to highly complex large-scale models with millions to billions of cells.\nTo the best of our knowledge, Biocellion is the only software framework that is specifically designed for discrete agent-based simulation of general multiscale multicellular biological system models and exploits high-performance parallel computing to simulate millions to billions of cells.\nBiocellion has three computational modules to (i) update individual discrete agent states, (ii) evaluate direct physico-mechanical interactions between discrete agent pairs in close proximity and (iii) track changes in the extracellular space to model indirect interactions among cells via diffusible molecules and interactions between cells and their environment.", 
  "affiliations": [
    " Computational Biology and Bioinformatics Group", 
    " Department of Computer Science Utah State University ", 
    " High-performance Computing Group Pacific Northwest National Laboratory ", 
    " Institute for Systems Biology"
  ], 
  "grants": [
    "Funding: Support for this research was provided by the Extreme Scale Computing Initiative, the Fundamental and Computational Sciences Directorate and the Technology Investment Program, as part of the Laboratory Directed Research and Development Program at Pacific Northwest National Laboratory (PNNL)."
  ], 
  "acks": " ", 
  "authors": [
    " Seunghwa Kang", 
    " Simon Kahan", 
    " Jason Mcdermott", 
    " Nicholas Flann", 
    " Ilya Shmulevich"
  ], 
  "keyWords": [
    [
      "computational", 
      "biocellion", 
      "modelling", 
      "agents", 
      "simulations", 
      "cells"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btu772", 
  "name": "Bioimagingbased detection of mislocalized proteins in human cancers by semisupervised learning", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Bioimaging-based detection of mislocalized proteins in human cancers by semi-supervised learning", 
  "toolName": "Bioimaging-based detection of mislocalized proteins in human cancers by semi-supervised learning", 
  "abstract": "Motivation: There is a long-term interest in the challenging task of finding translocated and mislo-cated cancer biomarker proteins. Bioimages of subcellular protein distribution are new data sources which have attracted much attention in recent years because of their intuitive and detailed descriptions of protein distribution. However, automated methods in large-scale biomarker screening suffer significantly from the lack of subcellular location annotations for bioimages from cancer tissues. The transfer prediction idea of applying models trained on normal tissue proteins to predict the subcellular locations of cancerous ones is arbitrary because the protein distribution patterns may differ in normal and cancerous states. Results: We developed a new semi-supervised protocol that can use unlabeled cancer protein data in model construction by an iterative and incremental training strategy. Our approach enables us to selectively use the low-quality images in normal states to expand the training sample space and provides a general way for dealing with the small size of annotated images used together with large unannotated ones. Experiments demonstrate that the new semi-supervised protocol can result in improved accuracy and sensitivity of subcellular location difference detection.", 
  "summary": "To enhance the performance of predicting subcellular locations of proteins in cancerous tissues, we consider adding some images from cancerous tissues into the training set to eliminate the transfer prediction error caused by the difference between the normal and cancer data.\naThe results have two lines: the first line is the predicted subcellular location labels in normal and cancer conditions, respectively, by the classifier; the second line is the P-values measuring the subcellular location changes when cancer occurs (column 3), which are calculated by the independent sample t test on the predicted scores for normal and cancer images.", 
  "affiliations": [
    " Institute of Image Processing and Pattern Recognition Ministry of Education of China and Key Laboratory of System Control and Information Processing Shanghai Jiao Tong University ", 
    " Department of Computational Medicine and Bioinformatics University of Michigan "
  ], 
  "grants": [
    "Funding\nThis work was supported in part by the National Natural Science Foundation of China [Nos."
  ], 
  "acks": " We are grateful to Dr. Jeffrey Brender and Dr. Richard Jang for reading the manuscript. This work was supported in part by the National Natural Science Foundation of China, Shanghai Science and Technology Commission, a Foundation for the Author of National Excellent Doctoral Dissertation of People's Republic of China and the National Institute of General Medical Sciences. Conflict of interest: none declared. ", 
  "authors": [
    " Ying-Ying Xu", 
    " Fan Yang", 
    " Yang Zhang", 
    " Hong-Bin Shen"
  ], 
  "keyWords": [
    [
      "proteins", 
      "predictions", 
      "sampling", 
      "cancerous", 
      "imaging", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-21T01:18:58Z"
}{
  "doi": "10.1093/bioinformatics/btu614", 
  "name": "BioVLABMMIANGS microRNAmRNA integrated analysis using highthroughput sequencing data", 
  "links": [
    "http://epigenomics.snu.ac.kr", 
    "http://www.etri.re.kr/eng/res/res", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://airavata.apache.org", 
    "http://bioconductor.org/packages/2.11/bioc/html/cummeRbund.html.266"
  ], 
  "title": "Sequence analysis BioVLAB-MMIA-NGS: microRNA\u2013mRNA integrated analysis using high-throughput sequencing data", 
  "toolName": "Sequence analysis BioVLAB-MMIA-NGS: microRNA\u2013mRNA integrated analysis using high-throughput sequencing data", 
  "abstract": "Motivation: It is now well established that microRNAs (miRNAs) play a critical role in regulating gene expression in a sequence-specific manner, and genome-wide efforts are underway to predict known and novel miRNA targets. However, the integrated miRNA\u2013mRNA analysis remains a major computational challenge, requiring powerful informatics systems and bioinformatics expertise. Results: The objective of this study was to modify our widely recognized Web server for the integrated mRNA\u2013miRNA analysis (MMIA) and its subsequent deployment on the Amazon cloud (BioVLAB-MMIA) to be compatible with high-throughput platforms, including next-generation sequencing (NGS) data (e.g. RNA-seq). We developed a new version called the BioVLAB-MMIA-NGS, deployed on both Amazon cloud and on a high-performance publicly available server called MAHA. By using NGS data and integrating various bio-informatics tools and databases, BioVLAB-MMIA-NGS offers several advantages. First, sequencing data is more accurate than array-based methods for determining miRNA expression levels. Second, potential novel miRNAs can be detected by using various computational methods for characterizing miRNAs. Third, because miRNA-mediated gene regulation is due to hybridization of an miRNA to its target mRNA, sequencing data can be used to identify many-to-many relationship between miRNAs and target genes with high accuracy. Availability and implementation: http://epigenomics.snu.ac.kr/ biovlab_mmia_ngs/", 
  "summary": "Here we present the next-generation sequencing (NGS) data-compatible BioVLABMMIA-NGS, an updated version of our array-based miRNA mRNA integrated analysis system mRNAmiRNA analysis (MMIA) (Nam et al., 2009)\n[10038768, The Development of Supercomputing System for the Genome Analysis], by Next- Generation BioGreen 21 Program [PJ009037022012]; Rural Development Administration, Republic of Korea, and by Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT & Future Planning (No.NRF2012M3C4A7033341).\nNat. Protoc., 35, 169175.\nNat. Genet., 39, 12781284.\nNat. Protoc., 7, 562578.", 
  "affiliations": [
    " Department of Computer Science School of Informatics and Computing Indiana University Bloomington ", 
    " Indiana University School of Medicine", 
    " School of Computer Science and Engineering Seoul National University "
  ], 
  "grants": [
    "Funding: This work was supported by the IT R&D program of MSIP/KEIT."
  ], 
  "acks": " ", 
  "authors": [
    " Heejoon Chae", 
    " Sungmin Rhee", 
    " Kenneth P Nephew", 
    " Sun Kim"
  ], 
  "keyWords": [
    [
      "data", 
      "genes", 
      "mirnas", 
      "analysis", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-01T07:57:30Z"
}{
  "doi": "10.1093/bioinformatics/btu507", 
  "name": "BitPAl a bitparallel general integerscoring sequence alignment algorithm", 
  "links": [
    "http://lobstah.bu.edu/BitPAl/BitPAl.html", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Sequence analysis BitPAl: a bit-parallel, general integer-scoring sequence alignment algorithm", 
  "toolName": "Sequence analysis BitPAl: a bit-parallel, general integer-scoring sequence alignment algorithm", 
  "abstract": "Motivation: Mapping of high-throughput sequencing data and other bulk sequence comparison applications have motivated a search for high-efficiency sequence alignment algorithms. The bit-parallel approach represents individual cells in an alignment scoring matrix as bits in computer words and emulates the calculation of scores by a series of logic operations composed of AND, OR, XOR, complement, shift and addition. Bit-parallelism has been successfully applied to the longest common subsequence (LCS) and edit-distance problems, producing fast algorithms in practice. Results: We have developed BitPAl, a bit-parallel algorithm for general , integer-scoring global alignment. Integer-scoring schemes assign integer weights for match, mismatch and insertion/deletion. The BitPAl method uses structural properties in the relationship between adjacent scores in the scoring matrix to construct classes of efficient algorithms , each designed for a particular set of weights. In timed tests, we show that BitPAl runs 7\u201325 times faster than a standard iterative algorithm. Availability and implementation: Source code is freely available for download at http://lobstah.bu.edu/BitPAl/BitPAl.html. BitPAl is implemented in C and runs on all major operating systems.", 
  "summary": "The operations are: (i) finding the locations because of a preceding higher \"V value using AND of appropriate \"V; \"H pairs (which intersect along a common diagonal in the Function Table) and collecting them together with ORs; (ii) shifting the initial vectors right one position for subsequent calculations; (iii) carrying through runs of \"Hmin computed in two operations, an ADDITION (+) as before and an XOR with \"Hmin to complement the bits within the \"Hmin runs (Fig. 6).", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was supported by the National Science Foundation (IIS-1017621 to G.B., DGE-0654108 to J.L."
  ], 
  "acks": " ", 
  "authors": [
    " Joshua Loving", 
    " Yozen Hernandez", 
    " Gary Benson", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "scoring", 
      "algorithmics", 
      "values", 
      "sequencing", 
      "bits", 
      "matching"
    ]
  ], 
  "sourcelinks": [
    "http://lobstah.bu.edu/BitPAl/BitPAl.html"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-30T00:14:11Z"
}{
  "doi": "10.1093/bioinformatics/btu524", 
  "name": "BioTextQuest a knowledge integration platform for literature mining and concept discovery", 
  "links": [
    "http://bioinformatics.med", 
    "http://biocompendium.embl.de", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining BioTextQuest + : a knowledge integration platform for literature mining and concept discovery", 
  "toolName": "Data and text mining BioTextQuest + : a knowledge integration platform for literature mining and concept discovery", 
  "abstract": "The iterative process of finding relevant information in biomedical literature and performing bioinformatics analyses might result in an endless loop for an inexperienced user, considering the exponential growth of scientific corpora and the plethora of tools designed to mine PubMed V R and related biological databases. Herein, we describe BioTextQuest + , a web-based interactive knowledge exploration platform with significant advances to its predecessor (BioTextQuest), aiming to bridge processes such as bioentity recognition , functional annotation, document clustering and data integration towards literature mining and concept discovery. BioTextQuest + enables PubMed and OMIM querying, retrieval of abstracts related to a targeted request and optimal detection of genes, proteins, molecular functions, pathways and biological processes within the retrieved documents. The front-end interface facilitates the browsing of document clustering per subject, the analysis of term co-occurrence, the generation of tag clouds containing highly represented terms per cluster and at-a-glance popup windows with information about relevant genes and proteins. Moreover, to support experimental research, BioTextQuest + addresses integration of its primary functionality with biological repositories and software tools able to deliver further bio-informatics services. The Google-like interface extends beyond simple use by offering a range of advanced parameterization for expert users. We demonstrate the functionality of BioTextQuest + through several exemplary research scenarios including author disambiguation, functional term enrichment, knowledge acquisition and concept discovery linking major human diseases, such as obesity and ageing. Availability: The service is accessible at http://bioinformatics.med.", 
  "summary": "BioTextQuest+ provides a workflow to query PubMed and OMIM databases (Hamosh et al., 2005) and feed an automatic pipeline for document preprocessing, clustering, visualization and data integration with other repositories.\nThe core component of BioTextQuest+ is based on a number of former implementations (Iliopoulos et al., 2001; Papanikolaou et al., 2011) aiming to extract significant biomedical terms from an abstract collection and subsequently cluster these abstracts into subjects according to their similarity based on the extracted terms.\nWith PubMed and OMIM repositories as starting points, BioTextQuest+ currently offers automated literature extraction, identification of significant bioentity terms, term-based document clustering, co-occurrence analysis as well as integration with a rich collection of biological databases and automated bioinformatics analysis.", 
  "affiliations": [
    " Esch sur Alzette Luxembourg Centre for Systems Biomedicine (LCSB) University of Luxembourg ", 
    " Division of Basic Sciences Medical School University of Crete ", 
    " Institute of Marine Biology, Biotechnology and Aquaculture (IMBBC) Hellenic Centre for Marine Research (HCMR) ", 
    " Department of Biological Sciences Bioinformatics Research Laboratory University of Cyprus "
  ], 
  "grants": [
    "Funding: This work was supported by the European Commission FP7 programmes INFLA-CARE (EC grant agreement number 223151), `Translational Potential' (EC grant agreement number 285948) and MARBIGEN (EC grant agreement number 264089)."
  ], 
  "acks": " The authors wish to thank the four anonymous reviewers for invaluable comments. I.I. and V.P.S. would like to thank I. Manolakis for fruitful discussions. ", 
  "authors": [
    " Nikolas Papanikolaou", 
    " Georgios A Pavlopoulos", 
    " Evangelos Pafilis", 
    " Theodosios Theodosiou", 
    " Reinhard Schneider", 
    " Venkata P Satagopam", 
    " Christos A Ouzounis", 
    " Aristides G Eliopoulos", 
    " Vasilis J Promponas", 
    " Ioannis Iliopoulos", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "clustering", 
      "biotextquest", 
      "termed", 
      "genes", 
      "proteins", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-07T03:03:22Z"
}{
  "doi": "10.1093/bioinformatics/btu368", 
  "name": "Blue correcting sequencing errors using consensus and context", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bioinformatics.csiro.au/Blue"
  ], 
  "title": "Sequence analysis Blue: correcting sequencing errors using consensus and context", 
  "toolName": "Sequence analysis Blue: correcting sequencing errors using consensus and context", 
  "abstract": "Motivation: Bioinformatics tools, such as assemblers and aligners, are expected to produce more accurate results when given better quality sequence data as their starting point. This expectation has led to the development of stand-alone tools whose sole purpose is to detect and remove sequencing errors. A good error-correcting tool would be a transparent component in a bioinformatics pipeline, simply taking sequence data in any of the standard formats and producing a higher quality version of the same data containing far fewer errors. It should not only be able to correct all of the types of errors found in real sequence data (substitutions, insertions, deletions and uncalled bases), but it has to be both fast enough and scalable enough to be usable on the large datasets being produced by current sequencing technologies, and work on data derived from both haploid and diploid organisms. Results: This article presents Blue, an error-correction algorithm based on k-mer consensus and context. Blue can correct substitution, deletion and insertion errors, as well as uncalled bases. It accepts both FASTQ and FASTA formats, and corrects quality scores for corrected bases. Blue also maintains the pairing of reads, both within a file and between pairs of files, making it compatible with downstream tools that depend on read pairing. Blue is memory efficient, scalable and faster than other published tools, and usable on large sequencing datasets. On the tests undertaken, Blue also proved to be generally more accurate than other published algorithms, resulting in more accurately aligned reads and the assembly of longer contigs containing fewer errors. One significant feature of Blue is that its k-mer consensus table does not have to be derived from the set of reads being corrected. This decoupling makes it possible to correct one dataset, such as small set of 454 mate-pair reads, with the consensus derived from another dataset, such as Illumina reads derived from the same DNA sample. Such cross-correction can greatly improve the quality of small (and expensive) sets of long reads, leading to even better assemblies and higher quality finished genomes. Availability and implementation: The code for Blue and its related tools are available from http://www.bioinformatics.csiro.au/Blue. These programs are written in C# and run natively under Windows and under Mono on Linux.", 
  "summary": "The ability of Blue to `cross-correct' reads is demonstrated by correcting this 454 dataset with the k-mer consensus table generated from the ERA000206 Illumina dataset, using a `-min' parameter value derived from the Illumina data.\nBlue had to be sufficiently fast and memory-efficient to allow it to correct today's large datasets using reasonable resources, and effectively transparent so it could be used within existing analytical workflow tools such as Galaxy, just taking in a sequencing dataset and writing it out again after removing as many errors as possible while maintaining file formats, quality scores and read pairings.", 
  "affiliations": [
    " CSIRO Animal, Food and Health Sciences", 
    " CSIRO Computational Informatics"
  ], 
  "grants": [
    "Funding: This work was funded by the CSIRO Transformational Biology Capability Platform (TBCP).", 
    "The TBCP also provided funding for the Helicoverpa genome project."
  ], 
  "acks": " ", 
  "authors": [
    " Paul Greenfield", 
    " Konsta Duesing", 
    " Alexie Papanicolaou", 
    " Denis C Bauer"
  ], 
  "keyWords": [
    "blue correcting sequencing errors", 
    [
      "datasets", 
      "sequencers", 
      "genomes", 
      "correctable", 
      "reads", 
      "error"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bioinformatics.csiro.au/Blue"
  ], 
  "technologies": [], 
  "dateCreated": "2014-06-12T02:50:27Z"
}{
  "doi": "10.1093/bioinformatics/btu594", 
  "name": "CCharPPI web server computational characterization of proteinprotein interactions from structure", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://life.bsc.es/pid/ccharppi"
  ], 
  "title": "Structural bioinformatics CCharPPI web server: computational characterization of protein\u2013protein interactions from structure", 
  "toolName": "Structural bioinformatics CCharPPI web server: computational characterization of protein\u2013protein interactions from structure", 
  "abstract": "The atomic structures of protein\u2013protein interactions are central to understanding their role in biological systems, and a wide variety of biophysical functions and potentials have been developed for their characterization and the construction of predictive models. These tools are scattered across a multitude of stand-alone programs, and are often available only as model parameters requiring reimple-mentation. This acts as a significant barrier to their widespread adoption. CCharPPI integrates many of these tools into a single web server. It calculates up to 108 parameters, including models of electrostatics, desolvation and hydrogen bonding, as well as interface packing and complementarity scores, empirical potentials at various resolutions, docking potentials and composite scoring functions. Availability and implementation: The server does not require registration by the user and is freely available for non-commercial academic use at", 
  "summary": "Up to 108 intermolecular parameters are calculated for the input proteinprotein interface/s, including 43 potential functions, which have been reimplemented (Chuang et al., 2008; Feng et al., 2010; Lu et al., 2003; Liu and Vakser, 2011; Liu et al., 2004; Mintseris et al., 2007; Moal and Fernandez-Recio, 2013; Pokarowski et al., 2005; Rajgaria et al., 2008, 2006; Shen and Sali, 2006; Tobi, 2010; Tobi and Bahar, 2006), as well as terms calculated with 11 stand-alone programs (Feliu et al., 2011; Li and Liang, unpublished; Lu et al., 2008; Mitra and Pal, 2010; Pierce and Weng, 2007, 2008; Ravikant and Elber, 2010; Viswanath et al., 2013; Yang and Zhou, 2008a,b; Zhang and Zhang, 2010; Zhou and Skolnick, 2011) and 4 packages: FireDock (Andrusier et al., 2007), PyRosetta (Chaudhury et al., 2010), SIPPER (Pons et al., 2011) and PyDock (Cheng et al., 2007).", 
  "affiliations": [
    " Department of Life Sciences Joint BSC-IRB Research Programme in Computational Biology Barcelona Supercomputing Center "
  ], 
  "grants": [
    "Funding: The research leading to these results has received funding from the People Programme (Marie Curie Actions) of the European Unions Seventh Framework Programme (FP7/20072013) under REA grant agreement PIEF-GA-2012-327899 and grant BIO2013-48213-R from Spanish Ministry of Economy and Competitiveness."
  ], 
  "acks": " ", 
  "authors": [
    " Iain H Moal", 
    " Brian Jim Enez-Garc", 
    " Juan Fern Andez-Recio", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "interfaces", 
      "bioinformatics", 
      "proteins", 
      "modelled", 
      "potentials"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-03T06:17:49Z"
}{
  "doi": "10.1093/bioinformatics/btu500", 
  "name": "CCMpredfast and precise prediction of protein residueresidue contacts from correlated mutations", 
  "links": [
    "http://dictionary.cambridge.org/dictionary/british/speed-up", 
    "https://bitbucket.org/soedinglab/ccmpred", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "CCMpred\u2014fast and precise prediction of protein residue\u2013residue contacts from correlated mutations", 
  "toolName": "CCMpred\u2014fast and precise prediction of protein residue\u2013residue contacts from correlated mutations", 
  "abstract": "Motivation: Recent breakthroughs in protein residue\u2013residue contact prediction have made reliable de novo prediction of protein structures possible. The key was to apply statistical methods that can distinguish direct couplings between pairs of columns in a multiple sequence alignment from merely correlated pairs, i.e. to separate direct from indirect effects. Two classes of such methods exist, either relying on regularized inversion of the covariance matrix or on pseudo-likelihood maximization (PLM). Although PLM-based methods offer clearly higher precision, available tools are not sufficiently optimized and are written in interpreted languages that introduce additional overheads. This impedes the runtime and large-scale contact prediction for larger protein families, multi-domain proteins and protein\u2013protein interactions. Results: Here we introduce CCMpred, our performance-optimized PLM implementation in C and CUDA C. Using graphics cards in the price range of current six-core processors, CCMpred can predict contacts for typical alignments 35\u2013113 times faster and with the same precision as the most accurate published methods. For users without a CUDA-capable graphics card, CCMpred can also run in a CPU mode that is still 4\u201314 times faster. Thanks to our speed-ups (http://dictionary.cambridge.org/dictionary/british/speed-up) contacts for typical protein families can be predicted in 15\u201360 s on a consumer grade GPU and 1\u20136 min on a six-core CPU.", 
  "summary": "Modern contact prediction methods differ by their strategy in the disentangling step: the most accurate class of methods (Ekeberg et al., 2013; Kamisetty et al., 2013) such as plmDCA (Ekeberg et al., 2013) and GREMLIN (Kamisetty et al., 2013) learn the direct couplings as parameters of a Markov random field by maximizing its pseudo-likelihood, which has runtime complexity of O(NL2) where N is the number of homologous sequences in the MSA and L its number of columns.\nFor benchmarking the precision of contact prediction methods, we use the same set of 150 Pfam families with 1000 sequences and high-resolution structures (1.9 A) with identical input alignments as used in the PSICOV (Jones et al., 2012) method.", 
  "affiliations": [
    " Max Planck Institute for Biophysical Chemistry", 
    " Gene Center LMU Munich "
  ], 
  "grants": [
    "Funding: This work was funded by the Deutsche Forschungsgemeinschaft (Grants GRK1721 and SFB646) and the Bavarian Center for Molecular Biosystems (BioSysNet)."
  ], 
  "acks": " The authors would like to thank Markus Meier for the distribution over SCOP domain lengths and Jessica Andreani, Susann Vorberg, Markus and Armin Meier for helpful comments and discussions. ", 
  "authors": [
    " Stefan Seemayer", 
    " Markus Gruber", 
    " Johannes S \u20ac Oding", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "proteins", 
      "predictions", 
      "sequencing", 
      "ccmpred", 
      "alignments", 
      "contacting"
    ]
  ], 
  "sourcelinks": [
    "https://bitbucket.org/soedinglab/ccmpred"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btu420", 
  "name": "ccSOL omics a webserver for solubility prediction of endogenous and heterologous expression in Escherichia coli", 
  "links": [
    "http://creativecommons.org/licenses/by/3.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://s.tartaglialab.com", 
    "http://sbkb.org/tt", 
    "http://s.tartaglialab.com/page/ccsol_group"
  ], 
  "title": "Sequence analysis ccSOL omics: a webserver for solubility prediction of endogenous and heterologous expression in Escherichia coli", 
  "toolName": "Sequence analysis ccSOL omics: a webserver for solubility prediction of endogenous and heterologous expression in Escherichia coli", 
  "abstract": "Here we introduce ccSOL omics, a webserver for large-scale calculations of protein solubility. Our method allows (i) prote-ome-wide predictions; (ii) identification of soluble fragments within each sequences; (iii) exhaustive single-point mutation analysis. Results: Using coil/disorder, hydrophobicity, hydrophilicity,-sheet and-helix propensities, we built a predictor of protein solubility. Our approach shows an accuracy of 79% on the training set (36 990 Target Track entries). Validation on three independent sets indicates that ccSOL omics discriminates soluble and insoluble proteins with an accuracy of 74% on 31 760 proteins sharing 530% sequence similarity. Availability and implementation: ccSOL omics can be freely ac-cessed on the web at", 
  "summary": "In the past years, an in vitro reconstituted translation system allowed the large-scale investigation of Escherichia coli proteins solubility (Niwa et al., 2009), thus providing the opportunity for the development of predictive methods such as ccSOL (Agostini et al., 2012).\nFurthermore, we tested the algorithm on three independent datasets containing protein expression data [total of 31 760 entries taken from E.coli (Niwa et al., 2009), SOLpro (Magnan et al., 2009) and PROSO II (Smialowski et al., 2012)] and found 74% accuracy (Fig. 1D; see also Supplementary Material).", 
  "affiliations": [
    " Universitat Pompeu Fabra (UPF)", 
    " Gene Function and Evolution, Bioinformatics and Genomics Centre for Genomic Regulation (CRG) "
  ], 
  "grants": [
    "Funding: The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/20072013), through the European Research Council, under grant agreement RIBOMYLOME 309545, and from the Spanish Ministry of Economy and Competitiveness (SAF2011-\n\nDownloaded from http://bioinformatics.oxfordjournals.org/ at :: on August 8, 2016\n\nFig."
  ], 
  "acks": " ", 
  "authors": [
    " Federico Agostini", 
    " Davide Cirillo", 
    " Carmen Maria Livi", 
    " Riccardo Delli Ponti", 
    " Gian Gaetano Tartaglia", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "solubility prediction", 
    [
      "soluble", 
      "proteins", 
      "predictions", 
      "ccsol", 
      "bioinformatics", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-03T00:27:22Z"
}{
  "doi": "10.1093/bioinformatics/btu502", 
  "name": "CCBuilder an interactive webbased tool for building designing and assessing coiledcoil protein assemblies", 
  "links": [
    "http://coiledcoils.chm.bris.ac.uk/app/cc_builder", 
    "http://webglmol", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bioinf.org.uk/software/profit", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Structural bioinformatics CCBuilder: an interactive web-based tool for building, designing and assessing coiled-coil protein assemblies", 
  "toolName": "Structural bioinformatics CCBuilder: an interactive web-based tool for building, designing and assessing coiled-coil protein assemblies", 
  "abstract": "Motivation: The ability to accurately model protein structures at the atomistic level underpins efforts to understand protein folding, to engineer natural proteins predictably and to design proteins de novo. Homology-based methods are well established and produce impressive results. However, these are limited to structures presented by and resolved for natural proteins. Addressing this problem more widely and deriving truly ab initio models requires mathematical descriptions for protein folds; the means to decorate these with natural, engineered or de novo sequences; and methods to score the resulting models. Results: We present CCBuilder, a web-based application that tackles the problem for a defined but large class of protein structure, the-helical coiled coils. CCBuilder generates coiled-coil backbones, builds side chains onto these frameworks and provides a range of metrics to measure the quality of the models. Its straightforward graphical user interface provides broad functionality that allows users to build and assess models, in which helix geometry, coiled-coil architecture and topology and protein sequence can be varied rapidly. We demonstrate the utility of CCBuilder by assembling models for 653 coiled-coil structures from the PDB, which cover496% of the known coiled-coil types, and by generating models for rarer and de novo coiled-coil structures. Availability and implementation: CCBuilder is freely available, without registration, at", 
  "summary": "Although methods are available to model coiled coils (Crick, 1953a; Offer et al., 2002; Grigoryan and Degrado, 2011), these only generate the -helical backbones, which severely limits the modelling, prediction and design of coiled-coil structures.\nTo determine whether the backbone in the models is strained, the number of residues per -helical turn (n) was calculated, using Helanal (Bansal et al., 2000), for a set of 32 878, nonredundant, -helices selected from 2417 crystal structures with a resolution of better than 1.6 A and sequence identity 530%.\nDistribution of RMSD100 scores measured between the backbone atoms of models generated with CCBuilder and crystal structures of known coiled coils.", 
  "affiliations": [
    " School of Chemistry University of Bristol ", 
    " School of Biochemistry Medical Sciences Building University of Bristol University Walk "
  ], 
  "grants": [
    "thanks the EPSRC (EP/J001430/1), BBSRC (BB/ J008990/1; grant with RLB) and the ERC (340764) for financial support.", 
    "Funding: C.W.W."
  ], 
  "acks": " ", 
  "authors": [
    " Christopher W Wood", 
    " Marc Bruning", 
    " Amaurys A Ibarra", 
    " Gail J Bartlett", 
    " Andrew R Thomson", 
    " Richard B Sessions", 
    " R Leo Brady", 
    " Derek N Woolfson", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "coiled protein", 
    [
      "coils", 
      "helical", 
      "proteins", 
      "modelling", 
      "structures"
    ]
  ], 
  "sourcelinks": [
    "http://www.bioinf.org.uk/software/profit"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btu836", 
  "name": "CDvist a webserver for identification and visualization of conserved domains in protein sequences", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://cdvist.utk.edu"
  ], 
  "title": "CDvist: a webserver for identification and visualization of conserved domains in protein sequences", 
  "toolName": "CDvist: a webserver for identification and visualization of conserved domains in protein sequences", 
  "abstract": "Identification of domains in protein sequences allows their assigning to biological functions. Several webservers exist for identification of protein domains using similarity searches against various databases of protein domain models. However, none of them provides comprehensive domain coverage while allowing bulk querying and their visualization schemes can be improved. To address these issues, we developed CDvist (a comprehensive domain visualization tool), which combines the best available search algorithms and databases into a user-friendly framework. First, a given protein sequence is matched to domain models using high-specificity tools and only then unmatched segments are subjected to more sensitive algorithms resulting in a best possible comprehensive coverage. Bulk querying and rich visualization and download options provide improved functionality to domain architecture analysis. Availability and implementation: Freely available on the web at http://cdvist.", 
  "summary": "Searching tools such as RPS-BLAST (Marchler-Bauer et al., 2013), HMMER3 (Eddy, 2011) and HHpred/HHsearch (Hildebrand et al., 2009; Soding, 2005) are used to match sequences to domain models present in a given database.\nThe size of the protein sequence database grows dramatically, whereas its coverage by pre-computed domain models increases very slowly (Rekapalli et al., 2012).\nTo improve domain coverage, rather than using the entire sequence, CDvist iteratively identifies regions without significant domain match (orphan segments) and submits each one of them to similarity search against a user-determined sequence of databases until the entire protein sequence is covered or all databases have been searched (Fig. 1).", 
  "affiliations": [], 
  "grants": [
    "Funding\nNIH GM072285 (to I.B.Z.)."
  ], 
  "acks": " ", 
  "authors": [
    " Ogun Adebali", 
    " Davi R Ortega", 
    " Igor B Zhulin"
  ], 
  "keyWords": [
    "protein sequences", 
    [
      "sequence", 
      "proteins", 
      "searching", 
      "bioinformatics", 
      "domains", 
      "visualizations"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-12-20T04:13:19Z"
}{
  "doi": "10.1093/bioinformatics/btu605", 
  "name": "cddApp a Cytoscape app for accessing the NCBI conserved domain database", 
  "links": [
    "http://www.rbvi.ucsf.edu/cytoscape/cddApp/tutorial.shtml.4", 
    "http://www.rbvi.ucsf", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://github.com/RBVI/cddApp", 
    "http://apps"
  ], 
  "title": "Systems biology cddApp: a Cytoscape app for accessing the NCBI conserved domain database", 
  "toolName": "cddApp", 
  "abstract": "Motivation: cddApp is a Cytoscape extension that supports the annotation of protein networks with information about domains and specific functional sites from the National Center for Biotechnology Information's conserved domain database (CDD). CDD information is loaded for nodes annotated with NCBI numbers or UniProt identifiers and (optionally) Protein Data Bank structures. cddApp integrates with the Cytoscape apps structureViz2 and enhancedGraphics. Together, these three apps provide powerful tools to annotate nodes with CDD domain and site information and visualize that information in both network and structural contexts. Availability and implementation: cddApp is written in Java and freely available for download from the Cytoscape app store (http://apps. cytoscape.org). Documentation is provided at", 
  "summary": "ABSTRACT Motivation: cddApp is a Cytoscape extension that supports the annotation of protein networks with information about domains and specific functional sites from the National Center for Biotechnology Information's conserved domain database (CDD).\nThe National Center for Bioinformatics Information's (NCBI) conserved domain database (CDD) (Marchler-Bauer et al., 2011) is a repository of manually curated and computationally derived protein domain family models that are searchable through a Web interface or Web services.\nThe cddApp provides three main functions: using the CDD Web services to search for domains in proteins in the current network, visualizing the domain annotations and linking the domain annotations to the structure through a companion app.", 
  "affiliations": [
    " National Center for Biotechnology Information Associate Editor: Igor Jurisica National Library of Medicine National Institutes of Health ", 
    " Resource for Biocomputing, Visualization, and Informatics University of California "
  ], 
  "grants": [
    "Funding: This research was supported in part by the Intramural Research Program of the U.S. National Institutes of Health (NIH); National Library of Medicine; and by NIH National Institute of General Medical Science Grant [P41-GM103311]."
  ], 
  "sourcelinks": [
    "http://github.com/RBVI/cddApp", 
    "http://apps"
  ], 
  "acks": " ", 
  "authors": [
    " John H Morris", 
    " Allan Wu", 
    " Roxanne A Yamashita", 
    " Aron Marchler-Bauer", 
    " Thomas E Ferrin"
  ], 
  "keyWords": [
    "protein networks", 
    [
      "network", 
      "apps", 
      "proteins", 
      "cddapp", 
      "functionality", 
      "domains"
    ]
  ], 
  "github_data": {
    "name": "cddApp", 
    "contributors": [
      {
        "contributions": 38, 
        "html_url": "https://github.com/allanwu1986"
      }, 
      {
        "contributions": 23, 
        "html_url": "https://github.com/scootermorris"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/RBVI"
      }
    ], 
    "versions": [], 
    "created_at": "2013-07-09T16:46:11Z", 
    "updated_at": "2015-06-17T22:35:21Z", 
    "languages": [
      "Java"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/RBVI"
      }, 
      {
        "html_url": "https://github.com/allanwu1986"
      }, 
      {
        "html_url": "https://github.com/scootermorris"
      }
    ], 
    "owner": "https://github.com/RBVI", 
    "homepage": null
  }, 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-09-12T02:13:15Z"
}{
  "doi": "10.1093/bioinformatics/btu662", 
  "name": "Chimera a Bioconductor package for secondary analysis of fusion products", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Sequence analysis Chimera: a Bioconductor package for secondary analysis of fusion products", 
  "toolName": "Sequence analysis Chimera: a Bioconductor package for secondary analysis of fusion products", 
  "abstract": "Chimera is a Bioconductor package that organizes, annotates , analyses and validates fusions reported by different fusion detection tools; current implementation can deal with output from bellerophontes, chimeraScan, deFuse, fusionCatcher, FusionFinder, FusionHunter, FusionMap, mapSplice, Rsubread, tophat-fusion and STAR. The core of Chimera is a fusion data structure that can store fusion events detected with any of the aforementioned tools. Fusions are then easily manipulated with standard R functions or through the set of functionalities specifically developed in Chimera with the aim of supporting the user in managing fusions and discriminating false-positive results. Availability and implementation: Chimera is implemented as a Bioconductor package in R. The package and the vignette can be", 
  "summary": "As each fusion detection tool might rely on different gene annotations, the import function recovers the HUGO (Seal et al., 2011) symbols for the genes involved in the fusion, by overlapping the fusion break points to the genes' genomic location stored in the Bioconductor package (Gentleman et al., 2004) org.Hs.eg.db, which contains a genome-wide annotation based on Entrez Gene identifiers.\nThe supporting reads of a fusion (whether made available by the fusion detection tool or through the Chimera annotation functions) are assembled with GapFiller into a reference.", 
  "affiliations": [
    " Department of Mathematics and Computer Science University of Udine ", 
    " Department of Molecular Biotechnology and Health Sciences University of Torino ", 
    " Department of Computational and Quantitative Biology UMR 7238 CNRS -Universit e Pierre et Marie Curie ", 
    " Department of Computer Sciences University of Torino "
  ], 
  "grants": [
    "Funding: This work was supported by the Epigenomics Flagship Project EPIGEN and the European 7th frame-work program, Health.2012.1.2-1, NGS-PTL grant n. 306242."
  ], 
  "acks": " ", 
  "authors": [
    " Marco Beccuti", 
    " Matteo Carrara", 
    " Francesca Cordero", 
    " Fulvio Lazzarato", 
    " Susanna Donatelli", 
    " Francesca Nadalin", 
    " Alberto Policriti", 
    " Raffaele A Calogero"
  ], 
  "keyWords": [
    [
      "chimeras", 
      "functionalities", 
      "fusions", 
      "bioinformatics", 
      "genomic", 
      "tools", 
      "annotations"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-07T04:41:50Z"
}{
  "doi": "10.1093/bioinformatics/btv015", 
  "name": "CellCODE a robust latent variable approach to differential expression analysis for heterogeneous cell populations", 
  "links": [
    "http://www.broadinstitute.org/dmap/home", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.pitt"
  ], 
  "title": "CellCODE: a robust latent variable approach to differential expression analysis for heterogeneous cell populations", 
  "toolName": "CellCODE: a robust latent variable approach to differential expression analysis for heterogeneous cell populations", 
  "abstract": "Motivation: Identifying alterations in gene expression associated with different clinical states is important for the study of human biology. However, clinical samples used in gene expression studies are often derived from heterogeneous mixtures with variable cell-type composition, complicating statistical analysis. Considerable effort has been devoted to modeling sample heterogeneity, and presently, there are many methods that can estimate cell proportions or pure cell-type expression from mixture data. However, there is no method that comprehensively addresses mixture analysis in the context of differential expression without relying on additional proportion information, which can be inaccurate and is frequently unavailable. Results: In this study, we consider a clinically relevant situation where neither accurate proportion estimates nor pure cell expression is of direct interest, but where we are rather interested in detecting and interpreting relevant differential expression in mixture samples. We develop a method, Cell-type COmputational Differential Estimation (CellCODE), that addresses the specific statistical question directly, without requiring a physical model for mixture components. Our approach is based on latent variable analysis and is computationally transparent; it requires no additional experimental data, yet outperforms existing methods that use independent proportion measurements. CellCODE has few parameters that are robust and easy to interpret. The method can be used to track changes in proportion, improve power to detect differential expression and assign the differentially expressed genes to the correct cell type. Availability and implementation: The CellCODE R package can be downloaded at", 
  "summary": "The method can be used to track changes in proportion, improve power to detect differential expression and assign the differentially expressed genes to the correct cell type.\nIdeally, SPVs, which are eigengene-based summaries of cell-type marker genes, should not just correlate with true proportions, but should also be consistent, i.e. produce the same values for samples with the same cell-type proportions, independently of other sources of expression variation.\nWe simulate two clinical groups, which differ in their proportion distributions and in marker gene expression at the individual cell-type level.", 
  "affiliations": [
    " Department of Neurology Icahn School of Medicine at Mount Sinai ", 
    " Department of Computational and Systems Biology University of Pittsburgh "
  ], 
  "grants": [
    "Funding\nThis work was supported by NIH Contract HHSN272201000054C and NIH award U54HG008540."
  ], 
  "acks": " ", 
  "authors": [
    " Maria Chikina", 
    " Elena Zaslavsky", 
    " Stuart C Sealfon"
  ], 
  "keyWords": [
    "gene expression", 
    "expressed genes", 
    [
      "proportional", 
      "cellcode", 
      "express", 
      "types", 
      "markers"
    ]
  ], 
  "sourcelinks": [
    "http://www.broadinstitute.org/dmap/home", 
    "http://www.pitt"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2015-01-13T01:15:54Z"
}{
  "doi": "10.1093/bioinformatics/btu575", 
  "name": "Cell population identification using fluorescenceminusone controls with a oneclass classifying algorithm", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://fccf.mpiib-berlin.mpg.de/daten/drfz/bioinformatics/with"
  ], 
  "title": "Data and text mining Cell population identification using fluorescence-minus-one controls with a one-class classifying algorithm", 
  "toolName": "Data and text mining Cell population identification using fluorescence-minus-one controls with a one-class classifying algorithm", 
  "abstract": "Motivation: The tried and true approach of flow cytometry data analysis is to manually gate on each biomarker separately, which is feasible for a small number of biomarkers, e.g. less than five. However, this rapidly becomes confusing as the number of biomarker increases. Furthermore, multivariate structure is not taken into account. Recently, automated gating algorithms have been implemented, all of which rely on unsupervised learning methodology. However, all unsupervised learning outputs suffer the same difficulties in validation in the absence of external knowledge, regardless of application domain. Results: We present a new semi-automated algorithm for population discovery that is based on comparison to fluorescence-minus-one controls, thus transferring the problem into that of one-class classification , as opposed to being an unsupervised learning problem. The novel one-class classification algorithm is based on common principal components and can accommodate complex mixtures of multivariate densities. Computational time is short, and the simple nature of the calculations means the algorithm can easily be adapted to process large numbers of cells (10 6). Furthermore, we are able to find rare cell populations as well as populations with low biomarker concentration, both of which are inherently hard to do in an unsupervised learning context without prior knowledge of the samples' composition. Availability and implementation: R scripts are available via", 
  "summary": "ABSTRACT Motivation: The tried and true approach of flow cytometry data analysis is to manually gate on each biomarker separately, which is feasible for a small number of biomarkers, e.g. less than five.\nGiven that the cells are correlated, combining the output from multiple comparisons strengthens this (e.g. in the case when a population is positive for two BMs but only weakly so for one).\nThe proposed algorithm requires no tuning of parameters, and each full staining-FMO comparison (CPC proj poly) is linear in the number of cells, and easily parallelizable.\n(2011) Rapid cell population identification in flow cytometry data.", 
  "affiliations": [
    " Deutsches Rheuma-Forschungszentrum Associate Editor: Igor Jurisica "
  ], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " Kristen Feher", 
    " Jenny Kirsch", 
    " Andreas Radbruch", 
    " Hyun-Dong Chang", 
    " Toralf Kaiser"
  ], 
  "keyWords": [
    "cell population", 
    [
      "clustering", 
      "cells", 
      "data", 
      "biomarkers", 
      "populations"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-29T00:22:23Z"
}{
  "doi": "10.1093/bioinformatics/btu505", 
  "name": "Circleator flexible circular visualization of genomeassociated data with BioPerl and SVG", 
  "links": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://clovr.org", 
    "http://jonathancrabtree.github.io"
  ], 
  "title": "Genome analysis Circleator: flexible circular visualization of genome-associated data with BioPerl and SVG", 
  "toolName": "jonathancrabtree.github.io", 
  "abstract": "Circleator is a Perl application that generates circular figures of genome-associated data. It leverages BioPerl to support standard annotation and sequence file formats and produces publication quality SVG output. It is designed to be both flexible and easy to use. It includes a library of circular track types and predefined configuration files for common use-cases, including. (i) visualizing gene annotation and DNA sequence data from a GenBank flat file, (ii) displaying patterns of gene conservation in related microbial strains, (iii) showing Single Nucleotide Polymorphisms (SNPs) and indels relative to a reference genome and gene set and (iv) viewing RNA-Seq plots. Availability and implementation: Circleator is freely available under the Artistic License 2.0 from http://jonathancrabtree.github.io/ Circleator/ and is integrated with the CloVR cloud-based sequence analysis Virtual Machine (VM), which can be downloaded from", 
  "summary": "Circleator accepts reference sequence(s) and annotation in any BioPerlsupported format, including GenBank format; Sequence Alignment/Map and BGZF-compressed SAM (SAM/BAM) alignment files; output from Cufflinks (Trapnell et al., 2010), Tandem Repeats Finder (TRF) (Benson, 1999) and the BLAST Score Ratio (Rasko et al., 2005) utility; SNPs in Variant Call Format (VCF) and tab-delimited quantitative data, such as gene expression data.\nThe configuration file supports loops, which allow the same set of tracks to be displayed for 80 genomes in a SNP comparison without repeating everything 80 times; pseudo-tracks, which do not appear in the figure but can load data or perform data transformations (e.g. the compute-deserts track, which identifies all regions of a specified length that do not contain any features of a specified", 
  "affiliations": [
    " Institute for Genome Sciences University of Maryland School of Medicine "
  ], 
  "grants": [
    "Some combine analysis and visualization: BRIG (Alikhan et al., 2011) incorporates a BLAST-based prokaryotic genome comparison algorithm, and CGView Server (Grant and Stothard, 2008) is a CGView-based (Stothard and Wishart, 2005) web service that runs on-the-fly BLAST comparisons.", 
    "Grant,J.R.", 
    "Funding: This project has been funded by the National Institute of Allergy and Infectious Diseases, National Institutes of Health, Department of Health and Human Services under contract number HHSN272200900009C and grant U19AI110820."
  ], 
  "acks": " ", 
  "authors": [
    " Jonathan Crabtree", 
    " Sonia Agrawal", 
    " Anup Mahurkar", 
    " Garry S Myers", 
    " David A Rasko", 
    " Owen White", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "genomics", 
      "tracks", 
      "data", 
      "circleator", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://clovr.org", 
    "http://jonathancrabtree.github.io"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-30T00:14:11Z"
}{
  "doi": "10.1093/bioinformatics/btu794", 
  "name": "Clusteringbased model of cysteine coevolution improves disulfide bond connectivity prediction and reduces homologous sequence requirements", 
  "links": [
    "http://ibsquare.be/sephiroth", 
    "http://dislocate.biocomp", 
    "http://ibsquare.be/sephiroth.2.4", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Clustering-based model of cysteine co-evolution improves disulfide bond connectivity prediction and reduces homologous sequence requirements", 
  "toolName": "Clustering-based model of cysteine co-evolution improves disulfide bond connectivity prediction and reduces homologous sequence requirements", 
  "abstract": "Motivation: Cysteine residues have particular structural and functional relevance in proteins because of their ability to form covalent disulfide bonds. Bioinformatics tools that can accurately predict cysteine bonding states are already available, whereas it remains challenging to infer the disulfide connectivity pattern of unknown protein sequences. Improving accuracy in this area is highly relevant for the structural and functional annotation of proteins. Results: We predict the intra-chain disulfide bond connectivity patterns starting from known cysteine bonding states with an evolutionary-based unsupervised approach called Sephiroth that relies on high-quality alignments obtained with HHblits and is based on a coarse-grained cluster-based mode-lization of tandem cysteine mutations within a protein family. We compared our method with state-of-the-art unsupervised predictors and achieve a performance improvement of 25\u201327% while requiring an order of magnitude less of aligned homologous sequences ($10 3 instead of $10 4). Availability and implementation: The software described in this article and the datasets used are available at http://ibsquare.be/sephiroth.", 
  "summary": "Results: We predict the intra-chain disulfide bond connectivity patterns starting from known cysteine bonding states with an evolutionary-based unsupervised approach called Sephiroth that relies on high-quality alignments obtained with HHblits and is based on a coarse-grained cluster-based modelization of tandem cysteine mutations within a protein family.\nDisulfide connectivities have been predicted from MSAs this way using correlated mutation analysis methods (Rubinstein and Fiser 2008) and using a mutual information (MI) approach combined with a sparse inverse covariance estimation method (Savojardo et al., 2013) derived from the CP tool PSICOV (Jones et al., 2012), so improving the performances of their full annotation predictor (Savojardo et al., 2011).", 
  "affiliations": [], 
  "grants": [
    "and by the Brussels Institute for Research and Innovation (Innoviris) (grant BB2B 2010-1-12 to W.F.V.).", 
    "Funding\nThis work was funded by a Ph.D. grant of the Agency for Innovation by Science and Technology (IWT) (to D.R.)"
  ], 
  "acks": " ", 
  "authors": [
    " Daniele Raimondi", 
    " Gabriele Orlando", 
    " Wim F Vranken"
  ], 
  "keyWords": [
    [
      "methods", 
      "proteins", 
      "predictions", 
      "bioinformatics", 
      "sequencing", 
      "structures"
    ]
  ], 
  "sourcelinks": [
    "http://ibsquare.be/sephiroth"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-10T01:14:59Z"
}{
  "doi": "10.1093/bioinformatics/btv003", 
  "name": "Clonality inference in multiple tumor samples using phylogeny", 
  "links": [
    "http://sourceforge.net/projects/citup", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Clonality inference in multiple tumor samples using phylogeny", 
  "toolName": "citup", 
  "abstract": "Motivation: Intra-tumor heterogeneity presents itself through the evolution of subclones during cancer progression. Although recent research suggests that this heterogeneity has clinical implications , in silico determination of the clonal subpopulations remains a challenge. Results: We address this problem through a novel combinatorial method, named clonality inference in tumors using phylogeny (CITUP), that infers clonal populations and their frequencies while satisfying phylogenetic constraints and is able to exploit data from multiple samples. Using simulated datasets and deep sequencing data from two cancer studies, we show that CITUP predicts clonal frequencies and the underlying phylogeny with high accuracy. Availability and implementation: CITUP is freely available at: http://sourceforge.net/projects/citup/.", 
  "summary": "CITUP solves the mutation phylogeny problem by iterating through all tree topologies up to a fixed number of nodes Nmax , and solving the mutation assignment problem for each tree:\nTo ensure a reasonable running time for the QIP approach on larger (>20 mutations) problem sizes, we first cluster the mutations into N sets by their mutation frequency, where N is the number of nodes in the current tree topology.\nNote that since CITUP does not explicitly predict tumor purity, for each sample this value is estimated as 1:0  ars, where ars is the predicted subclonal frequency of the root node in that sample if the root node is not assigned any mutations.", 
  "affiliations": [
    " BC Cancer Agency", 
    " Vancouver Prostate Center", 
    " School of Computing Science Simon Fraser University "
  ], 
  "grants": [
    "Funding\nThis work is funded by Genome Canada (BCB/SIP 176ISO) and National Science and Engineering Research Council (NSERC) Discovery (298339) grants to CSS, and NSERC CREATE (139277) fellowship to S.M."
  ], 
  "acks": " We thank Andrew Roth from BC Cancer Agency for helpful discussions and our reviewers for their suggestions and comments. This work is funded by Genome Canada (BCB/SIP 176ISO) and National Science and Engineering Research Council (NSERC) Discovery (298339) grants to CSS, and NSERC CREATE (139277) fellowship to S.M. Conflict of Interest: none declared. ", 
  "authors": [
    " Salem Malikic", 
    " Andrew W Mcpherson", 
    " Nilgun Donmez", 
    " Cenk S Sahinalp"
  ], 
  "keyWords": [
    [
      "mutational", 
      "tumors", 
      "trees", 
      "subclones", 
      "citup", 
      "clonality"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net/projects/citup"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-08T02:10:15Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/citup/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/smalikic/", 
        "name": "Salem Malikic"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu714", 
  "name": "Characterization of structural variants with single molecule and hybrid sequencing approaches", 
  "links": [
    "http://blog.pacificbiosciences.com/2013/10/data-release-long-read-shotgun.html.Peters,B.A", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/PacificBiosciences/blasr/tree"
  ], 
  "title": "Genome analysis Characterization of structural variants with single molecule and hybrid sequencing approaches", 
  "toolName": "tree", 
  "abstract": "Motivation: Structural variation is common in human and cancer gen-omes. High-throughput DNA sequencing has enabled genome-scale surveys of structural variation. However, the short reads produced by these technologies limit the study of complex variants, particularly those involving repetitive regions. Recent 'third-generation' sequen-cing technologies provide single-molecule templates and longer sequencing reads, but at the cost of higher per-nucleotide error rates. Results: We present MultiBreak-SV, an algorithm to detect structural variants (SVs) from single molecule sequencing data, paired read sequencing data, or a combination of sequencing data from different platforms. We demonstrate that combining low-coverage third-generation data from Pacific Biosciences (PacBio) with high-coverage paired read data is advantageous on simulated chromosomes. We apply MultiBreak-SV to PacBio data from four human fosmids and show that it detects known SVs with high sensitivity and specificity. Finally, we perform a whole-genome analysis on PacBio data from a complete hydatidiform mole cell line and predict 1002 high-probability SVs, over half of which are confirmed by an Illumina-based assembly. Availability and implementation: MultiBreak-SV is available at", 
  "summary": "Methods that incorporate ambiguous alignments for paired reads improved SV detection (Hormozdiari et al., 2009, 2010; Lee et al., 2008; Quinlan et al., 2010); however, these methods report a single set of predictions that usually involves minimizing the total number of SVs. More recently, GASVPro (Sindi et al., 2012) demonstrated the effectiveness of a probabilistic model that considers many possible read mappings and incorporates both a paired-end and a read-depth signal to refine variant predictions.\n3.3.2 Novel adjacencies predicted by MultiBreakSV MultiBreak-SV returns the posterior probability of every multi-breakpoint-mapping; we use these probabilities to compute the probability than an adjacency is supported by k or more multi-breakpoint-mappings (see Methods and Supplementary Section 1.6).", 
  "affiliations": [
    " Pacific Biosciences", 
    " Department of Computer Science Brown University ", 
    " School of Natural Sciences University of California ", 
    " Center for Computational Molecular Biology Brown University "
  ], 
  "grants": [
    "Funding: This work was supported by grant R01HG5690 from the National Institutes of Health and a National Science Foundation CAREER Award CCF-1053753 to BJR."
  ], 
  "acks": " We thank Mark Chaisson at PacBio for his insight with running and tuning BLASR for long read alignments in the early stage of this work. ", 
  "authors": [
    " Anna Ritz", 
    " Ali Bashir", 
    " Suzanne Sindi", 
    " David Hsu", 
    " Iman Hajirasouliha", 
    " Benjamin J Raphael", 
    " "
  ], 
  "keyWords": [
    [
      "deletions", 
      "genomics", 
      "mappings", 
      "maps", 
      "reads", 
      "sequencing", 
      "alignments"
    ]
  ], 
  "sourcelinks": [
    "https://github.com/PacificBiosciences/blasr/tree"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-30T05:01:31Z"
}{
  "doi": "10.1093/bioinformatics/btu402", 
  "name": "CNVguided multiread allocation for ChIPseq", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.stat.wisc"
  ], 
  "title": "Genome analysis CNV-guided multi-read allocation for ChIP-seq", 
  "toolName": "Genome analysis CNV-guided multi-read allocation for ChIP-seq", 
  "abstract": "Motivation: In chromatin immunoprecipitation followed by high-throughput sequencing (ChIP-seq) and other short-read sequencing experiments, a considerable fraction of the short reads align to multiple locations on the reference genome (multi-reads). Inferring the origin of multi-reads is critical for accurately mapping reads to repetitive regions. Current state-of-the-art multi-read allocation algorithms rely on the read counts in the local neighborhood of the alignment locations and ignore the variation in the copy numbers of these regions. Copy-number variation (CNV) can directly affect the read densities and, therefore, bias allocation of multi-reads. Results: We propose cnvCSEM (CNV-guided ChIP-Seq by expectation -maximization algorithm), a flexible framework that incorporates CNV in multi-read allocation. cnvCSEM eliminates the CNV bias in multi-read allocation by initializing the read allocation algorithm with CNV-aware initial values. Our data-driven simulations illustrate that cnvCSEM leads to higher read coverage with satisfactory accuracy and lower loss in read-depth recovery (estimation). We evaluate the biological relevance of the cnvCSEM-allocated reads and the resultant peaks with the analysis of several ENCODE ChIP-seq datasets.", 
  "summary": "We first simulated short reads from a repetitive sequence-enriched segment of the human genome with synthetic binding events and copy numbers, and then compared the three read allocation methods (Uni, CSEM and cnvCSEM) in terms of coverage, read allocation accuracy and the read-depth recovery.\n(a) Simulated binding signal; (b) simulated CNVs; (c) convolution of the binding signal and CNV information; (d) true read depth; (e) simulated sample read depth; (f) read-depth estimates by Uni; (g) read-depth estimates by CSEM; (h) read-depth estimates by cnvCSEM; (i) segmental duplication annotation, where the two solid lines and the two dashed lines represent the two pairs of regions with high sequence similarity", 
  "affiliations": [
    " Department of Biostatistics and Medical Informatics"
  ], 
  "grants": [
    "Funding: This work was supported by National Institutes of Health Grants (HG007019 and HG003747 to S.K.)."
  ], 
  "acks": " ", 
  "authors": [
    " Qi Zhang", 
    " S \u20ac Und \u20ac Uz Keles\u00b81keles\u00b81"
  ], 
  "keyWords": [
    [
      "regions", 
      "genomics", 
      "reads", 
      "cnvcsem", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-06-26T04:26:56Z"
}{
  "doi": "10.1093/bioinformatics/btu845", 
  "name": "Coexpression analysis of highthroughput transcriptome sequencing data with Poisson mixture models", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Co-expression analysis of high-throughput transcriptome sequencing data with Poisson mixture models", 
  "toolName": "Co-expression analysis of high-throughput transcriptome sequencing data with Poisson mixture models", 
  "abstract": "Motivation: In recent years, gene expression studies have increasingly made use of high-through-put sequencing technology. In turn, research concerning the appropriate statistical methods for the analysis of digital gene expression (DGE) has flourished, primarily in the context of normalization and differential analysis. Results: In this work, we focus on the question of clustering DGE profiles as a means to discover groups of co-expressed genes. We propose a Poisson mixture model using a rigorous framework for parameter estimation as well as the choice of the appropriate number of clusters. We illustrate co-expression analyses using our approach on two real RNA-seq datasets. A set of simulation studies also compares the performance of the proposed model with that of several related approaches developed to cluster RNA-seq or serial analysis of gene expression data. Availability and and implementation: The proposed method is implemented in the open-source R package HTSCluster, available on CRAN.", 
  "summary": "PoisL: Originally proposed for SAGE data, the PoisL approach (Cai et al., 2004) assumes that, given the cluster k, genes follow a PoissXon distribution with mean lijk  wikjk, under the constraint that j kjk  1 for all k; the existence of replicates within each condition is not taken into account in the original method.\nThe model is parameterized to account for several characteristics of RNA-seq data, including: (i) a set of normalization factors (sjl) to account for systematic differences in library size among biological replicates, (ii) a per-gene offset parameter (wi) to account for differences among genes due to overall expression level and (iii) a condition-specific cluster effect (kjk).", 
  "affiliations": [
    " Inria Saclay -I \u02c6 le-de-France", 
    " Institut de Math\u00e9 matiques de Toulouse INSA de Toulouse Universit\u00e9 de Toulouse "
  ], 
  "grants": [
    "Funding\nA.R.", 
    "The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR), under grant MixStatSeq (ANR-13-JS01-0001-01)."
  ], 
  "acks": " The authors thank the members of the Statistics for Systems Biology (SSB) working group for their helpful and insightful comments, as well as the three anonymous reviewers for their helpful comments. The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR), under grant MixStatSeq (ANR-13-JS01-0001-01). ", 
  "authors": [
    " Andrea Rau", 
    " Cathy Maugis-Rabusseau", 
    " Marie-Laure Martin-Magniette", 
    " Gilles Celeux"
  ], 
  "keyWords": [
    "gene expression", 
    [
      "clustering", 
      "genes", 
      "data", 
      "expressed", 
      "modeled"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2015-01-07T03:18:32Z"
}{
  "doi": "10.1093/bioinformatics/btu863", 
  "name": "Combining treebased and dynamical systems for the inference of gene regulatory networks", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.gene-regulation", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Combining tree-based and dynamical systems for the inference of gene regulatory networks", 
  "toolName": "Combining tree-based and dynamical systems for the inference of gene regulatory networks", 
  "abstract": "Motivation: Reconstructing the topology of gene regulatory networks (GRNs) from time series of gene expression data remains an important open problem in computational systems biology. Existing GRN inference algorithms face one of two limitations: model-free methods are scalable but suffer from a lack of interpretability and cannot in general be used for out of sample predictions. On the other hand, model-based methods focus on identifying a dynamical model of the system. These are clearly interpretable and can be used for predictions; however, they rely on strong assumptions and are typically very demanding computationally. Results: Here, we propose a new hybrid approach for GRN inference, called Jump3, exploiting time series of expression data. Jump3 is based on a formal on/off model of gene expression but uses a non-parametric procedure based on decision trees (called 'jump trees') to reconstruct the GRN topology, allowing the inference of networks of hundreds of genes. We show the good performance of Jump3 on in silico and synthetic networks and applied the approach to identify regulatory interactions activated in the presence of interferon gamma. Availability and implementation: Our MATLAB implementation of Jump3 is available at", 
  "summary": "Jump3 is based on a formal on/off model of gene expression but uses a non-parametric procedure based on decision trees (called `jump trees') to reconstruct the GRN topology, allowing the inference of networks of hundreds of genes.\nThe basic idea of our GRN inference procedure is to learn for each target gene i a model fi in the form of a decision tree (or an ensemble of decision trees), which predicts the promoter state li at any time t from the expression levels of the candidate regulators at the same time t.", 
  "affiliations": [
    " School of Informatics University of Edinburgh "
  ], 
  "grants": [
    "Funding\nThis work was supported by the European Research Council [grant number MLCS306999].", 
    "The widely used TRANSFAC database (http://www.gene-regulation."
  ], 
  "acks": " We thank Peter Ghazal and Thorsten Forster for useful discussions on interferon gamma biology. This work was supported by the European Research Council ", 
  "authors": [
    " V\u00e2 ", 
    " Anh Huynh-Thu", 
    " Guido Sanguinetti"
  ], 
  "keyWords": [
    "model methods", 
    "method modelled", 
    [
      "jump", 
      "biologically", 
      "trees", 
      "modelling", 
      "networks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-09T02:26:44Z"
}{
  "doi": "10.1093/bioinformatics/btu493", 
  "name": "Comparison of the mammalian insulin signalling pathway to invertebrates in the context of FOXOmediated ageing", 
  "links": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://phylomedb.org/).The", 
    "http://www.ensembl.org", 
    "http://www.r-project.org", 
    "http://cegg.unige.ch/orthodb7", 
    "http://www.ebi.ac.uk/thornton-srv", 
    "http://www.ncbi", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.yworks.com", 
    "http://www.treefam.org"
  ], 
  "title": "BIOINFORMATICS DISCOVERY NOTE Systems biology Comparison of the mammalian insulin signalling pathway to invertebrates in the context of FOXO-mediated ageing", 
  "toolName": "BIOINFORMATICS DISCOVERY NOTE Systems biology Comparison of the mammalian insulin signalling pathway to invertebrates in the context of FOXO-mediated ageing", 
  "abstract": "Motivation: A large number of experimental studies on ageing focus on the effects of genetic perturbations of the insulin/insulin-like growth factor signalling pathway (IIS) on lifespan. Short-lived invertebrate laboratory model organisms are extensively used to quickly identify ageing-related genes and pathways. It is important to extrapolate this knowledge to longer lived mammalian organisms, such as mouse and eventually human, where such analyses are difficult or impossible to perform. Computational tools are needed to integrate and manipulate pathway knowledge in different species. Results: We performed a literature review and curation of the IIS and target of rapamycin signalling pathways in Mus Musculus. We compare this pathway model to the equivalent models in Drosophila mel-anogaster and Caenorhabtitis elegans. Although generally well-conserved, they exhibit important differences. In general, the worm and mouse pathways include a larger number of feedback loops and interactions than the fly. We identify 'functional orthologues' that share similar molecular interactions, but have moderate sequence similarity. Finally, we incorporate the mouse model into the web-service NetEffects and perform in silico gene perturbations of IIS components and analyses of experimental results. We identify sub-paths that, given a mutation in an IIS component, could potentially antagonize the primary effects on ageing via FOXO in mouse and via SKN-1 in worm. Finally, we explore the effects of FOXO knockouts in three different mouse tissues. Availability and implementation: http://www.ebi.ac.uk/thornton-srv/ software/NetEffects", 
  "summary": "ABSTRACT Motivation: A large number of experimental studies on ageing focus on the effects of genetic perturbations of the insulin/insulin-like growth factor signalling pathway (IIS) on lifespan.\nPreviously, we developed the web-service NetEffects to solve the problem of relating gene expression results to their effects at the protein signalling level of the IIS pathway and the ageing phenotype (Papatheodorou et al., 2012).\nPrevious work on mouse mutants of the IIS and TOR signalling pathways have clearly shown a role for the insulin pathway in the regulation of lifespan, with null S6K (Selman et al., 2009) mice and null IRS-1 (Selman et al., 2008) mice showing significant lifespan extension when compared with wild-type.", 
  "affiliations": [
    " EMBL-European Bioinformatics Institute Wellcome Trust Genome Campus "
  ], 
  "grants": [
    "Funding: This work was funded by the Wellcome Trust Strategic Award WT081394MA (I.P., J.M.T.)"
  ], 
  "acks": " ", 
  "authors": [
    " Irene Papatheodorou", 
    " Rudolfs Petrovs", 
    " Janet M Thornton"
  ], 
  "keyWords": [
    "foxo ageing", 
    "signalling pathway", 
    [
      "aging", 
      "cells", 
      "genes", 
      "foxos", 
      "pathways", 
      "signaling"
    ]
  ], 
  "sourcelinks": [
    "http://www.ebi.ac.uk/thornton-srv", 
    "http://www.ensembl.org", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btu656", 
  "name": "CompMap a referencebased compression program to speed up read mapping to related reference sequences", 
  "links": [
    "http://csse.szu.edu.cn/staff/zhuzx/CompMap", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.novocraft.com"
  ], 
  "title": "Sequence analysis CompMap: a reference-based compression program to speed up read mapping to related reference sequences", 
  "toolName": "Sequence analysis CompMap: a reference-based compression program to speed up read mapping to related reference sequences", 
  "abstract": "Exhaustive mapping of next-generation sequencing data to a set of relevant reference sequences becomes an important task in pathogen discovery and metagenomic classification. However, the runtime and memory usage increase as the number of reference sequences and the repeat content among these sequences increase. In many applications, read mapping time dominates the entire application. We developed CompMap, a reference-based compression program, to speed up this process. CompMap enables the generation of a non-redundant representative sequence for the input sequences. We have demonstrated that reads can be mapped to this representative sequence with a much reduced time and memory usage, and the mapping to the original reference sequences can be recovered with high accuracy. Availability and implementation: CompMap is implemented in C and freely available at", 
  "summary": "In pathogen discovery (Kostic et al., 2011) or metagenomic classification applications (Francis et al., 2013; Lindner and Renard, 2013), curating databases that correspond to a certain taxonomic rank (e.g. species or genus) and assign next-generation sequencing (NGS) reads to these taxa are fundamental tasks.\nCompMap is a reference-based compression program designed to reduce redundancies in a given set of related reference sequences, either heterologous or homogenous, by identifying, recording and eliminating repetitive subsequences.\nIn addition to that, when reference sequences are highly similar, existing aligners like BWA may not be able to identify exhaustive read mapping locations (Supplementary Materials).", 
  "affiliations": [
    " College of Computer Science and Software Engineering Shenzhen University ", 
    " Infectious Disease Initiative The Broad Institute "
  ], 
  "grants": [
    "Funding: National Natural Science Foundation of China (61471246 and 61205092), Guangdong Foundation of Outstanding Young Teachers in Higher Education Institutions (Yq2013141), Shenzhen Scientific Research and Development Funding Program (JCYJ20130329115450637, KQC2011 08300045A and ZYC201105170243A) and Guangdong Natural Science Foundation (S2012010009545)."
  ], 
  "acks": " ", 
  "authors": [
    " Zexuan Zhu", 
    " Linsen Li", 
    " Yongpeng Zhang", 
    " Yanli Yang", 
    " Xiao Yang", 
    " John Hancock"
  ], 
  "keyWords": [
    "read mapping", 
    "reference compression", 
    [
      "mapped", 
      "genomes", 
      "reads", 
      "references", 
      "compressed", 
      "bioinformatics", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-05T00:35:52Z"
}{
  "doi": "10.1093/bioinformatics/btu534", 
  "name": "Comparative assembly hubs Webaccessible browsers for comparative genomics", 
  "links": [
    "https://github.com/glennhickey/progressiveCactus/tree/comparative", 
    "https://github.com", 
    "https://github.com/benedictpaten/jobTree", 
    "http://www.ebi.ac.uk/genomes/bacteria.html", 
    "http://genome.ucsc.edu/goldenPath/help/gbib", 
    "https://github.com/glennhickey/progressiveCactus", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://genome.ucsc.edu/FAQ"
  ], 
  "title": "Genome analysis Comparative assembly hubs: Web-accessible browsers for comparative genomics", 
  "toolName": "comparative", 
  "abstract": "Motivation: Researchers now have access to large volumes of genome sequences for comparative analysis, some generated by the plethora of public sequencing projects and, increasingly, from individual efforts. It is not possible, or necessarily desirable, that the public genome browsers attempt to curate all these data. Instead, a wealth of powerful tools is emerging to empower users to create their own visualizations and browsers. Results: We introduce a pipeline to easily generate collections of Web-accessible UCSC Genome Browsers interrelated by an alignment. It is intended to democratize our comparative genomic browser resources, serving the broad and growing community of evolutionary genomicists and facilitating easy public sharing via the Internet. Using the alignment, all annotations and the alignment itself can be efficiently viewed with reference to any genome in the collection, symmetrically. A new, intelligently scaled alignment display makes it simple to view all changes between the genomes at all levels of resolution, from substitutions to complex structural rearrangements, including duplications. To demonstrate this work, we create a comparative assembly hub containing 57 Escherichia coli and 9 Shigella genomes and show examples that highlight their unique biology. Availability and implementation: The source code is available as open source at: https://github.com/glennhickey/progressiveCactus The E.coli and Shigella genome hub is now a public hub listed on the UCSC browser public hubs Web page.", 
  "summary": "Finally, to overcome the limitation of viewing the alignment only from the perspective of any single reference genome, each comparative assembly hub provides an automatically generated pangenome reference browser, using our recently developed algorithm (Nguyen et al.\nDuplications within the query genome create extra segments that overlap along the reference genome axis, e.g. Figure 2 shows a tandem repeat region of E.coli KO11FL 162099 displayed along the genome of E.coli KO11FL 52593 that was engineered by the chromosomal insertion of the Zymomonas mobilis pdc, adhB and cat genes into E.coli W for ethanol production purposes (Ohta et al.", 
  "affiliations": [
    " Center for Biomolecular Science and Engineering Howard Hughes Medical Institute UCSC ", 
    " Center for Biomolecular Sciences and Engineering CBSE/ITI UC Santa Cruz "
  ], 
  "grants": [
    "Funding: The authors acknowledge the support of NIH (1U41HG007234-01) and (1U41HG006992-2) and NHGRI/ NIH (5U01HG004695) for providing funding."
  ], 
  "sourcelinks": [
    "https://github.com/glennhickey/progressiveCactus/tree/comparative", 
    "https://github.com", 
    "https://github.com/benedictpaten/jobTree", 
    "http://www.ebi.ac.uk/genomes/bacteria.html", 
    "http://genome.ucsc.edu/goldenPath/help/gbib", 
    "https://github.com/glennhickey/progressiveCactus", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "acks": " The authors would like to thank the Howard Hughes Medical Institute. They also thank the three anonymous reviewers for providing excellent feedback. ", 
  "authors": [
    " Ngan Nguyen", 
    " Glenn Hickey", 
    " Brian J Raney", 
    " Joel Armstrong", 
    " Hiram Clawson", 
    " Ann Zweig", 
    " Donna Karolchik", 
    " William James Kent", 
    " David Haussler", 
    " Benedict Paten", 
    " John Hancock"
  ], 
  "keyWords": [
    "genomic browser", 
    [
      "displaying", 
      "genomering", 
      "tracks", 
      "browsers", 
      "hubs", 
      "alignments"
    ]
  ], 
  "github_data": {
    "name": "skaro", 
    "contributors": [
      {
        "contributions": 25, 
        "html_url": "https://github.com/technomancy"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/danjacka"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/syohex"
      }
    ], 
    "versions": [], 
    "created_at": "2013-07-09T04:13:07Z", 
    "updated_at": "2016-08-07T20:02:43Z", 
    "languages": [
      "Shell", 
      "Clojure", 
      "Emacs Lisp", 
      "OCaml", 
      "Racket", 
      "Scheme"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/ahmetus"
      }
    ], 
    "owner": "https://github.com/technomancy", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-08-20T05:11:29Z"
}{
  "doi": "10.1093/bioinformatics/btu495", 
  "name": "Compression and fast retrieval of SNP data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.dei.unipd.it"
  ], 
  "title": "Genetics and population analysis Compression and fast retrieval of SNP data", 
  "toolName": "Genetics and population analysis Compression and fast retrieval of SNP data", 
  "abstract": "Motivation: The increasing interest in rare genetic variants and epi-static genetic effects on complex phenotypic traits is currently pushing genome-wide association study design towards datasets of increasing size, both in the number of studied subjects and in the number of genotyped single nucleotide polymorphisms (SNPs). This, in turn, is leading to a compelling need for new methods for compression and fast retrieval of SNP data. Results: We present a novel algorithm and file format for compressing and retrieving SNP data, specifically designed for large-scale association studies. Our algorithm is based on two main ideas: (i) compress linkage disequilibrium blocks in terms of differences with a reference SNP and (ii) compress reference SNPs exploiting information on their call rate and minor allele frequency. Tested on two SNP datasets and compared with several state-of-the-art software tools, our compression algorithm is shown to be competitive in terms of compression rate and to outperform all tools in terms of time to load compressed data. Availability and implementation: Our compression and decompres-sion algorithms are implemented in a C++ library, are released under the GNU General Public License and are freely downloadable from", 
  "summary": "based on two main ideas: summarize LD blocks in terms of differences with a reference SNP and compress reference SNPs with the best among five types of codes, designed to exploit the information on the call rate and minor allele frequency of the SNPs. We compared our algorithm with one of the most widely adopted tools for genetic data analysis, the PLINK software, with the state of the art in GWAS data compression and retrieval, the SpeedGene software, with the general compression tool GZIP and with the specific genetic compression tool TGC.", 
  "affiliations": [
    " Department of Information Engineering University of Padova "
  ], 
  "grants": [
    "Funding: The research was supported by the European Union's Seventh Framework Program (FP7/2007-2013) for the Innovative Medicine Initiative under grant agreement n IMI/ 115006 (the SUMMIT consortium) and by the University of Padova's strategic project AACSE.", 
    "Tools based on our library could sit in the GWAS analysis pipeline right after variant calling and implement, for example, data quality control or association analysis, effectively exploiting the reduction in storage space and time to load the data granted by our library and file format."
  ], 
  "acks": " ", 
  "authors": [
    " Francesco Sambo", 
    " Barbara Di Camillo", 
    " Gianna Toffolo", 
    " Claudio Cobelli"
  ], 
  "keyWords": [
    "snp data", 
    "file format", 
    [
      "files", 
      "codes", 
      "genetics", 
      "compression", 
      "snps", 
      "coding", 
      "formats"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.dei.unipd.it"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btu625", 
  "name": "Comprehensive largescale assessment of intrinsic protein disorder", 
  "links": [
    "http://mobidb.bio.unipd.it/lsd", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Structural bioinformatics Comprehensive large-scale assessment of intrinsic protein disorder", 
  "toolName": "Structural bioinformatics Comprehensive large-scale assessment of intrinsic protein disorder", 
  "abstract": "Motivation: Intrinsically disordered regions are key for the function of numerous proteins. Due to the difficulties in experimental disorder characterization, many computational predictors have been developed with various disorder flavors. Their performance is generally measured on small sets mainly from experimentally solved structures, e.g. Protein Data Bank (PDB) chains. MobiDB has only recently started to collect disorder annotations from multiple experimental structures. Results: MobiDB annotates disorder for UniProt sequences, allowing us to conduct the first large-scale assessment of fast disorder predictors on 25 833 different sequences with X-ray crystallographic structures. In addition to a comprehensive ranking of predictors, this analysis produced the following interesting observations. (i) The predictors cluster according to their disorder definition, with a consensus giving more confidence. (ii) Previous assessments appear over-reliant on data annotated at the PDB chain level and performance is lower on entire UniProt sequences. (iii) Long disordered regions are harder to predict. (iv) Depending on the structural and functional types of the proteins, differences in prediction performance of up to 10% are observed. Availability: The datasets are available from Web site at URL:", 
  "summary": "Many are tuned for the disorder style used in the Critical Assessment of techniques for protein Structure Prediction (CASP), where the goal is to detect missing residues in the X-ray crystal (Monastyrskyy et al., 2014).\nThe top five predictor SOV performances for proteins with at least one disordered residue are separated into the three GO classes (Ashburner et al., 2000).\nHighly accurate predictors on UniProt sequences are vital, considering that users are invariably trying to understand disorder properties of unannotated proteins and not the PDB, which is already annotated with quality structural information.", 
  "affiliations": [
    " Department of Information Engineering University of Padua ", 
    " Institute for Advanced Simulation Forschungszentrum Juelich ", 
    " Department of Biomedical Sciences"
  ], 
  "grants": [
    "Funding: FIRB Futuro in Ricerca grant (RBFR08ZSXY to S.T.)."
  ], 
  "acks": " The authors are grateful to members of the BioComputing UP lab for insightful discussions. ", 
  "authors": [
    " Ian Walsh", 
    " Manuel Giollo", 
    " Tom As", 
    " Di Domenico", 
    " Carlo Ferrari", 
    " Olav Zimmermann", 
    " Silvio C E Tosatto", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "protein disorder", 
    [
      "predictors", 
      "performances", 
      "structured", 
      "proteins", 
      "sequences", 
      "disordered"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-23T02:41:35Z"
}{
  "doi": "10.1093/bioinformatics/btu726", 
  "name": "Computational framework for nextgeneration sequencing of heterogeneous viral populations using combinatorial pooling", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://alan.cs.gsu.edu/NGS/?qcontent/pooling"
  ], 
  "title": "Computational framework for next-generation sequencing of heterogeneous viral populations using combinatorial pooling", 
  "toolName": "Computational framework for next-generation sequencing of heterogeneous viral populations using combinatorial pooling", 
  "abstract": "Motivation: Next-generation sequencing (NGS) allows for analyzing a large number of viral sequences from infected patients, providing an opportunity to implement large-scale molecular surveillance of viral diseases. However, despite improvements in technology, traditional protocols for NGS of large numbers of samples are still highly cost and labor intensive. One of the possible cost-effective alternatives is combinatorial pooling. Although a number of pooling strategies for consensus sequencing of DNA samples and detection of SNPs have been proposed, these strategies cannot be applied to sequencing of highly heterogeneous viral populations. Results: We developed a cost-effective and reliable protocol for sequencing of viral samples, that combines NGS using barcoding and combinatorial pooling and a computational framework including algorithms for optimal virus-specific pools design and deconvolution of individual samples from sequenced pools. Evaluation of the framework on experimental and simulated data for hepatitis C virus showed that it substantially reduces the sequencing costs and allows deconvolution of viral populations with a high accuracy. Availability and implementation: The source code and experimental data sets are available at", 
  "summary": "Results: We developed a cost-effective and reliable protocol for sequencing of viral samples, that combines NGS using barcoding and combinatorial pooling and a computational framework including algorithms for optimal virus-specific pools design and deconvolution of individual samples from sequenced pools.\nWe propose a method for inference of samples from sequenced pools based on a novel maximum likelihood clustering algorithm for heterogeneous viral samples.\nViral Sample Pool Design (VSPD) Problem: given a graph G  V; E and T > 0, find a minimum set of cliques P  fP1; .", 
  "affiliations": [
    " Division of Viral Hepatitis Centers of Disease Control and Prevention ", 
    " Department of Computer Science Georgia State University ", 
    " Department of Computer Science and Engineering University of Connecticut "
  ], 
  "grants": [], 
  "acks": " We thank reviewers for helpful comments and Seth Sims and Vernard Martin (CDC/NCEZID) for help with CDC cluster. Conflict of interest: none declared. ", 
  "authors": [
    " Pavel Skums", 
    " Alexander Artyomenko", 
    " Olga Glebova", 
    " Sumathi Ramachandran", 
    " Ion Mandoiu", 
    " David S Campo", 
    " Zoya Dimitrova", 
    " Alex Zelikovsky", 
    " Yury Khudyakov"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "pooling", 
      "algorithms", 
      "sampling"
    ]
  ], 
  "sourcelinks": [
    "http://alan.cs.gsu.edu/NGS/?qcontent/pooling"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-31T00:44:56Z"
}{
  "doi": "10.1093/bioinformatics/btu837", 
  "name": "CONSRANK a server for the analysis comparison and ranking of docking models based on interresidue contacts", 
  "links": [
    "http://www.jmol.org", 
    "https://www", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://www.molnac.unisa.it/BioTools/consrank"
  ], 
  "title": "CONSRANK: a server for the analysis, comparison and ranking of docking models based on inter-residue contacts", 
  "toolName": "CONSRANK: a server for the analysis, comparison and ranking of docking models based on inter-residue contacts", 
  "abstract": "Herein, we present CONSRANK, a web tool for analyzing, comparing and ranking protein\u2013protein and protein\u2013nucleic acid docking models, based on the conservation of inter-residue contacts and its visualization in 2D and 3D interactive contact maps.", 
  "summary": "Summary: Herein, we present CONSRANK, a web tool for analyzing, comparing and ranking proteinprotein and proteinnucleic acid docking models, based on the conservation of interresidue contacts and its visualization in 2D and 3D interactive contact maps.\nCONSRANK output includes in the main page user-sortable and searchable tables reporting: (i) a list of the interresidue contacts observed in at least 1% of the models, with relative conservation rate (Vangone et al., 2012); (ii) the ranking of the submitted models based on the CONSRANK normalized score (Oliva et al., 2013); (iii) parameters (C50, C70, C90) reflecting the overall conservation of inter-residue contacts in the models ensemble (Vangone et al., 2012).", 
  "affiliations": [
    " Department of Chemistry and Biology University of Salerno ", 
    " Kaust Catalysis Center King Abdullah University of Science and Technology ", 
    " Dipartimento di Informatica ed Applicazioni"
  ], 
  "grants": [
    "Funding\nR.O."
  ], 
  "acks": " ", 
  "authors": [
    " Edrisse Chermak", 
    " Andrea Petta", 
    " Luigi Serra", 
    " Anna Vangone", 
    " Vittorio Scarano", 
    " Luigi Cavallo", 
    " Romina Oliva"
  ], 
  "keyWords": [
    "docking models", 
    [
      "contacts", 
      "proteins", 
      "vangone", 
      "consensus", 
      "bioinformatics", 
      "model"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-12-23T03:29:09Z"
}{
  "doi": "10.1093/bioinformatics/btu539", 
  "name": "Cordova Webbased management of genetic variation data", 
  "links": [
    "http://vvd.eng.uiowa.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/clcg/cordova", 
    "http://evs.gs.washington.edu/EVS", 
    "http://deafnessvariationdatabase.org"
  ], 
  "title": "Databases and ontologies Cordova: Web-based management of genetic variation data", 
  "toolName": "cordova", 
  "abstract": "Cordova is an out-of-the-box solution for building and maintaining an online database of genetic variations integrated with pathogenicity prediction results from popular algorithms. Our primary motivation for developing this system is to aid researchers and clin-ician\u2013scientists in determining the clinical significance of genetic variations. To achieve this goal, Cordova provides an interface to review and manually or computationally curate genetic variation data as well as share it for clinical diagnostics and the advancement of research. Availability and implementation: Cordova is open source under the MIT license and is freely available for download at https://", 
  "summary": "Cordova provides a collection of tools for research and clinical genetic testing designed to (i) annotate variations from popular pathogenicity prediction tools; (ii) collect published allele frequencies; (iii) provide an interface for reviewing and curating variation, pathogenicity and allele frequency data; and (iv) share these annotations to inform fellow clinicians and researchers with up-to-date data.\nWhen a variation is first submitted, Cordova will attempt to auto-populate data from dbNSFP2 (Liu et al., 2013), Exome Variant Server (2013), The 1000 Genomes Project Consortium (2012) and OtoSCOPEVR .", 
  "affiliations": [
    " Department of Biomedical Engineering", 
    " Department of Electrical and Computer Engineering", 
    " Department of Ophthalmology and Visual Sciences", 
    " Department of Otolaryngology\u2014Head & Neck Surgery Carver College of Medicine "
  ], 
  "grants": [
    "Its output is configurable and can be tailored to local database needs, including customized interpretations of prediction and conservation scores from dbNSFP2.", 
    "Cordova supports six pathogenicity prediction scores available from dbNSFP2, including SIFT (Kumar et al., 2009), Polyphen2 (Adzhubei et al., 2010), MutationTaster (Schwarz et al., 2010), LRT (Chun and Fay, 2009), phyloP (Siepel et al., 2006) and GERP++ (Davydov et al., 2010).", 
    "Funding: This work was supported by the National Institutes of Health (NIH), National Institute on Deafness and Other Communication Disorders Grants DC011674 (A.E.S.", 
    "It uses compressed versions of annotation files to reduce their total footprint by one-third, and runs approximately eight times faster than dbNSFP2's native querying application for large sets of annotations.", 
    "When a variation is first submitted, Cordova will attempt to auto-populate data from dbNSFP2 (Liu et al., 2013), Exome Variant Server (2013), The 1000 Genomes Project Consortium (2012) and OtoSCOPEVR .", 
    "(2013) dbNSFP v2.0: a database of human non-synonymous SNVs and their functional predictions and annotations.", 
    "and an NIH pre-doctoral research fellowship GM082729 (to A.P.D.)."
  ], 
  "sourcelinks": [
    "https://github.com/clcg/cordova"
  ], 
  "acks": " ", 
  "authors": [
    " Sean S Ephraim", 
    " Nikhil Anand", 
    " Adam P Deluca", 
    " Kyle R Taylor", 
    " Diana L Kolbe", 
    " Allen C Simpson", 
    " Hela Azaiez", 
    " Christina M Sloan", 
    " A Eliot Shearer", 
    " Andrea R Hallier", 
    " Thomas L Casavant", 
    " Todd E Scheetz", 
    " Richard J H Smith", 
    " Terry A Braun"
  ], 
  "keyWords": [
    "genetic variation data", 
    [
      "variations", 
      "cordova", 
      "genetics", 
      "lovd", 
      "databases"
    ]
  ], 
  "github_data": {
    "name": "Cordova", 
    "contributors": [
      {
        "contributions": 57, 
        "html_url": "https://github.com/brianleroux"
      }, 
      {
        "contributions": 35, 
        "html_url": "https://github.com/heynemann"
      }, 
      {
        "contributions": 9, 
        "html_url": "https://github.com/tonylukasavage"
      }, 
      {
        "contributions": 4, 
        "html_url": "https://github.com/infil00p"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/hermwong"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/wailqill"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/pamelafox"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/brianleroux/Cordova/zipball/0.2.3", 
        "tarball_url": "https://api.github.com/repos/brianleroux/Cordova/tarball/0.2.3", 
        "name": "0.2.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/brianleroux/Cordova/zipball/0.2.2", 
        "tarball_url": "https://api.github.com/repos/brianleroux/Cordova/tarball/0.2.2", 
        "name": "0.2.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/brianleroux/Cordova/zipball/0.2.1", 
        "tarball_url": "https://api.github.com/repos/brianleroux/Cordova/tarball/0.2.1", 
        "name": "0.2.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/brianleroux/Cordova/zipball/0.2.0", 
        "tarball_url": "https://api.github.com/repos/brianleroux/Cordova/tarball/0.2.0", 
        "name": "0.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/brianleroux/Cordova/zipball/0.1.1", 
        "tarball_url": "https://api.github.com/repos/brianleroux/Cordova/tarball/0.1.1", 
        "name": "0.1.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/brianleroux/Cordova/zipball/0.1.0", 
        "tarball_url": "https://api.github.com/repos/brianleroux/Cordova/tarball/0.1.0", 
        "name": "0.1.0"
      }
    ], 
    "created_at": "2011-04-04T23:08:11Z", 
    "updated_at": "2016-03-03T05:16:43Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/brianleroux"
      }
    ], 
    "owner": "https://github.com/brianleroux", 
    "homepage": ""
  }, 
  "technologies": [
    "PHP"
  ], 
  "dateCreated": "2014-08-15T05:12:51Z"
}{
  "doi": "10.1093/bioinformatics/btu591", 
  "name": "Consensus Genotyper for Exome Sequencing CGES improving the quality of exome variant genotypes", 
  "links": [
    "http://www.globus", 
    "http://www.globus.org/genomics", 
    "http://genome.sph.umich.edu/wiki/Vt", 
    "http://sourceforge.net/projects/atlas2", 
    "http://www.broadinstitute.org/gatk/guide/best-practices", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Sequence analysis Consensus Genotyper for Exome Sequencing (CGES): improving the quality of exome variant genotypes", 
  "toolName": "atlas2", 
  "abstract": "Motivation: The development of cost-effective next-generation sequencing methods has spurred the development of high-throughput bioinformatics tools for detection of sequence variation. With many disparate variant-calling algorithms available, investigators must ask, 'Which method is best for my data?' Machine learning research has shown that so-called ensemble methods that combine the output of multiple models can dramatically improve classifier performance. Here we describe a novel variant-calling approach based on an ensemble of variant-calling algorithms, which we term the Consensus Genotyper for Exome Sequencing (CGES). CGES uses a two-stage voting scheme among four algorithm implementations. While our ensemble method can accept variants generated by any variant-calling algorithm , we used GATK2.8, SAMtools, FreeBayes and Atlas-SNP2 in building CGES because of their performance, widespread adoption and diverse but complementary algorithms. Results: We apply CGES to 132 samples sequenced at the Hudson Alpha Institute for Biotechnology (HAIB, Huntsville, AL) using the Nimblegen Exome Capture and Illumina sequencing technology. Our sample set consisted of 40 complete trios, two families of four, one parent\u2013child duo and two unrelated individuals. CGES yielded the fewest total variant calls (N CGES =139 897), the highest Ts/Tv ratio (3.02), the lowest Mendelian error rate across all genotypes (0.028%), the highest rediscovery rate from the Exome Variant Server (EVS; 89.3%) and 1000 Genomes (1KG; 84.1%) and the highest positive predictive value (PPV; 96.1%) for a random sample of previously validated de novo variants. We describe these and other quality control (QC) metrics from consensus data and explain how the CGES pipeline can be used to generate call sets of varying quality stringency, including consensus calls present across all four algorithms , calls that are consistent across any three out of four algorithms , calls that are consistent across any two out of four algorithms or a more liberal set of all calls made by any algorithm. Availability and implementation: To enable accessible, efficient and reproducible analysis, we implement CGES both as a stand-alone command line tool available for download in GitHub and as a set of Galaxy tools and workflows configured to execute on parallel computers.", 
  "summary": "Here we describe a novel variant-calling approach based on an ensemble of variant-calling algorithms, which we term the Consensus Genotyper for Exome Sequencing (CGES).\nCGES yielded the fewest total variant calls (NCGES=139897), the highest Ts/Tv ratio (3.02), the lowest Mendelian error rate across all genotypes (0.028%), the highest rediscovery rate from the Exome Variant Server (EVS; 89.3%) and 1000 Genomes (1KG; 84.1%) and the highest positive predictive value (PPV; 96.1%) for a random sample of previously validated de novo variants.\nThe four variant-calling algorithms used in this analysis are implemented in previously published programs GATK v2.8 (DePristo et al., 2011; McKenna, 2010a), SAMtools (Li et al., 2009a), Atlas-SNP2 (Challis et al., 2012) and FreeBayes (Garrison and Marth, 2012).", 
  "affiliations": [
    " Department of Psychiatry University of Illinois at Chicago ", 
    " Computation Institute University of Chicago ", 
    " Department of Medicine Section of Genetic Medicine "
  ], 
  "grants": [
    "2.2 NGS\nSequencing for the majority of samples was performed at the HAIB (Hunstville, AL) as a part of the NIH Autism Sequencing Consortium\n\nFig.", 
    "Funding: This work was supported in part by P50 HD055751 (EHC), U19 GM61393 (NJC), P60 DK20595 (NJC), P50 MH094267 (NJC), R01 MH089482 (JSS), R24HL085343 (IF), by a Lever Award from the Chicago Biomedical Consortium and by the US Department of Energy under contract DE-AC0206CH11357."
  ], 
  "acks": " The authors wish to acknowledge the individuals who participated in research at the Autism Center for Excellence (University ", 
  "authors": [
    " Vassily Trubetskoy", 
    " Alex Rodriguez", 
    " Uptal Dave", 
    " Nicholas Campbell", 
    " Emily L Crawford", 
    " Edwin H Cook", 
    " James S Sutcliffe", 
    " Ian Foster", 
    " Ravi Madduri", 
    " Nancy J Cox", 
    " Lea K Davis"
  ], 
  "keyWords": [
    "variant genotypes", 
    "samples sequenced", 
    [
      "genotyping", 
      "sample", 
      "sequencing", 
      "cges", 
      "data", 
      "variants"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net/projects/atlas2", 
    "http://www.globus"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-01T07:57:30Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/atlas2/", 
    "languages": [
      "C", 
      "Ruby"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/ibgibson/", 
        "name": "Ian Gibson"
      }, 
      {
        "url": "https://sourceforge.net/u/challisd/", 
        "name": "Danny Challis"
      }, 
      {
        "url": "https://sourceforge.net/u/uevani/", 
        "name": "Uday Evani"
      }, 
      {
        "url": "https://sourceforge.net/u/li-an/", 
        "name": "Lilian"
      }, 
      {
        "url": "https://sourceforge.net/u/kklinear/", 
        "name": "taebeom"
      }, 
      {
        "url": "https://sourceforge.net/u/atlasmember/", 
        "name": "Atlas2Team"
      }, 
      {
        "url": "https://sourceforge.net/u/jy2/", 
        "name": "Jin Yu"
      }
    ], 
    "Development Status": [
      {
        "status": "4 - Beta"
      }
    ], 
    "license": [
      {
        "name": "BSD License"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu562", 
  "name": "Cosi2 an efficient simulator of exact and approximate coalescent with selection", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://broadinstitute.org/mpg/cosi2"
  ], 
  "title": "Genetics and population analysis Cosi2: an efficient simulator of exact and approximate coalescent with selection", 
  "toolName": "Genetics and population analysis Cosi2: an efficient simulator of exact and approximate coalescent with selection", 
  "abstract": "Motivation: Efficient simulation of population genetic samples under a given demographic model is a prerequisite for many analyses. Coalescent theory provides an efficient framework for such simulations , but simulating longer regions and higher recombination rates remains challenging. Simulators based on a Markovian approximation to the coalescent scale well, but do not support simulation of selection. Gene conversion is not supported by any published coalescent simulators that support selection. Results: We describe cosi2, an efficient simulator that supports both exact and approximate coalescent simulation with positive selection. cosi2 improves on the speed of existing exact simulators, and permits further speedup in approximate mode while retaining support for selection. cosi2 supports a wide range of demographic scenarios, including recombination hot spots, gene conversion, population size changes, population structure and migration. cosi2 implements coalescent machinery efficiently by tracking only a small subset of the Ancestral Recombination Graph, sampling only relevant recombination events, and using augmented skip lists to represent tracked genetic segments. To preserve support for selection in approximate mode, the Markov approximation is implemented not by moving along the chromosome but by performing a standard backwards in time coalescent simulation while restricting coalescence to node pairs with overlapping or near-overlapping genetic material. We describe the algorithms used by cosi2 and present comparisons with existing selection simulators. Availability and implementation: A free C+ + implementation of cosi2 is available at", 
  "summary": "Results: We describe cosi2, an efficient simulator that supports both exact and approximate coalescent simulation with positive selection.\nTo preserve support for selection in approximate mode, the Markov approximation is implemented not by moving along the chromosome but by performing a standard backwards-in-time coalescent simulation while restricting coalescence to node pairs with overlapping or near-overlapping genetic material.\nSimulators that do support positive selection (e.g. Ewing and Hermisson, 2010 and Teshima and Innan, 2009) suffer from the performance limitations of the traditional coalescent, and do not support all commonly needed features (variable genetic maps, gene conversion, structured populations) within a single framework.", 
  "affiliations": [
    " Department of Organismic and Evolutionary Biology Harvard University ", 
    " Broad Institute of MIT and Harvard"
  ], 
  "grants": [
    "Funding: PCS is funded by an NIH Innovator Award 1DP2OD006514-01 and by a Broad Institute SPARC award."
  ], 
  "acks": " ", 
  "authors": [
    " Ilya Shlyakhter", 
    " Pardis C Sabeti", 
    " Stephen F Schaffner", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    [
      "selection", 
      "cosi", 
      "sampling", 
      "recombinations", 
      "simulations", 
      "coalescences"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-23T04:14:10Z"
}{
  "doi": "10.1093/bioinformatics/btu795", 
  "name": "Computerassisted curation of a human regulatory core network from the biological literature", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://fastforward.sys-bio.net", 
    "http://cancergenome.nih.gov.Tikk,D"
  ], 
  "title": "Data and text mining Computer-assisted curation of a human regulatory core network from the biological literature Downloaded from", 
  "toolName": "Data and text mining Computer-assisted curation of a human regulatory core network from the biological literature Downloaded from", 
  "abstract": "Motivation: A highly interlinked network of transcription factors (TFs) orchestrates the context-dependent expression of human genes. ChIP-chip experiments that interrogate the binding of particular TFs to genomic regions are used to reconstruct gene regulatory networks at genome-scale, but are plagued by high false-positive rates. Meanwhile, a large body of knowledge on high-quality regulatory interactions remains largely unexplored, as it is available only in natural language descriptions scattered over millions of scientific publications. Such data are hard to extract and regulatory data currently contain together only 503 regulatory relations between human TFs. Results: We developed a text-mining-assisted workflow to systematically extract knowledge about regulatory interactions between human TFs from the biological literature. We applied this workflow to the entire Medline, which helped us to identify more than 45 000 sentences potentially describing such relationships. We ranked these sentences by a machine-learning approach. The top-2500 sentences contained 900 sentences that encompass relations already known in databases. By manually curating the remaining 1625 top-ranking sentences, we obtained more than 300 validated regulatory relationships that were not present in a regulatory database before. Full-text curation allowed us to obtain detailed information on the strength of experimental evidences supporting a relationship. Conclusions: We were able to increase curated information about the human core transcriptional network by >60% compared with the current content of regulatory databases. We observed improved performance when using the network for disease gene prioritization compared with the state-of-the-art. Availability and implementation: Web-service is freely accessible at http://fastforward.sys-bio.net/.", 
  "summary": "We automatically classified each of these sentences using a classifier trained on a manually annotated gold standard corpus of sentences describing regulatory relationships and inspected in detail the top-2500 sentences mentioning a pair of human TFs. 35% of those 2500 sentences report transcriptional interactions that were already covered by RegDBs. By manual curation, we found that 660 of the remaining 1625 sentences contained interesting information about gene regulatory relations, and further 322 sentences described co-operation or competition in transcription.", 
  "affiliations": [
    " Institute for Computer Science Humboldt-Universit\u00e4t zu Berlin Knowledge Management in Bioinformatics ", 
    " Department of Telecommunications and Media Informatics Budapest University of Technology and Economics "
  ], 
  "grants": [
    "As for the second source of knowledge, we compiled a regulatory dataset from three well-established gene regulatory databases, namely TRANSFAC (Wingender, 2008), TRRD (Kolchanov et al., 2002) and ORegAnno (Griffith et al., 2008).", 
    "Funding\nThis work was funded by German Academic Exchange Service and the German Federal Ministry of Education and Research [0315417B, 0316184A,B, 01GQ1001C, 0315261].", 
    "(B) Venndiagram of the regulatory relations between two TFs in the databases TRANSFAC, TRRD and ORegAnno\n\nAB\n\nA\n\nB\n\nC\n\nDownloaded from http://bioinformatics.oxfordjournals.org/ at :: on August 8, 2016\n\nCD\n\nE\n\nFig.", 
    "TRANSFAC, TRRD, ORegAnno) and our manual curation efforts was aggregated into a database, which is accessible by our web-service FastForward available at http://fastforward.sys-bio.net/.", 
    "The TRANSFAC project as an example of framework technology that supports the analysis of genomic regulation.", 
    "For manual curation, we filtered all candidates that were already present in TRANSFAC, TRRD or ORegAnno and also those candidates that were mentioned in publications already curated in one of these knowledge bases.", 
    "Of these, TRANSFAC contained the largest amount of relationships between human TFs (373), TRRD contained 183 and ORegAnno contained 22.", 
    "TRANSFAC (Wingender, 2008), TRRD (Kolchanov et al., 2002) and ORegAnno (Griffith et al., 2008) are more established databases specifically focusing on regulatory relationships.", 
    "We expanded this list by an additional 274 human TFs assembled from literature, TRANSFAC, TRRD and ORegAnno.", 
    "4.2 Curated regulatory databases\nWe compiled the existing knowledge for interactions between human TFs from the following regulatory databases (RegDBs): TRANSFAC [(Wingender, 2008), Release 12.1], TRRD (Kolchanov et al., 2002) and ORegAnno (Griffith et al., 2008)."
  ], 
  "acks": " ", 
  "authors": [
    " Philippe Thomas", 
    " Pawel Durek", 
    " Ill\u00e9 S Solt", 
    " Bertram Klinger", 
    " Franziska Witzel", 
    " Pascal Schulthess", 
    " Yvonne Mayer", 
    " Domonkos Tikk", 
    " Nils Bl\u00fc Thgen", 
    " Ulf Leser"
  ], 
  "keyWords": [
    [
      "regulatory", 
      "curating", 
      "networks", 
      "genes", 
      "sentences"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-30T01:39:32Z"
}{
  "doi": "10.1093/bioinformatics/btu658", 
  "name": "Computing autocatalytic sets to unravel inconsistencies in metabolic network reconstructions", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://users.minet.uni-jena.de/m3kach/ASBIG/ASBIG.zip"
  ], 
  "title": "Systems biology Computing autocatalytic sets to unravel inconsistencies in metabolic network reconstructions", 
  "toolName": "Systems biology Computing autocatalytic sets to unravel inconsistencies in metabolic network reconstructions", 
  "abstract": "Motivation: Genome-scale metabolic network reconstructions have been established as a powerful tool for the prediction of cellular phenotypes and metabolic capabilities of organisms. In recent years, the number of network reconstructions has been constantly increasing , mostly because of the availability of novel (semi-)automated procedures , which enabled the reconstruction of metabolic models based on individual genomes and their annotation. The resulting models are widely used in numerous applications. However, the accuracy and predictive power of network reconstructions are commonly limited by inherent inconsistencies and gaps. Results: Here we present a novel method to validate metabolic network reconstructions based on the concept of autocatalytic sets. Autocatalytic sets correspond to collections of metabolites that, besides enzymes and a growth medium, are required to produce all biomass components in a metabolic model. These autocatalytic sets are well-conserved across all domains of life, and their identification in specific genome-scale reconstructions allows us to draw conclusions about potential inconsistencies in these models. The method is capable of detecting inconsistencies, which are neglected by other gap-finding methods. We tested our method on the Model SEED, which is the largest repository for automatically generated genome-scale network reconstructions. In this way, we were able to identify a significant number of missing pathways in several of these reconstructions. Hence, the method we report represents a powerful tool to identify inconsistencies in large-scale metabolic networks. Availability and implementation: The method is available as source code on", 
  "summary": "These compounds constitute the crucial result of ASBIG, as their presence in the minimal seed set indicates potential gaps within the metabolic network reconstruction.\nFirst, the reconstruction of interest is examined via scope analysis with a predefined initial seed set (always containing the same metabolites regardless of the metabolic model investigated).\n3.2.4 Chlamydomonas reinhardtii The minimal seed set for iRC1080, the metabolic model of C.reinhardtii, contained add-on metabolites that indicated the lack of functionalities in some biosynthetic pathways.\nFor each investigated metabolic model, a minimal seed set of metabolites, which provides access to all fundamental metabolic pathways, is computed.", 
  "affiliations": [
    " Department of Bioorganic Chemistry Max Planck Institute for Chemical Ecology Experimental Ecology and Evolution Research Group ", 
    " Faculty of Biology and Pharmacy Research Group Theoretical Systems Biology Friedrich Schiller University Jena ", 
    " Department of Biomolecular Chemistry Leibniz Institute for Natural Product Research and Infection Biology -Hans Kn\u20ac oll Institute "
  ], 
  "grants": [
    "The use of metabolic models and associated methods has granted access to diverse scientific subjects, such as analysis of the bacterial metabolism (Yus et al., 2009), the prediction of growth rates of Escherichia coli (Varma and Palsson, 1994), the comparison of growth rates between wild type and mutant strains of E.coli (Segre` et al., 2002) and metabolic engineering (Wang et al., 2006).", 
    "Funding: This work was supported by the Jena School for Microbial Communication (JSMC) to S.W."
  ], 
  "acks": " ", 
  "authors": [
    " Ralf Schmidt", 
    " Silvio Waschina", 
    " Daniela Boettger-Schmidt", 
    " Christian Kost", 
    " Christoph Kaleta"
  ], 
  "keyWords": [
    [
      "asbig", 
      "modeling", 
      "metabolites", 
      "sets", 
      "compounds"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://users.minet.uni-jena.de/m3kach/ASBIG/ASBIG.zip"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-07T04:41:50Z"
}{
  "doi": "10.1093/bioinformatics/btu743", 
  "name": "CRISPRdirect software for designing CRISPRCas guide RNA with reduced offtarget sites", 
  "links": [
    "http://crispr.dbcls.jp", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://GGGenome.dbcls.jp", 
    "https://www.dna20.com/eCommerce", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "CRISPRdirect: software for designing CRISPR/ Cas guide RNA with reduced off-target sites", 
  "toolName": "CRISPRdirect: software for designing CRISPR/ Cas guide RNA with reduced off-target sites", 
  "abstract": "CRISPRdirect is a simple and functional web server for selecting rational CRISPR/Cas targets from an input sequence. The CRISPR/Cas system is a promising technique for genome engineering which allows target-specific cleavage of genomic DNA guided by Cas9 nuclease in complex with a guide RNA (gRNA), that complementarily binds to a 20 nt targeted sequence. The target sequence requirements are twofold. First, the 5 0-NGG protospacer adjacent motif (PAM) sequence must be located adjacent to the target sequence. Second, the target sequence should be specific within the entire genome in order to avoid off-target editing. CRISPRdirect enables users to easily select rational target sequences with minimized off-target sites by performing exhaustive searches against genomic sequences. The server currently incorporates the genomic sequences of human,", 
  "summary": "The CRISPR/Cas system is a promising technique for genome engineering which allows target-specific cleavage of genomic DNA guided by Cas9 nuclease in complex with a guide RNA (gRNA), that complementarily binds to a 20 nt targeted sequence.\nWeb servers for checking off-target sites for given 20 nt sequences are also available, such as Cas-OFFinder (Bae et al., 2014) and GGGenome (http:// GGGenome.dbcls.jp/).\nThe web server accepts an accession number, a genome coordinate or an arbitrary nucleotide sequence up to 10 kbp as input (Fig. 1A) and returns a list of CRISPR/Cas target candidates.", 
  "affiliations": [
    " Department of Biological Sciences Graduate School of Science University of Tokyo "
  ], 
  "grants": [
    "); Grant-in-Aid for Scientific Research from the Ministry of Education, Culture, Sports, Science and Technology (MEXT) of Japan (to Y.N.", 
    "Funding\nLife Science Database Integration Project, National Bioscience Database Center (NBDC) of Japan Science and Technology Agency (JST) (to Y.N."
  ], 
  "acks": " ", 
  "authors": [
    " Yuki Naito", 
    " Kimihiro Hino", 
    " Hidemasa Bono", 
    " Kumiko Ui-Tei", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "sciences", 
      "crisprdirect", 
      "targeting", 
      "genomes", 
      "sequences"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    " REST "
  ], 
  "dateCreated": "2014-11-21T01:18:58Z"
}{
  "doi": "10.1093/bioinformatics/btu731", 
  "name": "CytoCom a Cytoscape app to visualize query and analyse disease comorbidity networks", 
  "links": [
    "http://www.cl.cam.ac.uk/mam211", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.icd9data.com", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Genetics and population analysis CytoCom: a Cytoscape app to visualize, query and analyse disease comorbidity networks", 
  "toolName": "Genetics and population analysis CytoCom: a Cytoscape app to visualize, query and analyse disease comorbidity networks", 
  "abstract": "CytoCom is an interactive plugin for Cytoscape that can be used to search, explore, ana", 
  "summary": "CytoCom: a Cytoscape app to visualize, query and analyse disease comorbidity networks\nSummary: CytoCom is an interactive plugin for Cytoscape that can be used to search, explore, analyse and visualize human disease comorbidity network.\nCytoCom constructs disease comorbidity network (DCN) in which nodes are diseases and edges indicate the comorbidity associations among them.\nTo explore a particular disease, user needs to enter the seed disease code in the search box; CytoCom makes use of Cytoscape to construct the network and create a network view of disease association.", 
  "affiliations": [
    " Computer Laboratory University of Cambridge "
  ], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " Mohammad Ali Moni", 
    " Haoming Xu", 
    " Pietro Li\u00f2"
  ], 
  "keyWords": [
    "disease comorbidity networks", 
    [
      "visualization", 
      "based", 
      "users", 
      "comorbidities", 
      "diseases", 
      "cytocom", 
      "network"
    ]
  ], 
  "sourcelinks": [
    "http://www.cl.cam.ac.uk/mam211"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-08T01:28:35Z"
}{
  "doi": "10.1093/bioinformatics/btu394", 
  "name": "DAFGA diversity analysis of functional gene amplicons", 
  "links": [
    "https://github.com/outbig/DAFGA", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://fungene.cme.msu.edu"
  ], 
  "title": "Sequence analysis DAFGA: diversity analysis of functional gene amplicons", 
  "toolName": "DAFGA", 
  "abstract": "Diversity analysis of functional marker genes provides physiological insights into microbial guilds that perform an ecologically relevant process. However, it is challenging to group functional gene sequences to valid taxonomic units, primarily because of differences in the evolutionary rates of individual genes and possible horizontal gene transfer events. We developed a python script package named DAFGA, which estimates the evolutionary rate of a particular functional gene in a standardized manner by relating its sequence divergence to that of the 16S rRNA gene. As a result, DAFGA provides gene-specific parameter sets for operational taxonomic unit clustering and taxo-nomic assignment at desired rank, and it can be implemented into the diversity measurements offered by QIIME.", 
  "summary": "Massively parallel sequencing of 16S rRNA genes allows us to explore the phylogenetic diversity and taxonomic composition of microbial communities in any environment (e.g. Serkebaeva et al., 2013).\nThe most critical step in the diversity analysis of functional marker genes and rRNA genes is to cluster taxonomically homogeneous sequences into valid OTUs. DAFGA offers a standardized procedure to determine the ER of functional genes in relation to that of the 16S rRNA gene, and it provides sequence identity thresholds that correspond to different taxonomic ranks.", 
  "affiliations": [
    " Department of Biogeochemistry Max Planck Institute for Terrestrial Microbiology "
  ], 
  "grants": [
    "Funding: Yongkyu Kim received a postdoctoral fellowship of the Max Planck Society."
  ], 
  "sourcelinks": [
    "https://github.com/outbig/DAFGA"
  ], 
  "acks": " ", 
  "authors": [
    " Yongkyu Kim", 
    " Werner Liesack", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "clustering", 
      "taxonomically", 
      "dafga", 
      "genes", 
      "bioinformatics", 
      "sequencing"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v3.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/outbig/DAFGA/master/COPYING"
      }
    ], 
    "name": "DAFGA", 
    "contributors": [
      {
        "contributions": 13, 
        "html_url": "https://github.com/outbig"
      }
    ], 
    "versions": [], 
    "created_at": "2014-04-10T11:41:42Z", 
    "updated_at": "2015-03-31T12:56:07Z", 
    "languages": [
      "Python", 
      "Gnuplot"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/outbig"
      }, 
      {
        "html_url": "https://github.com/bioinfonm"
      }
    ], 
    "owner": "https://github.com/outbig", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-06-18T06:14:23Z"
}{
  "doi": "10.1093/bioinformatics/btu703", 
  "name": "DANN a deep learning approach for annotating the pathogenicity of genetic variants", 
  "links": [
    "https://github", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://cbcl.ics.uci.edu"
  ], 
  "title": "DANN: a deep learning approach for annotating the pathogenicity of genetic variants", 
  "toolName": "github", 
  "abstract": "Annotating genetic variants, especially non-coding variants, for the purpose of identifying pathogenic variants remains a challenge. Combined annotation-dependent depletion (CADD) is an algorithm designed to annotate both coding and non-coding variants, and has been shown to outper-form other annotation algorithms. CADD trains a linear kernel support vector machine (SVM) to differentiate evolutionarily derived, likely benign, alleles from simulated, likely deleterious, variants. However, SVMs cannot capture non-linear relationships among the features, which can limit performance. To address this issue, we have developed DANN. DANN uses the same feature set and training data as CADD to train a deep neural network (DNN). DNNs can capture non-linear relationships among features and are better suited than SVMs for problems with a large number of samples and features. We exploit Compute Unified Device Architecture-compatible graphics processing units and deep learning techniques such as dropout and momentum training to accelerate the DNN training. DANN achieves about a 19% relative reduction in the error rate and about a 14% relative increase in the area under the curve (AUC) metric over CADD's SVM methodology.", 
  "summary": "DANN uses the same feature set and training data as CADD to train a deep neural network (DNN).\nCADD trains a linear kernel support vector machine (SVM) to separate observed genetic variants from simulated genetic variants.\nROC curves comparing performances of the neural network (DANN), support vector machine (SVM), and logistic regression (LR) models in discriminating (a) `simulated' variants from `observed' variants in the testing set and (b) pathogenic ClinVar variants from likely benign ESP alleles (DAF  5%)", 
  "affiliations": [
    " Department of Computer Science"
  ], 
  "grants": [
    "Funding\nNational Institute of Biomedical Imaging and Bioengineering, National Research Service Award (EB009418) from the University of California, Irvine, Center for Complex Biological Systems (NIH R01HG006870)."
  ], 
  "sourcelinks": [
    "https://github", 
    "https://cbcl.ics.uci.edu"
  ], 
  "acks": " The authors gratefully acknowledge Martin Kircher for helping us understand CADD and providing the datasets. ", 
  "authors": [
    " Daniel Quang", 
    " Yifei Chen", 
    " Xiaohui Xie"
  ], 
  "keyWords": [
    [
      "training", 
      "features", 
      "performances", 
      "functional", 
      "dann", 
      "variants", 
      "annotations", 
      "linearly"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "BSD 3-clause \"New\" or \"Revised\" License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/michael/github/master/LICENSE"
      }
    ], 
    "name": "github", 
    "contributors": [
      {
        "contributions": 88, 
        "html_url": "https://github.com/AurelioDeRosa"
      }, 
      {
        "contributions": 44, 
        "html_url": "https://github.com/clayreimann"
      }, 
      {
        "contributions": 41, 
        "html_url": "https://github.com/ingalls"
      }, 
      {
        "contributions": 38, 
        "html_url": "https://github.com/aendrew"
      }, 
      {
        "contributions": 31, 
        "html_url": "https://github.com/mattpass"
      }, 
      {
        "contributions": 12, 
        "html_url": "https://github.com/darvin"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/iamdanfox"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/coderaiser"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/ctalau"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/STRd6"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/captn3m0"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/jlord"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/knsh14"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/mtscout6"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/maxogden"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/randalpinto"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/raphink"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/tricknotes"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/ele828"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/kpdecker"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/tristen"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/PeterDaveHello"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/arosenberg01"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/BernhardBezdek"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/mscdex"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/cassioscabral"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/dafortune"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/klcodanr"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/divergentdave"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/incrop"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.3.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.3.0", 
        "name": "v2.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.2.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.2.0", 
        "name": "v2.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.1.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.1.0", 
        "name": "v2.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.0.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.0.0", 
        "name": "v2.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.3.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.3.0", 
        "name": "v1.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.2.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.2.1", 
        "name": "v1.2.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.2.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.2.0", 
        "name": "v1.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.1.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.1.0", 
        "name": "v1.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.0.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.0.0", 
        "name": "v1.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.7", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.7", 
        "name": "v0.10.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.6", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.6", 
        "name": "v0.10.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.5", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.5", 
        "name": "v0.10.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.4", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.4", 
        "name": "v0.10.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.3", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.3", 
        "name": "v0.10.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.2", 
        "name": "v0.10.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.1", 
        "name": "v0.10.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.0", 
        "name": "v0.10.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.9.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.9.2", 
        "name": "v0.9.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.9.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.9.0", 
        "name": "v0.9.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.8.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.8.1", 
        "name": "v0.8.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.8.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.8.0", 
        "name": "v0.8.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.7.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.7.0", 
        "name": "v0.7.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.6.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.6.0", 
        "name": "v0.6.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.2", 
        "name": "v0.5.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.1", 
        "name": "v0.5.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.0", 
        "name": "v0.5.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.4.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.4.0", 
        "name": "v0.4.0"
      }
    ], 
    "created_at": "2012-03-06T18:08:53Z", 
    "updated_at": "2016-08-09T15:55:59Z", 
    "languages": [
      "Shell", 
      "JavaScript", 
      "HTML"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/GamesDev"
      }, 
      {
        "html_url": "https://github.com/fokusferit"
      }, 
      {
        "html_url": "https://github.com/rocel"
      }, 
      {
        "html_url": "https://github.com/thinkxl"
      }, 
      {
        "html_url": "https://github.com/schlagobers"
      }, 
      {
        "html_url": "https://github.com/jasonyandell"
      }, 
      {
        "html_url": "https://github.com/t0n1"
      }, 
      {
        "html_url": "https://github.com/timrchavez"
      }, 
      {
        "html_url": "https://github.com/apsrose"
      }, 
      {
        "html_url": "https://github.com/Bediako"
      }, 
      {
        "html_url": "https://github.com/kooyeed"
      }, 
      {
        "html_url": "https://github.com/PivotLogix"
      }, 
      {
        "html_url": "https://github.com/Timothee"
      }, 
      {
        "html_url": "https://github.com/esimionato"
      }, 
      {
        "html_url": "https://github.com/alixcan"
      }, 
      {
        "html_url": "https://github.com/FrediBach"
      }, 
      {
        "html_url": "https://github.com/antiface"
      }, 
      {
        "html_url": "https://github.com/ghostx2013"
      }, 
      {
        "html_url": "https://github.com/dnordstrom"
      }, 
      {
        "html_url": "https://github.com/bernardoantunes"
      }, 
      {
        "html_url": "https://github.com/petrosh"
      }, 
      {
        "html_url": "https://github.com/cloudtrends"
      }, 
      {
        "html_url": "https://github.com/cgkio"
      }, 
      {
        "html_url": "https://github.com/greenflag"
      }, 
      {
        "html_url": "https://github.com/loopByte"
      }, 
      {
        "html_url": "https://github.com/hasithaAlex"
      }, 
      {
        "html_url": "https://github.com/malei0311"
      }, 
      {
        "html_url": "https://github.com/majicmike"
      }, 
      {
        "html_url": "https://github.com/wuwenvogue"
      }, 
      {
        "html_url": "https://github.com/ingalls"
      }
    ], 
    "owner": "https://github.com/michael", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-23T00:08:46Z"
}{
  "doi": "10.1093/bioinformatics/btu631", 
  "name": "Curation integration and visualization of bacterial virulence factors in PATRIC", 
  "links": [
    "http://patricbrc.org", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://www.phidias.us/victors", 
    "http://patricbrc.org/portal/portal/patric/SpecialtyGene", 
    "http://www.mgc.ac.cn/VFs", 
    "http://www", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Databases and ontologies Curation, integration and visualization of bacterial virulence factors in PATRIC", 
  "toolName": "Databases and ontologies Curation, integration and visualization of bacterial virulence factors in PATRIC", 
  "abstract": "Motivation: We've developed a highly curated bacterial virulence factor (VF) library in PATRIC (Pathosystems Resource Integration Center, www.patricbrc.org) to support infectious disease research. Although several VF databases are available, there is still a need to incorporate new knowledge found in published experimental evidence and integrate these data with other information known for these specific VF genes, including genomic and other omics data. This integration supports the identification of VFs, comparative studies and hypothesis generation, which facilitates the understanding of virulence and pathogenicity. Results: We have manually curated VFs from six prioritized NIAID (National Institute of Allergy and Infectious Diseases) category A\u2013C bacterial pathogen genera, Mycobacterium, Salmonella, Escherichia, Shigella, Listeria and Bartonella, using published literature. This curated information on virulence has been integrated with data from genomic functional annotations, trancriptomic experiments, protein\u2013 protein interactions and disease information already present in PATRIC. Such integration gives researchers access to a broad array of information about these individual genes, and also to a suite of tools to perform comparative genomic and transcriptomics analysis that are available at PATRIC. Availability and implementation: All tools and data are freely available at PATRIC (http://patricbrc.org).", 
  "summary": "By leveraging the curated VFs, the information from both VFDB and Victors and the preexisting PATRIC resources, PATRIC's VF module enables users to access the comprehensive omics, and host-pathogen disease information about the VF genes, perform comparative genomic and transcriptomics analysis for these VFs and store the analyzed information in their private workspace.\nIn addition to manually curating data associated with virulence, we have computationally integrated data from two VF resources, allowing researches to see the homologs of all genes identified as virulent across the 22 000 genomes currently in PATRIC.", 
  "affiliations": [
    " Virginia Bioinformatics Institute Virginia Tech "
  ], 
  "grants": [
    "4 CONCLUSION AND FUTURE DIRECTIONS\nPathogenicity of bacteria continues to be an intense focus of research and funding.", 
    "Funding: This project has been funded with Federal funds from the National Institute of Allergy and Infectious Diseases, National Institutes of Health, Department of Health and Human Services, under Contract No."
  ], 
  "acks": " The authors would like to acknowledge Drs. Stephen Cammer, Joseph Gillespie and Yan Zhang for their help with the curation project and Dr. Yongqun He for kindly providing the data from Victors. ", 
  "authors": [
    " Chunhong Mao", 
    " David Abraham", 
    " Alice R Wattam", 
    " Meredith J C Wilson", 
    " Maulik Shukla", 
    " Hyun Seung Yoo", 
    " Bruno W Sobral"
  ], 
  "keyWords": [
    [
      "genomics", 
      "curating", 
      "virulence", 
      "genes", 
      "patric"
    ]
  ], 
  "sourcelinks": [
    "http://patricbrc.org/portal/portal/patric/SpecialtyGene", 
    "http://patricbrc.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu527", 
  "name": "Crossvalidation under separate sampling strong bias and how to correct it", 
  "links": [
    "http://gsp.tamu", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining Cross-validation under separate sampling: strong bias and how to correct it", 
  "toolName": "Data and text mining Cross-validation under separate sampling: strong bias and how to correct it", 
  "abstract": "Motivation: It is commonly assumed in pattern recognition that cross-validation error estimation is 'almost unbiased' as long as the number of folds is not too small. While this is true for random sampling, it is not true with separate sampling, where the populations are independently sampled, which is a common situation in bioinformatics. Results: We demonstrate, via analytical and numerical methods, that classical cross-validation can have strong bias under separate sampling , depending on the difference between the sampling ratios and the true population probabilities. We propose a new separate-sampling cross-validation error estimator, and prove that it satisfies an 'almost unbiased' theorem similar to that of random-sampling cross-validation. We present two case studies with previously published data, which show that the results can change drastically if the correct form of cross-validation is used. Availability and implementation: The source code in C+ +, along", 
  "summary": "We propose a new separate-sampling cross-validation error estimator, and prove that it satisfies an `almost unbiased' theorem similar to that of random-sampling crossvalidation.\ncross-validation error rate under separate sampling is E\"^ ncvkjN0=n0.\nIf k0=n0 and k1=n1, then the k0; k1-fold cross-validation estimators defined previously reduce to separate-sampling leave-\nThe x-axis corresponds to the sampling ratio r, the y-axis gives the bias, the solid lines are for the proposed separate-sampling cross-validation, the dashed lines are for classical cross-validation, and the colors code the value of c.\nIn Figure 4, the white bars are the expected classical cross-validation error rates; the shaded bars are the estimated error rates using the separate-sampling cross-validation scheme.", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was supported by NSF award CCF-0845407 (Braga-Neto) and NIH grant 2R25CA090301 (Nutrition, Biostatistics and Bioinformatics) from the National Cancer Institute."
  ], 
  "acks": " ", 
  "authors": [
    " Ulisses M Braga-Neto", 
    " Amin Zollanvari", 
    " Edward R Dougherty"
  ], 
  "keyWords": [
    "error estimation", 
    [
      "estimating", 
      "errors", 
      "data", 
      "classification", 
      "samplings"
    ]
  ], 
  "sourcelinks": [
    "http://gsp.tamu"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-15T05:12:51Z"
}{
  "doi": "10.1093/bioinformatics/btv084", 
  "name": "Design of shortest doublestranded DNA sequences covering all kmers with applications to proteinbinding microarrays and synthetic enhancers", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Erratum Design of shortest double-stranded DNA sequences covering all k-mers with applications to protein-binding microarrays and synthetic enhancers", 
  "toolName": "Erratum Design of shortest double-stranded DNA sequences covering all k-mers with applications to protein-binding microarrays and synthetic enhancers", 
  "abstract": "", 
  "summary": "In Theorem 1 `if' should be replaced by `iff' and should read as follows: For odd k, an RC complete sequence s achieves the lower bound (Proposition 1) iff there exist two edge-disjoint paths with no repeating edges, corresponding to s and RC(s), that together cover all edges of the de Bruijn graph of order k  1.\n1. Initially all edges are unmarked, F  R  ;, and A  fug, an arbitrary vertex.\n4. Pick any starting vertex v  x1; .\n; xk1 from A.\n5. While there exists an unmarked edge e  x1; .\n; xk\noutgoing from v do 6.\n; xk; A  A [ fvg.\n9. Remove v from A.", 
  "affiliations": [], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " Yaron Orenstein", 
    " Ron Shamir"
  ], 
  "keyWords": [
    [
      "access", 
      "edges", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-03-25T03:18:04Z"
}{
  "doi": "10.1093/bioinformatics/btu719", 
  "name": "deML robust demultiplexing of Illumina sequences using a likelihoodbased approach", 
  "links": [
    "http://bioinf.eva", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/ws6", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Sequence analysis deML: robust demultiplexing of Illumina sequences using a likelihood-based approach", 
  "toolName": "ws6", 
  "abstract": "Motivation: Pooling multiple samples increases the efficiency and lowers the cost of DNA sequenc-ing. One approach to multiplexing is to use short DNA indices to uniquely identify each sample. After sequencing, reads must be assigned in silico to the sample of origin, a process referred to as demultiplexing. Demultiplexing software typically identifies the sample of origin using a fixed number of mismatches between the read index and a reference index set. This approach may fail or misassign reads when the sequencing quality of the indices is poor. Results: We introduce deML, a maximum likelihood algorithm that demultiplexes Illumina sequences. deML computes the likelihood of an observed index sequence being derived from a specified sample. A quality score which reflects the probability of the assignment being correct is generated for each read. Using these quality scores, even very problematic datasets can be demultiplexed and an error threshold can be set. Availability and implementation: deML is freely available for use under the GPL", 
  "summary": "After sequencing, reads must be assigned in silico to the sample of origin, a process referred to as demultiplexing.\nIncreased error rates--particularly during sequencing of the index-- can lead to a higher number of mismatches and hinders assignment to the correct sample.\nBy simulating increasing error in the indices we show that, especially at high error rates, deML with default quality cutoffs enables the user to demultiplex several fold more sequences than the vendor's default demultiplexer or other methods based on fixed mismatches.\nUsing the sample assignment provided by deML for the reads mapping to the PhiX, the rate of false assignment was computed as a function", 
  "affiliations": [
    " Department of Evolutionary Genetics Max Planck Institute for Evolutionary Anthropology "
  ], 
  "grants": [
    "Acknowledgements\nWe acknowledge the Max Planck Society for funding and Natural Sciences and Engineering Research Council of Canada (NSERC) for PGS D scholarship to G.R."
  ], 
  "sourcelinks": [
    "https://github.com/ws6", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "acks": " We acknowledge the Max Planck Society for funding and Natural Sciences and Engineering Research Council of Canada (NSERC) for PGS D scholarship to G.R. Conflict of Interest: none declared. ", 
  "authors": [
    " Gabriel Renaud", 
    " Udo Stenzel", 
    " Tomislav Maricic", 
    " Victor Wiebe", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "deml", 
      "assignments", 
      "reads", 
      "samples", 
      "bioinformatics", 
      "sequencing", 
      "demultiplexing"
    ]
  ], 
  "github_data": {
    "name": "WS6", 
    "contributors": [
      {
        "contributions": 8, 
        "html_url": "https://github.com/carmip"
      }, 
      {
        "contributions": 5, 
        "html_url": "https://github.com/solencel"
      }
    ], 
    "versions": [], 
    "created_at": "2016-05-02T19:05:25Z", 
    "updated_at": "2016-05-02T19:13:27Z", 
    "languages": [
      "JavaScript", 
      "HTML", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/carmip"
      }, 
      {
        "html_url": "https://github.com/solencel"
      }
    ], 
    "owner": "https://github.com/carmip", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-31T00:44:56Z"
}{
  "doi": "10.1093/bioinformatics/btu862", 
  "name": "DDIGin detecting diseasecausing genetic variations due to frameshifting indels and nonsense mutations employing sequence and structural properties at nucleotide and protein levels", 
  "links": [
    "http://sparks-lab.org/ddig", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetics and population analysis DDIG-in: detecting disease-causing genetic variations due to frameshifting indels and nonsense mutations employing sequence and structural properties at nucleotide and protein levels Downloaded from", 
  "toolName": "Genetics and population analysis DDIG-in: detecting disease-causing genetic variations due to frameshifting indels and nonsense mutations employing sequence and structural properties at nucleotide and protein levels Downloaded from", 
  "abstract": "Motivation: Frameshifting (FS) indels and nonsense (NS) variants disrupt the protein-coding sequence downstream of the mutation site by changing the reading frame or introducing a premature termination codon, respectively. Despite such drastic changes to the protein sequence, FS indels and NS variants have been discovered in healthy individuals. How to discriminate disease-causing from neutral FS indels and NS variants is an understudied problem. Results: We have built a machine learning method called DDIG-in (FS) based on real human genetic variations from the Human Gene Mutation Database (inherited disease-causing) and the 1000 Genomes Project (GP) (putatively neutral). The method incorporates both sequence and predicted structural features and yields a robust performance by 10-fold cross-validation and independent tests on both FS indels and NS variants. We showed that human-derived NS variants and FS indels derived from animal orthologs can be effectively employed for independent testing of our method trained on human-derived FS indels. DDIG-in (FS) achieves a Matthews correlation coefficient (MCC) of 0.59, a sensitivity of 86%, and a specificity of 72% for FS indels. Application of DDIG-in (FS) to NS variants yields essentially the same performance (MCC of 0.43) as a method that was specifically trained for NS variants. DDIG-in (FS) was shown to make a significant improvement over existing techniques. Availability and implementation: The DDIG-in web-server for predicting NS variants, FS indels, and non-frameshifting (NFS) indels is available at http://sparks-lab.org/ddig. Contact: yaoqi.zhou@griffith.edu.au", 
  "summary": "The method incorporates both sequence and predicted structural features and yields a robust performance by 10-fold cross-validation and independent tests on both FS indels and NS variants.\nBy utilizing disease-causing variants annotated in the HGMD (Stenson et al., 2014) and putatively neutral variants from the 1000 GP (McVean et al., 2010), we found that the most discriminative feature for FS indels and NS variants was the disruption of DNA conservation, rather than the disruption of protein structure as in the case of NFS indels.", 
  "affiliations": [
    " Department of Medical and Molecular Genetics Indiana University School of Medicine ", 
    " Institute of Medical Genetics Cardiff University "
  ], 
  "grants": [
    "Funding\nThis work was supported in part by National Health and Medical Research Council (1059775) of Australia to Y.Z."
  ], 
  "acks": " The authors thank Jing Hu for sharing the SIFT Indel neutral dataset. This work was supported in part by National Health and Medical Research Council (1059775) of Australia to Y.Z. NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy, and the Australian Research Council through the ICT Centre of Excellence program. The authors also gratefully acknowledge the support of the Griffith University eResearch Services Team and the use of the High Performance Computing Cluster 'Gowonda' to complete this research. This project has also been undertaken with the aid of the research cloud resources provided by the Queensland Cyber Infrastructure Foundation (QCIF). The funders had no role in study design , data collection and analysis, decision to publish, or preparation of the manuscript. Conflict of Interest: none declared. ", 
  "authors": [
    " Lukas Folkman", 
    " Yuedong Yang", 
    " Zhixiu Li", 
    " Bela Stantic", 
    " Abdul Sattar", 
    " Matthew Mort", 
    " David N Cooper", 
    " Yunlong Liu", 
    " Yaoqi Zhou"
  ], 
  "keyWords": [
    [
      "indels", 
      "proteins", 
      "methods", 
      "sequencing", 
      "variants", 
      "features"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-09T02:26:44Z"
}{
  "doi": "10.1093/bioinformatics/btv008", 
  "name": "Deep profiling of multitube flow cytometry data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Deep profiling of multitube flow cytometry data", 
  "toolName": "Deep profiling of multitube flow cytometry data", 
  "abstract": "Motivation: Deep profiling the phenotypic landscape of tissues using high-throughput flow cytom-etry (FCM) can provide important new insights into the interplay of cells in both healthy and diseased tissue. But often, especially in clinical settings, the cytometer cannot measure all the desired markers in a single aliquot. In these cases, tissue is separated into independently analysed samples , leaving a need to electronically recombine these to increase dimensionality. Nearest-neighbour (NN) based imputation fulfils this need but can produce artificial subpopulations. Clustering-based NNs can reduce these, but requires prior domain knowledge to be able to parameterize the clustering, so is unsuited to discovery settings. Results: We present flowBin, a parameterization-free method for combining multitube FCM data into a higher-dimensional form suitable for deep profiling and discovery. FlowBin allocates cells to bins defined by the common markers across tubes in a multitube experiment, then computes aggregate expression for each bin within each tube, to create a matrix of expression of all markers assayed in each tube. We show, using simulated multitube data, that flowType analysis of flowBin output reproduces the results of that same analysis on the original data for cell types of >10% abundance. We used flowBin in conjunction with classifiers to distinguish normal from cancerous cells. We used flowBin together with flowType and RchyOptimyx to profile the immunophenotypic landscape of NPM1-mutated acute myeloid leukemia, and present a series of novel cell types associated with that mutation.", 
  "summary": "This process is common for modern clinical diagnostic FCM data; especially, when immunophenotyping leukemias, where the standard method is to include the pan-leukocyte marker (CD45) in each tube, and use this in combination with right angle scattered light (side-scattered light; SSC) to identify leukemic blasts in each tube separately (Lacombe et al., 1997).\nK-means clustering with a high value for K and NN joining has been used successfully in the past for identifying cell populations in FCM data (Aghaeepour et al., 2011).\nTo compare flowBin to the per-cell NN merging of Pedreira et al., we created a small, synthetic example using real data containing peripheral blood mononuclear cells stained for CD3, CD4 and CD8.", 
  "affiliations": [
    " Terry Fox Laboratory BC Cancer Agency ", 
    " Department of Hematopathology Vancouver General Hospital "
  ], 
  "grants": [], 
  "acks": " This work was supported by Natural Sciences and Engineering Research Council, National Institutes of Health/R01EB008400, the Canadian Cancer Society, the Michael Smith Foundation for Health Research, the International Society for the Advancement of Cytometry scholars program (NA), the Canadian Institutes of Health Research/Michael Smith Foundation for Health Research scholarship for strategic training in bioinformatics (NA) and UBCs 4YF scholarship (NA). ", 
  "authors": [
    " Kieran O 'neill", 
    " Nima Aghaeepour", 
    " Jeremy Parker", 
    " Donna Hogge", 
    " Aly Karsan", 
    " Bakul Dalal", 
    " Ryan R Brinkman"
  ], 
  "keyWords": [
    [
      "data", 
      "cells", 
      "analysis", 
      "binning", 
      "flowbin", 
      "bins"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2015-01-20T01:59:41Z"
}{
  "doi": "10.1093/bioinformatics/btu341", 
  "name": "Detecting differential protein expression in largescale population proteomics", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.stanford.edu/%7eclairesr/software.html"
  ], 
  "title": "Gene expression Detecting differential protein expression in large-scale population proteomics", 
  "toolName": "Gene expression Detecting differential protein expression in large-scale population proteomics", 
  "abstract": "Motivation: Mass spectrometry (MS)-based high-throughput quantitative proteomics shows great potential in large-scale clinical bio-marker studies, identifying and quantifying thousands of proteins in biological samples. However, there are unique challenges in analyzing the quantitative proteomics data. One issue is that the quantification of a given peptide is often missing in a subset of the experiments, especially for less abundant peptides. Another issue is that different MS experiments of the same study have significantly varying numbers of peptides quantified, which can result in more missing peptide abundances in an experiment that has a smaller total number of quantified peptides. To detect as many biomarker proteins as possible, it is necessary to develop bioinformatics methods that appropriately handle these challenges. Results: We propose a Significance Analysis for Large-scale Proteomics Studies (SALPS) that handles missing peptide intensity values caused by the two mechanisms mentioned above. Our model has a robust performance in both simulated data and prote-omics data from a large clinical study. Because varying patients' sample qualities and deviating instrument performances are not avoidable for clinical studies performed over the course of several years, we believe that our approach will be useful to analyze large-scale clinical proteomics data. Availability and Implementation: R codes for SALPS are available at", 
  "summary": "Results: We propose a Significance Analysis for Large-scale Proteomics Studies (SALPS) that handles missing peptide intensity values caused by the two mechanisms mentioned above.\nThis model reduced to a probit regression when all peptide intensity values in one group were missing, while it reduced to a linear regression when all peptides intensity values were present.\nFor example, in the simulation dataset, where b=0:50; 1=0:75; 2=0:50, the numbers of truly differentially expressed proteins detected by LinearI and LinearC are 475% of the true differential proteins detected by SALPS, but this percentage reduced to550% for Karpievitch et al.", 
  "affiliations": [
    " Massachusetts General Hospital Harvard Medical School ", 
    " Stanford Genome Technology Center Stanford University ", 
    " Biological Sciences Division and Environmental Molecular Sciences Laboratory Pacific Northwest National Laboratory "
  ], 
  "grants": [
    "Funding: This research was supported by National Institutes of Health grants (T32-GM007035 to R.G.T., R01-GM101401 to R.G.T.", 
    "and Shriners Research Grant (85500-BOS to W.X.)."
  ], 
  "acks": " We thank R. Tibshirani, M. Monroe, O. Vitek, J. Seok, W. Xu, H. Gao and A. Kaushal for helpful discussion. In particular, we wish to acknowledge the efforts of many individuals at participating institutions of the Inflammation and Host Response to Injury Program that generated the human monocyte proteomics data reported here. ", 
  "authors": [
    " So Young Ryu", 
    " Wei-Jun Qian", 
    " David G Camp", 
    " Richard D Smith", 
    " Ronald G Tompkins", 
    " Ronald W Davis", 
    " Wenzhong Xiao"
  ], 
  "keyWords": [
    [
      "proteomics", 
      "peptides", 
      "values", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://www.stanford.edu/%7eclairesr/software.html"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-06-14T00:23:03Z"
}{
  "doi": "10.1093/bioinformatics/btu722", 
  "name": "Detecting differential peaks in ChIPseq signals with ODIN", 
  "links": [
    "http://costalab.org/wp/odin", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/taoliu/MACS"
  ], 
  "title": "Genome analysis Detecting differential peaks in ChIP-seq signals with ODIN", 
  "toolName": "MACS", 
  "abstract": "Motivation: Detection of changes in deoxyribonucleic acid (DNA)\u2013 protein interactions from ChIP-seq data is a crucial step in unraveling the regulatory networks behind biological processes. The simplest variation of this problem is the differential peak calling (DPC) problem. Here, one has to find genomic regions with ChIP-seq signal changes between two cellular conditions in the interaction of a protein with DNA. The great majority of peak calling methods can only analyze one ChIP-seq signal at a time and are unable to perform DPC. Recently, a few approaches based on the combination of these peak callers with statistical tests for detecting differential digital expression have been proposed. However, these methods fail to detect detailed changes of protein\u2013DNA interactions. Results: We propose an One-stage DIffereNtial peak caller (ODIN); an Hidden Markov Model-based approach to detect and analyze differential peaks (DPs) in pairs of ChIP-seq data. ODIN performs genomic signal processing, peak calling and p-value calculation in an integrated framework. We also propose an evaluation methodology to compare ODIN with competing methods. The evaluation method is based on the association of DPs with expression changes in the same cellular conditions. Our empirical study based on several ChIP-seq experiments from transcription factors, histone modifications and simulated data shows that ODIN outperforms considered competing methods in most scenarios. Availability and implementation: http://costalab.org/wp/odin.", 
  "summary": "These twostage differential peak callers (DPC) first combine peaks that are called on individual ChIP-seq conditions using SPCs. Next, they count the number of reads for each candidate peak, perform signal normalization and apply statistical tests assuming a differential count model like EdgeR (Robinson et al., 2010) or DESeq (Anders and Huber, 2010).\nThese results are further supported with simulated data, where ODIN outperforms competing methods on scenarios with few reads and complex DPs. Moreover, we present the first approach to evaluate DPs methods, which is based on the association of DPs with expression changes in the same cellular conditions; and a methodology to simulate pairs of ChIP-seq read libraries with DPs. Calling DPs is an extremely important but so far poorly explored problem of ChIP-seq analysis.", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was supported by the Interdisciplinary Center for Clinical Research (IZKF Aachen), RWTH Aachen University Medical School, Aachen, Germany; the Excellence Initiative of the German Federal and State Governments and the German Research Foundation through Grant GSC 111;\n\n3474\n\n\fDetecting DPs in ChIP-seq signals with ODIN\n\nand the START-program (AZ 22/13) of the Faculty of Medicine, RWTH Aachen."
  ], 
  "sourcelinks": [
    "https://github.com/taoliu/MACS"
  ], 
  "acks": " ", 
  "authors": [
    " Manuel Allhoff", 
    " Kristin Ser", 
    " Heike Chauvistr", 
    " Qiong Lin", 
    " Martin Zenke", 
    " Ivan G Costa"
  ], 
  "keyWords": [
    [
      "methods", 
      "genomics", 
      "data", 
      "peaks", 
      "signaling"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "BSD 3-clause \"New\" or \"Revised\" License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/taoliu/MACS/master/COPYING"
      }
    ], 
    "name": "MACS", 
    "contributors": [
      {
        "contributions": 397, 
        "html_url": "https://github.com/taoliu"
      }, 
      {
        "contributions": 154, 
        "html_url": "https://github.com/benjschiller"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/bgruening"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/mr-c"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/ghuls"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/jayhesselberth"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/daler"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/humburg"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/liqingtian"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.10_6_6_2012", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.10_6_6_2012", 
        "name": "v2.0.10_6_6_2012"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.9", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.9", 
        "name": "v2.0.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.8", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.8", 
        "name": "v2.0.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.7", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.7", 
        "name": "v2.0.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.6", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.6", 
        "name": "v2.0.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.5", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.5", 
        "name": "v2.0.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.4", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.4", 
        "name": "v2.0.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.3", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.3", 
        "name": "v2.0.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2.0.2", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2.0.2", 
        "name": "v2.0.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v2", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v2", 
        "name": "v2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v1.4.2", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v1.4.2", 
        "name": "v1.4.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v1.4.1", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v1.4.1", 
        "name": "v1.4.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/v1.4.0", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/v1.4.0", 
        "name": "v1.4.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/tag_at_May_31_2012", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/tag_at_May_31_2012", 
        "name": "tag_at_May_31_2012"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/initial_merging_by_Ben", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/initial_merging_by_Ben", 
        "name": "initial_merging_by_Ben"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/2015.4.20", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/2015.4.20", 
        "name": "2015.4.20"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/2.1.0.20140616", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/2.1.0.20140616", 
        "name": "2.1.0.20140616"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/1.4rc2", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/1.4rc2", 
        "name": "1.4rc2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/taoliu/MACS/zipball/1.4beta", 
        "tarball_url": "https://api.github.com/repos/taoliu/MACS/tarball/1.4beta", 
        "name": "1.4beta"
      }
    ], 
    "created_at": "2011-03-02T19:40:25Z", 
    "updated_at": "2016-08-05T20:42:26Z", 
    "languages": [
      "Python", 
      "Shell", 
      "C", 
      "Groff", 
      "Makefile"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/taoliu"
      }, 
      {
        "html_url": "https://github.com/jmadzo"
      }, 
      {
        "html_url": "https://github.com/qinqian"
      }, 
      {
        "html_url": "https://github.com/tianping"
      }, 
      {
        "html_url": "https://github.com/pveber"
      }, 
      {
        "html_url": "https://github.com/raygozag"
      }, 
      {
        "html_url": "https://github.com/davidliwei"
      }, 
      {
        "html_url": "https://github.com/yaomin"
      }, 
      {
        "html_url": "https://github.com/jdiez"
      }, 
      {
        "html_url": "https://github.com/christear"
      }, 
      {
        "html_url": "https://github.com/minghuiwang1981"
      }, 
      {
        "html_url": "https://github.com/jbyars"
      }, 
      {
        "html_url": "https://github.com/hsmorikawa"
      }, 
      {
        "html_url": "https://github.com/eco32i"
      }, 
      {
        "html_url": "https://github.com/bgruening"
      }, 
      {
        "html_url": "https://github.com/liqingtian"
      }, 
      {
        "html_url": "https://github.com/topkent"
      }, 
      {
        "html_url": "https://github.com/jcchai"
      }, 
      {
        "html_url": "https://github.com/xzt41"
      }, 
      {
        "html_url": "https://github.com/jerryhuang01"
      }, 
      {
        "html_url": "https://github.com/hellomaalex"
      }, 
      {
        "html_url": "https://github.com/dbrg77"
      }, 
      {
        "html_url": "https://github.com/vogelwk"
      }, 
      {
        "html_url": "https://github.com/RGunning"
      }, 
      {
        "html_url": "https://github.com/yfu"
      }, 
      {
        "html_url": "https://github.com/IvoryC"
      }, 
      {
        "html_url": "https://github.com/IanCodes"
      }, 
      {
        "html_url": "https://github.com/saeedsaberi"
      }, 
      {
        "html_url": "https://github.com/your-highness"
      }, 
      {
        "html_url": "https://github.com/Elisaby"
      }
    ], 
    "owner": "https://github.com/taoliu", 
    "homepage": "http://liulab.dfci.harvard.edu/MACS/"
  }, 
  "technologies": [
    "Python", 
    "R"
  ], 
  "dateCreated": "2014-11-05T05:28:21Z"
}{
  "doi": "10.1093/bioinformatics/btu699", 
  "name": "Detecting time periods of differential gene expression using Gaussian processes an application to endothelial cells exposed to radiotherapy dose fraction", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Detecting time periods of differential gene expression using Gaussian processes: an application to endothelial cells exposed to radiotherapy dose fraction Downloaded from", 
  "toolName": "Detecting time periods of differential gene expression using Gaussian processes: an application to endothelial cells exposed to radiotherapy dose fraction Downloaded from", 
  "abstract": "Motivation: Identifying the set of genes differentially expressed along time is an important task in two-sample time course experiments. Furthermore, estimating at which time periods the differential expression is present can provide additional insight into temporal gene functions. The current differential detection methods are designed to detect difference along observation time intervals or on single measurement points, warranting dense measurements along time to characterize the full temporal differential expression patterns. Results: We propose a novel Bayesian likelihood ratio test to estimate the differential expression time periods. Applying the ratio test to systems of genes provides the temporal response timings and durations of gene expression to a biological condition. We introduce a novel non-stationary Gaussian process as the underlying expression model, with major improvements on model fitness on perturbation and stress experiments. The method is robust to uneven or sparse measurements along time. We assess the performance of the method on realistically simulated dataset and compare against state-of-the-art methods. We additionally apply the method to the analysis of primary human endothelial cells under an ionizing radiation stress to study the transcriptional perturbations over 283 measured genes in an attempt to better understand the role of endothelium in both normal and cancer tissues during radiotherapy. As a result, using the cascade of differential expression periods, domain literature and gene enrichment analysis, we gain insights into the dynamic response of endothelial cells to irradiation. Availability and implementation: R package 'nsgp' is available at www.ibisc.fr/en/logiciels_arobas Contact", 
  "summary": "Detecting time periods of differential gene expression using Gaussian processes: an application to endothelial cells exposed to radiotherapy dose fraction\nWe consider the Gaussian process regression (GPR) models, which have been commonly applied to model time course gene expression (Schliep et al., 2005; Lawrence et al., 2007), and are an apt model for likelihood ratio estimation (Stegle et al., 2010).\nWe model the gene expression using the GPR models and estimate the time periods of differential gene expression under irradiation.\nIn this article, we have proposed a novel Bayesian likelihood ratio test for detecting time-periods of differential gene expression in time course data.", 
  "affiliations": [
    " AMIB UMR CNRS 8623 INRIA-Saclay LRI Universit\u00e9 Paris Sud ", 
    " IBISC Universite d'E \u00b4 vry Val d'Essonne ", 
    " Institut de Radioprotection et de S\u00fb ret\u00e9 Nucl\u00e9 aire LRTE "
  ], 
  "grants": [
    "Funding\nThis work was supported by Electricite de France (Groupe Gestion ProjetRadioprotection) and Institut de Radioprotec-tion et de Su^ rete nucleaire (programme ROSIRIS)."
  ], 
  "acks": " ", 
  "authors": [
    " Markus Heinonen", 
    " Olivier Guipaud", 
    " Fabien Milliat", 
    " Val\u00e9 Rie Buard", 
    " B\u00e9 Atrice Micheau", 
    " Georges Tarlet", 
    " Marc Benderitter", 
    " Farida Zehraoui", 
    " Florence D 'alch\u00e9-Buc", 
    " "
  ], 
  "keyWords": [
    "differential gene expression", 
    "genes differentially expressed", 
    [
      "differentiated", 
      "cells", 
      "timings", 
      "times", 
      "modelling", 
      "expressions"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-30T05:01:31Z"
}{
  "doi": "10.1093/bioinformatics/btu823", 
  "name": "Detection of circular permutations within protein structures using CECP", 
  "links": [
    "http://biojava.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.rcsb.org/pdb/workbench/workbench.do", 
    "http://github.com/biojava/biojava"
  ], 
  "title": "Detection of circular permutations within protein structures using CE-CP", 
  "toolName": "biojava", 
  "abstract": "Motivation: Circular permutation is an important type of protein rearrangement. Natural circular permutations have implications for protein function, stability and evolution. Artificial circular permutations have also been used for protein studies. However, such relationships are difficult to detect for many sequence and structure comparison algorithms and require special consideration. Results: We developed a new algorithm, called Combinatorial Extension for Circular Permutations (CE-CP), which allows the structural comparison of circularly permuted proteins. CE-CP was designed to be user friendly and is integrated into the RCSB Protein Data Bank. It was tested on two collections of circularly permuted proteins. Pairwise alignments can be visualized both in a desktop application or on the web using Jmol and exported to other programs in a variety of formats. Availability and implementation: The CE-CP algorithm can be accessed through the RCSB website at", 
  "summary": "Detection of circular permutations within protein structures using CE-CP\nResults: We developed a new algorithm, called Combinatorial Extension for Circular Permutations (CE-CP), which allows the structural comparison of circularly permuted proteins.\nHere we describe a method, Combinatorial Extension with Circular Permutations (CE-CP), for the identification of circular permutations based on protein structure.\nAll 11 pairs of circularly permuted proteins from the dataset were correctly identified by CE-CP, with most residues matching the reference alignment within 04 residues.\nCE-CP is a readily available and easy to use tool for detecting circular permutations from protein structures.\n(2001) Circularly permuted proteins in the protein structure database.", 
  "affiliations": [
    " San Diego Supercomputer Center RCSB Protein Data Bank University of California "
  ], 
  "grants": [
    "Funding\nThis work was supported by the National Science Foundation [grant number DBI-1338415]; the Intramural Research Program of the National Center for Biotechnology Information, National Library of Medicine; National Institutes of Health [grant number T32GM8806]; and the Department of Energy."
  ], 
  "sourcelinks": [
    "http://github.com/biojava/biojava", 
    "http://biojava.org"
  ], 
  "acks": " We would like to thank Guido Capitani for help proofreading the manuscript, and Wei-Cheng Lo and Ping-Chiang Lyu for providing access to the CPDB alignments. This work was supported by the National Science Foundation ", 
  "authors": [
    " Spencer E Bliven", 
    " Philip E Bourne", 
    " Andreas Prlic\u00b43prlic\u00b4prlic\u00b43"
  ], 
  "keyWords": [
    "protein structures", 
    "circular permutations", 
    "circularly permuted", 
    [
      "alignments", 
      "structural", 
      "proteins", 
      "permutation"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU Lesser General Public License v2.1"
      }, 
      {
        "link": "https://raw.githubusercontent.com/biojava/biojava/master/LICENSE"
      }
    ], 
    "name": "biojava", 
    "contributors": [
      {
        "contributions": 2036, 
        "html_url": "https://github.com/andreasprlic"
      }, 
      {
        "contributions": 509, 
        "html_url": "https://github.com/josemduarte"
      }, 
      {
        "contributions": 468, 
        "html_url": "https://github.com/sbliven"
      }, 
      {
        "contributions": 439, 
        "html_url": "https://github.com/lafita"
      }, 
      {
        "contributions": 196, 
        "html_url": "https://github.com/abradle"
      }, 
      {
        "contributions": 179, 
        "html_url": "https://github.com/dmyersturnbull"
      }, 
      {
        "contributions": 104, 
        "html_url": "https://github.com/jjgao"
      }, 
      {
        "contributions": 89, 
        "html_url": "https://github.com/markchapman"
      }, 
      {
        "contributions": 88, 
        "html_url": "https://github.com/andrewyatz"
      }, 
      {
        "contributions": 75, 
        "html_url": "https://github.com/pwrose"
      }, 
      {
        "contributions": 70, 
        "html_url": "https://github.com/emckee2006"
      }, 
      {
        "contributions": 63, 
        "html_url": "https://github.com/heuermh"
      }, 
      {
        "contributions": 57, 
        "html_url": "https://github.com/willishf"
      }, 
      {
        "contributions": 57, 
        "html_url": "https://github.com/kohchuanhock"
      }, 
      {
        "contributions": 54, 
        "html_url": "https://github.com/paolopavan"
      }, 
      {
        "contributions": 54, 
        "html_url": "https://github.com/aalhossary"
      }, 
      {
        "contributions": 47, 
        "html_url": "https://github.com/foisys"
      }, 
      {
        "contributions": 40, 
        "html_url": "https://github.com/ucarion"
      }, 
      {
        "contributions": 29, 
        "html_url": "https://github.com/homiak"
      }, 
      {
        "contributions": 26, 
        "html_url": "https://github.com/benjamintboyle"
      }, 
      {
        "contributions": 21, 
        "html_url": "https://github.com/julesjacobsen"
      }, 
      {
        "contributions": 20, 
        "html_url": "https://github.com/jgrzebyta"
      }, 
      {
        "contributions": 19, 
        "html_url": "https://github.com/larsonmattr"
      }, 
      {
        "contributions": 19, 
        "html_url": "https://github.com/brandstaetter"
      }, 
      {
        "contributions": 17, 
        "html_url": "https://github.com/hollandr"
      }, 
      {
        "contributions": 11, 
        "html_url": "https://github.com/m-ezzat"
      }, 
      {
        "contributions": 11, 
        "html_url": "https://github.com/sroughley"
      }, 
      {
        "contributions": 9, 
        "html_url": "https://github.com/d-cameron"
      }, 
      {
        "contributions": 9, 
        "html_url": "https://github.com/gwaldon"
      }, 
      {
        "contributions": 8, 
        "html_url": "https://github.com/edlunde-dnastar"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/svn-trunk", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/svn-trunk", 
        "name": "svn-trunk"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/rcsb_1005", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/rcsb_1005", 
        "name": "rcsb_1005"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/rcsb_1002", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/rcsb_1002", 
        "name": "rcsb_1002"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/first_version", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/first_version", 
        "name": "first_version"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-5.0.0-alpha5", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-5.0.0-alpha5", 
        "name": "biojava-5.0.0-alpha5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-5.0.0-alpha4", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-5.0.0-alpha4", 
        "name": "biojava-5.0.0-alpha4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-5.0.0-alpha3", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-5.0.0-alpha3", 
        "name": "biojava-5.0.0-alpha3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-5.0.0-alpha2", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-5.0.0-alpha2", 
        "name": "biojava-5.0.0-alpha2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-5.0.0-alpha1", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-5.0.0-alpha1", 
        "name": "biojava-5.0.0-alpha1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-4.2.4", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-4.2.4", 
        "name": "biojava-4.2.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-4.2.3", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-4.2.3", 
        "name": "biojava-4.2.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-4.2.2", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-4.2.2", 
        "name": "biojava-4.2.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-4.2.1", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-4.2.1", 
        "name": "biojava-4.2.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-4.2.0", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-4.2.0", 
        "name": "biojava-4.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-4.1.0", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-4.1.0", 
        "name": "biojava-4.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-4.0.0", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-4.0.0", 
        "name": "biojava-4.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.1.0", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.1.0", 
        "name": "biojava-3.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0.8", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0.8", 
        "name": "biojava-3.0.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0.7", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0.7", 
        "name": "biojava-3.0.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0.6", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0.6", 
        "name": "biojava-3.0.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0.5", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0.5", 
        "name": "biojava-3.0.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0.4", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0.4", 
        "name": "biojava-3.0.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0.3", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0.3", 
        "name": "biojava-3.0.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0.2", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0.2", 
        "name": "biojava-3.0.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0.1", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0.1", 
        "name": "biojava-3.0.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0", 
        "name": "biojava-3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0-beta4", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0-beta4", 
        "name": "biojava-3.0-beta4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0-beta3", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0-beta3", 
        "name": "biojava-3.0-beta3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0-beta1", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0-beta1", 
        "name": "biojava-3.0-beta1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/biojava/biojava/zipball/biojava-3.0-alpha6", 
        "tarball_url": "https://api.github.com/repos/biojava/biojava/tarball/biojava-3.0-alpha6", 
        "name": "biojava-3.0-alpha6"
      }
    ], 
    "created_at": "2013-04-03T16:27:17Z", 
    "updated_at": "2016-08-09T09:15:35Z", 
    "languages": [
      "Python", 
      "Shell", 
      "HTML", 
      "Java"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/andreasprlic"
      }, 
      {
        "html_url": "https://github.com/sbliven"
      }, 
      {
        "html_url": "https://github.com/heuermh"
      }, 
      {
        "html_url": "https://github.com/dasmoth"
      }, 
      {
        "html_url": "https://github.com/markchapman"
      }, 
      {
        "html_url": "https://github.com/rfinkers"
      }, 
      {
        "html_url": "https://github.com/draeger"
      }, 
      {
        "html_url": "https://github.com/hwillis"
      }, 
      {
        "html_url": "https://github.com/homiak"
      }, 
      {
        "html_url": "https://github.com/aalhossary"
      }, 
      {
        "html_url": "https://github.com/footie11"
      }, 
      {
        "html_url": "https://github.com/gwaldon"
      }, 
      {
        "html_url": "https://github.com/hollandr"
      }, 
      {
        "html_url": "https://github.com/pwrose"
      }, 
      {
        "html_url": "https://github.com/foisys"
      }, 
      {
        "html_url": "https://github.com/willishf"
      }, 
      {
        "html_url": "https://github.com/markusgumbel"
      }, 
      {
        "html_url": "https://github.com/suvo"
      }, 
      {
        "html_url": "https://github.com/leonqli"
      }, 
      {
        "html_url": "https://github.com/emckee2006"
      }, 
      {
        "html_url": "https://github.com/josemduarte"
      }, 
      {
        "html_url": "https://github.com/phidias51"
      }, 
      {
        "html_url": "https://github.com/andylaw"
      }, 
      {
        "html_url": "https://github.com/brianrepko"
      }, 
      {
        "html_url": "https://github.com/foreveremain"
      }, 
      {
        "html_url": "https://github.com/darnells"
      }, 
      {
        "html_url": "https://github.com/gcapitani"
      }, 
      {
        "html_url": "https://github.com/tonywang0613"
      }, 
      {
        "html_url": "https://github.com/mkleen"
      }, 
      {
        "html_url": "https://github.com/larsonmattr"
      }
    ], 
    "owner": "https://github.com/biojava", 
    "homepage": "http://biojava.org"
  }, 
  "technologies": [], 
  "dateCreated": "2014-12-14T01:33:54Z"
}{
  "doi": "10.1093/bioinformatics/btu736", 
  "name": "Development of a robust classifier for quality control of reversephase protein arrays", 
  "links": [
    "http://www.vigenetech.com/Micro", 
    "http://rforge.r-project.org/projects/supercurve", 
    "http://bioinformatics.mdanderson.org/MOAR", 
    "http://www.r-project.org", 
    "http://bioinformatics.mdanderson.org/main/OOMPA:Overview", 
    "http://www", 
    "http://bioinformatics.mdanderson.org/OOMPA", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining Development of a robust classifier for quality control of reverse-phase protein arrays", 
  "toolName": "Data and text mining Development of a robust classifier for quality control of reverse-phase protein arrays", 
  "abstract": "Motivation: High-throughput reverse-phase protein array (RPPA) technology allows for the parallel measurement of protein expression levels in approximately 1000 samples. However, the many steps required in the complex protocol (sample lysate preparation, slide printing, hybridization, washing and amplified detection) may create substantial variability in data quality. We are not aware of any other quality control algorithm that is tuned to the special characteristics of RPPAs. Results: We have developed a novel classifier for quality control of RPPA experiments using a generalized linear model and logistic function. The outcome of the classifier, ranging from 0 to 1, is defined as the probability that a slide is of good quality. After training, we tested the classifier using two independent validation datasets. We conclude that the classifier can distinguish RPPA slides of good quality from those of poor quality sufficiently well such that normalization schemes, protein expression patterns and advanced biological analyses will not be drastically impacted by erroneous measurements or systematic variations. Availability and implementation: The classifier, implemented in the \" SuperCurve \" R package, can be freely downloaded at http://bioinformatics.mdanderson.org/main/OOMPA:Overview or http://r-forge.r-project.org/projects/supercurve/. The data used to develop and validate the classifier are available at http://bioinformatics.mdanderson.org/MOAR.", 
  "summary": "To tackle that problem, we define the quality of an RPPA slide using an exhaustive set of quantitative predictors in a generalized linear model (GLM; Dobson and Barnett, 2008; Hardin and Hilbe, 2007) and a logistic function by which an outcome of a GLM calculation is transformed to the probability that the slide is of good quality.\nquality by the classifier mostly had observable signals on a relatively clear background, but had either weak positive control spots or saturated sample signals that were not responsive to the sample lysate dilution (Fig. 3C).", 
  "affiliations": [
    " Department of Bioinformatics and Computational Biology", 
    " Department of Systems Biology"
  ], 
  "grants": [
    "Funding\nThis work was supported in part by the National Institutes of Health and the National Cancer Institute through the Cancer Center Support Grant to MD Anderson Cancer Center [grant number CA016672]."
  ], 
  "acks": " The authors thank Lee Ann Chastain and Rebecca I. Partida for proofreading the manuscript. ", 
  "authors": [
    " Zhenlin Ju", 
    " Wenbin Liu", 
    " Paul L Roebuck", 
    " Doris R Siwak", 
    " Nianxiang Zhang", 
    " Yiling Lu", 
    " Michael A Davies", 
    " Rehan Akbani", 
    " John N Weinstein", 
    " Gordon B Mills", 
    " Kevin R Coombes"
  ], 
  "keyWords": [
    [
      "spotted", 
      "cancer", 
      "sampling", 
      "slides", 
      "modeling", 
      "quality"
    ]
  ], 
  "sourcelinks": [
    "http://www.vigenetech.com/Micro", 
    "http://rforge.r-project.org/projects/supercurve", 
    "http://www.r-project.org", 
    "http://bioinformatics.mdanderson.org/main/OOMPA:Overview", 
    "http://www", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-11-08T01:28:35Z"
}{
  "doi": "10.1093/bioinformatics/btu519", 
  "name": "Detection of active transcription factor binding sites with the combination of DNase hypersensitivity and histone modifications", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://costalab.org/wp/dh-hmm"
  ], 
  "title": "Genome analysis Detection of active transcription factor binding sites with the combination of DNase hypersensitivity and histone modifications", 
  "toolName": "Genome analysis Detection of active transcription factor binding sites with the combination of DNase hypersensitivity and histone modifications", 
  "abstract": "Motivation: The identification of active transcriptional regulatory elements is crucial to understand regulatory networks driving cellular processes such as cell development and the onset of diseases. It has recently been shown that chromatin structure information, such as DNase I hypersensitivity (DHS) or histone modifications, significantly improves cell-specific predictions of transcription factor binding sites. However, no method has so far successfully combined both DHS and histone modification data to perform active binding site prediction. Results: We propose here a method based on hidden Markov models to integrate DHS and histone modifications occupancy for the detection of open chromatin regions and active binding sites. We have created a framework that includes treatment of genomic signals, model training and genome-wide application. In a comparative analysis, our method obtained a good trade-off between sensitivity versus specificity and superior area under the curve statistics than competing methods. Moreover, our technique does not require further training or sequence information to generate binding location predictions. Therefore, the method can be easily applied on new cell types and allow flexible downstream analysis such as de novo motif finding.", 
  "summary": "We propose here an HMM-based approach to integrate both DHS (DNase-seq) and histone modifications (ChIP-seq) for the detection of open chromatin regions and active TFBSs. We and others have previously observed that the peak-dip-peak patterns of the DHS profile happen inside the dip of the histone modification profiles (Gusma~o et al., 2012; Kundaje et al., 2012; see Fig. 1A).\nThis is the first approach combining local genomic profiles of histone modification and DHS for the detection of open chromatin and active TFBSs. We evaluate our and competing methods with public data from H1-hESC and K562 cell types.", 
  "affiliations": [
    " Max Planck Institute for Biology of Ageing Computational RNA Biology Lab and Bioinformatics Core ", 
    " Institute for Biomedical Engineering IZKF Computational Biology Research Group RWTH Aachen University Medical School "
  ], 
  "grants": [
    "Funding: Interdisciplinary Center for Clinical Research (IZKF Aachen), RWTH Aachen University Medical School, Aachen, Germany; and Brazilian research agencies: FACEPE and CNPq.", 
    "(2006) TRANSFAC and its module TRANSCompel: transcriptional gene regulation in eukaryotes."
  ], 
  "acks": " The authors would like to thank Pablo A. Jaskowiak, Sonja Haenzelmann, Manuel Allhoff, Joseph Kuo, Terry Furey and Shane Neph for providing predictions and sharing code and the anonymous referees for relevant suggestions. ", 
  "authors": [
    " Eduardo G Gusmao", 
    " Christoph Dieterich", 
    " Martin Zenke", 
    " Ivan G Costa"
  ], 
  "keyWords": [
    [
      "signals", 
      "genomics", 
      "data", 
      "methods", 
      "predictions"
    ]
  ], 
  "sourcelinks": [
    "http://costalab.org/wp/dh-hmm"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-02T07:42:41Z"
}{
  "doi": "10.1093/bioinformatics/btu563", 
  "name": "diCalIBD demographyaware inference of identitybydescent tracts in unrelated individuals", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://sourceforge.net/projects/dical-ibd"
  ], 
  "title": "Genetics and population analysis diCal-IBD: demography-aware inference of identity-by-descent tracts in unrelated individuals", 
  "toolName": "dical-ibd", 
  "abstract": "We present a tool, diCal-IBD, for detecting identity-by-descent (IBD) tracts between pairs of genomic sequences. Our method builds on a recent demographic inference method based on the coalescent with recombination, and is able to incorporate demographic information as a prior. Simulation study shows that diCal-IBD has significantly higher recall and precision than that of existing single-nucleotide polymorphism\u2013based IBD detection methods, while retaining reasonable accuracy for IBD tracts as small as 0.1 cM. Availability: http://sourceforge.net/projects/", 
  "summary": "The notion of identity-by-descent (IBD) between distantly related individuals is playing an increasing role in a variety of genetic analyses, including association mapping (Browning and Thompson, 2012), inferring past demographic history (Palamara et al., 2012; Ralph and Coop, 2013) and detecting signals of natural selection (Albrechtsen et al., 2010).\nThese methods are based on characterizing similar haplotypes [e.g. GERMLINE (Gusev et al., 2009)] or considering patterns of linkage disequilibrium [e.g. fastIBD, Refined IBD and IBDseq (Browning and Browning, 2011, 2013a, b)], but they do not explicitly model genealogical relationships between genomic sequences.\ndiCal-IBD uses a recently developed demographic inference method called diCal (Sheehan et al., 2013).", 
  "affiliations": [
    " Bioinformatics Research Centre Department of Computer Science Aarhus University ", 
    " Biophysics Graduate Group"
  ], 
  "grants": [
    "from CiBER at UC Berkeley, an NIH grant R01GM094402 (Y.S.S.)", 
    "Funding: This research is supported in part by an NSF IGERT grant (J.A.N.)"
  ], 
  "acks": " We thank Sara Sheehan, Jack Kamm, Matthias Steinr \u20ac ucken and other members of the Song group for helpful discussions. ", 
  "authors": [
    " Paula Tataru", 
    " Jasmine A Nirody", 
    " Yun S Song", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    [
      "genomics", 
      "genetics", 
      "methods", 
      "dical", 
      "populations", 
      "detection", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net/projects/dical-ibd"
  ], 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-08-22T03:08:27Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/dical-ibd/", 
    "languages": [
      "Python"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/paulatataru/", 
        "name": "Paula Tataru"
      }
    ], 
    "Development Status": [
      {
        "status": "5 - Production/Stable"
      }, 
      {
        "status": "1 - Planning"
      }
    ], 
    "license": [
      {
        "name": "BSD License"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu686", 
  "name": "DIANAalgorithmic improvements for analysis of dataindependent acquisition MS data", 
  "links": [
    "https://pypi.python.org/pypi", 
    "http://quantitativeproteomics.org/diana", 
    "https://pypi.python.org/pypi/pyprophet/0.9.1", 
    "http://quan", 
    "https://github.com/fick", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://numpy.org"
  ], 
  "title": "Gene expression DIANA\u2014algorithmic improvements for analysis of data-independent acquisition MS data", 
  "toolName": "fick", 
  "abstract": "Motivation: Data independent acquisition mass spectrometry has emerged as a reproducible and sensitive alternative in quantitative proteomics, where parsing the highly complex tandem mass spectra requires dedicated algorithms. Recently, targeted data extraction was proposed as a novel analysis strategy for this type of data, but it is important to further develop these concepts to provide quality-controlled , interference-adjusted and sensitive peptide quantification. Results: We here present the algorithm DIANA and the classifier PyProphet, which are based on new probabilistic sub-scores to classify the chromatographic peaks in targeted data-independent acquisition data analysis. The algorithm is capable of providing accurate quantitative values and increased recall at a controlled false discovery rate, in a complex gold standard dataset. Importantly, we further demonstrate increased confidence gained by the use of two complementary data-independent acquisition targeted analysis algorithms, as well as increased numbers of quantified peptide precursors in complex biological samples. Availability and implementation: DIANA is implemented in scala and python and available as open source (Apache 2.0 license) or pre-compiled binaries from http://quantitativeproteomics.org/diana. PyProphet can be installed from PyPi (https://pypi.python.org/pypi/ pyprophet).", 
  "summary": "Data-independent acquisition MS (DIA-MS) was originally used to improve peptide identification rates (Purvine et al., 2003; Plumb et al., 2006; Panchaud et al., 2009), but lately workflows using DIA-MS combined with targeted data extraction have been described in attempts to combine the reproducibility of SRM with the throughput of shotgun MS (Gillet et al., 2012; Weisbrod et al., 2012; Egertson et al., 2013).\nTargeted extraction of the spiked-in peptide traces from the 60 DIA-MS maps generated 20 520 extracted chromatograms, which were analyzed manually in the OpenSWATH publication (Ro st et al., 2014).", 
  "affiliations": [
    " Department of Clinical Sciences Lund University ", 
    " Department of Immunotechnology Lund University ", 
    " Department of Biology Institute of Molecular Systems Biology ", 
    " SIT University of Zurich ", 
    " ITS Scientific IT Services ETH Zurich "
  ], 
  "grants": [
    "were supported by the Swedish Research Council (projects 2008:3356 and 621-2012-3559), the Swedish Foundation for Strategic Research (grant FFL4), the Crafoord Foundation (grant 20100892), Stiftelsen Olle Engkvist Byggmastare, the Wallenberg Academy Fellow KAW (2012.0178) and European research council starting grant (ERC-2012-StG-309831).", 
    "Funding: J.T."
  ], 
  "sourcelinks": [
    "https://pypi.python.org/pypi", 
    "http://numpy.org", 
    "https://pypi.python.org/pypi/pyprophet/0.9.1", 
    "http://quan", 
    "https://github.com/fick", 
    "http://quantitativeproteomics.org/diana"
  ], 
  "acks": " The authors thank Ufuk Kirik for the helpful discussions on the algorithms. Funding: J.T. and J.M. were supported by the Swedish Research Council (projects 2008:3356 and 621-2012-3559), the Swedish Foundation for Strategic Research (grant FFL4), the Crafoord Foundation (grant 20100892), Stiftelsen Olle Engkvist Byggm\u20ac astare, the Wallenberg Academy Fellow KAW ", 
  "authors": [
    " Johan Teleman", 
    " Hannes L R \u20ac Ost", 
    " George Rosenberger", 
    " Uwe Schmitt", 
    " Lars Malmstr", 
    " \u20ac Om", 
    " Johan Malmstr", 
    " \u20ac Om", 
    " Fredrik Levander"
  ], 
  "keyWords": [
    [
      "diana", 
      "proteomics", 
      "peptides", 
      "data", 
      "analysis"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "Other"
      }, 
      {
        "link": "https://raw.githubusercontent.com/tumtumtum/Fickle/master/LICENSE"
      }
    ], 
    "name": "Fickle", 
    "contributors": [
      {
        "contributions": 239, 
        "html_url": "https://github.com/tumtumtum"
      }, 
      {
        "contributions": 29, 
        "html_url": "https://github.com/JeffGos"
      }, 
      {
        "contributions": 16, 
        "html_url": "https://github.com/samcook"
      }
    ], 
    "versions": [], 
    "created_at": "2013-06-05T18:14:27Z", 
    "updated_at": "2016-01-21T15:52:27Z", 
    "languages": [
      "ASP", 
      "C#", 
      "JavaScript", 
      "Pascal", 
      "Objective-C", 
      "HTML", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/tumtumtum"
      }, 
      {
        "html_url": "https://github.com/JeffGos"
      }
    ], 
    "owner": "https://github.com/tumtumtum", 
    "homepage": null
  }, 
  "technologies": [
    "R", 
    "Java"
  ], 
  "dateCreated": "2014-10-28T04:40:37Z"
}{
  "doi": "10.1093/bioinformatics/btu660", 
  "name": "Deviance residualsbased sparse PLS and sparse kernel PLS regression for censored data", 
  "links": [
    "http://cran.rproject.org/web/packages/plsRcox/index.html", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://sites.uclouvain"
  ], 
  "title": "Data and text mining Deviance residuals-based sparse PLS and sparse kernel PLS regression for censored data", 
  "toolName": "Data and text mining Deviance residuals-based sparse PLS and sparse kernel PLS regression for censored data", 
  "abstract": "Motivation: A vast literature from the past decade is devoted to relating gene profiles and subject survival or time to cancer recurrence. Biomarker discovery from high-dimensional data, such as transcrip-tomic or single nucleotide polymorphism profiles, is a major challenge in the search for more precise diagnoses. The proportional hazard regression model suggested by Cox (1972), to study the relationship between the time to event and a set of covariates in the presence of censoring is the most commonly used model for the analysis of survival data. However, like multivariate regression, it supposes that more observations than variables, complete data, and not strongly correlated variables are available. In practice, when dealing with high-dimensional data, these constraints are crippling. Collinearity gives rise to issues of over-fitting and model misidentification. Variable selection can improve the estimation accuracy by effectively identifying the subset of relevant predictors and enhance the model interpretability with parsimonious representation. To deal with both collinearity and variable selection issues, many methods based on least absolute shrinkage and selection operator penalized Cox proportional hazards have been proposed since the reference paper of Tibshirani. Regularization could also be performed using dimension reduction as is the case with partial least squares (PLS) regression. We propose two original algorithms named sPLSDR and its non-linear kernel counterpart DKsPLSDR, by using sparse PLS regression (sPLS) based on deviance residuals. We compared their predicting performance with state-of-the-art algorithms on both simulated and real reference benchmark datasets. Results: sPLSDR and DKsPLSDR compare favorably with other methods in their computational time, prediction and selectivity, as indicated by results based on benchmark datasets. Moreover, in the framework of PLS regression, they feature other useful tools, including biplots representation, or the ability to deal with missing data. Therefore, we view them as a useful addition to the toolbox of estimation and prediction methods for the widely used Cox's model in the high-dimensional and low-sample size settings. Availability and implementation: The R-package plsRcox is available on the CRAN and is maintained by Fr ed eric Bertrand. http://cran.r-project.org/web/packages/plsRcox/index.html.", 
  "summary": "Partial least squares (PLS) regression, that can be viewed as a regularization method based on dimension reduction, was developed as a chemometric tool in an attempt to find reliable predictive models with spectral data (Tenenhaus, 1998; Wold et al., 1983).\nWe benchmarked the new sPLSDR and DKsPLSDR algorithms against the following existing ones: coxpath (Park and Hastie, 2007), coxnet (Simon et al., 2011), PLS-Cox (Bastien and Tenenhaus, 2001), autoPLS-Cox (PLS-Cox with a hard-thresholding approach and automatic selection of the maximal number of components, Bastien et al., 2005), LARS-LassoDR (Segal, 2006), Cox-PLS (Nguyen and Rocke, 2002), PLSDR (Bastien, 2008), DKPLSDR (Bastien, 2008), uniCox (Tibshirani, 2009) and glcoxph (Sohn et al., 2009).", 
  "affiliations": [
    " UMR 7501 IRMA CNRS Labex IRMIA Universit e de Strasbourg ", 
    " L'Or eal Recherche & Innovation"
  ], 
  "grants": [
    "Funding: CNRS UMR 7501, INSERM EA3430, University of Strasbourg, L'Oreal, Recherche et Innovation and Labex IRMIA."
  ], 
  "acks": " The authors are grateful to the referees for their very helpful comments. They also wish to thank Caroline Chaigne and Claude Bouillon who assisted in the proofreading of the manuscript. ", 
  "authors": [
    " Philippe Bastien", 
    " Fr Ed Eric Bertrand", 
    " Nicolas Meyer", 
    " Myriam Maumy-Bertrand"
  ], 
  "keyWords": [
    [
      "data", 
      "algorithms", 
      "regressions", 
      "modelling"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-07T04:41:50Z"
}{
  "doi": "10.1093/bioinformatics/btu672", 
  "name": "Differential regulation enrichment analysis via the integration of transcriptional regulatory network and gene expression data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinfo.au.tsinghua.edu.cn/dragen"
  ], 
  "title": "Differential regulation enrichment analysis via the integration of transcriptional regulatory network and gene expression data", 
  "toolName": "Differential regulation enrichment analysis via the integration of transcriptional regulatory network and gene expression data", 
  "abstract": "Motivation: Although many gene set analysis methods have been proposed to explore associations between a phenotype and a group of genes sharing common biological functions or involved in the same biological process, the underlying biological mechanisms of identified gene sets are typically unexplained. Results: We propose a method called Differential Regulation-based enrichment Analysis for GENe sets (DRAGEN) to identify gene sets in which a significant proportion of genes have their transcriptional regulatory patterns changed in a perturbed phenotype. We conduct comprehensive simulation studies to demonstrate the capability of our method in identifying differentially regulated gene sets. We further apply our method to three human microarray expression datasets, two with hormone treated and control samples and one concerning different cell cycle phases. Results indicate that the capability of DRAGEN in identifying phenotype-associated gene sets is significantly superior to those of four existing methods for analyzing differentially expressed gene sets. We conclude that the proposed differential regulation enrichment analysis method, though exploratory in nature, complements the existing gene set analysis methods and provides a promising new direction for the interpretation of gene expression data.", 
  "summary": "Results: We propose a method called Differential Regulation-based enrichment Analysis for GENe sets (DRAGEN) to identify gene sets in which a significant proportion of genes have their transcriptional regulatory patterns changed in a perturbed phenotype.\nSpecifically, we achieve this goal by putting forward an approach called Differential Regulation based enrichment Analysis for GENe sets (DRAGEN) that integrates gene expression data and a transcriptional regulatory network to identify differentially regulated gene sets.\nExisting methods largely depend on gene expression data alone to detect differentially regulated gene sets that may be associated with a phenotype of interest, overlooking the fact that alterations of gene expression levels may actually result from the changes of regulatory patterns.", 
  "affiliations": [
    " Bioinformatics Division and Center for Synthetic & Systems Biology Department of Automation MOE Key Laboratory of Bioinformatics TNLIST Tsinghua University "
  ], 
  "grants": [
    "TRANSFAC (Matys et al., 2003) and HTRIdb (Bovolenta et al., 2012)].", 
    "(2003) TRANSFAC: transcriptional regulation, from patterns to profiles.", 
    "Funding: This research was partially supported by the National Basic Research Program of China [2012CB316504], the National High Technology Research and Development Program of China [2012AA020401], the National Science Foundation grant [DBI1262107], the National Natural Science Foundation of China [61175002]."
  ], 
  "acks": " The authors thank Prof. Xuegong Zhang, Prof. Shirley Liu and Prof. Zhiping Wen for several helpful suggestions. We are also grateful to Winston Haynes for sharing with us programs and the anonymous reviewers for their constructive comments. Funding: This research was partially supported by the National Basic Research Program of China, the National High Technology Research and Development Program of China, the National Science Foundation grant, the National Natural Science Foundation of China. Conflict of interest: none declared. ", 
  "authors": [
    " Shining Ma", 
    " Tao Jiang", 
    " Rui Jiang"
  ], 
  "keyWords": [
    [
      "differentially", 
      "genes", 
      "sets", 
      "regulatory", 
      "data", 
      "dragen"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-17T05:05:29Z"
}{
  "doi": "10.1093/bioinformatics/btu844", 
  "name": "Diskbased compression of data from genome sequencing", 
  "links": [
    "http://www.ncbi.nlm.nih.gov/sra/SRX001540", 
    "http://compression.ru/ds/ppmdj1.rar", 
    "http://www.ncbi.nlm.nih.gov/genome", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://support.illumina.com/sequencing/sequencing_software/casava.ilmn", 
    "http://sun.aei.polsl.pl/orcom"
  ], 
  "title": "Disk-based compression of data from genome sequencing", 
  "toolName": "Disk-based compression of data from genome sequencing", 
  "abstract": "Motivation: High-coverage sequencing data have significant, yet hard to exploit, redundancy. Most FASTQ compressors cannot efficiently compress the DNA stream of large datasets, since the redundancy between overlapping reads cannot be easily captured in the (relatively small) main memory. More interesting solutions for this problem are disk based, where the better of these two, from Cox et al. (2012), is based on the Burrows\u2013Wheeler transform (BWT) and achieves 0.518 bits per base for a 134.0 Gbp human genome sequencing collection with almost 45-fold coverage. Results: We propose overlapping reads compression with minimizers, a compression algorithm dedicated to sequencing reads (DNA only). Our method makes use of a conceptually simple and easily parallelizable idea of minimizers, to obtain 0.317 bits per base as the compression ratio, allowing to fit the 134.0 Gbp dataset into only 5.31 GB of space. Availability and implementation: http://sun.aei.polsl.pl/orcom under a free license.", 
  "summary": "In this article, we present a new reference-free compressor for FASTQ data, Overlapping Reads COmpression with Minimizers (ORCOM), achieving compression ratios surpassing the best known solutions.\nMoreover, ORCOM, BWT-SAP and SRcomp compress the DNA stream only, whereas the remaining compressors have full FASTQ files on the input (with fake remaining streams in case of simulated reads presented in Section 3.2), what hampers their performance in compression speed and memory use (the compression ratios are however given for the DNA stream only).\nWe presented ORCOM, a lightweight solution for grouping and compressing overlapping reads in DNA sequencing data.", 
  "affiliations": [
    " Institute of Informatics Silesian University of Technology ", 
    " Institute of Applied Computer Science Lodz University of Technology "
  ], 
  "grants": [
    "The infrastructure supported by POIG.02.03.01-24-099/13 grant: `GeCONiIUpper Silesian Center for Computational Science and Engineering'.", 
    "Funding\nThe Polish National Science Centre under the project DEC-2012/05/B/ST6/ 03148 and also the European Union from the European Social Fund within the INTERKADRA project UDAPOKL-04.01.01-00-014/10-00 (partially)."
  ], 
  "acks": " The authors thank the reviewers for helpful comments. ", 
  "authors": [
    " Szymon Grabowski", 
    " Sebastian Deorowicz", 
    " \u0141ukasz Roguski"
  ], 
  "keyWords": [
    [
      "reads", 
      "datasets", 
      "sequencers", 
      "compression"
    ]
  ], 
  "sourcelinks": [
    "http://support.illumina.com/sequencing/sequencing_software/casava.ilmn", 
    "http://compression.ru/ds/ppmdj1.rar"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-24T01:14:33Z"
}{
  "doi": "10.1093/bioinformatics/btu525", 
  "name": "DNAApp a mobile application for sequencing data analysis", 
  "links": [
    "http://creativecommons.org/licenses/by-nc/4.0", 
    "http://lifescientist.com.au/content/biotechnology/article/the-rise-of-smartphone-health-and-medical-apps1072193834", 
    "http://mobihealth", 
    "http://www.geospiza.com", 
    "https://smallbusiness.yahoo.com/advisor/smartphone-tablet-cont", 
    "http://tinyurl.com/DNAAppuser", 
    "http://www.technelysium.com", 
    "http://mobihealthnews.com/29253/surveydoctors-prefer-tablets-for-journal-articles-smartphones-for-mostother-tasks", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Sequence analysis DNAApp: a mobile application for sequencing data analysis", 
  "toolName": "Sequence analysis DNAApp: a mobile application for sequencing data analysis", 
  "abstract": "There have been numerous applications developed for decoding and visualization of ab1 DNA sequencing files for Windows and MAC platforms, yet none exists for the increasingly popular smartphone operating systems. The ability to decode sequen-cing files cannot easily be carried out using browser accessed Web tools. To overcome this hurdle, we have developed a new native app called DNAApp that can decode and display ab1 sequencing file on Android and iOS. In addition to in-built analysis tools such as reverse complementation, protein translation and searching for specific sequences, we have incorporated convenient functions that would facilitate the harnessing of online Web tools for a full range of analysis. Given the high usage of Android/iOS tablets and smartphones, such bioinformatics apps would raise productivity and facilitate the high demand for analyzing sequencing data in biomedical research.", 
  "summary": "To overcome this hurdle, we have developed a new native app called DNAApp that can decode and display ab1 sequencing file on Android and iOS.\nGiven the high usage of Android/iOS tablets and smartphones, such bioinformatics apps would raise productivity and facilitate the high demand for analyzing sequencing data in biomedical research.\nWith the recent rise of iPhone/iPad and Android devices, scientists, along with others, have been benefiting from the convenience of mobility in their everyday lives (Comstock, 2014; see http://mobihealthnews.com/29253/surveydoctors-prefer-tablets-for-journal-articles-smartphones-for-mostother-tasks/), Dufau et al., 2011; Evanko, 2010; Sutton and Fraser, 2013; see http://lifescientist.com.au/content/biotechnology/article/the-rise-of-smartphone-health-and-medical-apps1072193834).", 
  "affiliations": [
    " Agency for Science Technology, and Research (A*STAR) Bioinformatics Institute "
  ], 
  "grants": [
    "Funding: This work is funded by the Joint Council Office, Agency for Science, Technology, and Research, Singapore\nConflict of interest: none declared."
  ], 
  "acks": " The authors thank Mr Keane Lim for the writing of the DNAApp user guide. ", 
  "authors": [
    " Phi-Vu Nguyen", 
    " Chandra Shekhar Verma", 
    " Samuel ", 
    " Ken-En Gan"
  ], 
  "keyWords": [
    "sequencing data", 
    "sequence analysis", 
    [
      "sequences", 
      "dnaapp", 
      "bioinformatics", 
      "users", 
      "singapore"
    ]
  ], 
  "sourcelinks": [
    "http://www.technelysium.com"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-06T05:13:41Z"
}{
  "doi": "10.1093/bioinformatics/btu744", 
  "name": "DISOPRED3 precise disordered region predictions with annotated proteinbinding activity", 
  "links": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinf.cs.ucl.ac.uk/disopred"
  ], 
  "title": "DISOPRED3: precise disordered region predictions with annotated protein-binding activity", 
  "toolName": "DISOPRED3: precise disordered region predictions with annotated protein-binding activity", 
  "abstract": "Motivation: A sizeable fraction of eukaryotic proteins contain intrinsically disordered regions (IDRs), which act in unfolded states or by undergoing transitions between structured and unstruc-tured conformations. Over time, sequence-based classifiers of IDRs have become fairly accurate and currently a major challenge is linking IDRs to their biological roles from the molecular to the systems level. Results: We describe DISOPRED3, which extends its predecessor with new modules to predict IDRs and protein-binding sites within them. Based on recent CASP evaluation results, DISOPRED3 can be regarded as state of the art in the identification of IDRs, and our self-assessment shows that it significantly improves over DISOPRED2 because its predictions are more specific across the whole board and more sensitive to IDRs longer than 20 amino acids. Predicted IDRs are annotated as protein binding through a novel SVM based classifier, which uses profile data and additional sequence derived features. Based on benchmarking experiments with full cross-validation, we show that this predictor generates precise assignments of disordered protein binding regions and that it compares well with other publicly available tools. Availability and implementation:", 
  "summary": "The performance of the predictor based on profile data, IDR location and length and window composition was also compared with publicly available tools and with a naive approach that randomly labels the target sequence amino acids as either disordered protein binding or not with equal probability.\nThe benefits of the SVM for protein-binding region annotation is demonstrated by the massive reduction in the number of false positive assignments that would be made by tagging all predicted disordered residues as protein binding (method `DISOPRED3 no DPB SVM').", 
  "affiliations": [
    " Department of Computer Science Bioinformatics Group University College London "
  ], 
  "grants": [
    "Funding\nThis work was supported by the UK Biotechnology and Biological Sciences Research Council [Reference BB/J002925/1]."
  ], 
  "acks": " The authors kindly acknowledge the members of the Bioinformatics group for valuable and interesting discussions. In particular, they thank Dr Federico Minneci for help with setting up the online server. This work was supported by the UK Biotechnology and Biological Sciences Research Council ", 
  "authors": [
    " David T Jones", 
    " Domenico Cozzetto"
  ], 
  "keyWords": [
    [
      "structurally", 
      "proteins", 
      "predictions", 
      "disopred", 
      "sequences", 
      "idrs"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-13T05:02:10Z"
}{
  "doi": "10.1093/bioinformatics/btu684", 
  "name": "DOSE an RBioconductor package for disease ontology semantic and enrichment analysis", 
  "links": [
    "http://www.bioconductor.org/packages/release/bioc", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://goo.gl"
  ], 
  "title": "Systems biology DOSE: an R/Bioconductor package for disease ontology semantic and enrichment analysis", 
  "toolName": "Systems biology DOSE: an R/Bioconductor package for disease ontology semantic and enrichment analysis", 
  "abstract": "Disease ontology (DO) annotates human genes in the context of disease. DO is important annotation in translating molecular findings from high-throughput data to clinical relevance. DOSE is an R package providing semantic similarity computations among DO terms and genes which allows biologists to explore the similarities of diseases and of gene functions in disease perspective. Enrichment analyses including hypergeometric model and gene set enrichment analysis are also implemented to support discovering disease associations of high-throughput biological data. This allows biologists to verify disease relevance in a biological experiment and identify unexpected disease associations. Comparison among gene clusters is also supported. Availability and implementation: DOSE is released under Artistic-2.0 License. The source code and documents are freely available through Bioconductor", 
  "summary": "To address the shortcoming of lack of R/Bioconductor package that designed for computation of semantic and enrichment analyses based on DO, we present DOSE, that allows measuring semantic similarity among DO terms and genes using several information-content and graph-structure based algorithms.\nIt provides five algorithms for DO and gene semantic similarity measurements (Fig. 1A); hypergeometric test for identifying significant disease association of gene list (Fig. 1B and C); GSEA for interpreting genome wide expression profiles in disease context (Fig. 1D) and comparison of significant disease associations among different gene sets (Fig. 1E).", 
  "affiliations": [
    " College of Life Science and Technology Key Laboratory of Functional Protein Research of Guangdong Higher Education Institutes Jinan University ", 
    " Guangdong Information Center Associate Editor: Igor Jurisica "
  ], 
  "grants": [
    "Funding: This work was supported by the National Natural Science Foundation of China (21271086 to Q.-Y.H.)"
  ], 
  "acks": " ", 
  "authors": [
    " Guangchuang Yu", 
    " Li-Gen Wang", 
    " Guang-Rong Yan", 
    " Qing-Yu He"
  ], 
  "keyWords": [
    "gene functions", 
    [
      "diseases", 
      "similarities", 
      "genes", 
      "functional", 
      "dose", 
      "bioinformatics", 
      "biological"
    ]
  ], 
  "sourcelinks": [
    "http://www.bioconductor.org/packages/release/bioc"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btv028", 
  "name": "drexplorer A tool to explore doseresponse relationships and drugdrug interactions", 
  "links": [
    "http://www.R-project.org.Ritz,C", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.icesi.edu.co/CRAN/web", 
    "https://github.com/nickytong/drexplorer"
  ], 
  "title": "Data and text mining drexplorer: A tool to explore dose\u2013response relationships and drug\u2013drug interactions", 
  "toolName": "drexplorer", 
  "abstract": "Motivation: Nonlinear dose\u2013response models are primary tools for estimating the potency [e.g. half-maximum inhibitory concentration (IC) known as IC50] of anti-cancer drugs. We present drexplorer software, which enables biologists to evaluate replicate reproducibility, detect outlier data points, fit different models, select the best model, estimate IC values at different percentiles and assess drug\u2013drug interactions. drexplorer serves as a computation engine within the R environment and a graphical interface for users who do not have programming backgrounds. Availability and implementation: The drexplorer R package is freely available from GitHub at https://github.com/nickytong/drexplorer. A graphical user interface is shipped with the package.", 
  "summary": "Motivation: Nonlinear doseresponse models are primary tools for estimating the potency [e.g. half-maximum inhibitory concentration (IC) known as IC50] of anti-cancer drugs.\nWe present drexplorer software, which enables biologists to evaluate replicate reproducibility, detect outlier data points, fit different models, select the best model, estimate IC values at different percentiles and assess drugdrug interactions.\nWe developed drexplorer software in the R environment (R Core Team, 2014) to encompass several aspects of doseresponse analysis: assess reproducibility of replicated experiments, detect outlier data points, fit different models, identify the best model, estimate inhibitory concentration (IC) values and evaluate drugdrug interactions.", 
  "affiliations": [
    " Department of Biomedical Informatics The Ohio State University ", 
    " Departments of Bioinformatics and Computational Biology", 
    " Biostatistics The University of Texas MD Anderson Cancer Center ", 
    " Thoracic and Head and Neck Medical Oncology"
  ], 
  "grants": [
    "Acknowledgements\nThis project was partially supported by the NCI/NIH through the Lung SPORE (P50 CA070907) and Cancer Center Support Grant (CA016672), and by the Mary K. Chapman Foundation."
  ], 
  "sourcelinks": [
    "https://github.com/nickytong/drexplorer"
  ], 
  "acks": " This project was partially supported by the NCI/NIH through the Lung SPORE (P50 CA070907) and Cancer Center Support Grant (CA016672), and by the Mary K. Chapman Foundation. of Interest: none declared. ", 
  "authors": [
    " Pan Tong", 
    " Kevin R Coombes", 
    " Faye M Johnson", 
    " Lauren A Byers", 
    " Lixia Diao", 
    " Diane D Liu", 
    " J Jack Lee", 
    " John V Heymach", 
    " Jing Wang"
  ], 
  "keyWords": [
    [
      "doseresponse", 
      "responses", 
      "drugs", 
      "modelling", 
      "data", 
      "drexplorer"
    ]
  ], 
  "github_data": {
    "name": "drexplorer", 
    "contributors": [
      {
        "contributions": 107, 
        "html_url": "https://github.com/nickytong"
      }
    ], 
    "versions": [], 
    "created_at": "2014-02-11T00:09:41Z", 
    "updated_at": "2016-03-18T21:34:11Z", 
    "languages": [
      "R", 
      "HTML"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/nickytong"
      }, 
      {
        "html_url": "https://github.com/AshUK"
      }
    ], 
    "owner": "https://github.com/nickytong", 
    "homepage": ""
  }, 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2015-01-20T01:59:41Z"
}{
  "doi": "10.1093/bioinformatics/btu645", 
  "name": "DupliPHYWeb a web server for DupliPHY and DupliPHYML", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Genome analysis DupliPHY-Web: a web server for DupliPHY and DupliPHY-ML", 
  "toolName": "Genome analysis DupliPHY-Web: a web server for DupliPHY and DupliPHY-ML", 
  "abstract": "Gene duplication and loss are important processes in the evolution of gene families. Moreover, growth of families by duplication and retention is an important mechanism by which organisms gain new functions. Therefore the ability to infer the evolutionary histories of families is an important step in understanding the evolution of function. We have recently developed DupliPHY, a software tool to infer gene family histories using parsimony and maximum likelihood. Here, we present DupliPHY-Web a web server for DupliPHY that implements additional maximum likelihood functionality and provides users an intuitive interface to run DupliPHY. Availability and implementation: DupliPHY-Web is available at www. bioinf.manchester.ac.uk/dupliphy/", 
  "summary": "Previously, we have released the software DupliPHY (Ames et al., 2012) that provides weighted parsimony and maximum likelihood methods to accurately infer the evolutionary histories of gene families.\nDupliPHY works on gene copy number counts to infer gene family evolution and is different from tree reconciliation methods that infer duplication and loss events based on gene and species trees (Larget et al., 2010).\nAdditionally, DupliPHY-ML has been updated with a new model of gene family evolution and to make the program more configurable.\nDupliPHY-Web provides an intuitive web interface for the DupliPHY programs, enabling users to conveniently run gene family evolution analyses.", 
  "affiliations": [
    " Faculty of Life Sciences University of Manchester "
  ], 
  "grants": [
    "Funding: Funding is provided by the BBSRC: (BB/I020489/1)."
  ], 
  "acks": " The authors would like to thank David Talavera, Shaun Kandathil and Abayomi Olabode for feedback during development. ", 
  "authors": [
    " Ryan M Ames", 
    " Simon C Lovell", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "files", 
      "genomics", 
      "genes", 
      "trees", 
      "dupliphy", 
      "bioinformatics", 
      "ames"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-10-08T00:30:13Z"
}{
  "doi": "10.1093/bioinformatics/btu526", 
  "name": "DrugCellline Browser interactive canvas visualization of cancer drugcellline viability assay datasets", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.maayanlab.net/LINCS/DCB"
  ], 
  "title": "Systems biology Drug/Cell-line Browser: interactive canvas visualization of cancer drug/cell-line viability assay datasets", 
  "toolName": "Systems biology Drug/Cell-line Browser: interactive canvas visualization of cancer drug/cell-line viability assay datasets", 
  "abstract": "Recently, several high profile studies collected cell viability data from panels of cancer cell lines treated with many drugs applied at different concentrations. Such drug sensitivity data for cancer cell lines provide suggestive treatments for different types and subtypes of cancer. Visualization of these datasets can reveal patterns that may not be obvious by examining the data without such efforts. Here we introduce Drug/Cell-line Browser (DCB), an online interactive HTML5 data visualization tool for interacting with three of the recently published datasets of cancer cell lines/drug-viability studies. DCB uses clustering and canvas visualization of the drugs and the cell lines, as well as a bar graph that summarizes drug effectiveness for the tissue of origin or the cancer subtypes for single or multiple drugs. DCB can help in understanding drug response patterns and prioritizing drug/ cancer cell line interactions by tissue of origin or cancer subtype. Availability and implementation: DCB is an open source Web-based tool that is freely available at:", 
  "summary": "DCB uses clustering and canvas visualization of the drugs and the cell lines, as well as a bar graph that summarizes drug effectiveness for the tissue of origin or the cancer subtypes for single or multiple drugs.\nHere, we present Drug/Cell-line Browser (DCB), a HTML5 Web-based application that uses canvas clustering (Tan et al., 2013) to visualize three recent cell viability assay datasets: from the Cancer Cell Line Encyclopedia (CCLE) project (Barretina et al., 2012), from the Genomics of Drug Sensitivity (GDS) in cancer project (Yang et al., 2013) and from an anticancer compound study for breast cancer (Heiser et al., 2012).", 
  "affiliations": [
    " Department of Pharmacology and Systems Therapeutics Systems Biology Center New York Icahn School of Medicine at Mount Sinai ", 
    " Center for Cancer Research Massachusetts General Hospital Cancer Center Harvard Medical School "
  ], 
  "grants": [
    "Funding: This work was supported in part by grants from the NIH: R01GM098316-01A1, R01DK088541-01A1, U54HG006097-02S1 and U54CA189201 to A.M."
  ], 
  "acks": " ", 
  "authors": [
    " Qiaonan Duan", 
    " Zichen Wang", 
    " Nicolas F Fernandez", 
    " Andrew D Rouillard", 
    " Christopher M Tan", 
    " Cyril H Benes", 
    " Avi Ma 'ayan", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "cancer", 
      "similarity", 
      "drugs", 
      "cells", 
      "tissues", 
      "lines"
    ]
  ], 
  "sourcelinks": [
    "http://www.maayanlab.net/LINCS/DCB"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-07T03:03:22Z"
}{
  "doi": "10.1093/bioinformatics/btu770", 
  "name": "DISSECT an assignmentfree Bayesian discovery method for species delimitation under the multispecies coalescent", 
  "links": [
    "http://www.indriid.com", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://tree.bio.ed.ac.uk/software/beast"
  ], 
  "title": "DISSECT: an assignment-free Bayesian discovery method for species delimitation under the multispecies coalescent", 
  "toolName": "DISSECT: an assignment-free Bayesian discovery method for species delimitation under the multispecies coalescent", 
  "abstract": "Motivation: The multispecies coalescent model provides a formal framework for the assignment of individual organisms to species, where the species are modeled as the branches of the sp tree. None of the available approaches so far have simultaneously co-estimated all the relevant parameters in the model, without restricting the parameter space by requiring a guide tree and/or prior assignment of individuals to clusters or species. Results: We present DISSECT, which explores the full space of possible clusterings of individuals and species tree topologies in a Bayesian framework. It uses an approximation to avoid the need for reversible-jump Markov Chain Monte Carlo, in the form of a prior that is a modification of the birth\u2013death prior for the species tree. It incorporates a spike near zero in the density for node heights. The model has two extra parameters: one controls the degree of approximation and the second controls the prior distribution on the numbers of species. It is implemented as part of BEAST and requires only a few changes from a standard *BEAST analysis. The method is evaluated on simulated data and demonstrated on an empirical dataset. The method is shown to be insensitive to the degree of approximation, but quite sensitive to the second parameter, suggesting that large numbers of sequences are needed to draw firm conclusions. Availability and implementation:", 
  "summary": "All current MSCSD methods are either heuristic (e.g. O'Meara, 2010), dependent on a guide tree (e.g. Satler et al., 2013; Yang and Rannala, 2010; note however that a paper by Yang and Rannala [2014] appeared during the revision of this article, where the requirement of a user-supplied guide tree is eliminated) or are validation methods, which require prior assignment of individuals to clusters or species.\nThe boxplots show the values of the error metric over 50 replicates as the number of genes (G  3, 9, or 27), the amount of lineage sorting (shortest branches of species tree a  0.2, b  1.0, c  5.0 coalescent units) and prior on x (B82: Beta $ (8,2) hyperprior, k  3, 5 or 8) vary.", 
  "affiliations": [
    " Department of Biological and Environmental Sciences University of Gothenburg "
  ], 
  "grants": [
    "Funding\nThis work was supported by the Swedish Research Council [grant 2012-3719 to B.O.].", 
    "(2011) found that low rates (<0.1 migrant per generation) of migration had virtually no effect on the accuracy of BP&P in a simulation study.", 
    "However, at least when sample size is small, a single sampled recent migrant can cause severe effects.", 
    "The coalescent prior on the gene trees will affect them in a way that single recent introgressions will be `pushed back' by other gene trees that reflect the `true' speciation event, such that the coalescent time for the migrant may be biased."
  ], 
  "acks": " The authors thank four anonymous reviewers and the editors for valuable comments on earlier versions of this article. This work was supported by the Swedish Research Council ", 
  "authors": [
    " Graham Jones", 
    " Zeynep Aydin", 
    " Bengt Oxelman"
  ], 
  "keyWords": [
    [
      "priors", 
      "modeled", 
      "data", 
      "species", 
      "trees"
    ]
  ], 
  "sourcelinks": [
    "http://www.indriid.com", 
    "http://tree.bio.ed.ac.uk/software/beast", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-11-25T02:08:03Z"
}{
  "doi": "10.1093/bioinformatics/btu621", 
  "name": "EasyStrata evaluation and visualization of stratified genomewide association metaanalysis data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.broadinstitute.org/collaboration/giant).Funding"
  ], 
  "title": "Genome analysis EasyStrata: evaluation and visualization of stratified genome-wide association meta-analysis data", 
  "toolName": "Genome analysis EasyStrata: evaluation and visualization of stratified genome-wide association meta-analysis data", 
  "abstract": "The R package EasyStrata facilitates the evaluation and visualization of stratified genome-wide association meta-analyses (GWAMAs) results. It provides (i) statistical methods to test and account for between-strata difference as a means to tackle gene\u2013strata interaction effects and (ii) extended graphical features tailored for stratified GWAMA results. The software provides further features also suitable for general GWAMAs including functions to annotate, exclude or highlight specific loci in plots or to extract independent subsets of loci from genome-wide datasets. It is freely available and includes a user-friendly scripting interface that simplifies data handling and allows for combining statistical and graphical functions in a flexible fashion. Availability: EasyStrata is available for free (under the GNU General Public License v3) from our Web site www.genepi-regensburg.de/ easystrata and from the CRAN R package repository cran.r-project. org/web/packages/EasyStrata/.", 
  "summary": "2.1.1 Statistical functionality To evaluate stratified GWAMA results, we have implemented statistical approaches to estimate (i) the overall (i.e. strata-combined) effect by meta-analysis of the m strata results (Cox and Hinkley, 1979; Stouffer, 1949); (ii) the joint effect calculated from m strata results (Aschard et al., 2010); (iii) the difference between two strata results as a means to test for GxS effects (Randall et al., 2013); and (iv) the heterogeneity between m strata (Cochran, 1954) (see Supplementary Table S2 for a summary of implemented statistics).", 
  "affiliations": [
    " Division of Genetic Epidemiology Department of Medical Genetics, Molecular and Clinical Pharmacology Innsbruck Medical University ", 
    " Department of Statistical Bioinformatics Institute for Functional Genomics University of Regensburg ", 
    " Department of Genetic Epidemiology University of Regensburg "
  ], 
  "grants": [
    "Funding: German Federal Ministry of Education and Research (BMBF 01ER1206); National Institutes of Health (NIH, R01DK075787/01A1, CFDA 93 848); Swiss National Science Foundation (31003A-143914); Swiss Institute of Bioinformatics."
  ], 
  "acks": " EasyStrata was developed and tested using data from the Genetic Investigation of ANthropometric Traits Consortium (GIANT, http://www.broadinstitute.org/collaboration/giant). ", 
  "authors": [
    " Thomas W Winkler", 
    " Zoltan Kutalik", 
    " Mathias Gorski", 
    " Claudio Lottaz", 
    " Florian Kronenberg", 
    " Iris M Heid", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "associations", 
      "genomics", 
      "genetics", 
      "specifically", 
      "easystrata", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://www.broadinstitute.org/collaboration/giant).Funding"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-09-27T03:04:09Z"
}{
  "doi": "10.1093/bioinformatics/btu499", 
  "name": "eDriver a novel method to identify protein regions driving cancer", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/eduardporta/e-Driver.git"
  ], 
  "title": "Systems biology e-Driver: a novel method to identify protein regions driving cancer", 
  "toolName": "e-Driver.git", 
  "abstract": "Motivation: Most approaches used to identify cancer driver genes focus, true to their name, on entire genes and assume that a gene, treated as one entity, has a specific role in cancer. This approach may be correct to describe effects of gene loss or changes in gene expression ; however, mutations may have different effects, including their relevance to cancer, depending on which region of the gene they affect. Except for rare and well-known exceptions, there are not enough data for reliable statistics for individual positions, but an intermediate level of analysis, between an individual position and the entire gene, may give us better statistics than the former and better resolution than the latter approach. Results: We have developed e-Driver, a method that exploits the internal distribution of somatic missense mutations between the pro-tein's functional regions (domains or intrinsically disordered regions) to find those that show a bias in their mutation rate as compared with other regions of the same protein, providing evidence of positive selection and suggesting that these proteins may be actual cancer drivers. We have applied e-Driver to a large cancer genome dataset from The Cancer Genome Atlas and compared its performance with that of four other methods, showing that e-Driver identifies novel candidate cancer drivers and, because of its increased resolution, provides deeper insights into the potential mechanism of cancer driver genes identified by other methods. Availability and implementation: A Perl script with e-Driver and the files to reproduce the results described here can be downloaded from https://", 
  "summary": "Results: We have developed e-Driver, a method that exploits the internal distribution of somatic missense mutations between the protein's functional regions (domains or intrinsically disordered regions) to find those that show a bias in their mutation rate as compared with other regions of the same protein, providing evidence of positive selection and suggesting that these proteins may be actual cancer drivers.\nHere we present e-Driver, a novel method that identifies protein functional regions (PFRs) that show a bias in their mutation rates.\nTo identify PFRs under selection pressure, e-Driver first retrieves all missense mutations in a cancer cohort located in any given protein as well as the mutation coordinates and maps them to the protein's functional regions.", 
  "affiliations": [
    " Bioinformatics and Systems Biology Program Associate Editor: Igor Jurisica Sanford-Burnham Medical Research Institute "
  ], 
  "grants": [
    "Funding: This work has been supported by the Human Frontiers Science Program grant RGP0027/2011."
  ], 
  "sourcelinks": [
    "https://github.com/eduardporta/e-Driver.git"
  ], 
  "acks": " The authors want to thank their colleagues from the SBMRI bioinformatics group: specifically, Lukasz Jaroszewski for providing information and prediction for novel human protein domains and Thomas Hrabe for his help in preparing some of the figures. ", 
  "authors": [
    " Eduard Porta-Pardo", 
    " Adam Godzik"
  ], 
  "keyWords": [
    "driver genes", 
    [
      "mutational", 
      "drivers", 
      "proteins", 
      "cancers", 
      "domains", 
      "gene"
    ]
  ], 
  "github_data": {
    "name": "easy-driver.github.io", 
    "contributors": [
      {
        "contributions": 22, 
        "html_url": "https://github.com/LiFaytheGoblin"
      }
    ], 
    "versions": [], 
    "created_at": "2016-01-09T17:21:46Z", 
    "updated_at": "2016-01-17T17:03:24Z", 
    "languages": [
      "HTML", 
      "JavaScript"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/LiFaytheGoblin"
      }
    ], 
    "owner": "https://github.com/easy-driver", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btu687", 
  "name": "EMEM efficient computation of maximal exact matches for very large genomes", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.csd.uwo.ca/$ilie/E-MEM"
  ], 
  "title": "Genome analysis E-MEM: efficient computation of maximal exact matches for very large genomes", 
  "toolName": "Genome analysis E-MEM: efficient computation of maximal exact matches for very large genomes", 
  "abstract": "Motivation: Alignment of similar whole genomes is often performed using anchors given by the maximal exact matches (MEMs) between their sequences. In spite of significant amount of research on this problem, the computation of MEMs for large genomes remains a challenging problem. The leading current algorithms employ full text indexes, the sparse suffix array giving the best results. Still, their memory requirements are high, the parallelization is not very efficient, and they cannot handle very large genomes. Results: We present a new algorithm, efficient computation of MEMs (E-MEM) that does not use full text indexes. Our algorithm uses much less space and is highly amenable to parallelization. It can compute all MEMs of minimum length 100 between the whole human and mouse genomes on a 12 core machine in 10 min and 2 GB of memory; the required memory can be as low as 600 MB. It can run efficiently gen-omes of any size. Extensive testing and comparison with currently best algorithms is provided. Availability and implementation: The source code of E-MEM is freely available at:", 
  "summary": "E-MEM: efficient computation of maximal exact matches for very large genomes\nThe large popularity of whole-genome alignment programs, most notably that of the MUMmer software (Kurtz et al., 2004), attracted a lot of attention to the MEM computation problem, with the purpose of enabling the alignment of larger genomes within reasonable amount of memory.\nOur tests show that essaMEM is currently the best program for MEM computation in large genomes.\nN.K. implemented E-MEM, contributed to improving the design, installed the competing programs, downloaded the genome sequences, and performed all tests.", 
  "affiliations": [
    " Department of Computer Science University of Western Ontario "
  ], 
  "grants": [
    "Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant R3143A01 (L.I.)."
  ], 
  "acks": " Performance evaluation has been performed using the facilities of the Shared Hierarchical Academic Research Computing Network (SHARCNET: www.sharcnet.ca) and Compute/Calcul Canada. ", 
  "authors": [
    " Nilesh Khiste", 
    " Lucian Ilie", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "bioinformatics", 
      "genomics", 
      "computational", 
      "mems", 
      "programs"
    ]
  ], 
  "sourcelinks": [
    "http://www.csd.uwo.ca/$ilie/E-MEM"
  ], 
  "technologies": [
    "C++"
  ], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btu834", 
  "name": "Empowering biologists with multiomics data colorectal cancer as a paradigm", 
  "links": [
    "http://interactome.dfci.harvard", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.netgestalt.org", 
    "http://www"
  ], 
  "title": "Empowering biologists with multi-omics data: colorectal cancer as a paradigm", 
  "toolName": "Empowering biologists with multi-omics data: colorectal cancer as a paradigm", 
  "abstract": "Motivation: Recent completion of the global proteomic characterization of The Cancer Genome Atlas (TCGA) colorectal cancer (CRC) cohort resulted in the first tumor dataset with complete molecular measurements at DNA, RNA and protein levels. Using CRC as a paradigm, we describe the application of the NetGestalt framework to provide easy access and interpretation of multi-omics data. Results: The NetGestalt CRC portal includes genomic, epigenomic, transcriptomic, proteomic and clinical data for the TCGA CRC cohort, data from other CRC tumor cohorts and cell lines, and existing knowledge on pathways and networks, giving a total of more than 17 million data points. The portal provides features for data query, upload, visualization and integration. These features can be flexibly combined to serve various needs of the users, maximizing the synergy among omics data, human visualization and quantitative analysis. Using three case studies, we demonstrate that the portal not only provides user-friendly data query and visualization but also enables efficient data integration within a single omics data type, across multiple omics data types, and over biological networks. Availability and implementation: The NetGestalt CRC portal can be freely accessed at http://www. netgestalt.org.", 
  "summary": "In NetGestalt, single binary tracks can be defined based on experimental data (e.g. recurrently mutated genes or genes significantly correlated with survival time), existing knowledge on pathways, or user provided gene lists.\n2.3.3 Details-on-demand For all data tracks, after zooming into a network area with less than a pre-specified number of genes (i.e. 500 in the current implementation), a node-link diagram can be used to visualize detailed interaction relationships between all genes in the area.\nNetGestalt brings functionally related genes together to facilitate the interpretation and integration of multi-omics data within the context of biological networks.", 
  "affiliations": [
    " Department of Biomedical Informatics"
  ], 
  "grants": [
    "Cancer epidemiology, biomarkers & prevention: a publication of the American Association for Cancer Research, cosponsored by the American Society of Preventive Oncology, 17, 111117.", 
    "Funding\nThis work was supported by contract 13XS029 from Leidos Biomedical Research, Inc."
  ], 
  "acks": " ", 
  "authors": [
    " Jing Zhu", 
    " Zhiao Shi", 
    " Jing Wang", 
    " Bing Zhang"
  ], 
  "keyWords": [
    [
      "genes", 
      "levels", 
      "cancers", 
      "data", 
      "networks", 
      "netgestalt"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-20T04:13:19Z"
}{
  "doi": "10.1093/bioinformatics/btu639", 
  "name": "Enlight webbased integration of GWAS results with biological annotations", 
  "links": [
    "http://enlight.usc.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://eqtl.uchicago.edu"
  ], 
  "title": "Genetics and population analysis Enlight: web-based integration of GWAS results with biological annotations", 
  "toolName": "Genetics and population analysis Enlight: web-based integration of GWAS results with biological annotations", 
  "abstract": "Identifying causal variants remains a key challenge in post-GWAS (genome-wide association study) era, as many GWAS single-nucleotide polymorphisms (SNPs) (including imputed ones) fall into non-coding regions, making it difficult to associate statistical significance with predicted functionality. Therefore, we created a web-based tool, Enlight, which overlays functional annotation information , such as histone modification states, methylation patterns, transcription factor binding sites, eQTL and higher-order chromosomal structure, to GWAS results. Availability and implementation: Accessible by a Web browser at", 
  "summary": "ABSTRACT Summary: Identifying causal variants remains a key challenge in postGWAS (genome-wide association study) era, as many GWAS singlenucleotide polymorphisms (SNPs) (including imputed ones) fall into non-coding regions, making it difficult to associate statistical significance with predicted functionality.\nTo date, there are 14 012 genome-wide significant variants in the GWAS catalog (Hindorff et al., 2009).\nrs2071278 is genome-wide significantly associated with serum complement C3 and C4 levels (Yang et al., 2012), important measurements in the assessment of rheumatoid arthritis (Makinde et al., 1989).", 
  "affiliations": [], 
  "grants": [
    "Funding: National Institute of Health [R01 HG006465]."
  ], 
  "acks": " ", 
  "authors": [
    " Yunfei Guo", 
    " David V Conti", 
    " Kai Wang", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    "genome association", 
    [
      "associations", 
      "genomics", 
      "gwas", 
      "enlight", 
      "functionality", 
      "variants"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-28T00:25:27Z"
}{
  "doi": "10.1093/bioinformatics/btv045", 
  "name": "ENVIRONMENTS and EOL identification of Environment Ontology terms in text and the annotation of the Encyclopedia of Life", 
  "links": [
    "http://eol.org/traitbank", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://environments.hcmr.gr", 
    "http://download.jensenlab.org/EOL/.6", 
    "http://eol.org", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining ENVIRONMENTS and EOL: identification of Environment Ontology terms in text and the annotation of the Encyclopedia of Life", 
  "toolName": "Data and text mining ENVIRONMENTS and EOL: identification of Environment Ontology terms in text and the annotation of the Encyclopedia of Life", 
  "abstract": "The association of organisms to their environments is a key issue in exploring biodiversity patterns. This knowledge has traditionally been scattered, but textual descriptions of taxa and their habitats are now being consolidated in centralized resources. However, structured annotations are needed to facilitate large-scale analyses. Therefore, we developed ENVIRONMENTS, a fast dictionary based tagger capable of identifying Environment Ontology (ENVO) terms in text. We evaluate the accuracy of the tagger on a new manually curated corpus of 600 Encyclopedia of Life (EOL) species pages. We use the tagger to associate taxa with environments by tagging EOL text content monthly, and integrate the results into the EOL to disseminate them to a broad audience of users. Availability and implementation: The software and the corpus are available under the open-source BSD and the CC-BY-NC-SA 3.0 licenses, respectively, at http://environments.", 
  "summary": "ENVIRONMENTS and EOL: identification of Environment Ontology terms in text and the annotation of the Encyclopedia of Life\nHaving the environmental information contained in EOL annotated in the form of ENVO terms, rather than as free text, would enhance search capabilities and enable users to easily compile summary statistics on, for example, the ecological distribution of any taxa.\nENVIRONMENTS identifies ENVO terms in text using the same fast dictionary-based tagging engine as in Pafilis et al.\nTo this end, we have integrated the ENVIRONMENTS tagger with the EOL web resource to provide users with ENVO terms for each taxon.", 
  "affiliations": [
    " Disease Systems Biology Program Faculty of Health and Medical Sciences Novo Nordisk Foundation Center for Protein Research University of Copenhagen ", 
    " Marine Biological Laboratory", 
    " Institute of Marine Biology, Biotechnology and Aquaculture Hellenic Centre for Marine Research "
  ], 
  "grants": [
    "Funding\nThe Encyclopedia Of Life Rubenstein Fellows Program [CRDF EOL-3306613/E33066], the LifeWatchGreece Research Infrastructure [384676-94/ GSRT/ NSRF(C&E)] and the Novo Nordisk Foundation Center for Protein Research [NNF14CC0001]."
  ], 
  "acks": " ", 
  "authors": [
    " Evangelos Pafilis", 
    " Sune P Frankild", 
    " Julia Schnetzer", 
    " Lucia Fanini", 
    " Sarah Faulwetter", 
    " Christina Pavloudi", 
    " Katerina Vasileiadou", 
    " Patrick Leary", 
    " Jennifer Hammock", 
    " Katja Schulz", 
    " Cynthia Sims Parr", 
    " Christos Arvanitidis", 
    " Lars Juhl Jensen"
  ], 
  "keyWords": [
    [
      "terms", 
      "text", 
      "environments", 
      "research", 
      "synonyms", 
      "annotations"
    ]
  ], 
  "sourcelinks": [
    "http://environments.hcmr.gr", 
    "http://eol.org", 
    "http://download.jensenlab.org/EOL/.6"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-26T01:18:20Z"
}{
  "doi": "10.1093/bioinformatics/btu853", 
  "name": "ENViz a Cytoscape App for integrated statistical analysis and visualization of samplematched data with multiple data types", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "ENViz: a Cytoscape App for integrated statistical analysis and visualization of sample-matched data with multiple data types", 
  "toolName": "ENViz: a Cytoscape App for integrated statistical analysis and visualization of sample-matched data with multiple data types", 
  "abstract": "ENViz (Enrichment Analysis and Visualization) is a Cytoscape app that performs joint enrichment analysis of two types of sample matched datasets in the context of systematic annotations. Such datasets may be gene expression or any other high-throughput data collected in the same set of samples. The enrichment analysis is done in the context of pathway information, gene ontology or any custom annotation of the data.", 
  "summary": "Such datasets may be gene expression or any other high-throughput data collected in the same set of samples.\nStill, only a few tools support routine joint analysis of sample cohorts with multiple genomic measurement results (GomezCabrero et al., 2014).\nENViz follows an enrichment analysis approach, driven by three input matrices: (i) the primary data matrix (e.g. genes expression measurement across a set of samples), (ii) the annotation matrix\nAn example dataset, based on data published in (Enerly et al., 2011) and formatted for ENViz, can be downloaded from http://www.\nNat. Genet., 25, 2529.\nNat. Protoc., 2, 23662382.", 
  "affiliations": [
    " Agilent Laboratories", 
    " Blue Oak Software", 
    " Agilent Laboratories Tel-Aviv "
  ], 
  "grants": [
    "Conflict of Interest: I am an employee and hold stock of Agilent Technologies, the manufacturer of genomic microassays and library preparation assays\n\nupstream of next generation sequencing, and am currently conducting research sponsored by the company as part of my employment."
  ], 
  "acks": " We thank Allan Kuchinsky who identified the potential for weaving a joint data analysis approach into Cytoscape. Even though Allan was constantly fighting cancer and its complications, he led our team with great enthusiasm to cross countless obstacles and make ENViz a reality. This work is dedicated to the memory of Allan Kuchinsky, a Cytoscape enthusiast and pioneer. Conflict of Interest: I am an employee and hold stock of Agilent Technologies, the manufacturer of genomic microassays and library preparation assays upstream of next generation sequencing, and am currently conducting research sponsored by the company as part of my employment. Enviz analysis supports all relevant data, independent of the measurement technology provider. ", 
  "authors": [
    " Israel Steinfeld", 
    " Roy Navon", 
    " Michael L Creech", 
    " Zohar Yakhini", 
    " Anya Tsalenko", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "cytoscape", 
      "analysis", 
      "annotations", 
      "data", 
      "enrichments", 
      "enviz"
    ]
  ], 
  "sourcelinks": [
    "http://www"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-11T01:27:33Z"
}{
  "doi": "10.1093/bioinformatics/btu760", 
  "name": "Efficient searching and annotation of metabolic networks using chemical similarity", 
  "links": [
    "http://tyolab.north", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Systems biology Efficient searching and annotation of metabolic networks using chemical similarity", 
  "toolName": "Systems biology Efficient searching and annotation of metabolic networks using chemical similarity", 
  "abstract": "Motivation: The urgent need for efficient and sustainable biological production of fuels and high-value chemicals has elicited a wave of in silico techniques for identifying promising novel pathways to these compounds in large putative metabolic networks. To date, these approaches have primarily used general graph search algorithms, which are prohibitively slow as putative metabolic networks may exceed 1 million compounds. To alleviate this limitation, we report two methods\u2014SimIndex (SI) and SimZyme\u2014which use chemical similarity of 2D chemical fingerprints to efficiently navigate large metabolic networks and propose enzymatic connections between the constituent nodes. We also report a Byers\u2013Waterman type pathway search algorithm for further paring down pertinent networks. Results: Benchmarking tests run with SI show it can reduce the number of nodes visited in searching a putative network by 100-fold with a computational time improvement of up to 10 5-fold. Subsequent Byers\u2013Waterman search application further reduces the number of nodes searched by up to 100-fold, while SimZyme demonstrates $90% accuracy in matching query substrates with enzymes. Using these modules, we have designed and annotated an alternative to the methylery-thritol phosphate pathway to produce isopentenyl pyrophosphate with more favorable thermo-dynamics than the native pathway. These algorithms will have a significant impact on our ability to use large metabolic networks that lack annotation of promiscuous reactions. Availability and implementation: Python files will be available for download at http://tyolab.north western.edu/tools/.", 
  "summary": "Similar successful approaches have been taken to enable rapid searching of extant metabolic networks such as KEGG for links to known pathway mapping (Hattori et al., 2010) and validation of biochemical reactions in large databases (Felix and Valiente, 2007).\ndetermining enzyme promiscuity, a similarity-based approach coupled with the enzyme-substrate data from a large database such as BRENDA (Scheer et al., 2011) is suitable for rapid in silico annotation of novel biosynthetic pathways to value-added compounds within biochemical networks as demonstrated by the de novo MEP pathway.", 
  "affiliations": [
    " Department of Chemical and Biological Engineering Northwestern University "
  ], 
  "grants": [
    "Funding\nNIH Biotechnology Training [T32 GM008449, DP]; DOE Computer Science Fellowship (AS); Bill and Melinda Gates Foundation [OPP1044008]."
  ], 
  "acks": " ", 
  "authors": [
    " Dante A Pertusi", 
    " Andrew E Stine", 
    " Linda J Broadbelt", 
    " Keith E J Tyo"
  ], 
  "keyWords": [
    [
      "reactions", 
      "similarity", 
      "enzymes", 
      "searching", 
      "pathways", 
      "networks"
    ]
  ], 
  "sourcelinks": [
    "http://tyolab.north"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-23T01:09:29Z"
}{
  "doi": "10.1093/bioinformatics/btu592", 
  "name": "Estimation of GFPtagged RNA numbers from temporal fluorescence intensity data", 
  "links": [
    "http://www.cs.tut.fi/%7ehakkin22/jumpdet", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Gene expression Estimation of GFP-tagged RNA numbers from temporal fluorescence intensity data", 
  "toolName": "Gene expression Estimation of GFP-tagged RNA numbers from temporal fluorescence intensity data", 
  "abstract": "Motivation: MS2-GFP-tagging of RNA is currently the only method to measure intervals between consecutive transcription events in live cells. For this, new transcripts must be accurately detected from intensity time traces. Results: We present a novel method for automatically estimating RNA numbers and production intervals from temporal data of cell fluores-cence intensities that reduces uncertainty by exploiting temporal information. We also derive a robust variant, more resistant to outliers caused e.g. by RNAs moving out of focus. Using Monte Carlo simulations , we show that the quantification of RNA numbers and production intervals is generally improved compared with previous methods. Finally, we analyze data from live Escherichia coli and show statistically significant differences to previous methods. The new methods can be used to quantify numbers and production intervals of any fluorescent probes, which are present in low copy numbers, are brighter than the cell background and degrade slowly. Availability: Source code is available under Mozilla Public License at", 
  "summary": "Second, we apply our method and the previous methods on novel data extracted from time-lapse microscopy measurements of live E.coli cells expressing MS2-GFP and RNA target to show that, for large number of cells, statistically significant differences in the results can be detected between the new and previous methods, in both RNA numbers and RNA production time intervals.\nWe also applied the new and the previous methods on novel data from time-lapse images of live E.coli cells expressing RNA target for MS2-GFP to show that, if the data contains large number of cells, statistically significant differences in the results can be detected, in both the RNA numbers and RNA production time intervals.", 
  "affiliations": [
    " Department of Signal Processing Laboratory of Biosystem Dynamics, Computational Systems Biology Research Group Tampere University of Technology "
  ], 
  "grants": [
    "Funding: Work supported by Jenny and Antti Wihuri Foundation [to A.H.]; Academy of Finland [257603 to A.S.R."
  ], 
  "acks": " ", 
  "authors": [
    " Antti H ", 
    " Andre S Ribeiro"
  ], 
  "keyWords": [
    [
      "cells", 
      "estimation", 
      "methods", 
      "times"
    ]
  ], 
  "sourcelinks": [
    "http://www.cs.tut.fi/%7ehakkin22/jumpdet"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-05T05:34:33Z"
}{
  "doi": "10.1093/bioinformatics/btu609", 
  "name": "EUROCarbDBCCRC a EUROCarbDB node for storing glycomics standard data", 
  "links": [
    "http://jcggdb.jp/GlycoPOD", 
    "https://code", 
    "http://glycomics.ccrc.uga.edu/eurocarb", 
    "http://glycomics.ccrc.uga.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://code.google", 
    "http://glycomics.ccrc.uga.edu/GlycomicsWiki/Main_Page).243"
  ], 
  "title": "Databases and ontologies EUROCarbDB(CCRC): a EUROCarbDB node for storing glycomics standard data", 
  "toolName": "Databases and ontologies EUROCarbDB(CCRC): a EUROCarbDB node for storing glycomics standard data", 
  "abstract": "Motivation: In the field of glycomics research, several different techniques are used for structure elucidation. Although multiple techniques are often used to increase confidence in structure assignments, most glycomics databases allow storing of only a single type of experimental data. In addition, the methods used to prepare a sample for analysis is seldom recorded making it harder to reproduce the analytical data and results. Results: We have extended the freely available EUROCarbDB framework to allow the submission of experimental data and the reporting of several orthogonal experimental datasets. The features aim to increase the understandability and reproducibility of the reported data. Availability and implementation: The installation with the glycan standards is available at", 
  "summary": "For example, GLYCOSCIENCES.de (Lu tteke et al., 2006) and the Bacterial Carbohydrate Structure Database (Egorova and Toukach, 2014) contain nuclear magnetic resonance (NMR) data that have been extracted from the literature for different glycan structures.\nThe aim of the project was the establishment of a publicly available database framework for the creation of a network of homogeneous databases, allowing research groups worldwide to upload annotated glycan structures and data obtained by MS, NMR and HPLC experiments.\nAlthough storing experimental data along with the glycan structures, biological annotation, and their literature references was a fundamental goal of the EUROCarbDB project, approaches to storing techniques for sample preparation and methods of experimental analysis had not been developed.", 
  "affiliations": [
    " Department of Computer Science", 
    " Complex Carbohydrate Research Center University of Georgia "
  ], 
  "grants": [
    "Funding: This work was supported by Consortium for Functional Glycomics bridging grant [5U54GM062116-10]; and National Institute of General Medical Sciences, a part of the National Institutes of Health [8P41GM103490].", 
    "After the funding for CarbBank was discontinued, several independent new databases were created, often by importing most or all of the CarbBank data and sometimes adding experimental data."
  ], 
  "acks": " The authors would like to express their deep gratitude to the EUROCarbDB team who created the freely available open-source framework, which they built upon. ", 
  "authors": [
    " Khalifeh Al Jadda", 
    " Melody P Porterfield", 
    " Robert Bridger", 
    " Christian Heiss", 
    " Michael Tiemeyer", 
    " Lance Wells", 
    " John A Miller", 
    " William S York", 
    " Rene Ranzinger"
  ], 
  "keyWords": [
    [
      "glycomics", 
      "experiments", 
      "protocols", 
      "databases"
    ]
  ], 
  "sourcelinks": [
    "https://code.google", 
    "https://code"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-13T00:23:17Z"
}{
  "doi": "10.1093/bioinformatics/btu665", 
  "name": "ExomeAI detection of recurrent allelic imbalance in tumors using wholeexome sequencing data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://genomequebec.mcgill.ca", 
    "http://genomequebec.mcgill.ca/exomeai"
  ], 
  "title": "Sequence analysis ExomeAI: detection of recurrent allelic imbalance in tumors using whole-exome sequencing data", 
  "toolName": "Sequence analysis ExomeAI: detection of recurrent allelic imbalance in tumors using whole-exome sequencing data", 
  "abstract": "Whole-exome sequencing (WES) has extensively been used in cancer genome studies; however, the use of WES data in the study of loss of heterozygosity or more generally allelic imbalance (AI) has so far been very limited, which highlights the need for user-friendly and flexible software that can handle low-quality datasets. We have developed a statistical approach, ExomeAI, for the detection of recurrent AI events using WES datasets, specifically where matched normal samples are not available. Availability: ExomeAI is a web-based application, publicly available at: http://", 
  "summary": "To our knowledge, there is no publicly available software for identification of recurrent genomic AI segments, using WES data, shared across multiple tumor-only samples (Liu et al., 2013).\ndetect recurrent AI across cancer genomes by analyzing batches of WES data, and specifically in the absence of matched normal samples.\nSince then, we have successfully applied our method to several other cancer types such as glioma, renal cell carcinoma (unpublished data) and ETMR (Kleinman et al., 2014) (see Supplementary information) and showed that the approach is sensitive to clearly detect the recurrent AI across cancer genomes, in the absence of paired normal samples.", 
  "affiliations": [
    " Department of Human Genetics Faculty of Medicine McGill University and Genome Quebec Innovation Center "
  ], 
  "grants": [
    "Funding: This work was supported by the Genome Quebec, Genome Canada and the Canadian Institutes of Health Research [grant number 77764]; Tier II Canada Research Chair award (to J.M.)."
  ], 
  "acks": " We wish to thank Information Technology (IT) group at Genome Quebec Innovation Center, specifically, Terrance Mcquilkin, Alexandru Guja and Marc-Andre Labonte for providing infrastructure facilities and hosting ExomeAI server. ", 
  "authors": [
    " Javad Nadaf", 
    " Jacek Majewski", 
    " Somayyeh Fahiminiya"
  ], 
  "keyWords": [
    "exomeai detection", 
    [
      "detecting", 
      "segmentation", 
      "genomes", 
      "cancer"
    ]
  ], 
  "sourcelinks": [
    "http://genomequebec.mcgill.ca"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-09T01:49:44Z"
}{
  "doi": "10.1093/bioinformatics/btu727", 
  "name": "eQTL epistasis detecting epistatic effects and inferring hierarchical relationships of genes in biological pathways", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://biome"
  ], 
  "title": "Genome analysis eQTL epistasis: detecting epistatic effects and inferring hierarchical relationships of genes in biological pathways", 
  "toolName": "Genome analysis eQTL epistasis: detecting epistatic effects and inferring hierarchical relationships of genes in biological pathways", 
  "abstract": "Motivation: Epistasis is the interactions among multiple genetic variants. It has emerged to explain the 'missing heritability' that a marginal genetic effect does not account for by genome-wide association studies, and also to understand the hierarchical relationships between genes in the genetic pathways. The Fisher's geometric model is common in detecting the epistatic effects. However, despite the substantial successes of many studies with the model, it often fails to discover the functional dependence between genes in an epistasis study, which is an important role in inferring hierarchical relationships of genes in the biological pathway. Results: We justify the imperfectness of Fisher's model in the simulation study and its application to the biological data. Then, we propose a novel generic epistasis model that provides a flexible solution for various biological putative epistatic models in practice. The proposed method enables one to efficiently characterize the functional dependence between genes. Moreover, we suggest a statistical strategy for determining a recessive or dominant link among epistatic expression quantitative trait locus to enable the ability to infer the hierarchical relationships. The proposed method is assessed by simulation experiments of various settings and is applied to human brain data regarding schizophrenia. Availability and implementation: The MATLAB source codes are publicly available at: http://biome cis.uta.edu/epistasis.", 
  "summary": "eQTL epistasis: detecting epistatic effects and inferring hierarchical relationships of genes in biological pathways\nGiven the genetic effect sizes of two loci and the epistatic model (Model IModel IV) at each case study, SNP data at two loci (x1; x2) of a hundred samples were randomly generated, where r1 and r2 were varied by adding the positively normally distributed random variables (jN0; 0:1j) to the given values in Table 2.\nThe epistatic effects, in which only one genetic effect size is significant to another by Equation (15), were used for the construction, since the epistasis relationship clearly provides a clue to infer the hierarchy between genes.", 
  "affiliations": [
    " Department of Psychiatry University of Illinois at Chicago ", 
    " Department of Computer Science and Engineering University of Texas at Arlington ", 
    " Department of Mathematics University of Texas at Arlington "
  ], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " Mingon Kang", 
    " Chunling Zhang", 
    " Hyung-Wook Chun", 
    " Chris Ding", 
    " Chunyu Liu", 
    " Jean Gao"
  ], 
  "keyWords": [
    "epistatic effects", 
    [
      "epistasis", 
      "modeling", 
      "genetics", 
      "effect"
    ]
  ], 
  "sourcelinks": [
    "http://biome"
  ], 
  "technologies": [
    "MATLAB"
  ], 
  "dateCreated": "2014-10-31T00:44:56Z"
}{
  "doi": "10.1093/bioinformatics/btu741", 
  "name": "EVpedia a community web portal for extracellular vesicles research", 
  "links": [
    "http://www.isev", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://evpedia.info", 
    "http://www"
  ], 
  "title": "EVpedia: a community web portal for extracellular vesicles research", 
  "toolName": "EVpedia: a community web portal for extracellular vesicles research", 
  "abstract": "Motivation: Extracellular vesicles (EVs) are spherical bilayered proteolipids, harboring various bio-active molecules. Due to the complexity of the vesicular nomenclatures and components, online searches for EV-related publications and vesicular components are currently challenging. Results: We present an improved version of EVpedia, a public database for EVs research. This community web portal contains a database of publications and vesicular components, identification of orthologous vesicular components, bioinformatic tools and a personalized function. EVpedia includes 6879 publications, 172 080 vesicular components from 263 high-throughput datasets, and has been accessed more than 65 000 times from more than 750 cities. In addition, about 350 members from 73 international research groups have participated in developing EVpedia. This free web-based database might serve as a useful resource to stimulate the emerging field of EV research.", 
  "summary": "Researchers in this field have coined dozens of different names for EVs, especially for more complex eukaryotic cell-derived EVs as listed in Box (`Extracellular Vesicles: Diverse Nomenclature') (Choi et al., 2014; Gould and Raposo, 2013; Kim et al., 2013).\nThe explosion of EV data has justified the need for databases that catalog proteins, nucleic acids and lipids associated with EVs. Currently, three databases exist for EV research including ExoCarta (Simpson et al., 2012), EVpedia (Kim et al., 2013) and Vesiclepedia (Kalra et al., 2012).\nThe updated EVpedia has five functional modules for systematic analyses of EVs derived from prokaryotic and eukaryotic cells (Fig. 2): (i) a database of publications and principal investigators, (ii) a database of vesicular proteins, mRNAs, miRNAs, lipids and", 
  "affiliations": [
    " Department of Nephrology and Hypertension University Medical Center Utrecht ", 
    " Institute of Environmental Health Sciences and Hospital Infection Control, Medical Center University of Freiburg ", 
    " Ludwig Institute for Cancer Research Melbourne Austin Hospital ", 
    " Section of Oncology Department of Clinical Sciences Lund University ", 
    " Department of Rheumatology and Inflammation Research Sahlgrenska Academy University of Gothenburg ", 
    " Department of Clinical Immunology Polish-American Institute of Paediatrics Jagiellonian University Medical College ", 
    " Department of Clinical Immunology Aalborg University Hospital ", 
    " Section of Pulmonary, Critical Care and Sleep Medicine Department of Internal Medicine Yale University School of Medicine New Haven ", 
    " Tumour Microenvironment Laboratory QIMR Berghofer Medical Research Institute ", 
    " Department of Cell Biology and Physiology Washington University School of Medicine ", 
    " The Feinstein Institute for Medical Research", 
    " Program in Cellular and Molecular Medicine at Boston Children's Hospital and Department of Cell Biology, Harvard Medical School", 
    " Laboratory of Molecular Biology of Parasites and Vectors", 
    " Spinal Cord Injury & Tissue Regeneration Center Salzburg (SCI-TReCS) Paracelsus Medical University ", 
    " Ontario Cancer Institute", 
    " UMR837 JEAN-PIERRE Aubert Research Centre, Lille INSERM ", 
    " Department of Molecular Biology Ume\u00e5 University ", 
    " Cancer Biology Program Cedars-Sinai Medical Center ", 
    " Krefting Research Centre, Sahlgrenska Academy University of Gothenburg ", 
    " Division of Signaling and Functional Genomics German Cancer Research Center ", 
    " Department of Genetics, Cell-and Immunobiology Semmelweis University ", 
    " Department of Nuclear Medicine Seoul National University College of Medicine ", 
    " The University of Queensland Diamantina Institute Translational Research Institute The University of Queensland ", 
    " Department of Biochemistry and Molecular Biology, BIO21 Molecular Science and Biotechnology Institute The University of Melbourne ", 
    " Division of Biochemistry and Biotechnology Department of Biosciences University of Helsinki ", 
    " Department of Paediatrics University of Melbourne ", 
    " Australian Prostate Cancer Research Centre-Queensland Institute of Health and Biomedical Innovation Queensland University of Technology ", 
    " Department of Medical Physics Jagiellonian University ", 
    " Department of Health Science and Technology Aalborg University ", 
    " Jai Prakash University", 
    " Department of Life Sciences Pohang University of Science and Technology ", 
    " Department of Clinical Chemistry, Academic Medical Center", 
    " Department of Mechanical Engineering Pohang University of Science and Technology ", 
    " Department of Molecular and Comparative Pathobiology Instituto de Investigaci\u00f3 n Sanitaria Princesa The Johns Hopkins University School of Medicine ", 
    " Department of Microbiology Morehouse School of Medicine ", 
    " Institute of Cancer & Genetics School of Medicine Velindre Cancer Centre Cardiff University ", 
    " Department of Neurology, College of Medicine University of Tennessee Health Science Center ", 
    " Department of Hematology, Hemostasis, Oncology and Stem Cell Transplantation Hannover Medical School ", 
    " Department of Radiation Oncology and Experimental Cancer Research Laboratory of Experimental Cancer Research Ghent University Hospital ", 
    " The Florey Institute of Neuroscience and Mental Health The University of Melbourne ", 
    " Department of Cellular and Molecular Biology Graduate School of Biomedical Science Hiroshima University ", 
    " Division of Integrative Biosciences and Biotechnology Pohang University of Science and Technology ", 
    " Department of Biochemistry & Cell Biology Faculty of Veterinary Medicine Utrecht University ", 
    " Icahn School of Medicine at Mount Sinai Cardiovascular Research Institute ", 
    " 53 Inflammation Division Institute for Medical Research Laboratory of Medical Genomics, A.C. Camargo Cancer Center ", 
    " CSIR-Centre for Cellular and Molecular Biology", 
    " Department of Otorhinolaryngology and Research Group Gene Vectors University of Muenchen ", 
    " ICREA Barcelona Centre for International Health Research "
  ], 
  "grants": [
    "EVpedia statistics\n\nAll Eukaryotes Prokaryotes\n\nPublications Articles Principal investigators\nProteomes Studies Datasets Proteins\nTranscriptomes mRNA Studies Datasets mRNAs miRNA Studies Datasets miRNAs\nLipidomes Studies Datasets Lipids\nMetabolomes Studies Datasets Metabolites\nParticipating Laboratories (countries) Accesses (countries)\n\n6879 3336\n117 176 78 971\n17 28 74 430\n11 29 18 119\n22 29 550\n1 1 10\n73 (20) 66 617 (73)\n\n6021 2886\n97 148 74 696\n17 28 74 430\n11 29 18 119\n21 28 534\n1 1 10\n\n858 483\n20 28 4275\n0 0 0\n0 0 0\n1 1 16\n0 0 0\n\nFunding\nThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) [No.", 
    "A120273] and a grant from KRIBB Research Initiative Program.", 
    "2014-023004], the Ministry of Health and Welfare grant funded by the Korea government [No."
  ], 
  "acks": " ", 
  "authors": [
    " Dae-Kyum Kim", 
    " Jaewook Lee", 
    " Sae Rom", 
    " Kim ", 
    " \u2021 ", 
    " Dong-Sic Choi", 
    " \u2021 ", 
    " Yae Jin Yoon", 
    " Ji Hyun Kim", 
    " \u2021 ", 
    " Gyeongyun Go", 
    " \u2021 ", 
    " Dinh Nhung", 
    " \u2021 Kahye", 
    " Hong ", 
    " \u2021 ", 
    " Su Chul Jang", 
    " Si-Hyun Kim", 
    " Kyong-Su Park", 
    " \u2021 Oh", 
    " Youn Kim", 
    " Hyun Taek Park", 
    " Ji Hye Seo", 
    " \u2021 ", 
    " Elena Aikawa", 
    " Monika Baj-Krzyworzeka", 
    " Bas W M Van Balkom", 
    " Mattias Belting", 
    " Lionel Blanc", 
    " Vincent Bond", 
    " Antonella Bongiovanni", 
    " Francesc E Borr\u00e0", 
    " Luc 10", 
    " Bu\u00e9", 
    " I 11", 
    " Buz\u00e1", 
    " Lesley 12", 
    " Cheng", 
    " Aled Clayton", 
    " Emanuele Cocucci", 
    " Charles S Dela Cruz", 
    " Dominic M Desiderio", 
    " Dolores Di Vizio", 
    " Karin Ekstr\u00f6", 
    " Juan M Falcon-Perez", 
    " Chris Gardiner", 
    " Bernd Giebel", 
    " David W Greening", 
    " Julia Christina Gross", 
    " Dwijendra Gupta", 
    " An Hendrix", 
    " Andrew F Hill", 
    " Michelle M Hill", 
    " Esther Nolte-'t Hoen", 
    " Jameel Inal", 
    " Medicharla V Jagannadham", 
    " Muthuvel Jayachandran", 
    " Young-Koo Jee", 
    " Malene J\u00f8rgensen", 
    " Kwang Pyo Kim", 
    " Yoon-Keun Kim", 
    " Thomas Kislinger", 
    " Cecilia L\u00e4 Sser 39", 
    " Dong Soo Lee", 
    " Hakmo Lee", 
    " Johannes Van Leeuwen", 
    " Thomas Lener", 
    " Ming-Lin Liu", 
    " Jan L\u00f6 Tvall 39", 
    " Antonio Marcilla", 
    " Suresh Mathivanan", 
    " Andreas M\u00f6 Ller 47", 
    " Jess Morhayim", 
    " Fran\u00e7 Ois Mullier", 
    " Irina Nazarenko", 
    " Rienk Nieuwland", 
    " Diana N Nunes", 
    " Ken Pang", 
    " Jaesung Park", 
    " Tushar Patel", 
    " Gabriella Pocsfalvi", 
    " Hernando Del Portillo", 
    " Ulrich Putz", 
    " Marcel I Ramirez", 
    " Marcio L Rodrigues", 
    " Tae-Young Roh", 
    " Felix Royo", 
    " Susmita Sahoo", 
    " Raymond Schiffelers", 
    " Shivani Sharma", 
    " Pia Siljander", 
    " Richard J Simpson", 
    " Carolina Soekmadji", 
    " Philip Stahl", 
    " Allan Stensballe", 
    " Ewa Ste", 
    " \u02db Pien\u00b470pien\u00b4pien\u00b470", 
    " Hidetoshi Tahara", 
    " Arne Trummer", 
    " Hadi Valadi", 
    " Laura J Vella", 
    " Sun Nyunt Wai", 
    " Kenneth Witwer", 
    " Mar\u00eda Y\u00e1 N \u02dc Ez-M\u00f3", 
    " Hyewon Youn", 
    " Reinhard Zeidler", 
    " Yong Song Gho"
  ], 
  "keyWords": [
    "vesicles research", 
    [
      "institute", 
      "university", 
      "vesicle", 
      "evpedia", 
      "researchers"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-12T04:44:24Z"
}{
  "doi": "10.1093/bioinformatics/btu762", 
  "name": "EPGA de novo assembly using the distributions of reads and insert size", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com"
  ], 
  "title": "EPGA: de novo assembly using the distributions of reads and insert size", 
  "toolName": "github.com", 
  "abstract": "Motivation: In genome assembly, the primary issue is how to determine upstream and downstream sequence regions of sequence seeds for constructing long contigs or scaffolds. When extending one sequence seed, repetitive regions in the genome always cause multiple feasible extension candidates which increase the difficulty of genome assembly. The universally accepted solution is choosing one based on read overlaps and paired-end (mate-pair) reads. However, this solution faces difficulties with regard to some complex repetitive regions. In addition, sequencing errors may produce false repetitive regions and uneven sequencing depth leads some sequence regions to have too few or too many reads. All the aforementioned problems prohibit existing as-semblers from getting satisfactory assembly results. Results: In this article, we develop an algorithm, called extract paths for genome assembly (EPGA), which extracts paths from De Bruijn graph for genome assembly. EPGA uses a new score function to evaluate extension candidates based on the distributions of reads and insert size. The distribution of reads can solve problems caused by sequencing errors and short repetitive regions. Through assessing the variation of the distribution of insert size, EPGA can solve problems introduced by some complex repetitive regions. For solving uneven sequencing depth, EPGA uses relative mapping to evaluate extension candidates. On real datasets, we compare the performance of EPGA and other popular assemblers. The experimental results demonstrate that EPGA can effectively obtain longer and more accurate contigs and scaffolds.", 
  "summary": "Because paired-end reads can span repetitive regions shorter than insert size, for one sequence seed ss and one downstream extension candidate s, we can estimate the correctness of s through its MRL(s).\nFor one sequence seed ss, its downstream extension candidate s and evaluating region se, EPGA uses the following three new strategies which make novel use of paired-end reads:\nIn EPGA, we mainly incorporate two new ideas for genome assembly: (i) we consider the distribution of reads to identify whether one extension candidate includes sequencing errors, rather than only using k-mer frequency; (ii) based on the distribution of insert size, we develop a new score function to overcome complex repetitive regions.", 
  "affiliations": [
    " Division of Biomedical Engineering University of Saskatchewan ", 
    " Department of Computer Science Georgia State University ", 
    " School of Information Science and Engineering Central South University "
  ], 
  "grants": [
    "Funding: This work was supported in part by the National Natural Science Foundation of China [61232001, 61420106009, 61379108] and the Program for New Century Excellent Talents in University [NCET-12-0547]."
  ], 
  "sourcelinks": [
    "https://github.com"
  ], 
  "acks": " ", 
  "authors": [
    " Junwei Luo", 
    " Jianxin Wang", 
    " Zhen Zhang", 
    " Fang-Xiang Wu", 
    " Min Li", 
    " Yi Pan"
  ], 
  "keyWords": [
    [
      "assemblies", 
      "epga", 
      "genomes", 
      "regions", 
      "reads", 
      "sequencing"
    ]
  ], 
  "github_data": {
    "name": "node-v0.x-archive", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/orangemocha"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/works", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/works", 
        "name": "works"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.7", 
        "name": "v0.12.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.6", 
        "name": "v0.12.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.5", 
        "name": "v0.12.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.4", 
        "name": "v0.12.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.3", 
        "name": "v0.12.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.2", 
        "name": "v0.12.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.1", 
        "name": "v0.12.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.0", 
        "name": "v0.12.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.16", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.16", 
        "name": "v0.11.16"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.15", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.15", 
        "name": "v0.11.15"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.14", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.14", 
        "name": "v0.11.14"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.13", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.13", 
        "name": "v0.11.13"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.12", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.12", 
        "name": "v0.11.12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.11", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.11", 
        "name": "v0.11.11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.10", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.10", 
        "name": "v0.11.10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.9", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.9", 
        "name": "v0.11.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.8", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.8", 
        "name": "v0.11.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.7", 
        "name": "v0.11.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.6", 
        "name": "v0.11.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.5", 
        "name": "v0.11.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.4", 
        "name": "v0.11.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.3", 
        "name": "v0.11.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.40", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.40", 
        "name": "v0.10.40"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.39", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.39", 
        "name": "v0.10.39"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.38", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.38", 
        "name": "v0.10.38"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.37", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.37", 
        "name": "v0.10.37"
      }
    ], 
    "created_at": "2009-05-27T16:29:46Z", 
    "updated_at": "2016-08-09T16:10:45Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/isaacs"
      }, 
      {
        "html_url": "https://github.com/koichik"
      }, 
      {
        "html_url": "https://github.com/igorzi"
      }, 
      {
        "html_url": "https://github.com/kunoichi-co"
      }, 
      {
        "html_url": "https://github.com/carloslemes"
      }, 
      {
        "html_url": "https://github.com/aboyon"
      }, 
      {
        "html_url": "https://github.com/arden"
      }, 
      {
        "html_url": "https://github.com/guyoun"
      }, 
      {
        "html_url": "https://github.com/vissul"
      }, 
      {
        "html_url": "https://github.com/bdtgzj"
      }, 
      {
        "html_url": "https://github.com/santigimeno"
      }, 
      {
        "html_url": "https://github.com/chrisbateskeegan"
      }, 
      {
        "html_url": "https://github.com/FlashSoft"
      }, 
      {
        "html_url": "https://github.com/charlesgan"
      }, 
      {
        "html_url": "https://github.com/BlaneCordes"
      }, 
      {
        "html_url": "https://github.com/mdgoody"
      }, 
      {
        "html_url": "https://github.com/gindis"
      }, 
      {
        "html_url": "https://github.com/chrisdickinson"
      }, 
      {
        "html_url": "https://github.com/jorgenio"
      }, 
      {
        "html_url": "https://github.com/joityui"
      }, 
      {
        "html_url": "https://github.com/wxnet2013"
      }, 
      {
        "html_url": "https://github.com/be5invis"
      }, 
      {
        "html_url": "https://github.com/ashleymcfarland"
      }, 
      {
        "html_url": "https://github.com/jimxl"
      }, 
      {
        "html_url": "https://github.com/bluefisher80"
      }, 
      {
        "html_url": "https://github.com/subhaze"
      }, 
      {
        "html_url": "https://github.com/yaoming6046"
      }, 
      {
        "html_url": "https://github.com/hirozhang"
      }, 
      {
        "html_url": "https://github.com/rhyw"
      }, 
      {
        "html_url": "https://github.com/rhaldkhein"
      }
    ], 
    "owner": "https://github.com/nodejs", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-11-19T01:15:08Z"
}{
  "doi": "10.1093/bioinformatics/btu797", 
  "name": "Exploiting hidden information interleaved in the redundancy of the genetic code without prior knowledge", 
  "links": [
    "http://www.cs.tau.ac.il/$tamirtul/Chimera/download.htm", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.ncbi.nlm"
  ], 
  "title": "Exploiting hidden information interleaved in the redundancy of the genetic code without prior knowledge", 
  "toolName": "Exploiting hidden information interleaved in the redundancy of the genetic code without prior knowledge", 
  "abstract": "Motivation: Dozens of studies in recent years have demonstrated that codon usage encodes various aspects related to all stages of gene expression regulation. When relevant high-quality large-scale gene expression data are available, it is possible to statistically infer and model these signals, enabling analysing and engineering gene expression. However, when these data are not available, it is impossible to infer and validate such models. Results: In this current study, we suggest Chimera\u2014an unsupervised computationally efficient approach for exploiting hidden high-dimensional information related to the way gene expression is encoded in the open reading frame (ORF), based solely on the genome of the analysed organism. One version of the approach, named Chimera Average Repetitive Substring (ChimeraARS), estimates the adaptability of an ORF to the intracellular gene expression machinery of a genome (host), by computing its tendency to include long substrings that appear in its coding sequences; the second version, named ChimeraMap, engineers the codons of a protein such that it will include long substrings of codons that appear in the host coding sequences, improving its adaptation to a new host's gene expression machinery. We demonstrate the applicability of the new approach for analysing and engineering heterologous genes and for analysing endogenous genes. Specifically, focusing on Escherichia coli, we show that it can exploit information that cannot be detected by conventional approaches (e.g. the CAI\u2014Codon Adaptation Index), which only consider single codon distributions; for example, we report correlations of up to 0.67 for the ChimeraARS measure with heterologous gene expression, when the CAI yielded no correlation. Availability and implementation: For non-commercial purposes, the code of the Chimera approach can be downloaded from", 
  "summary": "One version of the approach, named Chimera Average Repetitive Substring (ChimeraARS), estimates the adaptability of an ORF to the intracellular gene expression machinery of a genome (host), by computing its tendency to include long substrings that appear in its coding sequences; the second version, named ChimeraMap, engineers the codons of a protein such that it will include long substrings of codons that appear in the host coding sequences, improving its adaptation to a new host's gene expression machinery.", 
  "affiliations": [], 
  "grants": [
    "Funding\nThis study was supported in part by a fellowship from the Edmond J. Safra Center for Bioinformatics at Tel-Aviv University."
  ], 
  "acks": " ", 
  "authors": [
    " Hadas Zur", 
    " Tamir Tuller"
  ], 
  "keyWords": [
    "gene expression", 
    [
      "genomics", 
      "information", 
      "genes", 
      "chimeramap", 
      "codons", 
      "expressing"
    ]
  ], 
  "sourcelinks": [
    "http://www.cs.tau.ac.il/$tamirtul/Chimera/download.htm", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-30T01:39:32Z"
}{
  "doi": "10.1093/bioinformatics/btu620", 
  "name": "ExportAid database of RNA elements regulating nuclear RNA export in mammals", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.introni.it/ExportAid/ExportAid.html"
  ], 
  "title": "Databases and ontologies ExportAid: database of RNA elements regulating nuclear RNA export in mammals", 
  "toolName": "Databases and ontologies ExportAid: database of RNA elements regulating nuclear RNA export in mammals", 
  "abstract": "Motivation: Regulation of nuclear mRNA export or retention is carried out by RNA elements but the mechanism is not yet well understood. To understand the mRNA export process, it is important to collect all the involved RNA elements and their transacting factors. Results: By hand-curated literature screening we collected, in ExportAid database, experimentally assessed data about RNA elements regulating nuclear export or retention of endogenous, heter-ologous or artificial RNAs in mammalian cells. This database could help to understand the RNA export language and to study the possible export efficiency alterations owing to mutations or polymorphisms. Currently, ExportAid stores 235 and 96 RNA elements, respectively, increasing and decreasing export efficiency, and 98 neutral assessed sequences. Availability and implementation: Freely accessible without registration at http://www.introni.it/ExportAid/ExportAid.html. Database and web interface are implemented in Perl, MySQL, Apache and JavaScript with all major browsers supported.", 
  "summary": "Results: By hand-curated literature screening we collected, in ExportAid database, experimentally assessed data about RNA elements regulating nuclear export or retention of endogenous, heterologous or artificial RNAs in mammalian cells.\nBy hand-curated literature screening, we collected, in ExportAid database, experimentally assessed data about RNA elements regulating nuclear export or retention of endogenous, heterologous or artificial RNAs in mammalian cells.\nMoreover, ExportAid could be used in virology to study the effect of emerging viral mutants, or to design lentiviral gene therapy vectors with more efficient RNA export elements (Oh et al., 2007).", 
  "affiliations": [
    " Department of Specialistic Clinical and Odontostomatological Sciences Polytechnic University of Marche "
  ], 
  "grants": [], 
  "acks": " The authors acknowledge a kind thanks to Mrs Monica Glebocki for language editing. ", 
  "authors": [
    " Matteo Giulietti", 
    " Sara Armida Milantoni", 
    " Tatiana Armeni", 
    " Giovanni Principato", 
    " Francesco Piva"
  ], 
  "keyWords": [
    "rna elements", 
    [
      "sequences", 
      "virus", 
      "element", 
      "proteins", 
      "exporting"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu549", 
  "name": "FACTERA a practical method for the discovery of genomic rearrangements at breakpoint resolution", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://factera.stanford.edu"
  ], 
  "title": "Sequence analysis FACTERA: a practical method for the discovery of genomic rearrangements at breakpoint resolution", 
  "toolName": "Sequence analysis FACTERA: a practical method for the discovery of genomic rearrangements at breakpoint resolution", 
  "abstract": "For practical and robust de novo identification of genomic fusions and breakpoints from targeted paired-end DNA sequencing data, we developed Fusion And Chromosomal Translocation Enumeration and Recovery Algorithm (FACTERA). Our method has minimal external dependencies, works directly on a preexisting Binary Alignment/Map file and produces easily interpretable output. We demonstrate FACTERA's ability to rapidly identify breakpoint-resolution fusion events with high sensitivity and specificity in patients with non-small cell lung cancer, including novel rearrangements. We anticipate that FACTERA will be broadly applicable to the discovery and analysis of clinically relevant fusions from both targeted and genome-wide sequencing datasets. Availability and implementation: http://", 
  "summary": "Because previous methods for fusion discovery perform well in simulated data but tend to overestimate breakpoints in real tumor genomes (Schroder et al., 2014), FACTERA was designed to detect fusion genes with high specificity without compromising sensitivity.\nAs input, FACTERA requires (i) a Binary Alignment/Map (BAM) file of pairedend reads mapped by an alignment tool capable of `soft clipping', such as Burrows-Wheeler Aligner (BWA) (Li and Durbin, 2009), (ii) genomic coordinates (in Browser Extensible Data [BED] format) used to control the resolution of fusion discovery via the locations of genes, exons or other genomic units and (iii) a 2BIT reference genome to enable fast sequence retrieval (e.g. UCSC hg19.2 bit).", 
  "affiliations": [
    " Stanford Cancer Institute Stanford University "
  ], 
  "grants": [
    "Funding: This work was supported by the Stanford Cancer Institute Genomics Initiative (A.A.A., M.D."
  ], 
  "acks": " ", 
  "authors": [
    " Aaron M Newman", 
    " Scott V Bratman", 
    " Henning Stehr", 
    " Luke J Lee", 
    " Chih Long Liu", 
    " Maximilian Diehn", 
    " Ash A Alizadeh", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "genomics", 
      "fusions", 
      "breakpoints", 
      "reads", 
      "sequencing", 
      "factera"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-21T04:05:53Z"
}{
  "doi": "10.1093/bioinformatics/btu560", 
  "name": "ExpTreeDB Webbased query and visualization of manually annotated gene expression profiling experiments of human and mouse from GEO", 
  "links": [
    "http://biotech.bmi.ac.cn/ExpTreeDB.2.2", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://biotech.bmi.ac.cn/ExpTreeDB"
  ], 
  "title": "Databases and ontologies ExpTreeDB: Web-based query and visualization of manually annotated gene expression profiling experiments of human and mouse from GEO", 
  "toolName": "Databases and ontologies ExpTreeDB: Web-based query and visualization of manually annotated gene expression profiling experiments of human and mouse from GEO", 
  "abstract": "Motivation: Numerous public microarray datasets are valuable resources for the scientific communities. Several online tools have made great steps to use these data by querying related datasets with users' own gene signatures or expression profiles. However, dataset annotation and result exhibition still need to be improved. Results: ExpTreeDB is a database that allows for queries on human and mouse microarray experiments from Gene Expression Omnibus with gene signatures or profiles. Compared with similar applications, ExpTreeDB pays more attention to dataset annotations and result visu-alization. We introduced a multiple-level annotation system to depict and organize original experiments. For example, a tamoxifen-treated cell line experiment is hierarchically annotated as 'agent!drug!estro gen receptor antagonist!tamoxifen'. Consequently, retrieved results are exhibited by an interactive tree-structured graphics, which provide an overview for related experiments and might enlighten users on key items of interest. Availability and implementation: The database is freely available at", 
  "summary": "Unlike MSigDB used for querying annotated gene sets, online applications such as Microarray Rank Query (MARQ) and Gene Expression data Mining Toward Relevant Network Discovery (GEM-TREND) allow users to discover experiments in GEO that induce similar or opposite gene expression patterns to their own experiments with GSEA approach (Feng et al., 2009; Vazquez et al., 2010).\nAt the fourthlevel annotations, RRGLs annotated as `dimethyloxalylglycine' and `collagen/chondroitin tissue engineering scaffold mesh' were top-ranked among the retrieved RRGLs. The literature associated with these also reported that these two types of agents induced gene expression patterns like hypoxia (Elvidge et al., 2006; Klapperich and Bertozzi, 2004).", 
  "affiliations": [
    " Beijing Institute of Radiation Medicine"
  ], 
  "grants": [
    "Funding: This work was supported by National Natural Science Foundation of China (Grant No.", 
    "Program of International S&T Cooperation (Grant No.", 
    "81273488, 81230089), National Key Technologies R&D Program for New Drugs (Grant No."
  ], 
  "acks": " ", 
  "authors": [
    " Ming Ni", 
    " Fuqiang Ye", 
    " Juanjuan Zhu", 
    " Zongwei Li", 
    " Shuai Yang", 
    " Bite Yang", 
    " Lu Han", 
    " Yongge Wu", 
    " Ying Chen", 
    " Fei Li", 
    " Shengqi Wang", 
    " Xiaochen Bo"
  ], 
  "keyWords": [
    "annotated gene", 
    [
      "drugs", 
      "cells", 
      "genes", 
      "exptreedb", 
      "rrgls", 
      "annotations"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R", 
    "MATLAB"
  ], 
  "dateCreated": "2014-08-25T00:08:47Z"
}{
  "doi": "10.1093/bioinformatics/btu541", 
  "name": "Fast construction of FMindex for long sequence reads", 
  "links": [
    "http://bit.ly/beetlGH", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/lh3/ropebwt2", 
    "http://bit.ly/levy2007", 
    "http://bit.ly/mol12878", 
    "https://github.com/lh3/ropebwt"
  ], 
  "title": "Sequence analysis Fast construction of FM-index for long sequence reads", 
  "toolName": "ropebwt", 
  "abstract": "We present a new method to incrementally construct the FM-index for both short and long sequence reads, up to the size of a genome. It is the first algorithm that can build the index while implicitly sorting the sequences in the reverse (complement) lexicographical order without a separate sorting step. The implementation is among the fastest for indexing short reads and the only one that practically works for reads of averaged kilobases in length. Availability and implementation: https://github.com/", 
  "summary": "Algorithm 2 constructs RLO/RCLO-BWT in a similar manner to Algorithm 1 except that it inserts Pi to l; u, the suffix array interval of P's suffix starting at i + 1, and that BWT symbols in this interval are already sorted.\nVenter: 32M  875 bp (in average) human reads by Sanger sequencing (Levy et al.\nWe implemented the algorithm in ropeBWT2 and evaluated its performance together with BEETL (http://bit.ly/beetlGH), the original on-disk implementation of BCR and BCRext, ropeBWT-BCR (https://github.com/lh3/ropebwt), an inmemory reimplementation of BCR by us, and NVBio (http:// bit.ly/nvbioio), a GPU-based algorithm inspired by CX1 (Liu et al., 2014).", 
  "affiliations": [], 
  "grants": [
    "Funding: NHGRI U54HG003037; NIH GM100233."
  ], 
  "sourcelinks": [
    "http://bit.ly/beetlGH", 
    "https://github.com/lh3/ropebwt2", 
    "https://github.com/lh3/ropebwt"
  ], 
  "acks": " ", 
  "authors": [], 
  "keyWords": [
    [
      "sequencing", 
      "reads", 
      "sorting", 
      "algorithms", 
      "strings"
    ]
  ], 
  "github_data": {
    "name": "ropebwt", 
    "contributors": [
      {
        "contributions": 174, 
        "html_url": "https://github.com/lh3"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/lh3/ropebwt/zipball/rb-final", 
        "tarball_url": "https://api.github.com/repos/lh3/ropebwt/tarball/rb-final", 
        "name": "rb-final"
      }
    ], 
    "created_at": "2012-05-23T16:15:43Z", 
    "updated_at": "2015-04-05T05:08:58Z", 
    "languages": [
      "C"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/lh3"
      }, 
      {
        "html_url": "https://github.com/noporpoise"
      }, 
      {
        "html_url": "https://github.com/imadcat"
      }, 
      {
        "html_url": "https://github.com/ryan-williams"
      }
    ], 
    "owner": "https://github.com/lh3", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-08-09T01:36:21Z"
}{
  "doi": "10.1093/bioinformatics/btu496", 
  "name": "FARVAT a familybased rare variant association test", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://biostat.cau.ac.kr/farvat/.However"
  ], 
  "title": "Genetics and population analysis FARVAT: a family-based rare variant association test", 
  "toolName": "Genetics and population analysis FARVAT: a family-based rare variant association test", 
  "abstract": "Motivation: Individuals in each family are genetically more homogeneous than unrelated individuals, and family-based designs are often recommended for the analysis of rare variants. However, despite the importance of family-based samples analysis, few statistical methods for rare variant association analysis are available. Results: In this report, we propose a FAmily-based Rare Variant Association Test (FARVAT). FARVAT is based on the quasi-likelihood of whole families, and is statistically and computationally efficient for the extended families. FARVAT assumed that families were ascertained with the disease status of family members, and incorporation of the estimated genetic relationship matrix to the proposed method provided robustness under the presence of the population substruc-ture. Depending on the choice of working matrix, our method could be a burden test or a variance component test, and could be extended to the SKAT-O-type statistic. FARVAT was implemented in C++, and application of the proposed method to schizophrenia data and simulated data for GAW17 illustrated its practical importance. Availability: The software calculates various statistics for the analysis of related samples, and it is freely downloadable from http:// healthstats.snu.ac.kr/software/farvat. Contact:", 
  "summary": "Recently, Family Based Association Tests (FBAT) statistics (Laird et al., 2000) have been extended for application in rare variant association analysis: the burden test (De et al., 2013) and the variance component test (Ionita-Laza et al., 2013) have been proposed.\nIt should be noted that the proposed method was an extension of the MQLS statistic (Thornton and McPeek, 2007) to rare variant association analysis, and that MQLS becomes robust under the presence of population substructure if ( was estimated with large-scale genomic data (Thornton and McPeek, 2010).", 
  "affiliations": [
    " Department of Public Health Science Seoul National University ", 
    " Institute of Human Genetics University of Bonn ", 
    " Interdisciplinary Program in bioinformatics Seoul National University ", 
    " Harvard Medical School", 
    " Institute for Genomic Mathematics University of Bonn ", 
    " Department of Biostatistics Harvard School of Public Health ", 
    " Center for Genomic Medicine Brigham and Women's Hospital "
  ], 
  "grants": [
    "Funding: This study was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education [2013R1A1A2010437]; and by NRF grant funded by the Korea government (MSIP) (No."
  ], 
  "acks": " ", 
  "authors": [
    " Sungkyoung Choi", 
    " Sungyoung Lee", 
    " Sven Cichon", 
    " Markus M N \u20ac Othen", 
    " Christoph Lange", 
    " Taesung Park", 
    " Sungho Won", 
    " ", 
    " ", 
    " ", 
    " "
  ], 
  "keyWords": [
    "statistical methods", 
    [
      "genetically", 
      "method", 
      "testing", 
      "statistically", 
      "genomes"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://biostat.cau.ac.kr/farvat/.However"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-30T00:14:11Z"
}{
  "doi": "10.1093/bioinformatics/btu544", 
  "name": "Figmop a profile HMM to identify genes and bypass troublesome gene models in draft genomes", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/dave-the-scientist"
  ], 
  "title": "Genome analysis Figmop: a profile HMM to identify genes and bypass troublesome gene models in draft genomes", 
  "toolName": "dave-the-scientist", 
  "abstract": "Motivation: Gene models from draft genome assemblies of metazoan species are often incorrect, missing exons or entire genes, particularly for large gene families. Consequently, labour-intensive manual cur-ation is often necessary. We present Figmop (Finding Genes using Motif Patterns) to help with the manual curation of gene families in draft genome assemblies. The program uses a pattern of short sequence motifs to identify putative genes directly from the genome sequence. Using a large gene family as a test case, Figmop was found to be more sensitive and specific than a BLAST-based approach. The visualization used allows the validation of potential genes to be carried out quickly and easily, saving hours if not days from an analysis. Availability and implementation: Source code of Figmop is freely available for download at https://github.com/dave-the-scientist, implemented in C and Python and is supported on Linux, Unix and", 
  "summary": "Figmop has proved invaluable in our efforts to manually curate cytochrome P450 (CYP) gene family members, which have high sequence variability and differing intron/exon structure, within a draft genome assembly of the parasitic nematode Haemonchus contortus (Laing et al., 2013).\nThis pattern of motifs was then used by Figmop to search the genome assembly of Drosophila melanogaster, where it returned all of the 85 defined CYP genes, as well as two probable pseudogenes (results not shown).\n4 CONCLUSION We have created Figmop, a software that uses a pHMM to compare the motif patterns of a gene family against a test genome.", 
  "affiliations": [
    " Department of Ecosystem and Public Health", 
    " Department of Comparative Biology and Experimental Medicine Faculty of Veterinary Medicine University of Calgary "
  ], 
  "grants": [
    "Funding: This work is supported by NSERC CREATE (Natural Sciences and Engineering Research Council of Canada Collaborative Research and Training Experience) programme in HostParasite Interactions (#413888-2012)."
  ], 
  "acks": " The authors thank Aude Gilabert for breaking earlier versions of the program. ", 
  "authors": [
    " David M Curran", 
    " John S Gilleard", 
    " James D Wasmuth", 
    " John Hancock"
  ], 
  "keyWords": [
    "gene models", 
    "sequence motifs", 
    [
      "sequences", 
      "motif", 
      "cyps", 
      "genes", 
      "genomes", 
      "figmop", 
      "bioinformatics", 
      "model"
    ]
  ], 
  "sourcelinks": [
    "https://github.com/dave-the-scientist"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-08-13T13:59:18Z"
}{
  "doi": "10.1093/bioinformatics/btu725", 
  "name": "Fast and accurate site frequency spectrum estimation from low coverage sequence data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://jnpopgen.org"
  ], 
  "title": "Genetics and population analysis Fast and accurate site frequency spectrum estimation from low coverage sequence data", 
  "toolName": "Genetics and population analysis Fast and accurate site frequency spectrum estimation from low coverage sequence data", 
  "abstract": "Motivation: The distribution of allele frequencies across polymorphic sites, also known as the site frequency spectrum (SFS), is of primary interest in population genetics. It is a complete summary of sequence variation at unlinked sites and more generally, its shape reflects underlying population genetic processes. One practical challenge is that inferring the SFS from low coverage sequencing data in a straightforward manner by using genotype calls can lead to significant bias. To reduce bias, previous studies have used a statistical method that directly estimates the SFS from sequenc-ing data by first computing site allele frequency (SAF) likelihood for each site (i.e. the likelihood a site has each possible allele frequency conditional on observed sequence reads) using a dynamic programming (DP) algorithm. Although this method produces an accurate SFS, computing the SAF likelihood is quadratic in the number of samples sequenced. Results: To overcome this computational challenge, we propose an algorithm, 'score-limited DP' algorithm, which is linear in the number of genomes to compute the SAF likelihood. This algorithm works because in a lower triangular matrix that arises in the DP algorithm, all non-negligible values of the SAF likelihood are concentrated on a few cells around the best-guess allele counts. We show that our score-limited DP algorithm has comparable accuracy but is faster than the original DP algorithm. This speed improvement makes SFS estimation practical when using low coverage NGS data from a large number of individuals. Availability and implementation: The program will be available via a link from the Novembre lab website (http://jnpopgen.org/).", 
  "summary": "To compare the four algorithms (original, rescaled, banded and score-limited DP algorithm) for computing SAF likelihoods, we generated aligned short-read sequencing data by changing sequencing coverage (3, 5 and 10) and sample size (50, 100, 300, 500 and 1000 diploid individuals).\nFor this purpose, we used low-coverage sequencing data for 50 diploid GBR individuals in the 1000 Genome Project, and then compared the SAF likelihood computed by the three algorithms (rescaled, banded and score-limited) at multiple random sites with the same best-guess allele frequency in the sample.", 
  "affiliations": [
    " Department of Human Genetics University of Chicago ", 
    " Department of Biostatistics University of California "
  ], 
  "grants": [
    "Funding\nThis study was funded by National Institutes of Health [T32 HG002536 to E.H., GM053275 to J.S."
  ], 
  "acks": " We thank Darren Kessner for his assistance with the sequencing simulations. This study was funded by National Institutes of Health [T32 HG002536 to E.H., GM053275 to J.S. and HG007089 to J.N.]. Conflict of interest: none declared. ", 
  "authors": [
    " Eunjung Han", 
    " Janet S Sinsheimer", 
    " John Novembre"
  ], 
  "keyWords": [
    "sequence data", 
    [
      "sequencing", 
      "computational", 
      "algorithms", 
      "sites", 
      "genetical"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-10-31T00:44:56Z"
}{
  "doi": "10.1093/bioinformatics/btu422", 
  "name": "FindPath a Matlab solution for in silico design of synthetic metabolic pathways", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://metasys.insa-toulouse.fr"
  ], 
  "title": "FindPath: a Matlab solution for in silico design of synthetic metabolic pathways", 
  "toolName": "FindPath: a Matlab solution for in silico design of synthetic metabolic pathways", 
  "abstract": "Several methods and computational tools have been developed to design novel metabolic pathways. A major challenge is evaluating the metabolic efficiency of the designed pathways in the host organism. Here we present FindPath, a unified system to predict and rank possible pathways according to their metabolic efficiency in the cellular system. This tool uses a chemical reaction database to generate possible metabolic pathways and exploits constraint-based models (CBMs) to identify the most efficient synthetic pathway to achieve the desired metabolic function in a given host microorganism. FindPath can be used with common tools for CBM manipulation and uses the standard SBML format for both input and output files. Availability and implementation: http://metasys.insa-toulouse.fr/ software/findpath/.", 
  "summary": "FindPath can be used with common tools for CBM manipulation and uses the standard SBML format for both input and output files.\nSBML format.\nStarting with an SAR database of 29 reactions and a genome-scale model of 1412 reactions, i.e. iMM904 (Zomorrodi and Maranas, 2010), FindPath was able to identify 15 potential pathways in 51 min in an XEON E5410 (8 cores) and 16 GB of RAM.\nIt provides additional features to the COBRA-Toolbox and can also be used in addition--or in parallel--to other computational tools that accept SBML formats.\nNat. Protoc., 2, 727738.\nBiotechnol.\nBiotechnol.\nTrends Biotechnol., 21, 6469.\nNat. Rev.", 
  "affiliations": [], 
  "grants": [
    "Funding: PROMYSE, 7th FWP, KBBE.2011.3.6-04 Applying Synthetic Biology principles towards the cell factory notion in biotechnology."
  ], 
  "acks": " ", 
  "authors": [
    " Gilles Vieira", 
    " Marc Carnicer", 
    " Jean-Charles Portais", 
    " St Ephanie Heux"
  ], 
  "keyWords": [
    "metabolic pathways", 
    [
      "reactions", 
      "findpath", 
      "pathway", 
      "bioinformatics", 
      "modeling", 
      "metabolism"
    ]
  ], 
  "sourcelinks": [
    "http://metasys.insa-toulouse.fr"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-04T02:04:38Z"
}{
  "doi": "10.1093/bioinformatics/btu780", 
  "name": "Faster sequence homology searches by clustering subsequences", 
  "links": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.hmpdacc.org", 
    "http://www"
  ], 
  "title": "Faster sequence homology searches by clustering subsequences", 
  "toolName": "Faster sequence homology searches by clustering subsequences", 
  "abstract": "Motivation: Sequence homology searches are used in various fields. New sequencing technologies produce huge amounts of sequence data, which continuously increase the size of sequence databases. As a result, homology searches require large amounts of computational time, especially for metagenomic analysis. Results: We developed a fast homology search method based on database subsequence clustering , and implemented it as GHOSTZ. This method clusters similar subsequences from a database to perform an efficient seed search and ungapped extension by reducing alignment candidates based on triangle inequality. The database subsequence clustering technique achieved an $2-fold increase in speed without a large decrease in search sensitivity. When we measured with metage-nomic data, GHOSTZ is $2.2\u20132.8 times faster than RAPSearch and is $185\u2013261 times faster than BLASTX. Availability and implementation: The source code is freely available for download at", 
  "summary": "This method clusters similar subsequences from a database to perform an efficient seed search and ungapped extension by reducing alignment candidates based on triangle inequality.\nIn the database subsequence clustering and seed search processes, the query and database amino acid sequences are both converted to a reduced amino acid alphabet to increase search sensitivity.\nWe reduced the number of ungapped alignment extensions by clustering subsequences in a database, and achieved a 2-fold acceleration in processing speed without a drop in search sensitivity.\nThe proposed database subsequence clustering method could also be useful in proteome research, which requires a huge number of sequence homology searches.", 
  "affiliations": [
    " Graduate School of Information Science and Engineering Tokyo Institute of Technology "
  ], 
  "grants": [
    "Funding\nThis work was supported by a Grant-in-Aid for the Japan Society for the Promotion of Science (JSPS) Fellows [Grant number 248766], the Strategic Programs for Innovative Research (SPIRE) Field 1 Supercomputational Life Science of the Ministry of Education, Culture, Sports, Science and Technology (MEXT) in Japan and Cancer Research Development funding from the National Cancer Center, Japan."
  ], 
  "acks": " The authors thank Prof. Ken Kurokawa and Dr Takuji Yamada for their helpful discussion of metagenomic analysis. This work was supported by a Grant-in-Aid for the Japan Society for the Promotion of Science (JSPS) Fellows, the Strategic Programs for Innovative Research (SPIRE) Field 1 Supercomputational Life Science of the Ministry of Education, Culture, Sports, Science and Technology (MEXT) in Japan and Cancer Research Development funding from the National Cancer Center, Japan. Conflict of Interest: none declared. ", 
  "authors": [
    " Shuji Suzuki", 
    " Masanori Kakuta", 
    " Takashi Ishida", 
    " Yutaka Akiyama"
  ], 
  "keyWords": [
    "database subsequence", 
    "clustering subsequences", 
    [
      "sequencers", 
      "bioinformatics", 
      "searching", 
      "ghostz", 
      "databases", 
      "clustered"
    ]
  ], 
  "sourcelinks": [
    "http://www"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-11-29T02:15:47Z"
}{
  "doi": "10.1093/bioinformatics/btu807", 
  "name": "flowCL ontologybased cell population labelling in flow cytometry", 
  "links": [
    "http://www.bioconductor.org/packages/devel/bioc/html/flowCL.html", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://cell.ctde.net:8080", 
    "http://purl.obolibrary.org/obo", 
    "http://www.w3.org/TR/rdf-sparql-query"
  ], 
  "title": "Databases and ontologies flowCL: ontology-based cell population labelling in flow cytometry", 
  "toolName": "Databases and ontologies flowCL: ontology-based cell population labelling in flow cytometry", 
  "abstract": "Motivation: Finding one or more cell populations of interest, such as those correlating to a specific disease, is critical when analysing flow cytometry data. However, labelling of cell populations is not well defined, making it difficult to integrate the output of algorithms to external knowledge sources. Results: We developed flowCL, a software package that performs semantic labelling of cell populations based on their surface markers and applied it to labelling of the Federation of Clinical Immunology Societies Human Immunology Project Consortium lyoplate populations as a use case. Conclusion: By providing automated labelling of cell populations based on their immunopheno-type, flowCL allows for unambiguous and reproducible identification of standardized cell types. Availability and implementation: Code, R script and documentation are available under the Artistic 2.0 license through Bioconductor", 
  "summary": "Conclusion: By providing automated labelling of cell populations based on their immunophenotype, flowCL allows for unambiguous and reproducible identification of standardized cell types.\nThe core module of flowCL is an ontology labeller that attempts to provide a semantic identifier to a cell population based on its marker expression profile (i.e., the immunophenotype).\nUsing flowCL and the immunophenotype `CD3CD4CD8-CCR7-CD45RA', an exact match (all markers of the input immunophenotype, along with their specific abundance, correspond to all of the markers that make up the cell type's definition in CL) was returned and the population was labelled `effector CD4-positive, alpha-beta T cell' (Fig. 1, top left quadrant).", 
  "affiliations": [
    " School of Dental Medicine University at Buffalo ", 
    " Fred Hutchinson Cancer Research Center", 
    " Institute for Immunity Transplantation and Infection Stanford University School of Medicine ", 
    " Terry Fox Laboratory British Columbia Cancer Agency ", 
    " Molecular Biology and Biochemistry Department Simon Fraser University ", 
    " Department of Neurology University at Buffalo School of Medicine and Biomedical Sciences "
  ], 
  "grants": [
    "Funding\nThis work was supported by National Institutes of Health (NIH)/National Institute of Biomedical Imaging and Bioengineering (NIBIB) [R01 EB008400],\n\n\fflowCL\n\n1339\n\nHuman Immunology Project Consortium (HIPC) [U19 AI089986], Natural Sciences and Engineering Research Council of Canada, National Institute of General Medical Sciences (NIGMS) 2R01GM080646-06 (Protein Ontology) and HHSN272201200028C (ImmPort)."
  ], 
  "acks": " ", 
  "authors": [
    " M\u00e9 Lanie Courtot", 
    " Justin Meskas", 
    " Alexander D Diehl", 
    " Radina Droumeva", 
    " Raphael Gottardo", 
    " Adrin Jalali", 
    " Mohammad Jafar Taghiyar", 
    " Holden T Maecker", 
    " J Philip Mccoy", 
    " Alan Ruttenberg", 
    " Richard H Scheuermann", 
    " Ryan R Brinkman"
  ], 
  "keyWords": [
    [
      "cells", 
      "flowcl", 
      "labelling"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://cell.ctde.net:8080"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-12-07T01:08:27Z"
}{
  "doi": "10.1093/bioinformatics/btu491", 
  "name": "FisHiCal an R package for iterative FISHbased calibration of HiC data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://cran.r-project.org"
  ], 
  "title": "Genome analysis FisHiCal: an R package for iterative FISH-based calibration of Hi-C data", 
  "toolName": "Genome analysis FisHiCal: an R package for iterative FISH-based calibration of Hi-C data", 
  "abstract": "The fluorescence in situ hybridization (FISH) method has been providing valuable information on physical distances between loci (via image analysis) for several decades. Recently, high-throughput data on nearby chemical contacts between and within chromosomes became available with the Hi-C method. Here, we present FisHiCal, an R package for an iterative FISH-based Hi-C calibration that exploits in full the information coming from these methods. We describe here our calibration model and present 3D inference methods that we have developed for increasing its usability, namely, 3D reconstruction through local stress minimization and detection of spatial inconsistencies. We next confirm our calibration across three human cell lines and explain how the output of our methods could inform our model, defining an iterative calibration pipeline, with applications for quality assessment and meta-analysis. Availability and implementation: FisHiCal v1.1 is available from http://cran.r-project.org/.", 
  "summary": "Here we present FisHiCal, an R package for integrating Hi-C and FISH data, which offers a modular and easy-to-use tool for chromosomal spatial analysis.\nWith FisHiCal, researchers can prepare and apply FISH-based Hi-C calibration, which converts contact frequencies into distances while taking into consideration range limitations in Hi-C data.\nFisHiCal: an R package for iterative FISH-based calibration of Hi-C data\nWith more FISH and Hi-C data becoming available, building chromosomes maps with accurate scale and carrying time-series calibration analysis will be made possible with FisHiCal (Supplementary Figures S6 and S7).", 
  "affiliations": [
    " Computer Laboratory University of Cambridge ", 
    " Cambridge Systems Biology Centre University of Cambridge "
  ], 
  "grants": [
    "Funding: We thank FP7-Health-F5-2012, under grant agreement no 305280 (MIMOmics)."
  ], 
  "acks": " We thank Dr Sandra Goetze for kindly providing us with FISH measurements. We thank Dr Sabrina Tosi and Dr Chris Town for helpful discussion on technical and clinical aspects of FISH analysis. ", 
  "authors": [
    " Yoli Shavit", 
    " Fiona Kathryn Hamey", 
    " Pietro Lio", 
    " John Hancock"
  ], 
  "keyWords": [
    "fish calibration", 
    [
      "fishical", 
      "calibrated", 
      "spatially", 
      "genomes"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-25T04:43:31Z"
}{
  "doi": "10.1093/bioinformatics/btu677", 
  "name": "flowDensity reproducing manual gating of flow cytometry data by automated densitybased cell population identification", 
  "links": [
    "http://master.bioconductor.org/packages", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Systems biology flowDensity: reproducing manual gating of flow cytometry data by automated density-based cell population identification", 
  "toolName": "Systems biology flowDensity: reproducing manual gating of flow cytometry data by automated density-based cell population identification", 
  "abstract": "flowDensity facilitates reproducible, high-throughput analysis of flow cytometry data by automating a predefined manual gating approach. The algorithm is based on a sequential bivariate gating approach that generates a set of predefined cell populations. It chooses the best cutoff for individual markers using characteristics of the density distribution. The Supplementary Material is linked to the online version of the manuscript. Availability and implementation: R source code freely available through BioConductor", 
  "summary": "ABSTRACT Summary: flowDensity facilitates reproducible, high-throughput analysis of flow cytometry data by automating a predefined manual gating approach.\nTo address these issues, we developed flowDensity, an automated gating approach that emulates an expert's sequential 2D gating strategy to identify predefined cell populations using a sequential bivariate gating algorithm.\nflowDensity estimates the region around cell populations using characteristics of the marker density distribution (e.g. the number, height and width of peaks and the slope of the distribution curve).\nflowDensity's completely automated results match that of expert users when it was possible to set cell population boundaries in a data-driven manner (Fig. 1a).", 
  "affiliations": [
    " Bioinformatics Training Program University of British Columbia ", 
    " Vaccine and Infectious Disease Division Fred Hutchinson Cancer Research Center ", 
    " Terry Fox Laboratory BC Cancer Agency Research Centre "
  ], 
  "grants": [
    "Funding: This work was supported by National Institutes of Health (R01 EB008400), the Human Immunology Consortium (U19 AI089986) and National Science and Engineering Research Council."
  ], 
  "acks": " Thanks to Kelly Lundsten for helpful discussion on using FMO controls, Jonathan Bramson for control samples, Herv e Luche for mouse data and comments and Mike Jiang for software review. ", 
  "authors": [
    " Mehrnoush Malek", 
    " Mohammad Jafar Taghiyar", 
    " Lauren Chong", 
    " Greg Finak", 
    " Raphael Gottardo", 
    " Ryan R Brinkman"
  ], 
  "keyWords": [
    "cell population", 
    [
      "flowdensity", 
      "populations", 
      "cells", 
      "gating", 
      "bioinformatics", 
      "gate", 
      "data", 
      "researchers"
    ]
  ], 
  "sourcelinks": [
    "http://master.bioconductor.org/packages"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btu864", 
  "name": "Functional Gene Networks RBioc package to generate and analyse gene networks derived from functional enrichment and clustering", 
  "links": [
    "http://bioconductor.org/packages/release/bioc/html/FGNet.html", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Systems biology", 
  "toolName": "Systems biology", 
  "abstract": "Functional Gene Networks (FGNet) is an R/Bioconductor package that generates gene networks derived from the results of functional enrichment analysis (FEA) and annotation clustering. The sets of genes enriched with specific biological terms (obtained from a FEA platform) are transformed into a network by establishing links between genes based on common functional annotations and common clusters. The network provides a new view of FEA results revealing gene modules with similar functions and genes that are related to multiple functions. In addition to building the functional network, FGNet analyses the similarity between the groups of genes and provides a distance heatmap and a bipartite network of functionally overlapping genes. The application includes an interface to directly perform FEA queries using different external tools: DAVID, GeneTerm Linker, TopGO or GAGE; and a graphical interface to facilitate the use. Availability and implementation: FGNet is available in Bioconductor, including a tutorial.", 
  "summary": "Summary: Functional Gene Networks (FGNet) is an R/Bioconductor package that generates gene networks derived from the results of functional enrichment analysis (FEA) and annotation clustering.\nFGNet builds functional networks based on the groups obtained from clustering gene-term sets (gtsets, genes and terms associated by an enrichment p-value) returned by a FEA.\nThe package includes an interface to do queries with gene lists using four FEA tools: DAVID with Functional Annotation Clustering (that returns clustered gtsets, Cl); GAGE (that also provides clusters) (Luo et al., 2009); GeneCodis with GeneTerm Linker (that returns metagroups, Mg) and TopGO (that only returns gtsets) (Alexa et al., 2010).", 
  "affiliations": [
    " CSIC/USAL/IBSAL) Bioinformatics and Functional Genomics Research Group Cancer Research Center (Consejo Superior de Investigaciones Cient\u00edficas Universidad de Salamanca and Instituto de Investigaci\u00f3 n Biom\u00e9 dica de Salamanca "
  ], 
  "grants": [
    "Funding\nThis work was supported by the \"Accion Estrategica en Salud\" (AES) of the \"Instituto de Salud Carlos III\" (ISCiii) from the Spanish Government (projects granted to J.D.L.R.", 
    ": PS09/00843 and PI12/00624); and by the \"Consejeria de Educacio n\" of the \"Junta Castilla y Leon\" (JCyL) and the European Social Fund (ESF) with grants given to S.A. and C.D."
  ], 
  "acks": " ", 
  "authors": [
    " Sara Aibar", 
    " Celia Fontanillo", 
    " Conrad Droste", 
    " Javier De", 
    " Las Rivas"
  ], 
  "keyWords": [
    [
      "clustering", 
      "genes", 
      "fgnet", 
      "bioinformatics", 
      "networks", 
      "functionally"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2015-01-20T01:59:41Z"
}{
  "doi": "10.1093/bioinformatics/btu627", 
  "name": "FungiFun2 a comprehensive online resource for systematic analysis of gene lists from fungal species", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/bync/4.0", 
    "https://elbe.hki-jena.de/fungifun"
  ], 
  "title": "Databases and ontologies FungiFun2: a comprehensive online resource for systematic analysis of gene lists from fungal species", 
  "toolName": "Databases and ontologies FungiFun2: a comprehensive online resource for systematic analysis of gene lists from fungal species", 
  "abstract": "Systematically extracting biological meaning from omics data is a major challenge in systems biology. Enrichment analysis is often used to identify characteristic patterns in candidate lists. FungiFun is a user-friendly Web tool for functional enrichment analysis of fungal genes and proteins. The novel tool FungiFun2 uses a completely revised data management system and thus allows enrichment analysis for 298 currently available fungal strains published in standard databases. FungiFun2 offers a modern Web interface and creates interactive tables, charts and figures, which users can directly manipulate to their needs. Availability and implementation: FungiFun2, examples and tutorials are publicly available at https://elbe.hki-jena.de/", 
  "summary": "The novel tool FungiFun2 uses a completely revised data management system and thus allows enrichment analysis for 298 currently available fungal strains published in standard databases.\nOur group implemented the tool FungiFun (Priebe et al., 2010) supporting enrichment analysis for 28 species with a focus on fungal pathogens.\nFor data collection, FungiFun2 uses a semi-automatic procedure, which downloads gene to category associations and annotations (names and functions) from online databases.\ne. Gene Ontology (GO; Ashburner et al., 2000), Kyoto Encyclopedia of Genes and Genomes (KEGG; Kanehisa and Goto, 2000) and Functional Catalogue (FunCat; Ru pp et al., 2004).", 
  "affiliations": [
    " Research Group Systems Biology/Bioinformatics Leibniz Institute for Natural Product Research and Infection Biology -Hans-Kn \u20ac oll-Institute "
  ], 
  "grants": [
    "Funding: J.L."
  ], 
  "acks": " ", 
  "authors": [
    " Steffen Priebe", 
    " Christian Kreisel", 
    " Fabian Horn", 
    " Reinhard Guthke", 
    " J \u20ac Org Linde", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "genomics", 
      "databases", 
      "genes", 
      "fungifun", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-08T00:30:13Z"
}{
  "doi": "10.1093/bioinformatics/btu696", 
  "name": "FlaiMapper computational annotation of small ncRNAderived fragments using RNAseq highthroughput data", 
  "links": [
    "https://github.com/yhoogstrate/flaimapper", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "FlaiMapper: computational annotation of small ncRNA-derived fragments using RNA-seq high-throughput data", 
  "toolName": "flaimapper", 
  "abstract": "Motivation: Recent discoveries show that most types of small non-coding RNAs (sncRNAs) such as miRNAs, snoRNAs and tRNAs get further processed into putatively active smaller RNA species. Their roles, genetic profiles and underlying processing mechanisms are only partially understood. To find their quantities and characteristics, a proper annotation is essential. Here, we present FlaiMapper, a method that extracts and annotates the locations of sncRNA-derived RNAs (sncdRNAs). These sncdRNAs are often detected in sequencing data and observed as fragments of their precursor sncRNA. Using small RNA-seq read alignments, FlaiMapper is able to annotate fragments primarily by peak detection on the start and end position densities followed by filtering and a reconstruction process. Results: To assess performance of FlaiMapper, we used independent publicly available small RNA-seq data. We were able to detect fragments representing putative sncdRNAs from nearly all types of sncRNA, including 97.8% of the annotated miRNAs in miRBase that have supporting reads. Comparison of FlaiMapper-predicted boundaries of miRNAs with miRBase entries demonstrated that 89% of the start and 54% of the end positions are identical. Additional benchmarking showed that FlaiMapper is superior in performance compared with existing software. Further analysis indicated a variety of characteristics in the fragments, including sequence motifs and relations with RNA interacting factors. These characteristics set a good basis for further research on sncdRNAs. Availability and implementation: The platform independent GPL licensed Python 2.7 code is available at: https://github.com/yhoogstrate/flaimapper. Corresponding Linux-specific scripts and annotations can be found in the same repository.", 
  "summary": "Using small RNA-seq read alignments, FlaiMapper is able to annotate fragments primarily by peak detection on the start and end position densities followed by filtering and a reconstruction process.\nTo get an impression of the influence of sequencing depth on accuracy of start and end positions corresponding to miRNA and miRNA* predictions, the number of corresponding reads (intensity) was plotted as a function of the offset for dataset SRP002175.\nExamination of FlaiMapper-predicted sncdRNAs indicated different type specific characteristics: 50=30-end-specific variability in miRNAs, associations between AGO and relative fragment profiles in dataset SRP006788 and a position-specific sequence motif in a subset of the H/ACA-box fragments.", 
  "affiliations": [
    " Department of Urology Erasmus University Medical Center "
  ], 
  "grants": [
    "Funding\nThe research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n201438 and from the research programme ALW-VENI Grant 863.12.014 financed by the Netherlands Organisation for Scientific Research (NWO)."
  ], 
  "sourcelinks": [
    "https://github.com/yhoogstrate/flaimapper"
  ], 
  "acks": " The authors would also like to thank Bas Pigmans for his work on sequencing alignment methodology. ", 
  "authors": [
    " Youri Hoogstrate", 
    " Guido Jenster", 
    " Elena S Martens-Uzunova"
  ], 
  "keyWords": [
    [
      "mirnas", 
      "reads", 
      "sequencers", 
      "fragments", 
      "positively"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v3.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/yhoogstrate/flaimapper/master/LICENSE"
      }
    ], 
    "name": "flaimapper", 
    "contributors": [
      {
        "contributions": 224, 
        "html_url": "https://github.com/yhoogstrate"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/yhoogstrate/flaimapper/zipball/v1.2.2", 
        "tarball_url": "https://api.github.com/repos/yhoogstrate/flaimapper/tarball/v1.2.2", 
        "name": "v1.2.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/yhoogstrate/flaimapper/zipball/v1.2.0", 
        "tarball_url": "https://api.github.com/repos/yhoogstrate/flaimapper/tarball/v1.2.0", 
        "name": "v1.2.0"
      }
    ], 
    "created_at": "2013-10-31T10:30:28Z", 
    "updated_at": "2016-02-04T08:38:02Z", 
    "languages": [
      "Python", 
      "Shell", 
      "R"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/yhoogstrate"
      }, 
      {
        "html_url": "https://github.com/jcchai"
      }, 
      {
        "html_url": "https://github.com/billhibazzz"
      }
    ], 
    "owner": "https://github.com/yhoogstrate", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-23T00:08:46Z"
}{
  "doi": "10.1093/bioinformatics/btu576", 
  "name": "Frameshift alignment statistics and postgenomic applications", 
  "links": [
    "http://last.cbrc.jp/falp", 
    "http://www.ncbi.nlm.nih.gov/CBBresearch/Spouge", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Frameshift alignment: statistics and post-genomic applications", 
  "toolName": "Frameshift alignment: statistics and post-genomic applications", 
  "abstract": "Motivation: The alignment of DNA sequences to proteins, allowing for frameshifts, is a classic method in sequence analysis. It can help identify pseudogenes (which accumulate mutations), analyze raw DNA and RNA sequence data (which may have frameshift sequencing errors), investigate ribosomal frameshifts, etc. Often, however, only ad hoc approximations or simulations are available to provide the statistical significance of a frameshift alignment score. Results: We describe a method to estimate statistical significance of frameshift alignments, similar to classic BLAST statistics. (BLAST presently does not permit its alignments to include frameshifts.) We also illustrate the continuing usefulness of frameshift alignment with two 'post-genomic' applications: (i) when finding pseudogenes within the human genome, frameshift alignments show that most anciently conserved non-coding human elements are recent pseudogenes with conserved ancestral genes; and (ii) when analyzing metagenomic DNA reads from polluted soil, frameshift alignments show that most alignable metagenomic reads contain frameshifts, suggesting that metagenomic analysis needs to use frameshift alignment to derive accurate results. Availability and implementation: The statistical calculation is available in FALP", 
  "summary": "(BLAST presently does not permit its alignments to include frameshifts.) We also illustrate the continuing usefulness of frameshift alignment with two `post-genomic' applications: (i) when finding pseudogenes within the human genome, frameshift alignments show that most anciently conserved non-coding human elements are recent pseudogenes with conserved ancestral genes; and (ii) when analyzing metagenomic DNA reads from polluted soil, frameshift alignments show that most alignable metagenomic reads contain frameshifts, suggesting that metagenomic analysis needs to use frameshift alignment to derive accurate results.\n(1996) Alignments of DNA and protein sequences containing frameshift errors.", 
  "affiliations": [
    " National Center for Biotechnology Information National Library of Medicine ", 
    " Computational Biology Research Center National Institute of Advanced Industrial Science and Technology "
  ], 
  "grants": [
    "Funding: This research was supported by the Intramural Research Program of the NIH, National Library of Medicine."
  ], 
  "acks": " ", 
  "authors": [
    " Sergey L Sheetlin", 
    " Yonil Park", 
    " Martin C Frith", 
    " John L Spouge", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "proteins", 
      "genomics", 
      "alignments", 
      "sequencing", 
      "frameshifts"
    ]
  ], 
  "sourcelinks": [
    "http://last.cbrc.jp/falp", 
    "http://www.ncbi.nlm.nih.gov/CBBresearch/Spouge"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-30T00:28:26Z"
}{
  "doi": "10.1093/bioinformatics/btu673", 
  "name": "FuncPatch a web server for the fast Bayesian inference of conserved functional patches in protein 3D structures", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://info.mcmaster.ca/yifei/FuncPatch", 
    "http://dx.doi.rog/10.1038"
  ], 
  "title": "Sequence analysis FuncPatch: a web server for the fast Bayesian inference of conserved functional patches in protein 3D structures", 
  "toolName": "Sequence analysis FuncPatch: a web server for the fast Bayesian inference of conserved functional patches in protein 3D structures", 
  "abstract": "Motivation: A number of statistical phylogenetic methods have been developed to infer conserved functional sites or regions in proteins. Many methods, e.g. Rate4Site, apply the standard phylogenetic models to infer site-specific substitution rates and totally ignore the spatial correlation of substitution rates in protein tertiary structures, which may reduce their power to identify conserved functional patches in protein tertiary structures when the sequences used in the analysis are highly similar. The 3D sliding window method has been proposed to infer conserved functional patches in protein tertiary structures, but the window size, which reflects the strength of the spatial correlation, must be predefined and is not inferred from data. We recently developed GP4Rate to solve these problems under the Bayesian framework. Unfortunately, GP4Rate is computationally slow. Here, we present an intuitive web server, FuncPatch, to perform a fast approximate Bayesian inference of conserved functional patches in protein tertiary structures. Results: Both simulations and four case studies based on empirical data suggest that FuncPatch is a good approximation to GP4Rate. However, FuncPatch is orders of magnitudes faster than GP4Rate. In addition, simulations suggest that FuncPatch is potentially a useful tool complementary to Rate4Site, but the 3D sliding window method is less powerful than FuncPatch and Rate4Site. The functional patches predicted by FuncPatch in the four case studies are supported by experimental evidence, which corroborates the usefulness of FuncPatch. Availability and implementation: The software FuncPatch is freely available at the web site, http://info.mcmaster.ca/yifei/FuncPatch", 
  "summary": "Many methods, e.g. Rate4Site, apply the standard phylogenetic models to infer site-specific substitution rates and totally ignore the spatial correlation of substitution rates in protein tertiary structures, which may reduce their power to identify conserved functional patches in protein tertiary structures when the sequences used in the analysis are highly similar.\nIt is expected that the spatial correlation of substitution rates is present in this dataset, since a previous study based on Rate4Site has suggested that the inferred conserved sites are clustered together in the protein tertiary structure of the human Bcl-xL protein (Glaser et al., 2003).", 
  "affiliations": [
    " Department of Biology McMaster University "
  ], 
  "grants": [
    "Funding: This work was financially supported by a National Sciences and Engineering Research Council of Canada (NSERC) Discovery grant to GBG [grant number RGPIN140221-10]."
  ], 
  "acks": " We thank Ben Evans, Terri Porter, Wilson Sung and the anonymous reviewers for insightful comments on this work. ", 
  "authors": [
    " Yi-Fei Huang", 
    " G Brian Golding", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "methods", 
      "proteins", 
      "funcpatch", 
      "sites", 
      "rates", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [
    "http://info.mcmaster.ca/yifei/FuncPatch"
  ], 
  "technologies": [
    "C", 
    "C++"
  ], 
  "dateCreated": "2014-10-17T05:05:29Z"
}{
  "doi": "10.1093/bioinformatics/btu535", 
  "name": "gCUP rapid GPUbased HIV1 coreceptor usage prediction for nextgeneration sequencing", 
  "links": [
    "http://www.heiderlab.de", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Sequence analysis gCUP: rapid GPU-based HIV-1 co-receptor usage prediction for next-generation sequencing", 
  "toolName": "Sequence analysis gCUP: rapid GPU-based HIV-1 co-receptor usage prediction for next-generation sequencing", 
  "abstract": "Next-generation sequencing (NGS) has a large potential in HIV diagnostics, and genotypic prediction models have been developed and successfully tested in the recent years. However, albeit being highly accurate, these computational models lack computational efficiency to reach their full potential. In this study, we demonstrate the use of graphics processing units (GPUs) in combination with a computational prediction model for HIV tropism. Our new model named gCUP, parallelized and optimized for GPU, is highly accurate and can classify 4175 000 sequences per second on an NVIDIA GeForce GTX 460. The computational efficiency of our new model is the next step to enable NGS technologies to reach clinical significance in HIV diagnostics. Moreover, our approach is not limited to HIV tropism prediction, but can also be easily adapted to other settings, e.g. drug resistance prediction. Availability and implementation: The source code can be down-loaded at http://www.heiderlab.de Contact", 
  "summary": "There exist some computational models for HIV tropism prediction, e.g. Dybowski et al.\nthe different steps that are performed within gCUP, namely QC, alignment of the reads against reference V3 sequence and V3 extraction and hydrophobicity and ESP classification (Fig. 1B).\nIt can also be used for other prediction models, e.g. protease inhibitor resistance and reverse transcriptase inhibitors with comparable speedups (Heider et al., 2013; Lengauer and Sing, 2006), as the current implementation can handle sequences up to a maximum length of 65 535 amino acids, which is enough for all known proteins.", 
  "affiliations": [
    " Institute of Computer Science University of Muenster ", 
    " Department of Bioinformatics University of Applied Sciences Weihenstephan-Triesdorf "
  ], 
  "grants": [
    "Funding: This work was supported by the Straubing Center of Science."
  ], 
  "acks": " ", 
  "authors": [
    " Michael Olejnik", 
    " Michel Steuwer", 
    " Sergei Gorlatch", 
    " Dominik Heider"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "gcup", 
      "performance", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://www.heiderlab.de"
  ], 
  "technologies": [
    "R", 
    "OpenCL"
  ], 
  "dateCreated": "2014-08-15T05:12:51Z"
}{
  "doi": "10.1093/bioinformatics/btu669", 
  "name": "GeneNet Toolbox for MATLAB a flexible platform for the analysis of gene connectivity in biological networks", 
  "links": [
    "http://avigailtaylor.github.io/gntat14", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Systems biology GeneNet Toolbox for MATLAB: a flexible platform for the analysis of gene connectivity in biological networks", 
  "toolName": "gntat14", 
  "abstract": "We present GeneNet Toolbox for MATLAB (also available as a set of standalone applications for Linux). The toolbox, available as command-line or with a graphical user interface, enables biologists to assess connectivity among a set of genes of interest ('seed-genes') within a biological network of their choosing. Two methods are implemented for calculating the significance of connectivity among seed-genes: 'seed randomization' and 'network permutation'. Options include restricting analyses to a specified subnetwork of the primary biological network, and calculating connectivity from the seed-genes to a second set of interesting genes. Pre-analysis tools help the user choose the best connectivity-analysis algorithm for their network. The toolbox also enables visualization of the connections among seed-genes. GeneNet Toolbox functions execute in reasonable time for very large networks (10 million edges) on a desktop computer. Availability and implementation: GeneNet Toolbox is open source and freely available from http://avigailtaylor.github.io/gntat14.", 
  "summary": "The toolbox, available as command-line or with a graphical user interface, enables biologists to assess connectivity among a set of genes of interest (`seed-genes') within a biological network of their choosing.\nequal in size to the set of seed-genes, and obtain an empirical P-value by comparing the direct seed connectivity to the connectivity of the random gene-sets (see Supplementary Figure S2A); conversely, in `network permutation' (NP), we keep seed-genes the same, permute the edges of the network many times (while preserving node degree and network clustering structure), and obtain an empirical P-value by comparing the direct seed connectivity in the real versus permuted networks (see Supplementary Figure S2B and Supplementary Information for algorithmic details).", 
  "affiliations": [
    " The Wellcome Trust Centre for Human Genetics University of Oxford ", 
    " Department of Physiology, Anatomy and Genetics MRC Functional Genomics Unit University of Oxford "
  ], 
  "grants": [
    "Funding: This work was supported by the Medical Research Council (AT, CW); the Wellcome Trust [093941/Z/10/Z] (JS), [090532/Z/09/Z] (The Wellcome Trust Centre for Human Genetics); the European Union's Seventh Framework Programme project GENCODYS [241995] (CW, AT); and Somerville-Clarendon and Natural Sciences and Engineering Research Council of Canada Scholarships (TA)."
  ], 
  "sourcelinks": [
    "http://avigailtaylor.github.io/gntat14"
  ], 
  "acks": " ", 
  "authors": [
    " Avigail Taylor", 
    " Julia Steinberg", 
    " Tallulah S Andrews", 
    " Caleb Webber", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "biological networks", 
    "gene connectivity", 
    [
      "biology", 
      "permutations", 
      "network", 
      "genes", 
      "connections", 
      "seed", 
      "bioinformatics", 
      "users"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v3.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/avigailtaylor/gntat14/master/LICENSE.txt"
      }
    ], 
    "name": "gntat14", 
    "contributors": [
      {
        "contributions": 6, 
        "html_url": "https://github.com/avigailtaylor"
      }
    ], 
    "versions": [], 
    "created_at": "2014-10-15T16:31:25Z", 
    "updated_at": "2014-10-15T16:48:00Z", 
    "languages": [
      "Objective-C", 
      "Shell", 
      "M", 
      "Matlab", 
      "Perl"
    ], 
    "subscribers": [], 
    "owner": "https://github.com/avigailtaylor", 
    "homepage": null
  }, 
  "technologies": [
    "MATLAB", 
    "Perl"
  ], 
  "dateCreated": "2014-10-16T01:58:54Z"
}{
  "doi": "10.1093/bioinformatics/btu775", 
  "name": "genomation a toolkit to summarize annotate and visualize genomic intervals", 
  "links": [
    "http://goo.gl/vHTExn.FundingThe", 
    "http://bioinformatics.mdc-berlin.de/genomation", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "genomation: a toolkit to summarize, annotate and visualize genomic intervals", 
  "toolName": "genomation: a toolkit to summarize, annotate and visualize genomic intervals", 
  "abstract": "Biological insights can be obtained through computational integration of genomics data sets consisting of diverse types of information. The integration is often hampered by a large variety of existing file formats, often containing similar information, and the necessity to use complicated tools to achieve the desired results. We have built an R package, genomation, to expedite the extraction of biological information from high throughput data. The package works with a variety of genomic interval file types and enables easy summarization and annotation of high throughput data sets with given genomic annotations. Availability and implementation: The software is currently distributed under MIT artistic license and freely available at http://bioinformatics.mdc-berlin.de/genomation, and through the Bioconductor framework.", 
  "summary": "The package works with a variety of genomic interval file types and enables easy summarization and annotation of high throughput data sets with given genomic annotations.\nThe package provides functions for computing fast summary statistics about the level of association of genomic intervals with the desired annotation and enables simultaneous visualization of multiple highthroughput experiments over regions of interest through parallel heatmaps, meta-region (meta-gene, meta-promoter, etc.) plots, coverage distribution profiles and pie charts for overlap with annotation (features summarized in Fig. 1).\nIt significantly reduces the time needed for data processing and biological inference by providing a multitude of convenience functions for annotation, summarization and visualization of genomic intervals.", 
  "affiliations": [
    " Friedrich Miescher Institute for Biomedical Research", 
    " Department of Physiology and Biophysics and the Institute for Computational Biomedicine Weill Cornell Medical College ", 
    " Department of Molecular Biology Faculty of Science Bioinformatics Group University of Zagreb "
  ], 
  "grants": [
    "], EMBO Young Investigator Program [installation grant 1431/2006 to K.V.]", 
    "EC Seventh Framework Program [Integra-Life\n\ngrant 315997 to K.V.", 
    "Funding\nThe research in the laboratory of D.S.", 
    "and Croatian MSES [grant 119-0982913-1211 to K.V."
  ], 
  "acks": " ", 
  "authors": [
    " Altuna Akalin", 
    " Vedran Franke", 
    " Kristian Vlahovi\u010dek", 
    " Christopher E Mason", 
    " Dirk Sch\u00fc Beler"
  ], 
  "keyWords": [
    "genomics data", 
    "genomic intervals", 
    [
      "functionalities", 
      "genomation", 
      "interval", 
      "formatted", 
      "bioinformatics", 
      "biological"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.mdc-berlin.de/genomation"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-11-23T01:09:29Z"
}{
  "doi": "10.1093/bioinformatics/btu798", 
  "name": "GenePainter v 20 resolves the taxonomic distribution of intron positions", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://sourceforge.net", 
    "http://www.motorprotein.de/genepainter"
  ], 
  "title": "GenePainter v. 2.0 resolves the taxonomic distribution of intron positions", 
  "toolName": "sourceforge", 
  "abstract": "Conserved intron positions in eukaryotic genes can be used to reconstruct phylogenetic trees, to resolve ambiguous subfamily relationships in protein families and to infer the history of gene families. This version of GenePainter facilitates working with large datasets through options to select specific subsets for analysis and visualization, and through providing exhaustive statistics. GenePainter's application in phylogenetic analyses is considerably extended by the newly implemented integration of the exon\u2013intron pattern conservation with phylogenetic trees. Availability and implementation: The software along with detailed documentation is available at", 
  "summary": "Existing tools for the comparison of gene structures like Exalign (Pavesi et al., 2008), CIDA/CIWOG (Wilkerson et al., 2009), GECA (Fawal et al., 2012) and GenePainter (Hammesfahr et al., 2013) compare exon lengths or map intron positions to positions in multiple amino acid sequence alignments (MSA).\n2.0 provides a number of output files such as an extended multiple sequence alignment, gene structure alignments including a binary representation for evolutionary analyses, graphical outputs on base-pair and other scales (Fig. 1A), scripts for mapping intron positions onto protein structures at user-provided conservation levels, extensive statistics and an extended phylogenetic tree with intron gain and loss events plotted onto the respective branches as svg (Fig. 1B) and in Newick format.", 
  "affiliations": [
    " Department of NMR-based Structural Biology Max-Planck-Institute for Biophysical Chemistry Group Systems Biology of Motor Proteins "
  ], 
  "grants": [
    "Funding: This project has been funded by the Deutsche Forschungsgemeinschaft [DFG Grant KO 2251/13-1 to M.K.]"
  ], 
  "acks": " We thank Prof. Christian Griesinger for his continuous generous support, Fabian Meyer for helpful discussions and James Dong for extensive testing and bug reporting. ", 
  "authors": [
    " Stefanie M\u00fc", 
    " Marcel Hellkamp", 
    " Martin Kollmar"
  ], 
  "keyWords": [
    "eukaryotic genes", 
    [
      "introns", 
      "genomics", 
      "genepainter", 
      "eukaryotes", 
      "trees"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net", 
    "http://www.motorprotein.de/genepainter"
  ], 
  "technologies": [
    "Ruby"
  ], 
  "dateCreated": "2014-12-01T01:08:52Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/sourceforge/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/thimsmith/", 
        "name": "Tim Smith"
      }, 
      {
        "url": "https://sourceforge.net/u/brondsem/", 
        "name": "Dave Brondsema"
      }, 
      {
        "url": "https://sourceforge.net/u/wdavison/", 
        "name": "Wayne Davison"
      }
    ], 
    "Development Status": [
      {
        "status": "6 - Mature"
      }, 
      {
        "status": "5 - Production/Stable"
      }
    ], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu746", 
  "name": "GASS identifying enzyme active sites with genetic algorithms", 
  "links": [
    "http://scop.berkeley.edu", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Structural bioinformatics GASS: identifying enzyme active sites with genetic algorithms", 
  "toolName": "Structural bioinformatics GASS: identifying enzyme active sites with genetic algorithms", 
  "abstract": "Motivation: Currently, 25% of proteins annotated in Pfam have their function unknown. One way of predicting proteins function is by looking at their active site, which has two main parts: the catalytic site and the substrate binding site. The active site is more conserved than the other residues of the protein and can be a rich source of information for protein function prediction. This article presents a new heuristic method, named genetic active site search (GASS), which searches for given active site 3D templates in unknown proteins. The method can perform non-exact amino acid matches (conservative mutations), is able to find amino acids in different chains and does not impose any restrictions on the active site size. Results: GASS results were compared with those catalogued in the catalytic site atlas (CSA) in four different datasets and compared with two other methods: amino acid pattern search for substruc-tures and motif and catalytic site identification. The results show GASS can correctly identify >90% of the templates searched. Experiments were also run using data from the substrate binding sites prediction competition CASP 10, and GASS is ranked fourth among the 18 methods considered. Availability and implementation: Source code and datasets (dcc.ufmg.br/ $glpappa/gass).", 
  "summary": "The algorithm performs a protein-to-template matching using a sub-graph search method and a library of catalytic residue templates from catalytic site atlas (CSA; Porter et al., 2004)--a database of catalytic sites in enzymes of known 3D structure.\nIn the first, results generated by GASS are validated according to the enzymes catalytic sites catalogued in CSA (Bartlett et al., 2002).\nGASS and CSA results DS Enzymes Templates Catalytic sites\nOne example of an enzyme not annotated in CSA is 1ARC, where GASS identified the catalytic site HIS 57, ASP 113 and SER 194, which is in agreement with Tsunasawa et al.", 
  "affiliations": [
    " Advanced Campus at Itabira Universidade Federal de Itajub\u00e1 "
  ], 
  "grants": [
    "Funding\nThis work was supported by CAPES (BIOCOMPUTACIONAL process number 23038004007/2014-82, PVE process number 403076/2012-9), CNPq, FAPEMIG and all Brazilian funding agencies."
  ], 
  "acks": " Thanks to Daniel B. Roche and Douglas E. V. Pires for discussions and suggestions , and Fran\u00e7ois Marie Artiguenave and Genoscope staff (CEA, France). ", 
  "authors": [
    " Sandro C Izidoro", 
    " Raquel C De Melo-Minardi", 
    " Gisele L Pappa"
  ], 
  "keyWords": [
    [
      "templates", 
      "methods", 
      "enzymes", 
      "structurally", 
      "sites", 
      "gass"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-12T04:44:24Z"
}{
  "doi": "10.1093/bioinformatics/btu803", 
  "name": "GenomeCons a web server for manipulating multiple genome sequence alignments and their consensus sequences", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinfo.sls.kyushu-u"
  ], 
  "title": "GenomeCons: a web server for manipulating multiple genome sequence alignments and their consensus sequences", 
  "toolName": "GenomeCons: a web server for manipulating multiple genome sequence alignments and their consensus sequences", 
  "abstract": "Genome sequence alignments provide valuable information on many aspects of molecular biological processes. In this study, we developed a web server, GenomeCons, for manipulating multiple genome sequence alignments and their consensus sequences for high-throughput genome sequence analyses. This server facilitates the visual inspection of multiple genome sequence alignments for a set of genomic intervals at a time. This allows the user to examine how these sites are evolutionarily conserved over time for their functional importance. The server also reports consensus sequences for the input genomic intervals, which can be applied to downstream analyses such as the identification of common motifs in the regions determined by ChIP-seq experiments. Availability and implementation: GenomeCons is freely accessible at", 
  "summary": "GenomeCons is well suited for large-scale analyses of genome sequence alignments, for example, the identification of evolutionarily conserved transcription factor binding sites in the peak regions identified by ChIP-seq experiments.\nThe server returns the output in three formats: (i) multiple genome sequence alignments of the input regions; (ii) consensus sequences where the sites with low phastCons scores (Siepel et al., 2005), which is a metric that represents the conservation at each aligned position, are masked with `N's and (iii) consensus sequences divided by low phastCons scores.", 
  "affiliations": [], 
  "grants": [
    "Funding\nGrant-in-Aid for Scientific Research from the Ministry of Education, Culture, Sports, Science and Technology of Japan [26550089 to T.S."
  ], 
  "acks": " ", 
  "authors": [
    " Tetsuya Sato", 
    " Mikita Suyama"
  ], 
  "keyWords": [
    "sequence alignments", 
    [
      "accessible", 
      "regions", 
      "sequencing", 
      "alignment", 
      "genomecons"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-12-06T02:51:31Z"
}{
  "doi": "10.1093/bioinformatics/btu593", 
  "name": "Genomon ITDetector a tool for somatic internal tandem duplication detection from cancer genome sequencing data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://cancer.sanger.ac.uk/cancergenome/projects/census", 
    "https://github.com/ken0-1n/Genomon-ITDetector"
  ], 
  "title": "Sequence analysis Genomon ITDetector: a tool for somatic internal tandem duplication detection from cancer genome sequencing data", 
  "toolName": "Genomon-ITDetector", 
  "abstract": "Somatic internal tandem duplications (ITDs) are known to play important roles in cancer pathogenesis. Although recent advances in high-throughput sequencing technologies have enabled genome-wide detection of various types of genomic mutations, including single nucleotide variants, indels and structural variations, only a few studies have focused on ITDs. We have developed an analytical tool called 'Genomon ITDetector' for genome-wide detection of som-atic ITDs. After evaluating the sensitivity and precision of the proposed approach using synthetic data, we have demonstrated that it can successfully detect not only common ITDs involving FLT3, but also a number of ITDs affecting other putative driver genes in acute myeloid leukemia exome sequencing data.", 
  "summary": "Although recent advances in high-throughput sequencing technologies have enabled genome-wide detection of various types of genomic mutations, including single nucleotide variants, indels and structural variations, only a few studies have focused on ITDs. We have developed an analytical tool called `Genomon ITDetector' for genome-wide detection of somatic ITDs. After evaluating the sensitivity and precision of the proposed approach using synthetic data, we have demonstrated that it can successfully detect not only common ITDs involving FLT3, but also a number of ITDs affecting other putative driver genes in acute myeloid leukemia exome sequencing data.", 
  "affiliations": [
    " Department of Pathology and Tumor Biology Graduate School of Medicine Kyoto University ", 
    " Human Genome Center Institute of Medical Science Laboratory of DNA Information Analysis The University of Tokyo "
  ], 
  "grants": [
    "Funding: Funding for open access charge: Integrative Systems Understanding of Cancer for Advanced Diagnosis, Therapy and Prevention (Grant-in-Aid for Scientific Research on Innovative Areas from the Ministry of Education, Culture, Sports, Science and Technology, Japan (Grant Number: 22134004))."
  ], 
  "sourcelinks": [
    "https://github.com/ken0-1n/Genomon-ITDetector"
  ], 
  "acks": " The supercomputing resources were provided by Human Genome Center, the Institute of Medical Science, The University of Tokyo. The authors would like to thank The Cancer Genome Atlas (TCGA) project. Funding: Funding for open access charge: Integrative Systems Understanding of Cancer for Advanced Diagnosis, Therapy and Prevention (Grant-in-Aid for Scientific Research on Innovative Areas from the Ministry of Education, Culture, Sports, Science and Technology, Japan (Grant Number: 22134004)). of interest: none declared. ", 
  "authors": [
    " Kenichi Chiba", 
    " Yuichi Shiraishi", 
    " Yasunobu Nagata", 
    " Kenichi Yoshida", 
    " Seiya Imoto", 
    " Seishi Ogawa", 
    " Satoru Miyano"
  ], 
  "keyWords": [
    "cancer genome sequencing", 
    [
      "genomics", 
      "sequences", 
      "itds", 
      "reads", 
      "bioinformatics", 
      "alignments"
    ]
  ], 
  "github_data": {
    "name": "Genomon-ITDetector", 
    "contributors": [
      {
        "contributions": 63, 
        "html_url": "https://github.com/ken0-1n"
      }
    ], 
    "versions": [], 
    "created_at": "2013-08-13T07:31:50Z", 
    "updated_at": "2016-05-30T02:44:25Z", 
    "languages": [
      "Shell", 
      "Perl"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/ken0-1n"
      }, 
      {
        "html_url": "https://github.com/dotdotdotpaul"
      }
    ], 
    "owner": "https://github.com/ken0-1n", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-09-06T04:48:38Z"
}{
  "doi": "10.1093/bioinformatics/btu425", 
  "name": "GlycoDigest a tool for the targeted use of exoglycosidase digestions in glycan structure determination", 
  "links": [
    "https://bitbucket.org/sib-pig", 
    "http://glycodigest.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://unicarbkb.org/glycodigest", 
    "http://www.glycodigest.org"
  ], 
  "title": "Structural bioinformatics GlycoDigest: a tool for the targeted use of exoglycosidase digestions in glycan structure determination", 
  "toolName": "Structural bioinformatics GlycoDigest: a tool for the targeted use of exoglycosidase digestions in glycan structure determination", 
  "abstract": "Sequencing oligosaccharides by exoglycosidases, either sequentially or in an array format, is a powerful tool to unambiguously determine the structure of complex N-and O-link glycans. Here, we introduce GlycoDigest, a tool that simulates exoglycosidase digestion, based on controlled rules acquired from expert knowledge and experimental evidence available in GlycoBase. The tool allows the targeted design of glycosidase enzyme mixtures by allowing researchers to model the action of exoglycosidases, thereby validating and improving the efficiency and accuracy of glycan analysis. Availability and implementation: http://www.glycodigest.org.", 
  "summary": "GlycoBase is an High Performance Liquid Chromatography (HPLC) retention time database of experimentally characterized glycan structures, which provides comprehensive information on known exoglycosidase digestions (Campbell et al., 2008).\nFor each N- and O-link glycan structure with defined linkage information, a `GlycoDigest' link provides users the option to select a combination of enzymes for simulating exoglycosidase treatment(s).\nProposed structures for each undigested and digested glycan resulting from the action of the exoglycosidases can be searched in UniCarbKB (Campbell et al., 2014b) for associated published and experimental information, and for combined exoglycosidase-MS/MS characterization of oligosaccharides (Ali et al., 2012) using UniCarb-DB spectra (Hayes et al., 2011).", 
  "affiliations": [
    " Biomolecular Frontiers Research Centre Macquarie University ", 
    " GlycoScience Group National Institute for Bioprocessing Research and Training ", 
    " Department of Medical Biochemistry and Cell Biology University of Gothenburg ", 
    " Proteome Informatics Group Swiss Institute of Bioinformatics "
  ], 
  "grants": [
    "Funding: Swiss National Science Foundation (31003A_141215); Macquarie University Research Excellence Scheme Postgraduate Scholarship; The Australian National eResearch Collaboration Tools and Resources project (RT016)."
  ], 
  "acks": " ", 
  "authors": [
    " Lou Gotz", 
    " Jodie L Abrahams", 
    " Julien Mariethoz", 
    " Pauline M Rudd", 
    " Niclas G Karlsson", 
    " Nicolle H Packer", 
    " Matthew P Campbell", 
    " Frederique Lisacek", 
    " Janet Kelso"
  ], 
  "keyWords": [
    "glycan structure", 
    "exoglycosidase digestions", 
    [
      "digestion", 
      "structurally", 
      "glycodigest", 
      "exoglycosidases", 
      "bioinformatics", 
      "tools"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-07-12T03:16:15Z"
}{
  "doi": "10.1093/bioinformatics/btu588", 
  "name": "GenPlay MultiGenome a tool to compare and analyze multiple human genomes in a graphical interface", 
  "links": [
    "http://genplay.einstein.yu.edu", 
    "http://genplay.einstein.yu.edu/wiki/index.php/How", 
    "https://github.com/JulienLajugie/GenPlay", 
    "http://genplay.einstein.yu.edu/wiki/index.php/Projects", 
    "http://genplay.einstein.yu.edu/wiki/index.php/GRCh37/hg19", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis GenPlay Multi-Genome, a tool to compare and analyze multiple human genomes in a graphical interface", 
  "toolName": "GenPlay", 
  "abstract": "Parallel visualization of multiple individual human genomes is a complex endeavor that is rapidly gaining importance with the increasing number of personal, phased and cancer genomes that are being generated. It requires the display of variants such as SNPs, indels and structural variants that are unique to specific genomes and the introduction of multiple overlapping gaps in the reference sequence. Here, we describe GenPlay Multi-Genome, an application specifically written to visualize and analyze multiple human genomes in parallel. GenPlay Multi-Genome is ideally suited for the comparison of allele-specific expression and functional gen-omic data obtained from multiple phased genomes in a graphical interface with access to multiple-track operation. It also allows the analysis of data that have been aligned to custom genomes rather than to a standard reference and can be used as a variant calling format file browser and as a tool to compare different genome assembly , such as hg19 and hg38.", 
  "summary": "The meta-reference genome is created using user-provided variant calling format (VCF) files, which are text files containing the position, the sequence and genotype information for all the variants in one or more individuals (Danecek et al., 2011).\nhave generated three race-specific reference sequences and have shown that alignment to a closely related genome decreases error rates (Dewey et al., 2011).\nGenPlay Multi-Genome partly resolves these problems because data that are represented in any custom genomic coordinate system can be loaded concurrently with data represented in the reference genome coordinate system as long as a VCF file is available to create a meta-reference genome.", 
  "affiliations": [
    " Department of Cell Biology Albert Einstein College of Medicine "
  ], 
  "grants": [
    "Funding: J.L., N.F.", 
    "were supported by grants C024405 and C024172 from NYSTEM."
  ], 
  "sourcelinks": [
    "http://genplay.einstein.yu.edu/wiki/index.php/GRCh37/hg19", 
    "https://github.com/JulienLajugie/GenPlay", 
    "http://genplay.einstein.yu.edu"
  ], 
  "acks": " ", 
  "authors": [
    " Julien Lajugie", 
    " Nicolas Fourel", 
    " Eric E Bouhassira"
  ], 
  "keyWords": [
    "genomic data", 
    [
      "sequencing", 
      "genomics", 
      "genplay", 
      "alignments", 
      "bioinformatics"
    ]
  ], 
  "github_data": {
    "name": "GenPlay", 
    "contributors": [
      {
        "contributions": 596, 
        "html_url": "https://github.com/JulienLajugie"
      }, 
      {
        "contributions": 424, 
        "html_url": "https://github.com/nicolas-fourel"
      }
    ], 
    "versions": [], 
    "created_at": "2013-12-18T21:23:06Z", 
    "updated_at": "2014-06-26T15:10:16Z", 
    "languages": [
      "Java"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/JulienLajugie"
      }, 
      {
        "html_url": "https://github.com/nicolas-fourel"
      }, 
      {
        "html_url": "https://github.com/Kentalot"
      }, 
      {
        "html_url": "https://github.com/ywei"
      }
    ], 
    "owner": "https://github.com/JulienLajugie", 
    "homepage": "genplay.einstein.yu.edu"
  }, 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-09-02T00:17:48Z"
}{
  "doi": "10.1093/bioinformatics/btu559", 
  "name": "GlycoPattern a web platform for glycan array mining", 
  "links": [
    "http://www.functionalgl", 
    "http://glycopattern.emory.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.w3.org/TR", 
    "http://www.w3.org"
  ], 
  "title": "Structural bioinformatics GlycoPattern: a web platform for glycan array mining", 
  "toolName": "Structural bioinformatics GlycoPattern: a web platform for glycan array mining", 
  "abstract": "GlycoPattern is Web-based bioinformatics resource to support the analysis of glycan array data for the Consortium for Functional Glycomics. This resource includes algorithms and tools to discover structural motifs, a heatmap visualization to compare multiple experiments, hierarchical clustering of Glycan Binding Proteins with respect to their binding motifs and a structural search feature on the experimental data.", 
  "summary": "RINGS (Akune et al., 2010) is a Web portal providing algorithmic and data mining tools to aid glycobiology research that includes tools for drawing structures, mining glycan subtrees, pathway prediction and glycan profiles, but is limited in the tools available for mining across multiple glycan array experiments.\nMany notations representing textual glycan nomenclature are available, including International Union of Pure and Applied Chemistry (IUPAC) (McNaught, 1997), Linear Notation for Unique description of Carbohydrate Structures (LINUCS; Bohne-Lang et al., 2001), Kyoto Encyclopedia of Genes and Genomes (KEGG) Chemical Function (Hattori et al., 2003), LinearCode (Ehud et al., 2002), GLYDE-II (Packer et al., 2008) and GlycoCT (Herget et al., 2008).", 
  "affiliations": [
    " National Center For Functional Glycomics Emory University School of Medicine "
  ], 
  "grants": [
    "An NIH white paper prepared from discussions by the focus groups at a workshop on the NIH campus, Bethesda MD (September 11-13, 2006).", 
    "Funding: This work was supported by grants from the National Institutes of Health including U53 GM62116 and GM98791 (Consortium for Functional Glycomics), R01GM085447 (D.F.S."
  ], 
  "acks": " ", 
  "authors": [
    " Sanjay B Agravat", 
    " Joel H Saltz", 
    " Richard D Cummings", 
    " David F Smith"
  ], 
  "keyWords": [
    [
      "glycopattern", 
      "experiments", 
      "structures", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://www.functionalgl"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-21T04:05:53Z"
}{
  "doi": "10.1093/bioinformatics/btu618", 
  "name": "GLAD a mixedmembership model for heterogeneous tumor subtype classification", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://genomics.wpi.edu/glad"
  ], 
  "title": "Gene expression GLAD: a mixed-membership model for heterogeneous tumor subtype classification", 
  "toolName": "Gene expression GLAD: a mixed-membership model for heterogeneous tumor subtype classification", 
  "abstract": "Motivation: Genomic analyses of many solid cancers have demonstrated extensive genetic heterogeneity between as well as within individual tumors. However, statistical methods for classifying tumors by subtype based on genomic biomarkers generally entail an all-or-none decision, which may be misleading for clinical samples containing a mixture of subtypes and/or normal cell contamination. Results: We have developed a mixed-membership classification model, called GLAD, that simultaneously learns a sparse biomarker signature for each subtype as well as a distribution over subtypes for each sample. We demonstrate the accuracy of this model on simulated data, in-vitro mixture experiments, and clinical samples from the Cancer Genome Atlas (TCGA) project. We show that many TCGA samples are likely a mixture of multiple subtypes.", 
  "summary": "Results: We have developed a mixed-membership classification model, called GLAD, that simultaneously learns a sparse biomarker signature for each subtype as well as a distribution over subtypes for each sample.\nThese tumor subtypes are commonly identified and characterized by clustering the genomic data from hundreds of samples (Eisen et al., 1998; Hofree et al., 2013).\nThis process involves several hyperparameters: , a non-negative scalar governing the Laplace prior on the elements of x; , a K-vector of non-negative values controlling the possibly asymmetric Dirichlet prior on each sample's mixing proportions; and S 2 RM  M, the conditional covariance matrix for the normally distributed components of the gene expression vectors.", 
  "affiliations": [
    " Department of Statistics University of California "
  ], 
  "grants": [
    "are supported by seed funding from Worcester Polytechnic Institute."
  ], 
  "acks": " The authors would like to thank Sia Najafi for computational support. P.F. and H.S. are supported by seed funding from Worcester Polytechnic Institute. ", 
  "authors": [
    " Hachem Saddiki", 
    " Jon Mcauliffe", 
    " Patrick Flaherty", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "models", 
      "genes", 
      "glad", 
      "subtyping", 
      "sampling"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-30T05:52:55Z"
}{
  "doi": "10.1093/bioinformatics/btu852", 
  "name": "GlycoMine a machine learningbased approach for predicting N C and Olinked glycosylation in the human proteome", 
  "links": [
    "http://www.structbioinfor", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "GlycoMine: a machine learning-based approach for predicting N-, C-and O-linked glycosylation in the human proteome", 
  "toolName": "GlycoMine: a machine learning-based approach for predicting N-, C-and O-linked glycosylation in the human proteome", 
  "abstract": "Motivation: Glycosylation is a ubiquitous type of protein post-translational modification (PTM) in eukaryotic cells, which plays vital roles in various biological processes (BPs) such as cellular communication, ligand recognition and subcellular recognition. It is estimated that >50% of the entire human proteome is glycosylated. However, it is still a significant challenge to identify glyco-sylation sites, which requires expensive/laborious experimental research. Thus, bioinformatics approaches that can predict the glycan occupancy at specific sequons in protein sequences would be useful for understanding and utilizing this important PTM. Results: In this study, we present a novel bioinformatics tool called GlycoMine, which is a comprehensive tool for the systematic in silico identification of Clinked , N-linked, and O-linked glycosyla-tion sites in the human proteome. GlycoMine was developed using the random forest algorithm and evaluated based on a well-prepared up-to-date benchmark dataset that encompasses all three types of glycosylation sites, which was curated from multiple public resources. Heterogeneous sequences and functional features were derived from various sources, and subjected to further two-step feature selection to characterize a condensed subset of optimal features that contributed most to the type-specific prediction of glycosylation sites. Five-fold cross-validation and independent tests show that this approach significantly improved the prediction performance compared with four existing prediction tools: NetNGlyc, NetOGlyc, EnsembleGly and GPP. We demonstrated that this tool could identify candidate glycosylation sites in case study proteins and applied it to identify many high-confidence glycosylation target proteins by screening the entire human proteome. Availability and implementation: The webserver, Java Applet, user instructions, datasets, and predicted glycosylation sites in the human proteome are freely available at http://www.structbioinfor. org/Lab/GlycoMine/.", 
  "summary": "In this study, we propose a novel bioinformatics approach called GlycoMine that addresses these problems and significantly improves the prediction performance for C-, N- and O-linked glycosylation by integrating various informative features derived from protein sequences.\nSpecifically, an effective two-step feature selection method based on information gain (IG) (Kent, 1983) and minimum redundancy maximum relevance (mRMR) (Peng et al., 2005) is used to determine the features that are important for glycosylation site specificity, as well as identifying condensed subsets of optimal features that contribute most to the prediction.", 
  "affiliations": [
    " National Engineering Laboratory for Industrial Enzymes and Key Laboratory of Systems Microbial Biotechnology Tianjin Institute of Industrial Biotechnology Chinese Academy of Sciences ", 
    " College of Information Engineering Northwest A&F University ", 
    " Department of Biochemistry and Molecular Biology Monash University ", 
    " Centre for Research in Intelligent Systems Faculty of Information Technology Monash University "
  ], 
  "grants": [
    "Funding\nThis work was supported by grants from a Major Inter-disciplinary Project Grant Awarded by Monash University, National Natural Science Foundation of China [61202167, 61303169, 11250110508, 31350110507], Australian National Health and Medical Research Council (NHMRC) [490989] and Chinese Academy of Sciences (CAS)."
  ], 
  "acks": " ", 
  "authors": [
    " Fuyi Li", 
    " Chen Li", 
    " Mingjun Wang", 
    " Geoffrey I Webb", 
    " Yang Zhang", 
    " James C Whisstock", 
    " Jiangning Song"
  ], 
  "keyWords": [
    [
      "features", 
      "glycosylation", 
      "proteins", 
      "predictions", 
      "sites", 
      "sequences"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2015-01-08T02:10:15Z"
}{
  "doi": "10.1093/bioinformatics/btv032", 
  "name": "GrammR graphical representation and modeling of count data with application in metagenomics", 
  "links": [
    "http://cran", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining GrammR: graphical representation and modeling of count data with application in metagenomics", 
  "toolName": "Data and text mining GrammR: graphical representation and modeling of count data with application in metagenomics", 
  "abstract": "Motivation: Microbiota compositions have great implications in human health, such as obesity and other conditions. As such, it is of great importance to cluster samples or taxa to visualize and discover community substructures. Graphical representation of metagenomic count data relies on two aspects, measure of dissimilarity between samples/taxa and algorithm used to estimate coordinates to study microbiota communities. UniFrac is a dissimilarity measure commonly used in metagenomic research, but it requires a phylogenetic tree. Principal coordinate analysis (PCoA) is a popular algorithm for estimating two-dimensional (2D) coordinates for graphical representation, although alternative and higher-dimensional representations may reveal underlying community substructures invisible in 2D representations. Results: We adapt a new measure of dissimilarity, penalized Kendall's s-distance, which does not depend on a phylogenetic tree, and hence more readily applicable to a wider class of problems. Further, we propose to use metric multidimensional scaling (MDS) as an alternative to PCoA for graphical representation. We then devise a novel procedure for determining the number of clusters in conjunction with PAM (mPAM). We show superior performances with higher-dimensional representations. We further demonstrate the utility of mPAM for accurate clustering analysis, especially with higher-dimensional MDS models. Applications to two human microbiota datasets illustrate greater insights into the subcommunity structure with a higher-dimensional analysis.", 
  "summary": "For a random realization of data generated under Model I, the average silhouette width plots are given in Supplementary Figure S2, from which one can see that PAM and mPAM will lead to the same estimations of the numbers of clusters for all models.\nThe two 2D plots display over- and under-estimation of the true number of clusters under the l1-MDS and the PCoA models, respectively.\nHowever, this error in estimation is not seen in the 3D models, with both l1-MDS and PCoA producing the correct number of clusters without any misclassification (Supplementary Table S1).", 
  "affiliations": [
    " Department of Statistics The Ohio State University "
  ], 
  "grants": [
    "Funding\nThis work was partially supported by the United States National Science Foundation grant DMS-1220772."
  ], 
  "acks": " ", 
  "authors": [
    " Deepak Nag", 
    " Ayyala ", 
    " Shili Lin"
  ], 
  "keyWords": [
    "cluster samples", 
    [
      "clustering", 
      "sampled", 
      "modeling", 
      "data", 
      "pcoa"
    ]
  ], 
  "sourcelinks": [
    "http://cran"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-22T03:50:58Z"
}{
  "doi": "10.1093/bioinformatics/btu732", 
  "name": "GlycoRDF an ontology to standardize glycomics data in RDF", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.monosaccharidedb.org", 
    "http://csdb.glycoscience.ru/bacterial/core", 
    "http://www.glycoinfo.org/GlycoRDF", 
    "http://jcggdb.jp", 
    "https://github.com/ReneRanzinger", 
    "http://www.glyconavi.org", 
    "http://www.monosaccharidedb.org/rdf", 
    "https://github.com/ReneRanzinger/GlycoRDF/wiki/GlycoProtDB-documentationNote", 
    "http://www.glycoepitope.jp", 
    "https://bioportal.bioontology.org", 
    "http://www", 
    "http://bibliontology.com", 
    "https://github.com/ReneRanzinger/GlycoRDF", 
    "http://www.glycoinfo", 
    "http://rdf.publication/org/example", 
    "http://ws.glyconavi.org/WebTool/rdf.php"
  ], 
  "title": "GlycoRDF: an ontology to standardize glycomics data in RDF", 
  "toolName": "GlycoRDF", 
  "abstract": "Motivation: Over the last decades several glycomics-based bioinformatics resources and databases have been created and released to the public. Unfortunately, there is no common standard in the representation of the stored information or a common machine-readable interface allowing bioinformatics groups to easily extract and cross-reference the stored information. Results: An international group of bioinformatics experts in the field of glycomics have worked together to create a standard Resource Description Framework (RDF) representation for glycomics data, focused on glycan sequences and related biological source, publications and experimental data. This RDF standard is defined by the GlycoRDF ontology and will be used by database providers to generate common machine-readable exports of the data stored in their databases. Availability and implementation: The ontology, supporting documentation and source code used by database providers to generate standardized RDF are available online (http://www.glycoinfo. org/GlycoRDF/).", 
  "summary": "Here, we present our efforts to develop an ontology for generating standardized RDF for glycan structures and related data (known as GlycoRDF), which has been agreed and adopted by most database designers and developers in the glycomics discipline.\nSince each database provider is free to use the sequence format they prefer and there is currently no globally accepted identifier for glycan structures (Aoki-Kinoshita et al., 2013a, b), mapping RDF information from a glycan entry in one database to RDF information of the same glycan entry in another database is not possible using this information.", 
  "affiliations": [
    " Database Center for Life Science Research Organization of Information and Systems ", 
    " Research Center for Medical Glycoscience National Institute of Advanced Industrial Science and Technology ", 
    " Faculty of Engineering Soka University ", 
    " Graduate School of Medical and Dental Sciences Niigata University ", 
    " Biomolecular Frontiers Research Centre Macquarie University ", 
    " Institute of Veterinary Physiology and Biochemistry Justus-Liebig-University Giessen ", 
    " N.D. Zelinsky Institute of Organic Chemistry Russian Academy of Sciences ", 
    " Complex Carbohydrate Research Center University of Georgia ", 
    " Laboratory of Glyco-organic Chemistry The Noguchi Institute "
  ], 
  "grants": [
    "An NIH white paper prepared from discussions by the\n\n\fGlycoRDF\n\n925\n\nfocus groups at a workshop on the NIH campus, Bethesda MD (September 11-13, 2006).", 
    "Funding\nThis work was supported by the National Institute of General Medical Sciences (8P41GM103490 to R.R."
  ], 
  "sourcelinks": [
    "http://jcggdb.jp", 
    "http://www.monosaccharidedb.org", 
    "http://csdb.glycoscience.ru/bacterial/core", 
    "https://github.com/ReneRanzinger/GlycoRDF/wiki/GlycoProtDB-documentationNote", 
    "https://github.com/ReneRanzinger", 
    "http://www.monosaccharidedb.org/rdf", 
    "http://www", 
    "https://github.com/ReneRanzinger/GlycoRDF", 
    "http://www.glycoinfo", 
    "http://ws.glyconavi.org/WebTool/rdf.php"
  ], 
  "acks": " This work was supported by the National Institute of General Medical Sciences (8P41GM103490 to R.R.); National Bioscience Database Center of Japan Science and Technology Agency (Integrated Database Project to H.N.); Russian Foundation for Basic Research (N12-04-00324 to P.T.); and Australian National eResearch Collaboration Tools and Resources (RT016 to M.C.). Conflict of Interest: none declared. ", 
  "authors": [
    " Rene Ranzinger", 
    " Kiyoko F Aoki-Kinoshita", 
    " Matthew P Campbell", 
    " Shin Kawano", 
    " Thomas L\u00fc Tteke", 
    " Shujiro Okuda", 
    " Daisuke Shinmachi", 
    " Toshihide Shikanai", 
    " Hiromichi Sawaki", 
    " Philip Toukach", 
    " Masaaki Matsubara", 
    " Issaku Yamada", 
    " Hisashi Narimatsu"
  ], 
  "keyWords": [
    "glycomics data", 
    [
      "glycans", 
      "informations", 
      "databases"
    ]
  ], 
  "github_data": {
    "name": "GlycoRDF", 
    "contributors": [
      {
        "contributions": 12, 
        "html_url": "https://github.com/ReneRanzinger"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/alternativeTime"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/senaarpinar"
      }
    ], 
    "versions": [], 
    "created_at": "2014-05-02T01:51:22Z", 
    "updated_at": "2016-03-29T11:48:07Z", 
    "languages": [
      "Java", 
      "Web Ontology Language"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/ReneRanzinger"
      }, 
      {
        "html_url": "https://github.com/alternativeTime"
      }, 
      {
        "html_url": "https://github.com/glycosciences"
      }, 
      {
        "html_url": "https://github.com/kiyoko"
      }, 
      {
        "html_url": "https://github.com/mobiusklein"
      }, 
      {
        "html_url": "https://github.com/senaarpinar"
      }, 
      {
        "html_url": "https://github.com/shinmachi"
      }, 
      {
        "html_url": "https://github.com/linikujp"
      }, 
      {
        "html_url": "https://github.com/kozo2"
      }, 
      {
        "html_url": "https://github.com/aokinobu"
      }, 
      {
        "html_url": "https://github.com/yamadaissaku"
      }, 
      {
        "html_url": "https://github.com/PabloPsicotico"
      }
    ], 
    "owner": "https://github.com/ReneRanzinger", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-11-12T04:44:24Z"
}{
  "doi": "10.1093/bioinformatics/btu817", 
  "name": "GSDS 20 an upgraded gene feature visualization server", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://gsds", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Genome analysis GSDS 2.0: an upgraded gene feature visualization server", 
  "toolName": "Genome analysis GSDS 2.0: an upgraded gene feature visualization server", 
  "abstract": "Visualizing genes' structure and annotated features helps biologists to investigate their function and evolution intuitively. The Gene Structure Display Server (GSDS) has been widely used by more than 60 000 users since its first publication in 2007. Here, we reported the upgraded GSDS 2.0 with a newly designed interface, supports for more types of annotation features and formats, as well as an integrated visual editor for editing the generated figure. Moreover, a user-specified phylogenetic tree can be added to facilitate further evolutionary analysis. The full source code is also available for downloading. Availability and implementation: Web server and source code are freely available at http://gsds.", 
  "summary": "GSDS 2.0: an upgraded gene feature visualization server\nThe Gene Structure Display Server (GSDS) has been widely used by more than 60 000 users since its first publication in 2007.\nHere, we reported the upgraded GSDS 2.0 with a newly designed interface, supports for more types of annotation features and formats, as well as an integrated visual editor for editing the generated figure.\nDesigning to generate high-quality figures suitable for publication, we developed an online Gene Structure Display Server (GSDS) (Guo et al., 2007), which supported three input formats including sequences, accession number of GenBank (Benson et al., 2013) and exon positions.", 
  "affiliations": [
    " College of Life Sciences Center for Bioinformatics State Key Laboratory of Protein and Plant Gene Research Peking University ", 
    " Department of Systems Biology College of Life Science and Technology Huazhong University of Science and Technology "
  ], 
  "grants": [
    "J.J. was supported partly by the China Postdoctoral Science Foundation Grant [2014M560017].", 
    "Thus, we implemented a\n\nFunding\nThis work was supported by the China National 973 Program [2011CBA01103]; 863 Programs [2006AA02Z334, 2007AA02Z165]; and the State Key Laboratory of Protein and Plant Gene Research."
  ], 
  "acks": " We thank all comments from users of GSDS 1.0 and the developers of SVGedit tool. This work was supported by the China National 973 Program; 863 Programs; and the State Key Laboratory of Protein and Plant Gene Research. The research of G.G. was supported in part by the National Outstanding Youth Talent Initiative Program. J.J. was supported partly by the China Postdoctoral Science Foundation Grant. Conflict of Interest: none declared. ", 
  "authors": [
    " Bo Hu", 
    " Jinpu Jin", 
    " An-Yuan Guo", 
    " He Zhang", 
    " Jingchu Luo", 
    " Ge Gao"
  ], 
  "keyWords": [
    [
      "formats", 
      "visualization", 
      "genes", 
      "gsds", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://gsds"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-12T02:23:08Z"
}{
  "doi": "10.1093/bioinformatics/btu799", 
  "name": "Graphical algorithm for integration of genetic and biological data proof of principle using psoriasis as a model", 
  "links": [
    "http://www.R-project.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://genome.sph.umich.edu/wiki/MEAGA"
  ], 
  "title": "Systems biology Graphical algorithm for integration of genetic and biological data: proof of principle using psoriasis as a model", 
  "toolName": "Systems biology Graphical algorithm for integration of genetic and biological data: proof of principle using psoriasis as a model", 
  "abstract": "Motivation: Pathway analysis to reveal biological mechanisms for results from genetic association studies have great potential to better understand complex traits with major human disease impact. However, current approaches have not been optimized to maximize statistical power to identify enriched functions/pathways, especially when the genetic data derives from studies using platforms (e.g. Immunochip and Metabochip) customized to have pre-selected markers from previously identified top-rank loci. We present here a novel approach, called Minimum distance-based Enrichment Analysis for Genetic Association (MEAGA), with the potential to address both of these important concerns. Results: MEAGA performs enrichment analysis using graphical algorithms to identify sub-graphs among genes and measure their closeness in interaction database. It also incorporates a statistic summarizing the numbers and total distances of the sub-graphs, depicting the overlap between observed genetic signals and defined function/pathway gene-sets. MEAGA uses sampling technique to approximate empirical and multiple testing-corrected P-values. We show in simulation studies that MEAGA is more powerful compared to count-based strategies in identifying disease-associated functions/pathways, and the increase in power is influenced by the shortest distances among associated genes in the interactome. We applied MEAGA to the results of a meta-analysis of psoriasis using Immunochip datasets, and showed that associated genes are significantly enriched in immune-related functions and closer with each other in the protein\u2013protein interaction network. Availability and implementation:", 
  "summary": "We show in simulation studies that MEAGA is more powerful compared to count-based strategies in identifying disease-associated functions/pathways, and the increase in power is influenced by the shortest distances among associated genes in the interactome.\nFor each gene-set being tested, MEAGA identifies overlapping genes in association signals, and constructs Steiner Tree(s) in the biological interactome (e.g. PPI database) using Kou's algorithm.\nOur results show that when the genes from association signals are relatively closer with each other in the interactome (i.e. 1.3 to 4), MEAGA could obtain higher statistical power in identifying the true associated functions when comparing with count-based approach.", 
  "affiliations": [
    " Department of Dermatology University of Michigan ", 
    " Department of Biostatistics Center for Statistical Genetics University of Michigan "
  ], 
  "grants": [
    "is also supported by NIH grants R01 AR054966, R01 AR062382, R01 AR065183 and by the Ann Arbor Veterans Affairs Hospital.", 
    "are supported by National Institute of Health (NIH) grants R01 AR042742 and R01 AR050511, J.T.E.", 
    "Funding\nL.C.T.", 
    "is supported by research grants R01HG007022 from the National Human Genome Research Institute and R01EY022005 from the National Eye Institute."
  ], 
  "acks": " The authors thank the insightful comments from Xiaowei Zhan, Christian Fuchsberger and our reviewers. ", 
  "authors": [
    " Lam C Tsoi", 
    " James T Elder", 
    " Goncalo R Abecasis"
  ], 
  "keyWords": [
    "association studies", 
    [
      "associated", 
      "meaga", 
      "studied", 
      "genetics", 
      "networks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Python", 
    "R"
  ], 
  "dateCreated": "2014-12-06T02:51:31Z"
}{
  "doi": "10.1093/bioinformatics/btu764", 
  "name": "Graphical model for joint segmentation and tracking of multiple dividing cells", 
  "links": [
    "http://hci.iwr.uni-heidelberg.de/MIP/Research", 
    "http://arxiv.org/abs/1206.0111", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Graphical model for joint segmentation and tracking of multiple dividing cells", 
  "toolName": "Graphical model for joint segmentation and tracking of multiple dividing cells", 
  "abstract": "Motivation: To gain fundamental insight into the development of embryos, biologists seek to understand the fate of each and every embryonic cell. For the generation of cell tracks in embryo-genesis, so-called tracking-by-assignment methods are flexible approaches. However, as every two-stage approach, they suffer from irrevocable errors propagated from the first stage to the second stage, here from segmentation to tracking. It is therefore desirable to model segmentation and tracking in a joint holistic assignment framework allowing the two stages to maximally benefit from each other. Results: We propose a probabilistic graphical model, which both automatically selects the best segments from a time series of oversegmented images/volumes and links them across time. This is realized by introducing intra-frame and inter-frame constraints between conflicting segmentation and tracking hypotheses while at the same time allowing for cell division. We show the efficiency of our algorithm on a challenging 3D\u00fet cell tracking dataset from Drosophila embryogenesis and on a 2D\u00fet dataset of proliferating cells in a dense population with frequent overlaps. On the latter, we achieve results significantly better than state-of-the-art tracking methods. Availability and implementation: Source code and the 3D\u00fet Drosophila dataset along with our manual annotations will be freely available on http://hci.iwr.uni-heidelberg.de/MIP/Research/ tracking/", 
  "summary": "From this structure, a graphical model is constructed (stage IV): overlapping segmentation hypotheses are connected by intra-frame conflicts (black: conflicting segmentation hypotheses; blue: local evidence for the number of cells in one connected component) and inter-timestep transition hypotheses are modeled by binary random variables (yellow nodes) indicating whether the corresponding cell in t has moved to, divided to or is not associated with the corresponding cell in t  1.\n1. A graphical model for cell tracking (Schiegg et al., 2013) (based on a given segmentation), which can correct for falsely merged cells in a post-processing step.", 
  "affiliations": [
    " University of Heidelberg IWR/HCI ", 
    " European Molecular Biology Laboratory (EMBL) Cell Biology and Biophysics Unit "
  ], 
  "grants": [
    "Funding\nThis work was partially supported by the Heidelberg University Cluster of Excellence Cell Networks [grant number EXC81] and the HGS Mathcomp [DFG GSC 220]."
  ], 
  "acks": " We thank Christoph Klein (University of Heidelberg) for his assistance in manual tracking annotations. This work was partially supported by the Heidelberg University Cluster of Excellence Cell Networks and the HGS Mathcomp ", 
  "authors": [
    " Martin Schiegg", 
    " Philipp Hanslovsky", 
    " Carsten Haubold", 
    " Ullrich Koethe", 
    " Lars Hufnagel", 
    " Fred A Hamprecht"
  ], 
  "keyWords": [
    [
      "modeling", 
      "cells", 
      "trackings", 
      "segmentations", 
      "detections"
    ]
  ], 
  "sourcelinks": [
    "http://hci.iwr.uni-heidelberg.de/MIP/Research"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-19T01:15:08Z"
}{
  "doi": "10.1093/bioinformatics/btu747", 
  "name": "HiCorrector a fast scalable and memoryefficient package for normalizing largescale HiC data", 
  "links": [
    "http://creativecommons.org/licenses/by-nc/4.0/),which", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://zhoulab.usc.edu/Hi-Corrector"
  ], 
  "title": "Hi-Corrector: a fast, scalable and memory-efficient package for normalizing large-scale Hi-C data", 
  "toolName": "Hi-Corrector: a fast, scalable and memory-efficient package for normalizing large-scale Hi-C data", 
  "abstract": "Genome-wide proximity ligation assays, e.g. Hi-C and its variant TCC, have recently become important tools to study spatial genome organization. Removing biases from chromatin contact matrices generated by such techniques is a critical preprocessing step of subsequent analyses. The continuing decline of sequencing costs has led to an ever-improving resolution of the Hi-C data, resulting in very large matrices of chromatin contacts. Such large-size matrices, however, pose a great challenge on the memory usage and speed of its normalization. Therefore, there is an urgent need for fast and memory-efficient methods for normalization of Hi-C data. We developed Hi-Corrector, an easy-to-use, open source implementation of the Hi-C data normalization algorithm. Its salient features are (i) scalability\u2014the software is capable of normalizing Hi-C data of any size in reasonable times; (ii) memory efficiency\u2014the sequential version can run on any single computer with very limited memory, no matter how little; (iii) fast speed\u2014the parallel version can run very fast on multiple computing nodes with limited local memory. Availability and implementation: The sequential version is implemented in ANSI C and can be easily compiled on any system; the parallel version is implemented in ANSI C with the MPI library (a standardized and portable parallel environment designed for solving large-scale scientific problems). The package is freely available at", 
  "summary": "Its salient features are (i) scalability--the software is capable of normalizing Hi-C data of any size in reasonable times; (ii) memory efficiency--the sequential version can run on any single computer with very limited memory, no matter how little; (iii) fast speed--the parallel version can run very fast on multiple computing nodes with limited local memory.\nTo normalize large Hi-C matrices in a short time, we also designed a fast, scalable and Memory-Efficient Parallel algorithm (called IC-MEP) that can maximally exploit the parallelism of the normalization problem and make use of many commonly available computing resources.", 
  "affiliations": [
    " Molecular and Computational Biology Program Department of Biological Sciences University of Southern California "
  ], 
  "grants": [
    "Funding\nNational Science Foundation Grant CAREER 1150287 and the Arnold and Mabel Beckman foundation (BYI program) (to F.A.", 
    "); and National Institutes of Health Grant (NHLBI MAPGEN U01HL108634 to X.J.Z."
  ], 
  "acks": " ", 
  "authors": [
    " Wenyuan Li", 
    " Ke Gong", 
    " Qingjiao Li", 
    " Frank Alber", 
    " Xianghong Jasmine Zhou"
  ], 
  "keyWords": [
    [
      "timely", 
      "computationally", 
      "genomes", 
      "algorithms", 
      "memory", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-11-13T05:02:10Z"
}{
  "doi": "10.1093/bioinformatics/btu537", 
  "name": "HapMuC somatic mutation calling using heterozygous germ line variants near candidate mutations", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0", 
    "https://cghub", 
    "http://github.com/usuyama/hapmuc"
  ], 
  "title": "Sequence analysis HapMuC: somatic mutation calling using heterozygous germ line variants near candidate mutations", 
  "toolName": "hapmuc", 
  "abstract": "Motivation: Identifying somatic changes from tumor and matched normal sequences has become a standard approach in cancer research. More specifically, this requires accurate detection of somatic point mutations with low allele frequencies in impure and heterogeneous cancer samples. Although haplotype phasing information derived by using heterozygous germ line variants near candidate mutations would improve accuracy, no somatic mutation caller that uses such information is currently available. Results: We propose a Bayesian hierarchical method, termed HapMuC, in which power is increased by using available information on heterozygous germ line variants located near candidate mutations. We first constructed two generative models (the mutation model and the error model). In the generative models, we prepared candidate haplotypes, considering a heterozygous germ line variant if available, and the observed reads were realigned to the haplotypes. We then inferred the haplotype frequencies and computed the marginal likelihoods using a variational Bayesian algorithm. Finally, we derived a Bayes factor for evaluating the possibility of the existence of somatic mutations. We also demonstrated that our algorithm has superior spe-cificity and sensitivity compared with existing methods, as determined based on a simulation, the TCGA Mutation Calling Benchmark 4 data-sets and data from the COLO-829 cell line. Availability and implementation: The HapMuC source code is available from", 
  "summary": "Although we can fit the Bayesian latent Dirichlet model to the sequencing data of tumor and matched normal samples using some statistical methods, for evaluating the possibility of a variant being a true somatic mutation, we need to define a model that can be considered as a control against the model constructed under the premise of the existence of the mutation.\nWe investigated the relationships between somatic mutations and heterozygous germ line variants nearby, using exome tumor and matched normal sequences of 10 ccRCC patients (Sato et al., 2013; Shiraishi et al., 2013).", 
  "affiliations": [
    " Department of Pathology and Tumor Biology Graduate School of Medicine Kyoto University ", 
    " Human Genome Center Institute of Medical Science The University of Tokyo ", 
    " Department of Urology Graduate School of Medicine The University of Tokyo "
  ], 
  "grants": [
    "Funding: This work was partly supported by Grant-in-Aid for Scientific Research on Innovative Areas (22134004)."
  ], 
  "sourcelinks": [
    "http://github.com/usuyama/hapmuc"
  ], 
  "acks": " The super-computing resource was provided by Human Genome Center, the Institute of Medical Science, the University of Tokyo. ", 
  "authors": [
    " Naoto Usuyama", 
    " Yuichi Shiraishi", 
    " Yusuke Sato", 
    " Haruki Kume", 
    " Yukio Homma", 
    " Seishi Ogawa", 
    " Satoru Miyano", 
    " Seiya Imoto"
  ], 
  "keyWords": [
    [
      "genomics", 
      "mutational", 
      "hapmuc", 
      "haplotyping", 
      "sequencing", 
      "reading"
    ]
  ], 
  "github_data": {
    "name": "hapmuc", 
    "contributors": [], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/usuyama/hapmuc/zipball/ver1.0", 
        "tarball_url": "https://api.github.com/repos/usuyama/hapmuc/tarball/ver1.0", 
        "name": "ver1.0"
      }
    ], 
    "created_at": "2013-05-21T05:35:49Z", 
    "updated_at": "2015-09-18T17:14:22Z", 
    "languages": [
      "Python", 
      "Makefile", 
      "Shell", 
      "Ruby", 
      "C++"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/usuyama"
      }
    ], 
    "owner": "https://github.com/usuyama", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-08-15T05:12:51Z"
}{
  "doi": "10.1093/bioinformatics/btu504", 
  "name": "Greater power and computational efficiency for kernelbased association testing of sets of genetic variants", 
  "links": [
    "http://immunobase.org", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://research.microsoft.com/enus/um/redmond/projects/MSCompBio/Fastlmm"
  ], 
  "title": "Genetics and population analysis Greater power and computational efficiency for kernel-based association testing of sets of genetic variants", 
  "toolName": "Genetics and population analysis Greater power and computational efficiency for kernel-based association testing of sets of genetic variants", 
  "abstract": "Motivation: Set-based variance component tests have been identified as a way to increase power in association studies by aggregating weak individual effects. However, the choice of test statistic has been largely ignored even though it may play an important role in obtaining optimal power. We compared a standard statistical test\u2014 a score test\u2014with a recently developed likelihood ratio (LR) test. Further, when correction for hidden structure is needed, or gene\u2013 gene interactions are sought, state-of-the art algorithms for both the score and LR tests can be computationally impractical. Thus we develop new computationally efficient methods. Results: After reviewing theoretical differences in performance between the score and LR tests, we find empirically on real data that the LR test generally has more power. In particular, on 15 of 17 real datasets, the LR test yielded at least as many associations as the score test\u2014up to 23 more associations\u2014whereas the score test yielded at most one more association than the LR test in the two remaining datasets. On synthetic data, we find that the LR test yielded up to 12% more associations, consistent with our results on real data, but also observe a regime of extremely small signal where the score test yielded up to 25% more associations than the LR test, consistent with theory. Finally, our computational speedups now enable (i) efficient LR testing when the background kernel is full rank, and (ii) efficient score testing when the background kernel changes with each test, as for gene\u2013gene interaction tests. The latter yielded a factor of 2000 speedup on a cohort of size 13 500. Availability: Software available at", 
  "summary": "In addition to our systematic comparison of the score and LR tests in the standard setting, we also consider richer scenarios in which, for example, one may want to correct for confounding factors arising from family relatedness or population structure (Listgarten et al., 2013), or testing for genegene interactions between variants from pairs of sets (e.g. genes) (Li and Cui, 2012).\nAll synthetic experiments comparing statistical tests used a standard variance components model in which there was only one kernel--the one used to test a set of SNPs, as in all of the SKAT papers (Lee et al., 2012a and 2012b; Wu et al., 2011).", 
  "affiliations": [
    " eScience Research Group Microsoft Research "
  ], 
  "grants": [
    "Funding support for the `CIDR Visceral Adiposity Study' was provided through the Division of Aging Biology and the Division of Geriatrics and Clinical Gerontology, NIA.", 
    "Funding for GENEVA was provided by National Human Genome Research Institute grant U01HG004402 (E. Boerwinkle).", 
    "Funding: Funding for the research was provided by Microsoft Research.", 
    "Funding for the project was provided by the Wellcome Trust under award 076113 and 085475."
  ], 
  "acks": " This study makes use of data generated by the Wellcome Trust Case-Control Consortium. A full list of the investigators who contributed to the generation of the data is available from www. wtccc.org.uk. Funding for the project was provided by the Wellcome Trust under award 076113 and 085475. The Atherosclerosis Risk in Communities Study is carried out as a collaborative study supported by National Heart, Lung, and Blood Institute contracts (HHSN268201100005C, HHSN268 201100006C, HHSN268201100007C, HHSN268201100008C, HHSN268201100009C, HHSN268201100010C, HHSN2682011 00011C and HHSN268201100012C). The authors thank the staff and participants of the ARIC study for their important contributions . We thank R.B. Davies for questions on his Davies method code, and the reviewers for helpful comments. Funding: Funding for the research was provided by Microsoft Research. Funding for GENEVA was provided by National Human Genome Research Institute grant U01HG004402 (E. Boerwinkle). Funding support for the 'CIDR Visceral Adiposity Study' was provided through the Division of Aging Biology and the Division of Geriatrics and Clinical Gerontology, NIA. The CIDR Visceral Adiposity Study includes a genomewide association study funded as part of the Division of Aging Biology and the Division of Geriatrics and Clinical Gerontology, NIA. Assistance with phenotype harmonization and genotype cleaning, as well as with general study coordination, was provided by Heath ABC Study Investigators. Conflict of interest: none declared. ", 
  "authors": [
    " Christoph Lippert", 
    " Jing Xiang", 
    " Danilo Horta", 
    " Christian Widmer", 
    " Carl Kadie", 
    " David Heckerman", 
    " Jennifer Listgarten", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    [
      "genetics", 
      "settings", 
      "testing", 
      "phenotypes", 
      "modeling", 
      "computationally", 
      "sets"
    ]
  ], 
  "sourcelinks": [
    "http://immunobase.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-30T00:14:11Z"
}{
  "doi": "10.1093/bioinformatics/btu801", 
  "name": "HIPPIE a highthroughput identification pipeline for promoter interacting enhancer elements", 
  "links": [
    "http://wanglab.pcbi.upenn.edu/hippie", 
    "https://github.com/HiC-inspector", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bioinformatics.babraham.ac.uk"
  ], 
  "title": "HIPPIE: a high-throughput identification pipeline for promoter interacting enhancer elements", 
  "toolName": "HiC-inspector", 
  "abstract": "We implemented a high-throughput identification pipeline for promoter interacting en-hancer element to streamline the workflow from mapping raw Hi-C reads, identifying DNA\u2013DNA interacting fragments with high confidence and quality control, detecting histone modifications and DNase hypersensitive enrichments in putative enhancer elements, to ultimately extracting possible intra-and inter-chromosomal enhancer\u2013target gene relationships. Availability and implementation: This software package is designed to run on high-performance computing clusters with Oracle Grid Engine. The source code is freely available under the MIT license for academic and nonprofit use. The source code and instructions are available at the Wang lab website (http://wanglab.pcbi.upenn.edu/hippie/). It is also provided as an Amazon Machine Image to be used directly on Amazon Cloud with minimal installation. Contact:", 
  "summary": "Summary: We implemented a high-throughput identification pipeline for promoter interacting enhancer element to streamline the workflow from mapping raw Hi-C reads, identifying DNADNA interacting fragments with high confidence and quality control, detecting histone modifications and DNase hypersensitive enrichments in putative enhancer elements, to ultimately extracting possible intra- and inter-chromosomal enhancertarget gene relationships.\nFor instance, users can map their Hi-C reads with other algorithms and call peaks with HIPPIE starting at phase III; or one can directly import the interaction regions and utilize HIPPIE for enhancertarget gene identifications (phase IV).", 
  "affiliations": [
    " Department of Biology University of Pennsylvania ", 
    " Genomics and Computational Biology Graduate Program University of Pennsylvania "
  ], 
  "grants": [
    "Funding\nThis work was supported by National Institute on Aging [U24-AG041689] and National Institute of General Medical Sciences [R01-GM099962]."
  ], 
  "sourcelinks": [
    "http://wanglab.pcbi.upenn.edu/hippie", 
    "https://github.com/HiC-inspector"
  ], 
  "acks": " We thank Dr. Weixin Wang and Dr. Yuk Yee Leung for insightful discussions and the members of the Gregory and Wang labs for their comments and construction input on the manuscript. This work was supported by National Institute on Aging and National Institute of General Medical Sciences ", 
  "authors": [
    " Yih-Chii Hwang", 
    " Chiao-Feng Lin", 
    " Otto Valladares", 
    " John Malamon", 
    " Pavel P Kuksa", 
    " Qi Zheng", 
    " Brian D Gregory", 
    " Li-San Wang"
  ], 
  "keyWords": [
    [
      "genomics", 
      "hippie", 
      "interactions", 
      "mapping", 
      "maps", 
      "reads", 
      "bioinformatics"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v3.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/HiC-inspector/HiC-inspector/master/LICENSE"
      }
    ], 
    "name": "HiC-inspector", 
    "contributors": [
      {
        "contributions": 24, 
        "html_url": "https://github.com/toniher"
      }
    ], 
    "versions": [], 
    "created_at": "2012-10-18T09:39:55Z", 
    "updated_at": "2015-08-19T19:25:20Z", 
    "languages": [
      "HTML", 
      "JavaScript", 
      "Perl", 
      "R", 
      "Awk", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/toniher"
      }, 
      {
        "html_url": "https://github.com/lexxx233"
      }, 
      {
        "html_url": "https://github.com/Perry-Zhu"
      }, 
      {
        "html_url": "https://github.com/sirusb"
      }, 
      {
        "html_url": "https://github.com/knightwupz"
      }, 
      {
        "html_url": "https://github.com/khl0798"
      }
    ], 
    "owner": "https://github.com/HiC-inspector", 
    "homepage": ""
  }, 
  "technologies": [
    " Oracle "
  ], 
  "dateCreated": "2014-12-06T02:51:31Z"
}{
  "doi": "10.1093/bioinformatics/btu431", 
  "name": "Gustaf Detecting and correctly classifying SVs in the NGS twilight zone", 
  "links": [
    "https://github.com/genome/pindel", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com", 
    "http://www.seqan.de/projects"
  ], 
  "title": "Gustaf: Detecting and correctly classifying SVs in the NGS twilight zone", 
  "toolName": "github.com", 
  "abstract": "Motivation: The landscape of structural variation (SV) including complex duplication and translocation patterns is far from resolved. SV detection tools usually exhibit low agreement, are often geared toward certain types or size ranges of variation and struggle to correctly classify the type and exact size of SVs. Results: We present Gustaf (Generic mUlti-SpliT Alignment Finder), a sound generic multi-split SV detection tool that detects and classifies deletions, inversions, dispersed duplications and translocations of !30 bp. Our approach is based on a generic multi-split alignment strategy that can identify SV breakpoints with base pair resolution. We show that Gustaf correctly identifies SVs, especially in the range from 30 to 100 bp, which we call the next-generation sequencing (NGS) twilight zone of SVs, as well as larger SVs 4500 bp. Gustaf performs better than similar tools in our benchmark and is furthermore able to correctly identify size and location of dispersed duplications and translocations, which otherwise might be wrongly classified, for example, as large deletions. Availability and implementation: Project information, paper benchmark and source code are available via http://www.seqan.de/projects/ gustaf/.", 
  "summary": "SV detection tools usually exhibit low agreement, are often geared toward certain types or size ranges of variation and struggle to correctly classify the type and exact size of SVs. Results: We present Gustaf (Generic mUlti-SpliT Alignment Finder), a sound generic multi-split SV detection tool that detects and classifies deletions, inversions, dispersed duplications and translocations of !30 bp.\nGustaf is an SV detection tool that uses a split-read approach to detect exact breakpoints of SVs including indels, inversions, duplications and translocations.", 
  "affiliations": [
    " Department of Computer Science Freie Universit \u20ac at Berlin ", 
    " New York Genome Center"
  ], 
  "grants": [
    "Funding: K.T.", 
    "was funded by the DFG SPP1307 grants RE1712/3-1 and RE-1712/3-2 during algorithm development and initial manuscript preparation and is currently funded by DFG grant RE3474/2-1."
  ], 
  "sourcelinks": [
    "https://github.com/genome/pindel", 
    "http://www.seqan.de/projects", 
    "https://github.com"
  ], 
  "acks": " ", 
  "authors": [
    " Kathrin Trappe", 
    " Anne-Katrin Emde", 
    " Hans-Christian Ehrlich", 
    " Knut Reinert"
  ], 
  "keyWords": [
    [
      "alignments", 
      "reads", 
      "gustaf", 
      "genomes", 
      "bioinformatics"
    ]
  ], 
  "github_data": {
    "name": "node-v0.x-archive", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/orangemocha"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/works", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/works", 
        "name": "works"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.7", 
        "name": "v0.12.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.6", 
        "name": "v0.12.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.5", 
        "name": "v0.12.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.4", 
        "name": "v0.12.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.3", 
        "name": "v0.12.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.2", 
        "name": "v0.12.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.1", 
        "name": "v0.12.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.0", 
        "name": "v0.12.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.16", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.16", 
        "name": "v0.11.16"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.15", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.15", 
        "name": "v0.11.15"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.14", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.14", 
        "name": "v0.11.14"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.13", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.13", 
        "name": "v0.11.13"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.12", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.12", 
        "name": "v0.11.12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.11", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.11", 
        "name": "v0.11.11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.10", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.10", 
        "name": "v0.11.10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.9", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.9", 
        "name": "v0.11.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.8", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.8", 
        "name": "v0.11.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.7", 
        "name": "v0.11.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.6", 
        "name": "v0.11.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.5", 
        "name": "v0.11.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.4", 
        "name": "v0.11.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.3", 
        "name": "v0.11.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.40", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.40", 
        "name": "v0.10.40"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.39", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.39", 
        "name": "v0.10.39"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.38", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.38", 
        "name": "v0.10.38"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.37", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.37", 
        "name": "v0.10.37"
      }
    ], 
    "created_at": "2009-05-27T16:29:46Z", 
    "updated_at": "2016-08-09T16:10:45Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/isaacs"
      }, 
      {
        "html_url": "https://github.com/koichik"
      }, 
      {
        "html_url": "https://github.com/igorzi"
      }, 
      {
        "html_url": "https://github.com/kunoichi-co"
      }, 
      {
        "html_url": "https://github.com/carloslemes"
      }, 
      {
        "html_url": "https://github.com/aboyon"
      }, 
      {
        "html_url": "https://github.com/arden"
      }, 
      {
        "html_url": "https://github.com/guyoun"
      }, 
      {
        "html_url": "https://github.com/vissul"
      }, 
      {
        "html_url": "https://github.com/bdtgzj"
      }, 
      {
        "html_url": "https://github.com/santigimeno"
      }, 
      {
        "html_url": "https://github.com/chrisbateskeegan"
      }, 
      {
        "html_url": "https://github.com/FlashSoft"
      }, 
      {
        "html_url": "https://github.com/charlesgan"
      }, 
      {
        "html_url": "https://github.com/BlaneCordes"
      }, 
      {
        "html_url": "https://github.com/mdgoody"
      }, 
      {
        "html_url": "https://github.com/gindis"
      }, 
      {
        "html_url": "https://github.com/chrisdickinson"
      }, 
      {
        "html_url": "https://github.com/jorgenio"
      }, 
      {
        "html_url": "https://github.com/joityui"
      }, 
      {
        "html_url": "https://github.com/wxnet2013"
      }, 
      {
        "html_url": "https://github.com/be5invis"
      }, 
      {
        "html_url": "https://github.com/ashleymcfarland"
      }, 
      {
        "html_url": "https://github.com/jimxl"
      }, 
      {
        "html_url": "https://github.com/bluefisher80"
      }, 
      {
        "html_url": "https://github.com/subhaze"
      }, 
      {
        "html_url": "https://github.com/yaoming6046"
      }, 
      {
        "html_url": "https://github.com/hirozhang"
      }, 
      {
        "html_url": "https://github.com/rhyw"
      }, 
      {
        "html_url": "https://github.com/rhaldkhein"
      }
    ], 
    "owner": "https://github.com/nodejs", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-07-16T00:23:13Z"
}{
  "doi": "10.1093/bioinformatics/btu638", 
  "name": "HTSeqa Python framework to work with highthroughput sequencing data", 
  "links": [
    "http://www.bioinformatics.babraham.ac.uk/projects/fastqc", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://picard.sourceforge", 
    "http://code.google.com/p/pysam", 
    "https://pypi.python.org/pypi/HTSeq", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www-huber.embl.de/HTSeq"
  ], 
  "title": "Genome analysis HTSeq\u2014a Python framework to work with high-throughput sequencing data", 
  "toolName": "picard", 
  "abstract": "Motivation: A large choice of tools exists for many standard tasks in the analysis of high-throughput sequencing (HTS) data. However, once a project deviates from standard workflows, custom scripts are needed. Results: We present HTSeq, a Python library to facilitate the rapid development of such scripts. HTSeq offers parsers for many common data formats in HTS projects, as well as classes to represent data, such as genomic coordinates, sequences, sequencing reads, alignments, gene model information and variant calls, and provides data structures that allow for querying via genomic coordinates. We also present htseq-count, a tool developed with HTSeq that preprocesses RNA-Seq data for differential expression analysis by counting the overlap of reads with genes.", 
  "summary": "HTSeq offers parsers for many common data formats in HTS projects, as well as classes to represent data, such as genomic coordinates, sequences, sequencing reads, alignments, gene model information and variant calls, and provides data structures that allow for querying via genomic coordinates.\nWe also present htseq-count, a tool developed with HTSeq that preprocesses RNA-Seq data for differential expression analysis by counting the overlap of reads with genes.\nThe script htseq-count is a tool for RNA-Seq data analysis: Given a SAM/BAM file and a GTF or GFF file with gene models, it counts for each gene how many aligned reads overlap its exons.", 
  "affiliations": [
    " Genome Biology Unit, European Molecular Biology Laboratory"
  ], 
  "grants": [
    "Funding: S.A. and W.H."
  ], 
  "acks": " ", 
  "authors": [
    " Simon Anders", 
    " Paul Theodor Pyl", 
    " Wolfgang Huber"
  ], 
  "keyWords": [
    [
      "genomics", 
      "bioinformatical", 
      "python", 
      "classes", 
      "htseq", 
      "reading", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "https://pypi.python.org/pypi/HTSeq", 
    "http://picard.sourceforge", 
    "http://www-huber.embl.de/HTSeq", 
    "http://code.google.com/p/pysam"
  ], 
  "technologies": [
    "Python", 
    "R", 
    "C++"
  ], 
  "dateCreated": "2014-09-27T03:04:09Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/picard/", 
    "languages": [
      "Java"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/vdauwera/", 
        "name": "Geraldine A Van der Auwera"
      }, 
      {
        "url": "https://sourceforge.net/u/geoffjentry/", 
        "name": "Jeff Gentry"
      }, 
      {
        "url": "https://sourceforge.net/u/gbggrant/", 
        "name": "gbggrant"
      }, 
      {
        "url": "https://sourceforge.net/u/droazen/", 
        "name": "droazen"
      }, 
      {
        "url": "https://sourceforge.net/u/jacarey/", 
        "name": "jcarey"
      }, 
      {
        "url": "https://sourceforge.net/u/jacobbroad/", 
        "name": "Jacob"
      }, 
      {
        "url": "https://sourceforge.net/u/bradtaylor/", 
        "name": "Brad Taylor"
      }, 
      {
        "url": "https://sourceforge.net/u/tfenne/", 
        "name": "Tim Fennell"
      }, 
      {
        "url": "https://sourceforge.net/u/bhandsaker/", 
        "name": "Bob Handsaker"
      }, 
      {
        "url": "https://sourceforge.net/u/eitanbanks/", 
        "name": "Eric Banks"
      }, 
      {
        "url": "https://sourceforge.net/u/delangel1/", 
        "name": "Guillermo del Angel"
      }, 
      {
        "url": "https://sourceforge.net/u/nilshomer/", 
        "name": "Nils Homer"
      }, 
      {
        "url": "https://sourceforge.net/u/vadimzalunin/", 
        "name": "Vadim Zalunin"
      }, 
      {
        "url": "https://sourceforge.net/u/brilliantred/", 
        "name": "Kathleen Tibbetts"
      }, 
      {
        "url": "https://sourceforge.net/u/ami-gatk/", 
        "name": "Ami Levy Moonshine"
      }, 
      {
        "url": "https://sourceforge.net/u/alecw/", 
        "name": "Alec Wysoker"
      }, 
      {
        "url": "https://sourceforge.net/u/yfarjoun/", 
        "name": "Yossi"
      }, 
      {
        "url": "https://sourceforge.net/u/jrobinso/", 
        "name": "James Robinson"
      }, 
      {
        "url": "https://sourceforge.net/u/jmthibault/", 
        "name": "Joel Thibault"
      }
    ], 
    "Development Status": [
      {
        "status": "5 - Production/Stable"
      }
    ], 
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "name": "Apache License V2.0"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu518", 
  "name": "Hybrid Bayesianrank integration approach improves the predictive power of genomic dataset aggregation", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.pitt.edu"
  ], 
  "title": "Gene expression Hybrid Bayesian-rank integration approach improves the predictive power of genomic dataset aggregation", 
  "toolName": "Gene expression Hybrid Bayesian-rank integration approach improves the predictive power of genomic dataset aggregation", 
  "abstract": "Motivation: Modern molecular technologies allow the collection of large amounts of high-throughput data on the functional attributes of genes. Often multiple technologies and study designs are used to address the same biological question such as which genes are over-expressed in a specific disease state. Consequently, there is considerable interest in methods that can integrate across datasets to present a unified set of predictions. Results: An important aspect of data integration is being able to account for the fact that datasets may differ in how accurately they capture the biological signal of interest. While many methods to address this problem exist, they always rely either on dataset internal statistics, which reflect data structure and not necessarily biological relevance, or external gold standards, which may not always be available. We present a new rank aggregation method for data integration that requires neither external standards nor internal statistics but relies on Bayesian reasoning to assess dataset relevance. We demonstrate that our method outperforms established techniques and significantly improves the predictive power of rank-based aggregations. We show that our method, which does not require an external gold standard, provides reliable estimates of dataset relevance and allows the same set of data to be integrated differently depending on the specific signal of interest. Availability: The method is implemented in R and is freely available at", 
  "summary": "We present a new rank aggregation method for data integration that requires neither external standards nor internal statistics but relies on Bayesian reasoning to assess dataset relevance.\nWe use the complete set of signal and noise datasets to evaluate different rank aggregation approaches [mean ranks, Robust Rank Aggregation (RRA) (Kolde et al., 2012), Stuart (Stuart et al., 2003), and our method BIRRA].\nAggregating 10 signal and 30 noise datasets with several different methods we find that even on a dataset with 75% uninformative data all methods tested produce results that outperform individual signal datasets (Fig. 1) and the computationally sophisticated RRA and Stuart methods surpass mean ranks at low sensitivity.", 
  "affiliations": [
    " Department of Neurology Mount Sinai School of Medicine ", 
    " Department of Computational and Systems Biology University of Pittsburgh "
  ], 
  "grants": [
    "Funding: Funded by NIH Contract HHSN272201000054C and a grant from the Michael J Fox Foundation."
  ], 
  "acks": " ", 
  "authors": [
    " Marcus A Badgeley", 
    " Stuart C Sealfon", 
    " Maria D Chikina", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "datasets", 
      "methods", 
      "rankings", 
      "genes", 
      "integrations", 
      "birra"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-30T05:52:55Z"
}{
  "doi": "10.1093/bioinformatics/btu824", 
  "name": "HYCUD a computational tool for prediction of effective rotational correlation time in flexible proteins", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "HYCUD: a computational tool for prediction of effective rotational correlation time in flexible proteins", 
  "toolName": "HYCUD: a computational tool for prediction of effective rotational correlation time in flexible proteins", 
  "abstract": "Motivation: A large fraction of eukaryotic proteins contain unstructured tails or linkers. The presence of flexible regions allows these systems to experience a high level of mobility facilitating their biological function. The complex nature of protein rotation in such flexible modular systems precludes a straightforward application of hydrodynamic methods to calculate their rotational mo-tional properties. We describe the workflow of HYdrodynamic CoUpling of Domains (HYCUD), a program for prediction of effective rotational correlation times in multidomain proteins. The usage of HYCUD is demonstrated by its application to the ribosomal protein L7/L12. Rotational correlation times predicted by HYCUD might be used to detect molecular switch events mediated by disorder\u2013-order transitions in interdomain linkers. Availability and implementation: The source code and documentation are available at www. mpibpc.mpg.de/106144/software.", 
  "summary": "Here, we describe the workflow of a recently developed alternative approach (Rezaei-Ghaleh et al., 2013) called HYdrodynamic CoUpling of Domains (HYCUD) to predict the rotational correlation time (sc) of protein domains within flexible modular systems.\nHere, we first generated an ensemble of 5000 random structures for the dimeric L7/L12 protein using the EOM program (Bernado et al., 2007) (Supplementary Fig. S1).\nFurther support for the validity of HYCUD is provided by the excellent agreement between predicted and experimental sc of the 53-residue domain of protein X from Sendai virus (Houben et al., 2007) (Supplementary Fig. S3 and Supplementary Table S1) as well as several other examples reported in Rezaei-Ghaleh et al.", 
  "affiliations": [
    " Department of NMR-Based Structural Biology Max Planck Institute for Biophysical Chemistry "
  ], 
  "grants": [
    "Funding\nThis work was supported through the German Science Foundation Collaborative Research Center 860, Project B2 (M.Z.)"
  ], 
  "acks": " ", 
  "authors": [
    " Nasrollah Rezaei-Ghaleh", 
    " Frederik Klama", 
    " Francesca Munari", 
    " Markus Zweckstetter"
  ], 
  "keyWords": [
    [
      "domains", 
      "proteins", 
      "hydrodynamics", 
      "structural"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-12-14T01:33:54Z"
}{
  "doi": "10.1093/bioinformatics/btu809", 
  "name": "ICMA an integrated cardiac modeling and analysis platform", 
  "links": [
    "http://goo.gl/M4lJKH", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/ABI-Software-Laboratory/ICMA", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Data and text mining ICMA: an integrated cardiac modeling and analysis platform", 
  "toolName": "ICMA", 
  "abstract": "ICMA, a software framework to create 3D finite element models of the left ventricle from cardiac ultrasound or magnetic resonance imaging (MRI) data, has been made available as an open-source code. The framework is hardware vendor independent and uses speckle tracking (endocardial border detection) on ultrasound (MRI) imaging data in the form of DICOM. Standard American Heart Association segment-based strain analysis can be performed using a browser-based interface. The speckle tracking, border detection and model fitting methods are implemented in C\u00fe\u00fe using open-source tools. They are wrapped as web services and orchestrated via a JBOSS-based application server. Availability and implementation: The source code for ICMA is freely available under MPL 1.1 or GPL 2.0 or LGPL 2.1 license at https://github.com/ABI-Software-Laboratory/ICMA and a standalone virtual machine at http://goo.gl/M4lJKH for download.", 
  "summary": "Summary: ICMA, a software framework to create 3D finite element models of the left ventricle from cardiac ultrasound or magnetic resonance imaging (MRI) data, has been made available as an open-source code.\nWe developed a model guided speckle tracking method that tracks left ventricular wall motion in cardiac ultrasound images.\nThe speckle tracking module implements the speckle tracking method discussed in (Leitman et al., 2004) along with an adaptive left ventricular model (Young et al., 2000) to robustly follow the wall motion in noisy images and to speed up the process by restricting the tracking process to regions of interest.", 
  "affiliations": [
    " Auckland Bioengineering Institute University of Auckland "
  ], 
  "grants": [
    "Funding\nThis work was funded by National Space Biomedical Research Institute, USA (CA02203)."
  ], 
  "sourcelinks": [
    "http://goo.gl/M4lJKH", 
    "https://github.com/ABI-Software-Laboratory/ICMA", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "acks": " ", 
  "authors": [
    " Jagir R Hussan", 
    " Peter J Hunter", 
    " Patrick A Gladding", 
    " Neil Greenberg", 
    " Richard Christie", 
    " Alan Wu", 
    " Hugh Sorby", 
    " James D Thomas"
  ], 
  "keyWords": [
    [
      "modeling", 
      "software", 
      "based", 
      "imaging", 
      "data"
    ]
  ], 
  "github_data": {
    "name": "ICMA", 
    "contributors": [
      {
        "contributions": 15, 
        "html_url": "https://github.com/Jagirhussan"
      }
    ], 
    "versions": [], 
    "created_at": "2014-09-25T03:50:08Z", 
    "updated_at": "2016-02-14T05:52:54Z", 
    "languages": [
      "Groff", 
      "C", 
      "Shell", 
      "Java", 
      "XSLT", 
      "Python", 
      "JavaScript", 
      "C++", 
      "Perl", 
      "XQuery", 
      "HTML", 
      "ActionScript", 
      "CMake", 
      "PHP", 
      "Ruby", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/codecurve"
      }, 
      {
        "html_url": "https://github.com/rchristie"
      }, 
      {
        "html_url": "https://github.com/Jagirhussan"
      }, 
      {
        "html_url": "https://github.com/taopanpan"
      }, 
      {
        "html_url": "https://github.com/johnyquest2"
      }, 
      {
        "html_url": "https://github.com/alan-wu"
      }, 
      {
        "html_url": "https://github.com/idvr"
      }
    ], 
    "owner": "https://github.com/ABI-Software", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-12-07T01:08:27Z"
}{
  "doi": "10.1093/bioinformatics/btu503", 
  "name": "img2net automated networkbased analysis of imaged phenotypes", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://mathbiol.mpimpgolm.mpg.de/img2net"
  ], 
  "title": "Bioimage informatics img2net: automated network-based analysis of imaged phenotypes", 
  "toolName": "Bioimage informatics img2net: automated network-based analysis of imaged phenotypes", 
  "abstract": "Automated analysis of imaged phenotypes enables fast and reproducible quantification of biologically relevant features. Despite recent developments, recordings of complex networked structures, such as leaf venation patterns, cytoskeletal structures or traffic networks, remain challenging to analyze. Here we illustrate the applicability of img2net to automatedly analyze such structures by reconstructing the underlying network, computing relevant network properties and statistically comparing networks of different types or under different conditions. The software can be readily used for analyzing image data of arbitrary 2D and 3D network-like structures. Availability and Implementation: img2net is open-source software under the GPL and can be downloaded from http://mathbiol.mpimp-golm.mpg.de/img2net/, where supplementary information and data-sets for testing are provided.", 
  "summary": "The existing image-based methods for reconstruction of biological networks are typically designed for specific types of networks: tree-like networks for plant root architectures (Pound et al., 2013); fungal or leaf venation networks (Obara et al., 2012); or neuronal topology of the human connectome\nHere, we present a robust approach to reconstruct 2D and 3D (non-)biological spatial networks from gray-scale image data.\nWe also used img2net to analyze the network structure of the German autobahn, which was obtained from OpenStreetMap as an image by using a gray-scale coding of the speed limits (Breuer et al., 2014).", 
  "affiliations": [
    " Mathematical Modeling and Systems Biology Max Planck Institute of Molecular Plant Physiology "
  ], 
  "grants": [
    "Funding: D.B."
  ], 
  "acks": " ", 
  "authors": [
    " David Breuer", 
    " Zoran Nikoloski"
  ], 
  "keyWords": [
    [
      "biologically", 
      "networked", 
      "imaging", 
      "grids", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://mathbiol.mpimpgolm.mpg.de/img2net"
  ], 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btu698", 
  "name": "iDoComp a compression scheme for assembled genomes", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.stanford.edu"
  ], 
  "title": "iDoComp: a compression scheme for assembled genomes", 
  "toolName": "iDoComp: a compression scheme for assembled genomes", 
  "abstract": "Motivation: With the release of the latest next-generation sequencing (NGS) machine, the HiSeq X by Illumina, the cost of sequencing a Human has dropped to a mere $4000. Thus we are approaching a milestone in the sequencing history, known as the $1000 genome era, where the sequencing of individuals is affordable, opening the doors to effective personalized medicine. Massive generation of genomic data, including assembled genomes, is expected in the following years. There is crucial need for compression of genomes guaranteed of performing well simultaneously on different species, from simple bacteria to humans, which will ease their transmission, dissemination and analysis. Further, most of the new genomes to be compressed will correspond to individuals of a species from which a reference already exists on the database. Thus, it is natural to propose compression schemes that assume and exploit the availability of such references. Results: We propose iDoComp, a compressor of assembled genomes presented in FASTA format that compresses an individual genome using a reference genome for both the compression and the decompression. In terms of compression efficiency, iDoComp outperforms previously proposed algorithms in most of the studied cases, with comparable or better running time. For example, we observe compression gains of up to 60% in several cases, including H.sapiens data, when comparing with the best compression performance among the previously proposed algorithms. Availability: iDoComp is written in C and can be downloaded from: http://www.stanford.edu/ ~iochoa/iDoComp.html (We also provide a full explanation on how to run the program and an example with all the necessary files to run it.).", 
  "summary": "In 2012, another compression algorithm was presented in (Chern et al., 2012), where they showed some improved compression results with respect to GReEn. However, the algorithm they proposed has relatively high running time when applied to big datasets and it does not work in several cases (see Supplementary Data, Section VI for the results.).\nAnother advantage of the proposed algorithm is that the scheme employed for compression is very intuitive, in the sense that the compression consists mainly of generating instructions composed of the sequence of matches M and the two sets S and I that suffice to reconstruct the target genome given the reference genome.", 
  "affiliations": [
    " Department of Electrical Engineering Stanford University "
  ], 
  "grants": [
    "Funding\nThis work is partially funded by a Stanford Graduate Fellowships Program in Science and Engineering, a fellowship from the Basque Government, a grant from the Center for Science of Information (CSoI) and 1157849-1-QAZCC NSF grant."
  ], 
  "acks": " The authors would like to thank Golan Yona for providing the initial motivation for this work and for helpful discussions, and the anonymous reviewers for helpful suggestions. This work is partially funded by a Stanford Graduate Fellowships Program in Science and Engineering, a fellowship from the Basque Government, a grant from the Center for Science of Information (CSoI) and 1157849-1-QAZCC NSF grant. Conflict of interest: none declared. ", 
  "authors": [
    " Idoia Ochoa", 
    " Mikel Hernaez", 
    " Tsachy Weissman"
  ], 
  "keyWords": [
    [
      "idocomp", 
      "genomics", 
      "algorithms", 
      "compressions", 
      "bioinformatics", 
      "sequencing", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.stanford.edu"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-25T03:54:31Z"
}{
  "doi": "10.1093/bioinformatics/btu811", 
  "name": "Identifying cancerrelated microRNAs based on gene expression data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://comp-sysbio.org/miR_Path"
  ], 
  "title": "Identifying cancer-related microRNAs based on gene expression data Downloaded from", 
  "toolName": "Identifying cancer-related microRNAs based on gene expression data Downloaded from", 
  "abstract": "Motivation: MicroRNAs (miRNAs) are short non-coding RNAs that play important roles in post-transcriptional regulations as well as other important biological processes. Recently, accumulating evidences indicate that miRNAs are extensively involved in cancer. However, it is a big challenge to identify which miRNAs are related to which cancer considering the complex processes involved in tumors, where one miRNA may target hundreds or even thousands of genes and one gene may regulate multiple miRNAs. Despite integrative analysis of matched gene and miRNA expression data can help identify cancer-associated miRNAs, such kind of data is not commonly available. On the other hand, there are huge amount of gene expression data that are publicly accessible. It will significantly improve the efficiency of characterizing miRNA's function in cancer if we can identify cancer miRNAs directly from gene expression data. Results: We present a novel computational framework to identify the cancer-related miRNAs based solely on gene expression profiles without requiring either miRNA expression data or the matched gene and miRNA expression data. The results on multiple cancer datasets show that our proposed method can effectively identify cancer-related miRNAs with higher precision compared with other popular approaches. Furthermore, some of our novel predictions are validated by both differentially expressed miRNAs and evidences from literature, implying the predictive power of our proposed method. In addition, we construct a cancer-miRNA-pathway network, which can help explain how miRNAs are involved in cancer. Availability and implementation: The R code and data files for the proposed method are available at http://comp-sysbio.org/miR_Path/ Contact:", 
  "summary": "Under the circumstances, some approaches have been developed to predict cancer-related miRNAs by investigating matched gene and miRNA expression data as well as miRNA-gene regulations (Wuchty et al., 2013; Zhang et al., 2014).\nThe IMRE approach inferred miRNA expression levels from mRNA expression data based on putative miRNA targets, and those up-regulated or down-regulated miRNAs in cancer were identified and regarded related to cancer (Kuo et al., 2012).\nWith the miRNA expression data across 40 normal human tissues (Liang et al., 2007), we identified 16 miRNAs for colon, 15 for lung, 21 for stomach and 6 for breast (Supplementary Material S4), where these miRNAs are both specifically expressed in the corresponding tissues and predicted to be related to cancer.", 
  "affiliations": [
    " Shanghai Institutes for Biological Sciences Key Laboratory of Systems Biology Chinese Academy of Sciences ", 
    " School of Electronics and Information Engineering Tongji University ", 
    " LERIA University of Angers ", 
    " Department of Mathematics Shanghai University "
  ], 
  "grants": [
    "Funding\nThis work was partly supported by Strategic Priority Research Program of the Chinese Academy of Sciences [XDB13040700]; National Natural Science Foundation of China [91130032, 61103075]; Innovation Program of Shanghai Municipal Education Commission [13ZZ072] and Shanghai Pujiang Program [13PJD032]."
  ], 
  "acks": " ", 
  "authors": [
    " Xing-Ming Zhao", 
    " Ke-Qin Liu", 
    " Guanghui Zhu", 
    " Feng He", 
    " B\u00e9 Atrice Duval", 
    " Jean-Michel Richer", 
    " De-Shuang Huang", 
    " Chang-Jun Jiang", 
    " Jin-Kao Hao", 
    " Luonan Chen"
  ], 
  "keyWords": [
    [
      "cells", 
      "genes", 
      "mirnas", 
      "regulations", 
      "cancers", 
      "pathways"
    ]
  ], 
  "sourcelinks": [
    "http://comp-sysbio.org/miR_Path"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-12-14T01:33:54Z"
}{
  "doi": "10.1093/bioinformatics/btu766", 
  "name": "HyDRA gene prioritization via hybrid distancescore rank aggregation", 
  "links": [
    "http://www.mathworks.com/matlabcentral/fileexchange/11609-hungarianalgorithm", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://web"
  ], 
  "title": "HyDRA: gene prioritization via hybrid distance-score rank aggregation", 
  "toolName": "HyDRA: gene prioritization via hybrid distance-score rank aggregation", 
  "abstract": "Gene prioritization refers to a family of computational techniques for inferring disease genes through a set of training genes and carefully chosen similarity criteria. Test genes are scored based on their average similarity to the training set, and the rankings of genes under various similarity criteria are aggregated via statistical methods. The contributions of our work are threefold: (i) first, based on the realization that there is no unique way to define an optimal aggregate for rank-ings, we investigate the predictive quality of a number of new aggregation methods and known fusion techniques from machine learning and social choice theory. Within this context, we quantify the influence of the number of training genes and similarity criteria on the diagnostic quality of the aggregate and perform in-depth cross-validation studies; (ii) second, we propose a new approach to genomic data aggregation, termed HyDRA (Hybrid Distance-score Rank Aggregation), which combines the advantages of score-based and combinatorial aggregation techniques. We also propose incorporating a new top-versus-bottom (TvB) weighting feature into the hybrid schemes. The TvB feature ensures that aggregates are more reliable at the top of the list, rather than at the bottom, since only top candidates are tested experimentally; (iii) third, we propose an iterative procedure for gene discovery that operates via successful augmentation of the set of training genes by genes discovered in previous rounds, checked for consistency. Motivation: Fundamental results from social choice theory, political and computer sciences, and statistics have shown that there exists no consistent, fair and unique way to aggregate rankings. Instead, one has to decide on an aggregation approach using predefined set of desirable properties for the aggregate. The aggregation methods fall into two categories, score-and distance-based approaches, each of which has its own drawbacks and advantages. This work is motivated by the observation that merging these two techniques in a computationally efficient manner, and by incorporating additional constraints, one can ensure that the predictive quality of the resulting aggregation algorithm is very high. Results: We tested HyDRA on a number of gene sets, including autism, breast cancer, colorectal cancer, endometriosis, ischaemic stroke, leukemia, lymphoma and osteoarthritis. Furthermore, we performed iterative gene discovery for glioblastoma, meningioma and breast cancer, using a sequentially augmented list of training genes related to the Turcot syndrome, Li-Fraumeni condition and other diseases. The methods outperform state-of-the-art software tools such as ToppGene and Endeavour. Despite this finding, we recommend as best practice to take the union of top-ranked items produced by different methods for the final aggregated list. Availability and implementation: The HyDRA software may be downloaded from: http://web. engr.illinois.edu/$mkim158/HyDRA.zip", 
  "summary": "Test genes are scored based on their average similarity to the training set, and the rankings of genes under various similarity criteria are aggregated via statistical methods.\nWithin this context, we quantify the influence of the number of training genes and similarity criteria on the diagnostic quality of the aggregate and perform in-depth cross-validation studies; (ii) second, we propose a new approach to genomic data aggregation, termed HyDRA (Hybrid Distance-score Rank Aggregation), which combines the advantages of score-based and combinatorial aggregation techniques.\nNote that score and distance-based methods do not make use of quantitative information, such as, e.g., p-values (for the case of gene prioritization) or ratings (for the case of social choice theory and recommender systems).", 
  "affiliations": [
    " Department of Electrical and Computer Engineering University of Illinois at Urbana-Champaign "
  ], 
  "grants": [
    "Acknowledgements\nThe work was supported in part by the National Science Foundation (NSF) under grants CCF 0809895, CCF 1218764, CSoI-CCF 0939370, and IOS 1339388."
  ], 
  "acks": " The work was supported in part by the National Science Foundation (NSF) under grants CCF 0809895, CCF 1218764, CSoI-CCF 0939370, and IOS 1339388. Conflict of interest: none declared. ", 
  "authors": [
    " Minji Kim", 
    " Farzad Farnoud", 
    " Olgica Milenkovic"
  ], 
  "keyWords": [
    "rank aggregation", 
    "disease genes", 
    [
      "rankings", 
      "gene", 
      "methods", 
      "diseases", 
      "aggregating"
    ]
  ], 
  "sourcelinks": [
    "http://web"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-20T06:56:29Z"
}{
  "doi": "10.1093/bioinformatics/btu608", 
  "name": "Improved rat genome gene prediction by integration of ESTs with RNASeq information", 
  "links": [
    "http://hgdownload.soe.ucsc", 
    "http://rgd.mcw.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://hgdownload.soe.ucsc.edu"
  ], 
  "title": "Genome analysis Improved rat genome gene prediction by integration of ESTs with RNA-Seq information", 
  "toolName": "Genome analysis Improved rat genome gene prediction by integration of ESTs with RNA-Seq information", 
  "abstract": "Motivation: RNA-Seq (also called whole-transcriptome sequencing) is an emerging technology that uses the capabilities of next-generation sequencing to detect and quantify entire transcripts. One of its important applications is the improvement of existing genome annotations. RNA-Seq provides rapid, comprehensive and cost-effective tools for the discovery of novel genes and transcripts compared with expressed sequence tag (EST), which is instrumental in gene discovery and gene sequence determination. The rat is widely used as a laboratory disease model, but has a less well-annotated genome as compared with humans and mice. In this study, we incorporated deep RNA-Seq data from three rat tissues\u2014bone marrow, brain and kidney\u2014with EST data to improve the annotation of the rat genome. Results: Our analysis identified 32 197 transcripts, including 13 461 known transcripts, 13 934 novel isoforms and 4802 new genes, which almost doubled the numbers of transcripts in the current public rat genome database (rn5). Comparisons of our predicted protein-coding gene sets with those in public datasets suggest that RNA-Seq significantly improves genome annotation and identifies novel genes and isoforms in the rat. Importantly, the large majority of novel genes and isoforms are supported by direct evidence of RNA-Seq experiments. These predicted genes were integrated into the Rat Genome Database (RGD) and can serve as an important resource for functional studies in the research community. Availability and implementation: The predicted genes are available at http://rgd.", 
  "summary": "Comparisons of our predicted protein-coding gene sets with those in public datasets suggest that RNA-Seq significantly improves genome annotation and identifies novel genes and isoforms in the rat.\nWe compared our predicted protein-coding gene sets with public datasets and found that RNA-Seq significantly improved genome annotation and identified novel genes and gene isoforms in the rat.\nBy incorporating EST information with RNA-Seq data from bone marrow into gene prediction models, we annotated 16 834 transcripts, including 8857 known transcripts, 5937 novel isoforms (within known genes) and 2040 new genes, that show no overlaps with any genes in the public rat genome database (rn5).", 
  "affiliations": [
    " Division of Respiratory Medicine School of Medicine Sir Run Run Shaw Hospital Zhejiang University ", 
    " Department of Physiology and the Cancer Center Medical College of Wisconsin ", 
    " Institute of Bioinformatics Zhejiang University ", 
    " Human Molecular and Genetics Center Medical College of Wisconsin "
  ], 
  "grants": [
    "Funding: This work has been support in part by start-up from Advancing a Healthier Wisconsin Fund (FP00001701 and FP00001703), the Louisiana Hope Research Grant provided by Free to Breathe, Women Health Research Program at the Medical College of Wisconsin, National Natural Science Foundation of China (No."
  ], 
  "acks": " The authors thank Haris G. Vikis and three anonymous reviewers for reading and commenting on the manuscript. They also thank Gregory McQuestion for providing system support for various programs used in the study. ", 
  "authors": [
    " Liping Li", 
    " Enguo Chen", 
    " Chun Yang", 
    " Jun Zhu", 
    " Pushkala Jayaraman", 
    " Jeffrey De Pons", 
    " Catherine C Kaczorowski", 
    " Howard J Jacob", 
    " Andrew S Greene", 
    " Matthew R Hodges", 
    " Allen W Cowley", 
    " Mingyu Liang", 
    " Haiming Xu", 
    " Pengyuan Liu", 
    " Yan Lu"
  ], 
  "keyWords": [
    "gene prediction", 
    [
      "genomics", 
      "rats", 
      "transcription", 
      "genes", 
      "predictions", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [
    "http://hgdownload.soe.ucsc", 
    "http://rgd.mcw.edu", 
    "http://hgdownload.soe.ucsc.edu"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-13T00:23:17Z"
}{
  "doi": "10.1093/bioinformatics/btu430", 
  "name": "Improving peak detection in highresolution LCMS metabolomics data using preexisting knowledge and machine learning approach", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining Improving peak detection in high-resolution LC/MS metabolomics data using preexisting knowledge and machine learning approach", 
  "toolName": "Data and text mining Improving peak detection in high-resolution LC/MS metabolomics data using preexisting knowledge and machine learning approach", 
  "abstract": "Motivation: Peak detection is a key step in the preprocessing of untargeted metabolomics data generated from high-resolution liquid chromatography-mass spectrometry (LC/MS). The common practice is to use filters with predetermined parameters to select peaks in the LC/MS profile. This rigid approach can cause suboptimal performance when the choice of peak model and parameters do not suit the data characteristics. Results: Here we present a method that learns directly from various data features of the extracted ion chromatograms (EICs) to differentiate between true peak regions from noise regions in the LC/MS profile. It utilizes the knowledge of known metabolites, as well as robust machine learning approaches. Unlike currently available methods, this new approach does not assume a parametric peak shape model and allows maximum flexibility. We demonstrate the superiority of the new approach using real data. Because matching to known me-tabolites entails uncertainties and cannot be considered a gold standard , we also developed a probabilistic receiver-operating characteristic (pROC) approach that can incorporate uncertainties.", 
  "summary": "Results: Here we present a method that learns directly from various data features of the extracted ion chromatograms (EICs) to differentiate between true peak regions from noise regions in the LC/MS profile.\nFirst, from the results of all three methods, we removed peaks matched to the [M + H]+ ions of the half HMDB metabolites that were used as training data.\nIn this study, we presented a new machine learning approach for peak detection from high-resolution LC/MS data.", 
  "affiliations": [
    " Department of Biostatistics and Bioinformatics Rollins School of Public Health ", 
    " Department of Medicine School of Medicine Emory University "
  ], 
  "grants": [
    "Funding: NIH grants P20 HL113451, P01 AI096187, U19 AI090023 and U19 AI057266."
  ], 
  "acks": " The authors thank three anonymous reviewers whose comments helped to greatly improve this article. Funding: NIH grants P20 HL113451, P01 AI096187, U19 AI090023 and U19 AI057266. Conflict of Interest: none declared. ", 
  "authors": [
    " Tianwei Yu", 
    " Dean P Jones"
  ], 
  "keyWords": [
    "peak detection", 
    [
      "methods", 
      "differences", 
      "detectable", 
      "values", 
      "data", 
      "peaks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-09T04:13:37Z"
}{
  "doi": "10.1093/bioinformatics/btu194", 
  "name": "Improving the accuracy of the structure prediction of the third hypervariable loop of the heavy chains of antibodies", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.biocomputing.it/H3Loopred", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Structural bioinformatics Improving the accuracy of the structure prediction of the third hypervariable loop of the heavy chains of antibodies", 
  "toolName": "Structural bioinformatics Improving the accuracy of the structure prediction of the third hypervariable loop of the heavy chains of antibodies", 
  "abstract": "Motivation: Antibodies are able to recognize a wide range of antigens through their complementary determining regions formed by six hypervariable loops. Predicting the 3D structure of these loops is essential for the analysis and reengineering of novel antibodies with enhanced affinity and specificity. The canonical structure model allows high accuracy prediction for five of the loops. The third loop of the heavy chain, H3, is the hardest to predict because of its diversity in structure, length and sequence composition. Results: We describe a method, based on the Random Forest automatic learning technique, to select structural templates for H3 loops among a dataset of candidates. These can be used to predict the structure of the loop with a higher accuracy than that achieved by any of the presently available methods. The method also has the advantage of being extremely fast and returning a reliable estimate of the model quality. Availability and implementation: The source code is freely available at", 
  "summary": "The strategy consists of modeling the framework by homology while the prediction of five of the six loops is based on the canonical structure (CS) model that states that these loops (the light chains loops and the H1 and H2 loops of the heavy chain) can only assume a limited number of conformations and that these are determined by the presence of key residues in specific positions in the sequence of the antibody (Chothia and Lesk, 1987; Tramontano et al., 1990).\nOne of the most used approaches for antibody structure prediction is Rosetta Antibody (RA; Sircar et al., 2009; Sivasubramanian et al., 2009), which combines template selections with ab initio CDR H3 loop modeling (using loop fragments) and simultaneous optimization of the CDR loop conformations and Variable Light (VL)-Variable Heavy (VH) orientations.", 
  "affiliations": [
    " Department of Physics Sapienza University ", 
    " Department of Systems Biology Center for Biological Sequence Analysis Technical University of Denmark "
  ], 
  "grants": [
    "Funding: KAUST Award No."
  ], 
  "acks": " ", 
  "authors": [
    " Mario Abdel Messih", 
    " Rosalba Lepore", 
    " Paolo Marcatili", 
    " Anna Tramontano", 
    " Istituto Pasteur-Fondazione", 
    " Cenci Bolognetti", 
    " Italy Rome", 
    " Alfonso Associate", 
    " Valencia"
  ], 
  "keyWords": [
    [
      "loops", 
      "antibodies", 
      "structures", 
      "methods", 
      "modelling"
    ]
  ], 
  "sourcelinks": [
    "http://www.biocomputing.it/H3Loopred"
  ], 
  "technologies": [], 
  "dateCreated": "2014-06-16T20:35:17Z"
}{
  "doi": "10.1093/bioinformatics/btu806", 
  "name": "Improved gene tree error correction in the presence of horizontal gene transfer", 
  "links": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://compbio"
  ], 
  "title": "Improved gene tree error correction in the presence of horizontal gene transfer", 
  "toolName": "Improved gene tree error correction in the presence of horizontal gene transfer", 
  "abstract": "Motivation: The accurate inference of gene trees is a necessary step in many evolutionary studies. Although the problem of accurate gene tree inference has received considerable attention, most existing methods are only applicable to gene families unaffected by horizontal gene transfer. As a result, the accurate inference of gene trees affected by horizontal gene transfer remains a largely unaddressed problem. Results: In this study, we introduce a new and highly effective method for gene tree error correction in the presence of horizontal gene transfer. Our method efficiently models horizontal gene transfers, gene duplications and losses, and uses a statistical hypothesis testing framework [Shimodaira\u2013Hasegawa (SH) test] to balance sequence likelihood with topological information from a known species tree. Using a thorough simulation study, we show that existing phylogenetic methods yield inaccurate gene trees when applied to horizontally transferred gene families and that our method dramatically improves gene tree accuracy. We apply our method to a dataset of 11 cyanobacterial species and demonstrate the large impact of gene tree accuracy on downstream evolutionary analyses. Availability and implementation: An implementation of our method is available at http://compbio.", 
  "summary": "We reconcile gene trees reconstructed using RAxML and those inferred using TreeFix-DTL, AnGST and MowgliNNI with their corresponding species trees and show that improved topological accuracy translates into a direct improvement in correctly inferring evolutionary events like duplications, transfers, and losses (DTLs).\nIn our experimental study, we applied the methods RAxML (Stamatakis, 2006), NOTUNG (Durand et al., 2006), TreeFix (Wu et al., 2013), MowgliNNI (Nguyen et al., 2012), AnGST (David and Alm, 2011) and our new method TreeFix-DTL, to simulated datasets and evaluated the accuracy of the inferred gene trees.", 
  "affiliations": [
    " Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology "
  ], 
  "grants": [
    "Funding\nThis work was supported by a National Science Foundation [CAREER award 0644282 to M.K.", 
    "], and startup funds from the University of Connecticut [to M.S.B.]."
  ], 
  "acks": " ", 
  "authors": [
    " Mukul S Bansal", 
    " Yi-Chieh Wu", 
    " Eric J Alm", 
    " Manolis Kellis"
  ], 
  "keyWords": [
    "gene tree", 
    [
      "genes", 
      "methods", 
      "treefix", 
      "species"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-12-07T01:08:27Z"
}{
  "doi": "10.1093/bioinformatics/btu522", 
  "name": "Individuallevel analysis of differential expression of genes and pathways for personalized medicine", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Gene expression Individual-level analysis of differential expression of genes and pathways for personalized medicine", 
  "toolName": "Gene expression Individual-level analysis of differential expression of genes and pathways for personalized medicine", 
  "abstract": "Motivation: The differential expression analysis focusing on inter-group comparison can capture only differentially expressed genes (DE genes) at the population level, which may mask the heterogeneity of differential expression in individuals. Thus, to provide patient-specific information for personalized medicine, it is necessary to conduct differential expression analysis at the individual level. Results: We proposed a method to detect DE genes in individual disease samples by using the disrupted ordering in individual disease samples. In both simulated data and real paired cancer-normal sample data, this method showed excellent performance. It was found to be insensitive to experimental batch effects and data normal-ization. The landscape of stable gene pairs in a particular type of normal tissue could be predetermined using previously accumulated data, based on which dysregulated genes and pathways for any disease sample can be readily detected. The usefulness of the RankComp method in clinical settings was exemplified by the identification and application of prognostic markers for lung cancer.", 
  "summary": "Using the stable gene pairs obtained from the 210 normal lung tissue samples, with 5% FDR control, DE genes for each lung cancer sample from an independent paired cancernormal sample dataset (GSE27262) were detected using the RankComp method described in Section 2.3.\nThese results indicated that the RankComp method can reliably capture differential expression signals, especially the top-ranked signals, in cancer patients based on the landscape of stable gene pairs obtained in advance using previously accumulated data.", 
  "affiliations": [
    " Genomics Research Center Harbin Medical University ", 
    " College of Bioinformatics Science and Technology"
  ], 
  "grants": [
    "Funding: The Natural Science Foundation of China (91029717, 81071646, 81372213, 81201822 and 81030029) and Research Fund for the Doctoral Program of Higher Education of China (20112307110011)."
  ], 
  "acks": " ", 
  "authors": [
    " Hongwei Wang", 
    " Qiang Sun", 
    " Wenyuan Zhao", 
    " Lishuang Qi", 
    " Yunyan Gu", 
    " Pengfei Li", 
    " Mengmeng Zhang", 
    " Yang Li", 
    " Shu-Lin Liu", 
    " Zheng Guo"
  ], 
  "keyWords": [
    "cancer sample", 
    "gene expression", 
    [
      "methods", 
      "genes", 
      "cancers", 
      "samples", 
      "expressions", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-28T02:52:42Z"
}{
  "doi": "10.1093/bioinformatics/btu489", 
  "name": "Inferring conditionspecific miRNA activity from matched miRNA and mRNA expression data", 
  "links": [
    "http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.ebi.ac.uk/arrayexpress", 
    "http://www.broad.mit.edu", 
    "http://www.ncbi.nlm.nih.gov/geo/query/acc"
  ], 
  "title": "Gene expression Inferring condition-specific miRNA activity from matched miRNA and mRNA expression data", 
  "toolName": "Gene expression Inferring condition-specific miRNA activity from matched miRNA and mRNA expression data", 
  "abstract": "Motivation: MicroRNAs (miRNAs) play crucial roles in complex cellular networks by binding to the messenger RNAs (mRNAs) of protein coding genes. It has been found that miRNA regulation is often condition specific. A number of computational approaches have been developed to identify miRNA activity specific to a condition of interest using gene expression data. However, most of the methods only use the data in a single condition, and thus, the activity discovered may not be unique to the condition of interest. Additionally, these methods are based on statistical associations between the gene expression levels of miRNAs and mRNAs, so they may not be able to reveal real gene regulatory relationships, which are causal relationships. Results: We propose a novel method to infer condition-specific miRNA activity by considering (i) the difference between the regulatory behavior that an miRNA has in the condition of interest and its behavior in the other conditions; (ii) the causal semantics of miRNA\u2013mRNA relationships. The method is applied to the epithelial\u2013mesenchymal transition (EMT) and multi-class cancer (MCC) datasets. The validation by the results of transfection experiments shows that our approach is effective in discovering significant miRNA\u2013mRNA interactions. Functional and pathway analysis and literature validation indicate that the identified active miRNAs are closely associated with the specific biological processes, diseases and pathways. More detailed analysis of the activity of the active miRNAs implies that some active miRNAs show different regulation types in different conditions, but some have the same regulation types and their activity only differs in different conditions in the strengths of regulation.", 
  "summary": "To infer such condition-specific active miRNAs or their activity, firstly, for each identified significant miRNAmRNA causal interaction, we find out the median value of its causal effects calculated during the B times of bootstrapping, in each condition, respectively.\nThe comparison with five correlation methods demonstrates that causal effects provide a better measure than correlations in modeling the strengths of miRNAmRNA interactions, leading to more effective discovery of active miRNAs. As the main aim of the article is to identify condition-specific active miRNAs and their activity, we conduct function and", 
  "affiliations": [
    " Children's Cancer Institute Australia", 
    " School of Information Technology and Mathematical Sciences University of South Australia ", 
    " Faculty of Engineering Dali University ", 
    " Centre for Cancer Biology SA Pathology ", 
    " Kunming University of Science and Technology"
  ], 
  "grants": [
    "Funding: This work has been partially supported by the Applied Basic Research Foundation of Science and Technology of Yunnan Province (No: 2013FD038), the Australian Research Council Discovery grant DP130104090 and the Science Research Foundation for Youth Scholars of Dali University (No: KYQN201203)."
  ], 
  "acks": " ", 
  "authors": [
    " Junpeng Zhang", 
    " Thuc Duy Le", 
    " Lin Liu", 
    " Bing Liu", 
    " Jianfeng He", 
    " Gregory J Goodall", 
    " Jiuyong Li"
  ], 
  "keyWords": [
    [
      "methods", 
      "mirnas", 
      "cancers", 
      "data", 
      "micrornas", 
      "significantly"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-25T04:43:31Z"
}{
  "doi": "10.1093/bioinformatics/btv007", 
  "name": "Inferring singlecell gene expression mechanisms using stochastic simulation", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Inferring single-cell gene expression mechanisms using stochastic simulation", 
  "toolName": "Inferring single-cell gene expression mechanisms using stochastic simulation", 
  "abstract": "Motivation: Stochastic promoter switching between transcriptionally active (ON) and inactive (OFF) states is a major source of noise in gene expression. It is often implicitly assumed that transitions between promoter states are memoryless, i.e. promoters spend an exponentially distributed time interval in each of the two states. However, increasing evidence suggests that promoter ON/OFF times can be non-exponential, hinting at more complex transcriptional regulatory architec-tures. Given the essential role of gene expression in all cellular functions, efficient computational techniques for characterizing promoter architectures are critically needed. Results: We have developed a novel model reduction for promoters with arbitrary numbers of ON and OFF states, allowing us to approximate complex promoter switching behavior with Weibull-distributed ON/OFF times. Using this model reduction, we created bursty Monte Carlo expectation-maximization with modified cross-entropy method ('bursty MCEM 2 '), an efficient parameter estimation and model selection technique for inferring the number and configuration of promoter states from single-cell gene expression data. Application of bursty MCEM 2 to data from the en-dogenous mouse glutaminase promoter reveals nearly deterministic promoter OFF times, consistent with a multi-step activation mechanism consisting of 10 or more inactive states. Our novel approach to modeling promoter fluctuations together with bursty MCEM 2 provides powerful tools for characterizing transcriptional bursting across genes under different environmental conditions. Availability and implementation: R source code implementing bursty MCEM 2 is available upon request.", 
  "summary": "Using this model reduction, we created bursty Monte Carlo expectationmaximization with modified cross-entropy method (`bursty MCEM2'), an efficient parameter estimation and model selection technique for inferring the number and configuration of promoter states from single-cell gene expression data.\nBecause of intrinsic noise exhibited by the small numbers of molecules involved in transcription (e.g. 12 copies of DNA, few available copies of transcriptional regulators) (Raser and O'Shea, 2005), the promoter produces expression bursts by switching randomly between the transcriptionally active (ON) and inactive (OFF) states according to kinetic parameters (rate constants) that can be estimated from single-cell time series data (Suter et al., 2011).", 
  "affiliations": [
    " Department of Electrical and Computer Engineering University of Delaware ", 
    " Institute for Collaborative Biotechnologies University of California ", 
    " Department of Computer Science University of California "
  ], 
  "grants": [
    "were supported by NIH RO1-EB014877, DOE DE-SC0008975 and the Institute for Collaborative Biotechnologies through grant [W911NF-09-0001] from the U.S. Army Research Office.", 
    "Funding\nA.S. was supported by the National Science Foundation Grant [DMS1312926], University of Delaware Research Foundation (UDRF) and Oak Ridge Associated Universities (ORAU).", 
    "We also acknowledge computing support from the UCSB Center for Scientific Computing from the CNSI, MRL: an NSF MRSEC (DMR-1121053) and NSF CNS-0960316."
  ], 
  "acks": " ", 
  "authors": [
    " Bernie J Daigle", 
    " Mohammad Soltani", 
    " Linda R Petzold", 
    " Abhyudai Singh"
  ], 
  "keyWords": [
    [
      "states", 
      "promoters", 
      "data", 
      "modelling", 
      "times"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-09T02:26:44Z"
}{
  "doi": "10.1093/bioinformatics/btv006", 
  "name": "Integrating alignmentbased and alignmentfree sequence similarity measures for biological sequence classification", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www", 
    "https://collaborators.oicr.on.ca/vferretti/borozan_csss/csss.html", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Integrating alignment-based and alignment-free sequence similarity measures for biological sequence classification", 
  "toolName": "Integrating alignment-based and alignment-free sequence similarity measures for biological sequence classification", 
  "abstract": "Motivation: Alignment-based sequence similarity searches, while accurate for some type of sequences, can produce incorrect results when used on more divergent but functionally related sequences that have undergone the sequence rearrangements observed in many bacterial and viral genomes. Here, we propose a classification model that exploits the complementary nature of alignment-based and alignment-free similarity measures with the aim to improve the accuracy with which DNA and protein sequences are characterized. Results: Our model classifies sequences using a combined sequence similarity score calculated by adaptively weighting the contribution of different sequence similarity measures. Weights are determined independently for each sequence in the test set and reflect the discriminatory ability of individual similarity measures in the training set. Because the similarity between some sequences is determined more accurately with one type of measure rather than another, our classifier allows different sets of weights to be associated with different sequences. Using five different similarity measures, we show that our model significantly improves the classification accuracy over the current composition-and alignment-based models, when predicting the taxonomic lineage for both short viral sequence fragments and complete viral sequences. We also show that our model can be used effectively for the classification of reads from a real metagenome dataset as well as protein sequences. Availability and implementation: All the datasets and the code used in this study are freely available at https://collaborators.oicr.on.ca/vferretti/borozan_csss/csss.html.", 
  "summary": "The classification accuracy [see Equation (16)] for Dataset I obtained with the CSSS (1-NN classifier) and the five other models: PhymmBL (Brady and Salzberg, 2009), NBC (Rosen et al., 2011), Kraken (Wood and Salzberg, 2014), RAIphy (Nalbantoglu et al., 2011) and PAUDA (Huson and Xie, 2014) when predicting 147 different viral genera across 266 viral DNA sequences as a function of the viral fragment length", 
  "affiliations": [
    " Department of Informatics and Bio-computing MaRS Centre Ontario Institute for Cancer Research South Tower "
  ], 
  "grants": [
    "Funding\nThis work was conducted with the support of the Ontario Institute for Cancer Research through funding provided by the government of Ontario to the authors."
  ], 
  "acks": " ", 
  "authors": [
    " Ivan Borozan", 
    " Stuart Watt", 
    " Vincent Ferretti"
  ], 
  "keyWords": [
    [
      "scoring", 
      "based", 
      "models", 
      "measures", 
      "genomes", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [
    "https://collaborators.oicr.on.ca/vferretti/borozan_csss/csss.html"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-09T02:26:44Z"
}{
  "doi": "10.1093/bioinformatics/btu666", 
  "name": "Integrative data analysis indicates an intrinsic disordered domain character of Argonautebinding motifs", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.comgen.pl", 
    "http://www.comgen.pl/whub"
  ], 
  "title": "Sequence analysis Integrative data analysis indicates an intrinsic disordered domain character of Argonaute-binding motifs", 
  "toolName": "Sequence analysis Integrative data analysis indicates an intrinsic disordered domain character of Argonaute-binding motifs", 
  "abstract": "Motivation: Argonaute-interacting WG/GW proteins are characterized by the presence of repeated sequence motifs containing glycine (G) and tryptophan (W). The motifs seem to be remarkably adaptive to amino acid substitutions and their sequences show non-contiguity. Our previous approach to the detection of GW domains, based on scoring their gross amino acid composition, allowed annotation of several novel proteins involved in gene silencing. The accumulation of new experimental data and more advanced applications revealed some deficiency of the algorithm in prediction selectivity. Additionally, W-motifs, though critical in gene regulation, have not yet been annotated in any available online resources. Results: We present an improved set of computational tools allowing efficient management and annotation of W-based motifs involved in gene silencing. The new prediction algorithms provide novel function-alities by annotation of the W-containing domains at the local sequence motif level rather than by overall compositional properties. This approach represents a significant improvement over the previous method in terms of prediction sensitivity and selectivity. Application of the algorithm allowed annotation of a comprehensive list of putative Argonaute-interacting proteins across eukaryotes. An in-depth characterization of the domains' properties indicates its intrinsic disordered character. In addition, we created a knowledge-based portal (whub) that provides access to tools and information on RNAi-related trypto-phan-containing motifs. Availability and implementation: The web portal and tools are freely available at http://www.comgen.pl/whub.", 
  "summary": "AGO proteins interact with GW proteins, which are characterized by the presence of repeated sequence motifs containing two amino acids: glycine (G) and tryptophan (W).\nDeficiency of a comprehensive resource for functional GW/ GW proteins in any of the central protein [e.g. UniProt (The UniProt Consortium, 2014), RefSeq (Pruitt et al., 2012)] and/ or specialized domain databases [e.g. Pfam (Finn et al., 2014), InterPro (Hunter et al., 2012)] as well as quickly accumulating knowledge about the properties of the W-based AGO/CCR4NOT interacting proteins inspired us to develop new computational tools to facilitate efficient management and annotation of W-motifs.", 
  "affiliations": [
    " Institute of Molecular Biology and Biotechnology Faculty of Biology Laboratory of Computational Genomics-Bioinformatics Laboratory Adam Mickiewicz University "
  ], 
  "grants": [
    "Funding: This work was supported by grants from the National Center of Science [UMO-2011/03/B/NZ2/01416 to W.M.K."
  ], 
  "acks": " ", 
  "authors": [
    " Andrzej Zielezinski", 
    " Wojciech M Karlowski", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "domains", 
      "proteins", 
      "agos", 
      "motifs", 
      "sequences"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-11T03:33:58Z"
}{
  "doi": "10.1093/bioinformatics/btu423", 
  "name": "Intensity drift removal in LCMS metabolomics by common variance compensation", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://b2slab.upc.edu/softwareand-downloads/intensity-drift-correction"
  ], 
  "title": "Gene expression Intensity drift removal in LC/MS metabolomics by common variance compensation", 
  "toolName": "Gene expression Intensity drift removal in LC/MS metabolomics by common variance compensation", 
  "abstract": "Liquid chromatography coupled to mass spectrometry (LC/MS) has become widely used in Metabolomics. Several artefacts have been identified during the acquisition step in large LC/MS metabolomics experiments, including ion suppression, carryover or changes in the sensitivity and intensity. Several sources have been pointed out as responsible for these effects. In this context, the drift effects of the peak intensity is one of the most frequent and may even constitute the main source of variance in the data, resulting in misleading statistical results when the samples are analysed. In this article, we propose the introduction of a methodology based on a common variance analysis before the data normalization to address this issue. This methodology was tested and compared with four other methods by calculating the Dunn and Silhouette indices of the quality control classes. The results showed that our proposed methodology performed better than any of the other four methods. As far as we know, this is the first time that this kind of approach has been applied in the metabolomics context. Availability and implementation: The source code of the methods is available as the R package intCor at http://b2slab.upc.edu/software-and-downloads/intensity-drift-correction/.", 
  "summary": "Equalization methods based on a sample-wise correction for LC/ MS metabolomic data have also been tested and compared by Veselkov et al.\nWe propose a quality measure for peak intensity drift correction methods based on the standard clustering internal measures Dunn and Silhouette for the QC classes in the principal plane (the plane explaining maximum variability of the data) score plot of all the classes (including the study class).\nOverall, in the context of LC/MS drift correction, the proposed two-step methodology shows better clustering properties of the QC samples for large metabolomic studies than the median fold change method.", 
  "affiliations": [
    " Department of Nutrition and Food Science-XaRTA Faculty of Pharmacy, Food and Nutrition Torribera Campus Biomarkers & Nutrimetabolomic Lab INSA University of Barcelona ", 
    " Department d'Enginyeria de Sistemes CIBER-BBN Autom atica i Inform atica Industrial Universitat Polit\u00e8 cnica de Catalunya "
  ], 
  "grants": [
    "This work was partially funded by the Spanish Ministerio de Ciencia y Tecnologia through the (TEC2010-20886-C02-02 and TEC2010-20886-C02-01) grants, and the Ramo n y Cajal programme.", 
    "thanks the Generalitat de Catalunyas Agency for Management of University and Research Grants (AGAUR) for the predoctoral (FI-DGR 2011) fellowship.", 
    "Funding: Spanish national grants (AGL2009-13906-C02-01/ALI, AGL2010-10084-E, 2014 SGR 1063, 2014 SGR 1566), the CONSOLIDER INGENIO 2010 Programme, FUN-C-FOOD (CSD2007-063) from the Spanish Ministry of Economy and Competitiveness (MINECO), as well as FEDER (Fondo Europeo de Desarrollo Regional) and Merck Serono 2010 Research Grants (Fundacio n Salud 2000)."
  ], 
  "acks": " ", 
  "authors": [
    " Francesc Fern Andez-Albert", 
    " Rafael Llorach", 
    " Mar Garcia-Aloy", 
    " Andrey Ziyatdinov", 
    " Cristina Andres-Lacueva", 
    " Alexandre Perera", 
    " Joan Xxiii", 
    " Ziv Bar-Joseph"
  ], 
  "keyWords": [
    [
      "classes", 
      "data", 
      "methods", 
      "drifts", 
      "samples"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://b2slab.upc.edu/softwareand-downloads/intensity-drift-correction"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-03T00:27:22Z"
}{
  "doi": "10.1093/bioinformatics/btu569", 
  "name": "Interspecies inference of gene set enrichment in lung epithelial cells from proteomic and large transcriptomic datasets", 
  "links": [
    "http://bhanot.biomaps.rutgers", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Gene expression Inter-species inference of gene set enrichment in lung epithelial cells from proteomic and large transcriptomic datasets", 
  "toolName": "Gene expression Inter-species inference of gene set enrichment in lung epithelial cells from proteomic and large transcriptomic datasets", 
  "abstract": "Motivation: Translating findings in rodent models to human models has been a cornerstone of modern biology and drug development. However, in many cases, a naive 'extrapolation' between the two species has not succeeded. As a result, clinical trials of new drugs sometimes fail even after considerable success in the mouse or rat stage of development. In addition to in vitro studies, inter-species translation requires analytical tools that can predict the enriched gene sets in human cells under various stimuli from corresponding measurements in animals. Such tools can improve our understanding of the underlying biology and optimize the allocation of resources for drug development. Results: We developed an algorithm to predict differential gene set enrichment as part of the sbv IMPROVER (systems biology verification in Industrial Methodology for Process Verification in Research) Species Translation Challenge, which focused on phosphoproteomic and transcriptomic measurements of normal human bronchial epithe-lial (NHBE) primary cells under various stimuli and corresponding measurements in rat (NRBE) primary cells. We find that gene sets exhibit a higher inter-species correlation compared with individual genes, and are potentially more suited for direct prediction. Furthermore, in contrast to a similar cross-species response in protein phosphorylation states 5 and 25 min after exposure to stimuli, gene set enrichment 6 h after exposure is significantly different in NHBE cells compared with NRBE cells. In spite of this difference, we were able to develop a robust algorithm to predict gene set activation in NHBE with high accuracy using simple analytical methods. Availability and implementation: Implementation of all algorithms is available as source code (in Matlab) at http://bhanot.biomaps.rutgers. edu/wiki/codes_SC3_Predicting_GeneSets.zip, along with the relevant data used in the analysis. Gene sets, gene expression and protein phosphorylation data are available on request.", 
  "summary": "Results: We developed an algorithm to predict differential gene set enrichment as part of the sbv IMPROVER (systems biology verification in Industrial Methodology for Process Verification in Research) Species Translation Challenge, which focused on phosphoproteomic and transcriptomic measurements of normal human bronchial epithelial (NHBE) primary cells under various stimuli and corresponding measurements in rat (NRBE) primary cells.\nIn the specific challenge described here, the so-called Inter-Species Pathway Perturbation Challenge (Sub-challenge 3 or SC3), we used phosphorylation level and gene expression data from normal rat bronchial epithelial (NRBE) and normal human bronchial epithelial (NHBE) cells to develop a predictive model of the activation of pathways/gene sets in humans from measurements in rats.", 
  "affiliations": [
    " Kohn Hall University of California ", 
    " Computational Biology Associate Editor: Igor Jurisica IBM T.J. Watson Research Center ", 
    " Johann Bernoulli Institute for Mathematics and Computer Science University of Groningen "
  ], 
  "grants": [
    "Funding: S.H., G.B.", 
    "and A.D. were supported in part by the National Science Foundation under Grant No.", 
    "NSF PHY11-25915."
  ], 
  "acks": " S.H. and A.D. thank the Kavli Institute of Theoretical Physics at UCSB, and in particular Boris Shraiman, for support. G.B. thanks the Kavli Institute of Theoretical Physics at UCSB for its support during the early stages of this project, the Juelich Supercomputing Centre at the Forschungszentrum, Juelich, for its support when this research was being completed, and Tata Institute of Fundamental Research, Mumbai, India, for support and hospitality during the writing of the manuscript. P.M. and K.R. helped to edit the manuscript. E.B., R.N., P.M. and K.R. helped to develop and organize the challenge. Funding: S.H., G.B. and A.D. were supported in part by the National Science Foundation under Grant No. NSF PHY11-25915. of interest: The data and organization of challenge was performed under a joint research collaboration between IBM Research and Philip Morris International R&D (PMI), and was funded by PMI. ", 
  "authors": [
    " Sahand Hormoz", 
    " Gyan Bhanot", 
    " Michael Biehl", 
    " Erhan Bilal", 
    " Pablo Meyer", 
    " Raquel Norel", 
    " Kahn Rhrissorrakrai", 
    " Adel Dayarian"
  ], 
  "keyWords": [
    [
      "data", 
      "biologically", 
      "genes", 
      "humans", 
      "predictions"
    ]
  ], 
  "sourcelinks": [
    "http://bhanot.biomaps.rutgers"
  ], 
  "technologies": [
    "Processing"
  ], 
  "dateCreated": "2014-08-25T00:08:47Z"
}{
  "doi": "10.1093/bioinformatics/btu570", 
  "name": "Interspecies pathway perturbation prediction via datadriven detection of functional homology", 
  "links": [
    "https://www.ebi", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinformaticsprb.med.wayne.edu?p=50"
  ], 
  "title": "Gene expression Inter-species pathway perturbation prediction via data-driven detection of functional homology", 
  "toolName": "Gene expression Inter-species pathway perturbation prediction via data-driven detection of functional homology", 
  "abstract": "Motivation: Experiments in animal models are often conducted to infer how humans will respond to stimuli by assuming that the same biological pathways will be affected in both organisms. The limitations of this assumption were tested in the IMPROVER Species Translation Challenge, where 52 stimuli were applied to both human and rat cells and perturbed pathways were identified. In the Inter-species Pathway Perturbation Prediction sub-challenge, multiple teams proposed methods to use rat transcription data from 26 stimuli to predict human gene set and pathway activity under the same perturbations. Submissions were evaluated using three performance metrics on data from the remaining 26 stimuli. Results: We present two approaches, ranked second in this challenge , that do not rely on sequence-based orthology between rat and human genes to translate pathway perturbation state but instead identify transcriptional response orthologs across a set of training conditions. The translation from rat to human accomplished by these so-called direct methods is not dependent on the particular analysis method used to identify perturbed gene sets. In contrast, machine learning-based methods require performing a pathway analysis initially and then mapping the pathway activity between organisms. Unlike most machine learning approaches, direct methods can be used to predict the activation of a human pathway for a new (test) stimuli, even when that pathway was never activated by a training stimuli. Availability: Gene expression data are available from ArrayExpress (accession E-MTAB-2091), while software implementations are available from", 
  "summary": "In the Inter-species Pathway Perturbation Prediction sub-challenge, multiple teams proposed methods to use rat transcription data from 26 stimuli to predict human gene set and pathway activity under the same perturbations.\nThe criteria used to discuss these methods include (i) prediction performance (overall and for the most challenging scenarios) using metrics estimated as in the official team ranking but also in alternative ways, (ii) applicability of the methods to instances when a given pathway was activated by few or no stimuli in the training set and (iii) the dependence of the rat to human pathway activity translation on the particular gene set analysis method used.", 
  "affiliations": [
    " IBM Thomas J. Watson Research Center", 
    " Department of Biology Center for Genomics & Systems Biology New York University ", 
    " Perinatology Research Branch Eunice Kennedy Shriver National Institute of Child Health and Human Development NIH Bethesda, MD and Detroit "
  ], 
  "grants": [
    "4 2015, pages 501508 doi:10.1093/bioinformatics/btu570\n\nGene expression\n\nAdvance Access publication August 22, 2014\n\nInter-species pathway perturbation prediction via data-driven\ndetection of functional homology\nChristoph Hafemeister1,*, Roberto Romero2, Erhan Bilal3, Pablo Meyer3, Raquel Norel3, Kahn Rhrissorrakrai3, Richard Bonneau1,4 and Adi L. Tarca2,5,*\n1Department of Biology, Center for Genomics & Systems Biology, New York University, New York, NY 10003, 2Perinatology Research Branch, Eunice Kennedy Shriver National Institute of Child Health and Human Development, NIH, Bethesda, MD and Detroit, MI 48201, USA, 3IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, 4Computer Science Department, Courant institute of Mathematical Sciences, New York University, New York, NY 10012 and 5Department of Computer Science, Wayne State University, Detroit, MI 48202, USA\nAssociate Editor: Igor Jurisica\n\nABSTRACT Motivation: Experiments in animal models are often conducted to infer how humans will respond to stimuli by assuming that the same biological pathways will be affected in both organisms.", 
    "A.T. was also supported by the DSC best overall performer grant from Philip Morris International.", 
    "were supported, in part, by the Perinatology Research Branch, Division of Intramural Research, Eunice Kennedy Shriver National Institute of Child Health and Human Development, National Institutes of Health, Department of Health and Human Services (NICHD/ NIH) and, in part, with Federal funds from NICHD, NIH under Contract No.", 
    "were supported by US National Science Foundation grants 0922738, 0929338, 1158273 and IOS- 1126971 and National Institutes of Health GM 32877-21/ 22, RC1-AI087266, RC4-AI092765, PN2-EY016586, IU54CA143907-01 and EY016586-06.", 
    "Funding: A.T. and R.R."
  ], 
  "acks": " The development of the methods and preparation of submissions described in this article were performed by A.T., C.H., R.B. and R.R. independently of P.M., K.R., E.B. and R.N. who contributed to the organization of the challenge. P.M. and K.R. were involved in the manuscript writing and post-challenge data analyses. ", 
  "authors": [
    " Christoph Hafemeister", 
    " Roberto Romero", 
    " Erhan Bilal", 
    " Pablo Meyer", 
    " Raquel Norel", 
    " Kahn Rhrissorrakrai", 
    " Richard Bonneau", 
    " Adi L Tarca"
  ], 
  "keyWords": [
    [
      "methods", 
      "rankings", 
      "genes", 
      "humans", 
      "teams", 
      "pathways", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformaticsprb.med.wayne.edu?p=50"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-23T04:14:10Z"
}{
  "doi": "10.1093/bioinformatics/btu688", 
  "name": "IntSide a web server for the chemical and biological examination of drug side effects", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://intside.irbbarcelona.org"
  ], 
  "title": "Databases and ontologies IntSide: a web server for the chemical and biological examination of drug side effects", 
  "toolName": "Databases and ontologies IntSide: a web server for the chemical and biological examination of drug side effects", 
  "abstract": "Drug side effects are one of the main health threats worldwide , and an important obstacle in drug development. Understanding how adverse reactions occur requires knowledge on drug mechanisms at the molecular level. Despite recent advances, the need for tools and methods that facilitate side effect anticipation still remains. Here, we present IntSide, a web server that integrates chemical and biological information to elucidate the molecular mechanisms underlying drug side effects. IntSide currently catalogs 1175 side effects caused by 996 drugs, associated with drug features divided into eight categories, belonging to either biology or chemistry. On the biological side, IntSide reports drug targets and off-targets, pathways, molecular functions and biological processes. From a chemical viewpoint , it includes molecular fingerprints, scaffolds and chemical entities. Finally, we also integrate additional biological data, such as protein interactions and disease-related genes, to facilitate mechanis-tic interpretations. Availability and implementation: Our data and web resource are available online (http://intside.irbbarcelona.org/).", 
  "summary": "Here, we present IntSide, a web server that integrates chemical and biological information to elucidate the molecular mechanisms underlying drug side effects.\nIntSide currently catalogs 1175 side effects caused by 996 drugs, associated with drug features divided into eight categories, belonging to either biology or chemistry.\nIntSide provides information to unravel the molecular mechanisms underlying drug SEs. Compared with other resources, our web server offers faster and more straightforward access to biological and chemical features that are likely related to SEs of interest.", 
  "affiliations": [
    " Institute for Research in Biomedicine (IRB Barcelona) Joint IRB-BSC-CRG Program in Computational Biology "
  ], 
  "grants": [
    "Funding: This work was partially supported by the Spanish Ministerio de Ciencia e Innovacio n [BIO2010-22073] and the European commission through the SyStemAge project [Agreement no: 306240]."
  ], 
  "acks": " We would like to thank S. Jaeger (IRB Barcelona) for critically reading the manuscript, and R. Mosca and R. Olivella (IRB Barcelona) for assistance in the web server implementation. ", 
  "authors": [
    " Teresa Juan-Blanco", 
    " Miquel Duran-Frigola", 
    " Patrick Aloy"
  ], 
  "keyWords": [
    [
      "features", 
      "drugs", 
      "intside", 
      "chemical", 
      "effects", 
      "biological", 
      "molecular"
    ]
  ], 
  "sourcelinks": [
    "http://intside.irbbarcelona.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btu690", 
  "name": "Introducing dAUTObase a first step towards the global scale geoepidemiology of autoimmune syndromes and diseases", 
  "links": [
    "http://www.odata.org", 
    "http://www.cs.umd.edu/hcil/treemap/).Finally", 
    "http://www.autoimmune.org.au/arrc-research-participant-database/.aspx", 
    "http://www.bio", 
    "http://jqvmap.com", 
    "http://www.getpivot.com/silverlight/pivotviewer", 
    "http://www.biodata.gr", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.biodata.gr/dautobase"
  ], 
  "title": "Databases and ontologies Introducing dAUTObase: a first step towards the global scale geoepidemiology of autoimmune syndromes and diseases", 
  "toolName": "Databases and ontologies Introducing dAUTObase: a first step towards the global scale geoepidemiology of autoimmune syndromes and diseases", 
  "abstract": "Motivation: An autoimmune disorder occurs when the immune system mistakenly attacks and destroys its own healthy body tissues. The initiation of a geoepidemiological database, for recording autoim-mune incidents with a focus to clinical manifestations, demographic parameters and geographic background is crucial to detect correlations. Results: The dAUTObase collects an ever increasing number of pub-lications\u2014currently counting 435\u2014on autoimmune diseases' frequencies in various populations and ethnic groups. The respective data have been hosted by a web application developed for the task. It uses three data visualization tools: the PivotViewer, the Disease Treemap and the Disease World Map, to assist the effective data querying. Availability and implementation: The dAUTObase 2.0 version (www. biodata.gr/dautobase) needs no registration for querying, but data entry and modification is reserved for registered users (curators-administrators).", 
  "summary": "These tools allow users to compare the epidemiological indices of the documented in dAUTObase autoimmune diseases among different populations and thus identify hidden relationships between individual pieces of information (Chandran and Raychaudhuri, 2010; Youinou et al., 2010).\nThe dAUTObase provides a web-based multimedia visualization environment for population-based autoimmune diseases epidemiological data collection and retrieval utilizing state-of-the-art visualization technologies implemented in three alternative data querying and visualization interfaces.", 
  "affiliations": [
    " Department of Computer and Informatics Engineering Technological Educational Institute of Western Greece ", 
    " Department of Pharmacy University of Patras ", 
    " Department of Computer Engineering and Informatics University of Patras "
  ], 
  "grants": [
    "Research Funding Program: Thales.", 
    "Funding: This work was supported by the MED OPERATIONAL PROJECT under the grant named `OTREMED' and by the European Union (European Social Fund--ESF) and Greek national funds through the Operational Program `Education and Lifelong Learning' of the National Strategic Reference Framework (NSRF).", 
    "The epidemiological indices for the same disease vary between patients from different geographical regions--up to 10, (Youinou et al., 2010)--and in many cases variations are observed within the same country, as happens with psoriasis in Australia were the native (Aborigines) population is much less affected than recent immigrants from Europe (Chandran and Raychaudhuri, 2010).", 
    "And this is subject to funding and policy issues, especially among the developing countries."
  ], 
  "acks": " ", 
  "authors": [
    " Vassiliki A Gkantouna", 
    " Manousos E Kambouris", 
    " Emmanouil S Viennas", 
    " Zafeiria-Marina Ioannou", 
    " Michael Paraskevas", 
    " George Lagoumintzis", 
    " Zoi Zagoriti", 
    " George P Patrinos", 
    " Giannis E Tzimas", 
    " Konstantinos Poulas", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "autoimmunity", 
      "epidemiological", 
      "users", 
      "populations", 
      "dautobase", 
      "visualizations", 
      "data", 
      "diseases"
    ]
  ], 
  "sourcelinks": [
    "http://www.getpivot.com/silverlight/pivotviewer"
  ], 
  "technologies": [
    " SQL "
  ], 
  "dateCreated": "2014-10-21T03:20:28Z"
}{
  "doi": "10.1093/bioinformatics/btu842", 
  "name": "JCircos an interactive Circos plotter", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.australianprostatecentre.org/research/software/jcircos"
  ], 
  "title": "J-Circos: an interactive Circos plotter", 
  "toolName": "J-Circos: an interactive Circos plotter", 
  "abstract": "Circos plots are graphical outputs that display three dimensional chromosomal interactions and fusion transcripts. However, the Circos plot tool is not an interactive visualization tool, but rather a figure generator. For example, it does not enable data to be added dynamically nor does it provide information for specific data points interactively. Recently, an R-based Circos tool (RCircos) has been developed to integrate Circos to R, but similarly, Rcircos can only be used to generate plots. Thus, we have developed a Circos plot tool (J-Circos) that is an interactive visualiza-tion tool that can plot Circos figures, as well as being able to dynamically add data to the figure, and providing information for specific data points using mouse hover display and zoom in/out functions. J-Circos uses the Java computer language to enable, it to be used on most operating systems (Windows, MacOS, Linux). Users can input data into J-Circos using flat data formats, as well as from the Graphical user interface (GUI). J-Circos will enable biologists to better study more complex chromosomal interactions and fusion transcripts that are otherwise difficult to visualize from next-generation sequencing data. Availability and implementation: J-circos and its manual are freely available at http://www.austral-ianprostatecentre.org/research/software/jcircos", 
  "summary": "Thus, we have developed a Circos plot tool (J-Circos) that is an interactive visualization tool that can plot Circos figures, as well as being able to dynamically add data to the figure, and providing information for specific data points using mouse hover display and zoom in/out functions.\nJ-Circos uses widely accepted UCSC genome browser data formats, such as bigWig, bigBed, bedgraph and Bed format.\n(2) Circos-bridge plots: As mentioned, bridge tracks are used to display connections of two chromosomal loci (e.g. from fusion genes or chromosome conformation capture data) in J-Circos.\nJ-Circos uses the data format from the UCSC genome browser (Meyer et al., 2013).", 
  "affiliations": [
    " School of Information Technology Deakin University ", 
    " Institute of Health and Biomedical Innovation Australian Prostate Cancer Research Centre-Queensland Queensland University of Technology Princess Alexandra Hospital and Translational Research Institute "
  ], 
  "grants": [], 
  "acks": " This work is supported by the Australian Government Department of Health and a Queensland Government Smart Futures Premier's Fellowship. Conflict of Interest: none declared. ", 
  "authors": [
    " Jiyuan An", 
    " John Lai", 
    " Atul Sajjanhar", 
    " Jyotsna Batra", 
    " Chenwei Wang", 
    " Colleen C Nelson"
  ], 
  "keyWords": [
    [
      "files", 
      "genomics", 
      "chromosomes", 
      "circos", 
      "bioinformatics", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://www.australianprostatecentre.org/research/software/jcircos"
  ], 
  "technologies": [
    "R", 
    "Java", 
    "Perl"
  ], 
  "dateCreated": "2014-12-25T04:16:00Z"
}{
  "doi": "10.1093/bioinformatics/btu407", 
  "name": "Interspecies prediction of protein phosphorylation in the sbv IMPROVER species translation challenge", 
  "links": [
    "http://bhanot.biomaps.rutgers.edu/wiki", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://github.com", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Inter-species prediction of protein phosphorylation in the sbv IMPROVER species translation challenge", 
  "toolName": "github.com", 
  "abstract": "Motivation: Animal models are widely used in biomedical research for reasons ranging from practical to ethical. An important issue is whether rodent models are predictive of human biology. This has been addressed recently in the framework of a series of challenges designed by the systems biology verification for Industrial Methodology for Process Verification in Research (sbv IMPROVER) initiative. In particular, one of the sub-challenges was devoted to the prediction of protein phosphorylation responses in human bronchial epithelial cells, exposed to a number of different chemical stimuli, given the responses in rat bronchial epithelial cells. Participating teams were asked to make inter-species predictions on the basis of available training examples, comprising transcriptomics and phospho-proteomics data. Results: Here, the two best performing teams present their data-driven approaches and computational methods. In addition, post hoc analyses of the datasets and challenge results were performed by the participants and challenge organizers. The challenge outcome indicates that successful prediction of protein phosphorylation status in human based on rat phosphorylation levels is feasible. However, within the limitations of the computational tools used, the inclusion of gene expression data does not improve the prediction quality. The post hoc analysis of time-specific measurements sheds light on the signaling pathways in both species. Availability and implementation: A detailed description of the data-set, challenge design and outcome is available at www.sbvimprover. com. The code used by team IGB is provided under http://github.com/ uci-igb/improver2013. Implementations of the algorithms applied by team AMG are available at", 
  "summary": "Predictions of the protein phosphorylation status in human cells were to be made during the challenge for dataset (B), which corresponded to a different set of stimuli and comprised only the gene expression and phosphorylation data for rat.\nTest set performance of naive prediction schemes c5 and c25 obtained from the ratP measurements at 5 and 25 min separately, compared with the corresponding time-specific (top) and total (bottom) binarized human protein activation", 
  "affiliations": [
    " Kavli Institute for Theoretical Physics Associate Editor: Igor Jurisica University of California ", 
    " University of California", 
    " Computational Biology IBM T.J. Watson Research Center ", 
    " Johann Bernoulli Institute for Mathematics and Computer Science University of Groningen ", 
    " Department of Physics Department of Molecular Biology and Biochemistry Busch Campus Rutgers University "
  ], 
  "grants": [
    "Funding: S.H., G.B.", 
    "NSF PHY1125915.", 
    "were supported by grants from the National Institutes of Health under NIH LM010235, NIH NLM T15 LM07443 and NSF IIS-0513376.", 
    "and A.D. were supported in part by the National Science Foundation under Grant No."
  ], 
  "sourcelinks": [
    "http://bhanot.biomaps.rutgers.edu/wiki", 
    "http://github.com"
  ], 
  "acks": " ", 
  "authors": [
    " Michael Biehl", 
    " Peter Sadowski", 
    " Gyan Bhanot", 
    " Erhan Bilal", 
    " Adel Dayarian", 
    " Pablo Meyer", 
    " Raquel Norel", 
    " Kahn Rhrissorrakrai", 
    " Michael D Zeller", 
    " Sahand Hormoz"
  ], 
  "keyWords": [
    [
      "measurements", 
      "humans", 
      "predictions", 
      "phosphorylation", 
      "activations", 
      "data"
    ]
  ], 
  "github_data": {
    "name": "node-v0.x-archive", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/orangemocha"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/works", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/works", 
        "name": "works"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.7", 
        "name": "v0.12.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.6", 
        "name": "v0.12.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.5", 
        "name": "v0.12.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.4", 
        "name": "v0.12.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.3", 
        "name": "v0.12.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.2", 
        "name": "v0.12.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.1", 
        "name": "v0.12.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.0", 
        "name": "v0.12.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.16", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.16", 
        "name": "v0.11.16"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.15", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.15", 
        "name": "v0.11.15"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.14", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.14", 
        "name": "v0.11.14"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.13", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.13", 
        "name": "v0.11.13"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.12", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.12", 
        "name": "v0.11.12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.11", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.11", 
        "name": "v0.11.11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.10", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.10", 
        "name": "v0.11.10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.9", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.9", 
        "name": "v0.11.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.8", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.8", 
        "name": "v0.11.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.7", 
        "name": "v0.11.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.6", 
        "name": "v0.11.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.5", 
        "name": "v0.11.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.4", 
        "name": "v0.11.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.3", 
        "name": "v0.11.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.40", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.40", 
        "name": "v0.10.40"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.39", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.39", 
        "name": "v0.10.39"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.38", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.38", 
        "name": "v0.10.38"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.37", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.37", 
        "name": "v0.10.37"
      }
    ], 
    "created_at": "2009-05-27T16:29:46Z", 
    "updated_at": "2016-08-09T16:10:45Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/isaacs"
      }, 
      {
        "html_url": "https://github.com/koichik"
      }, 
      {
        "html_url": "https://github.com/igorzi"
      }, 
      {
        "html_url": "https://github.com/kunoichi-co"
      }, 
      {
        "html_url": "https://github.com/carloslemes"
      }, 
      {
        "html_url": "https://github.com/aboyon"
      }, 
      {
        "html_url": "https://github.com/arden"
      }, 
      {
        "html_url": "https://github.com/guyoun"
      }, 
      {
        "html_url": "https://github.com/vissul"
      }, 
      {
        "html_url": "https://github.com/bdtgzj"
      }, 
      {
        "html_url": "https://github.com/santigimeno"
      }, 
      {
        "html_url": "https://github.com/chrisbateskeegan"
      }, 
      {
        "html_url": "https://github.com/FlashSoft"
      }, 
      {
        "html_url": "https://github.com/charlesgan"
      }, 
      {
        "html_url": "https://github.com/BlaneCordes"
      }, 
      {
        "html_url": "https://github.com/mdgoody"
      }, 
      {
        "html_url": "https://github.com/gindis"
      }, 
      {
        "html_url": "https://github.com/chrisdickinson"
      }, 
      {
        "html_url": "https://github.com/jorgenio"
      }, 
      {
        "html_url": "https://github.com/joityui"
      }, 
      {
        "html_url": "https://github.com/wxnet2013"
      }, 
      {
        "html_url": "https://github.com/be5invis"
      }, 
      {
        "html_url": "https://github.com/ashleymcfarland"
      }, 
      {
        "html_url": "https://github.com/jimxl"
      }, 
      {
        "html_url": "https://github.com/bluefisher80"
      }, 
      {
        "html_url": "https://github.com/subhaze"
      }, 
      {
        "html_url": "https://github.com/yaoming6046"
      }, 
      {
        "html_url": "https://github.com/hirozhang"
      }, 
      {
        "html_url": "https://github.com/rhyw"
      }, 
      {
        "html_url": "https://github.com/rhaldkhein"
      }
    ], 
    "owner": "https://github.com/nodejs", 
    "homepage": ""
  }, 
  "technologies": [
    "C", 
    "R"
  ], 
  "dateCreated": "2014-07-04T02:04:38Z"
}{
  "doi": "10.1093/bioinformatics/btu729", 
  "name": "JAMSS proteomics mass spectrometry simulation in Java", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/optimus"
  ], 
  "title": "Data and text mining JAMSS: proteomics mass spectrometry simulation in Java", 
  "toolName": "optimus", 
  "abstract": "Countless proteomics data processing algorithms have been proposed, yet few have been critically evaluated due to lack of labeled data (data with known identities and quantities). Although labeling techniques exist, they are limited in terms of confidence and accuracy. In silico simulators have recently been used to create complex data with known identities and quantities. We propose Java Mass Spectrometry Simulator (JAMSS): a fast, self-contained in silico simulator capable of generating simulated MS and LC-MS runs while providing meta information on the provenance of each generated signal. JAMSS improves upon previous in silico simulators in terms of its ease to install, minimal parameters, graphical user interface, multithreading capability, retention time shift model and reproducibility.", 
  "summary": "Most recently, Mspire-simulator, a standalone simulator in the Ruby programming language, provided automatic charge modeling, realistic hourglass-shaped isotope traces (increased variance at lower intensities), direct control over post-translational modifications (PTMs), and the ability to extract simulation parameters from existing mzML files using machine learning (Noyce et al., 2013).\nIt can be used to generate LC-MS and MS data, allowing for the evaluation of a wide range of data processing algorithms such as isotope trace extraction (both in chromatographic and non-chromatographic applications-- Smith et al., 2012), isotopic envelope extraction, molecular envelope extraction and reduction, and correspondence (Smith et al., 2013b).", 
  "affiliations": [
    " Department of Computer Science University of Montana ", 
    " Department of Chemistry Brigham Young University "
  ], 
  "grants": [], 
  "sourcelinks": [
    "https://github.com/optimus"
  ], 
  "acks": " ", 
  "authors": [
    " Rob Smith", 
    " John T Prince"
  ], 
  "keyWords": [
    "jamss proteomics", 
    [
      "simulation", 
      "proteomic", 
      "processing", 
      "data", 
      "bioinformatics"
    ]
  ], 
  "github_data": {
    "name": "optimus", 
    "contributors": [
      {
        "contributions": 194, 
        "html_url": "https://github.com/magnars"
      }, 
      {
        "contributions": 11, 
        "html_url": "https://github.com/cjohansen"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/env0der"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/shaharz"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/arbscht"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/arohner"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/DomKM"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/favila"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/krisajenkins"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/xsc"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.19.0", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.19.0", 
        "name": "0.19.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.18.5", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.18.5", 
        "name": "0.18.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.18.4", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.18.4", 
        "name": "0.18.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.18.3", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.18.3", 
        "name": "0.18.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.18.2", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.18.2", 
        "name": "0.18.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.18.1", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.18.1", 
        "name": "0.18.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.18.0", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.18.0", 
        "name": "0.18.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.17.2", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.17.2", 
        "name": "0.17.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.17.1", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.17.1", 
        "name": "0.17.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.17.0", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.17.0", 
        "name": "0.17.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.16.0", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.16.0", 
        "name": "0.16.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.15.1", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.15.1", 
        "name": "0.15.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.15.0", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.15.0", 
        "name": "0.15.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.14.4", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.14.4", 
        "name": "0.14.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.14.3", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.14.3", 
        "name": "0.14.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.14.2", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.14.2", 
        "name": "0.14.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.14.1", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.14.1", 
        "name": "0.14.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.14.0", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.14.0", 
        "name": "0.14.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.13.6", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.13.6", 
        "name": "0.13.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.13.5", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.13.5", 
        "name": "0.13.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.13.4", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.13.4", 
        "name": "0.13.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.13.3", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.13.3", 
        "name": "0.13.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.13.2", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.13.2", 
        "name": "0.13.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.13.1", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.13.1", 
        "name": "0.13.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.13.0", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.13.0", 
        "name": "0.13.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.12.7", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.12.7", 
        "name": "0.12.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.12.6", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.12.6", 
        "name": "0.12.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.12.5", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.12.5", 
        "name": "0.12.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.12.4", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.12.4", 
        "name": "0.12.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/magnars/optimus/zipball/0.12.3", 
        "tarball_url": "https://api.github.com/repos/magnars/optimus/tarball/0.12.3", 
        "name": "0.12.3"
      }
    ], 
    "created_at": "2013-11-07T19:54:57Z", 
    "updated_at": "2016-08-09T16:35:45Z", 
    "languages": [
      "Shell", 
      "Clojure", 
      "JavaScript"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/magnars"
      }, 
      {
        "html_url": "https://github.com/cjohansen"
      }, 
      {
        "html_url": "https://github.com/Nagarc"
      }, 
      {
        "html_url": "https://github.com/Asterozemljopad"
      }, 
      {
        "html_url": "https://github.com/sebastiansen"
      }, 
      {
        "html_url": "https://github.com/luger1990"
      }, 
      {
        "html_url": "https://github.com/darth10"
      }, 
      {
        "html_url": "https://github.com/rakhmad"
      }, 
      {
        "html_url": "https://github.com/arbscht"
      }, 
      {
        "html_url": "https://github.com/BorisKourt"
      }, 
      {
        "html_url": "https://github.com/zane"
      }, 
      {
        "html_url": "https://github.com/ratson"
      }, 
      {
        "html_url": "https://github.com/DomKM"
      }, 
      {
        "html_url": "https://github.com/billhibazzz"
      }, 
      {
        "html_url": "https://github.com/iwillig"
      }, 
      {
        "html_url": "https://github.com/magoyette"
      }, 
      {
        "html_url": "https://github.com/arlicle"
      }
    ], 
    "owner": "https://github.com/magnars", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-11-05T05:28:21Z"
}{
  "doi": "10.1093/bioinformatics/btu568", 
  "name": "JAMM a peak finder for joint analysis of NGS replicates", 
  "links": [
    "http://code.google", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://sites.google.com/site"
  ], 
  "title": "JAMM: a peak finder for joint analysis of NGS replicates", 
  "toolName": "JAMM: a peak finder for joint analysis of NGS replicates", 
  "abstract": "Motivation: Although peak finding in next-generation sequencing (NGS) datasets has been addressed extensively, there is no consensus on how to analyze and process biological replicates. Furthermore, most peak finders do not focus on accurate determination of enrichment site widths and are not widely applicable to different types of datasets. Results: We developed JAMM (Joint Analysis of NGS replicates via Mixture Model clustering): a peak finder that can integrate information from biological replicates, determine enrichment site widths accurately and resolve neighboring narrow peaks. JAMM is a universal peak finder that is applicable to different types of datasets. We show that JAMM is among the best performing peak finders in terms of site detection accuracy and in terms of accurate determination of enrichment sites widths. In addition, JAMM's replicate integration improves peak spatial resolution, sorting and peak finding accuracy. Availability and implementation: JAMM is available for free and can run on Linux machines through the command line", 
  "summary": "Because there is no gold standard for benchmarking peak finders (Szalkowski and Schmid, 2011), we analyzed five different ENCODE transcription factor ChIP-Seq datasets, including CTCF-HeLa, CTCF-K562, NRSF-K562, MAX-K562 and SRF-GM12878 (see Supplementary Tables S1 and S5--we refer to those datasets as `accuracy-benchmark' datasets) using three different benchmarking methods: (i) motif finding precision (fraction of called peaks with motif matches) using FIMO (Grant et al., 2011), which uses a uniform zero-order background model, (ii) maximum cumulative motif likelihood using SpeakerScan (Megraw et al., 2009), which uses a first-order local background model and (iii) accuracy of recovery of manually curated positive peaks as reported by Rye et al.", 
  "affiliations": [
    " The Berlin Institute for Medical Systems Biology Max Delbr \u20ac uck Center for Molecular Medicine Berlin-Buch Associate Editor: Inanc Birol "
  ], 
  "grants": [
    "Motif precision analysis was done using FIMO (P-value, `accuracy-benchmark': 0.0001/ P-value,'bardet-benchmark': 0.001) (Grant et al., 2011), cumulative log likelihood analysis was done using SpeakerScan (background window: 150 bp) (Megraw et al., 2009) and the curated peak set provided by Rye et al.", 
    "Because there is no gold standard for benchmarking peak finders (Szalkowski and Schmid, 2011), we analyzed five different ENCODE transcription factor ChIP-Seq datasets, including CTCF-HeLa, CTCF-K562, NRSF-K562, MAX-K562 and SRF-GM12878 (see Supplementary Tables S1 and S5--we refer to those datasets as `accuracy-benchmark' datasets) using three different benchmarking methods: (i) motif finding precision (fraction of called peaks with motif matches) using FIMO (Grant et al., 2011), which uses a uniform zero-order background model, (ii) maximum cumulative motif likelihood using SpeakerScan (Megraw et al., 2009), which uses a first-order local background model and (iii) accuracy of recovery of manually curated positive peaks as reported by Rye et al.", 
    "Grant,C.E.", 
    "Funding: MMI was supported by the Max-Delbru ck-Center/New York University Exchange Program."
  ], 
  "acks": " ", 
  "authors": [
    " Mahmoud M Ibrahim", 
    " Scott A Lacadie", 
    " Uwe Ohler"
  ], 
  "keyWords": [
    [
      "genomics", 
      "peaks", 
      "replicates", 
      "bioinformatics", 
      "jamm", 
      "enrichment"
    ]
  ], 
  "sourcelinks": [
    "http://code.google"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-16T00:23:19Z"
}{
  "doi": "10.1093/bioinformatics/btu816", 
  "name": "JEPEG a summary statistics based tool for genelevel joint testing of functional variants", 
  "links": [
    "http://www.illumina.com", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://code.google.com/p/jepeg", 
    "http://gump.qimr.edu.au", 
    "http://gump.qimr.edu.au/VEGAS", 
    "http://www.uk10k.org", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis JEPEG: a summary statistics based tool for gene-level joint testing of functional variants", 
  "toolName": "Genome analysis JEPEG: a summary statistics based tool for gene-level joint testing of functional variants", 
  "abstract": "Motivation: Gene expression is influenced by variants commonly known as expression quantitative trait loci (eQTL). On the basis of this fact, researchers proposed to use eQTL/functional information univariately for prioritizing single nucleotide polymorphisms (SNPs) signals from genome-wide association studies (GWAS). However, most genes are influenced by multiple eQTLs which, thus, jointly affect any downstream phenotype. Therefore, when compared with the univariate prioritization approach , a joint modeling of eQTL action on phenotypes has the potential to substantially increase signal detection power. Nonetheless, a joint eQTL analysis is impeded by (i) not measuring all eQTLs in a gene and/or (ii) lack of access to individual genotypes. Results: We propose joint effect on phenotype of eQTL/functional SNPs associated with a gene (JEPEG), a novel software tool which uses only GWAS summary statistics to (i) impute the summary statistics at unmeasured eQTLs and (ii) test for the joint effect of all measured and imputed eQTLs in a gene. We illustrate the behavior/performance of the developed tool by analysing the GWAS meta-analysis summary statistics from the Psychiatric Genomics Consortium Stage 1 and the Genetic Consortium for Anorexia Nervosa. Conclusions: Applied analyses results suggest that JEPEG complements commonly used univari-ate GWAS tools by: (i) increasing signal detection power via uncovering (a) novel genes or (b) known associated genes in smaller cohorts and (ii) assisting in fine-mapping of challenging regions, e.g. major histocompatibility complex for schizophrenia. Availability and implementation: JEPEG, its associated database of eQTL SNPs and usage examples are publicly available at", 
  "summary": "The associated software consists of four major components (Fig. 1): (i) an extensive database of eQTL/ functional SNPs (Section 2.1), (ii) a module for directly imputing summary statistics of unmeasured eQTL/functional SNPs [i.e. Direct Imputation of summary STatistics (DIST) (Lee et al., 2013)] (Section 2.2), (iii) a module for testing the joint effect of all reliably measured/ imputed functional SNPs associated with a gene (i.e. JEPEG) (Section 2.3) and (iv) reference population panels available/needed for both imputation and joint testing (Section 2.4).", 
  "affiliations": [
    " Department of Psychiatry Virginia Institute for Psychiatric and Behavioral Genetics "
  ], 
  "grants": [
    "Funding\nThis work was supported by R25DA026119 (D.L."
  ], 
  "acks": " ", 
  "authors": [
    " Donghyung Lee", 
    " Vernell S Williamson", 
    " T Bernard Bigdeli", 
    " Brien P Riley", 
    " Ayman H Fanous", 
    " Vladimir I Vladimirov", 
    " Silviu-Alin Bacanu"
  ], 
  "keyWords": [
    [
      "associations", 
      "snps", 
      "jepeg", 
      "genes", 
      "functionally"
    ]
  ], 
  "sourcelinks": [
    "http://www.illumina.com", 
    "http://gump.qimr.edu.au", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://code.google.com/p/jepeg", 
    "http://gump.qimr.edu.au/VEGAS"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-12-14T01:33:54Z"
}{
  "doi": "10.1093/bioinformatics/btu808", 
  "name": "Kablammo an interactive webbased BLAST results visualizer", 
  "links": [
    "http://kablammo.wasmuthlab.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://blast.ncbi.nlm.nih.gov"
  ], 
  "title": "Kablammo: an interactive, web-based BLAST results visualizer", 
  "toolName": "Kablammo: an interactive, web-based BLAST results visualizer", 
  "abstract": "Motivation: Kablammo is a web-based application that produces interactive, vector-based visual-izations of sequence alignments generated by BLAST. These visualizations can illustrate many features , including shared protein domains, chromosome structural modifications and genome misassembly. Availability and implementation: Kablammo can be used at http://kablammo.wasmuthlab.org. For a local installation, the source code and instructions are available under the MIT license at http://", 
  "summary": "Kablammo: an interactive, web-based BLAST results visualizer\nMotivation: Kablammo is a web-based application that produces interactive, vector-based visualizations of sequence alignments generated by BLAST.\nThe Basic Local Alignment Search Tool (BLAST) rapidly finds similar subsequences shared between a query and database sequences (Altschul et al., 1997).\nMore expansive web-based bioinformatics tools may thus leverage Kablammo to interactively display sequence alignments.\nFigure 1 demonstrates one use of Kablammo, in which we have investigated gene structure in a genome assembly for the parasitic nematode Haemonchus contortus (Schwarz et al., 2013).", 
  "affiliations": [
    " Department of Ecosystem and Public Health Faculty of Veterinary Medicine University of Calgary "
  ], 
  "grants": [
    "Funding\nThis work was supported by an Alberta Innovates Health Solutions (AIHS) studentship to J.A.W."
  ], 
  "acks": " The authors thank David Curran and Andrew Rezansoff for testing Kablammo, and for their comments on the documentation and this manuscript. This work was supported by an Alberta Innovates Health Solutions (AIHS) studentship to J.A.W. and the Natural Sciences and Engineering Research Council of Canada (NSERC) Collaborative Research and Training Experience (CREATE) program in Host-Parasite Interactions. Conflict of Interest: none declared. ", 
  "authors": [
    " Jeff A Wintersinger", 
    " James D Wasmuth"
  ], 
  "keyWords": [
    [
      "interactively", 
      "genes", 
      "kablammo", 
      "bioinformatics", 
      "sequencing", 
      "visualizations", 
      "blast", 
      "alignments"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-12-07T01:08:27Z"
}{
  "doi": "10.1093/bioinformatics/btu728", 
  "name": "Joint amalgamation of most parsimonious reconciled gene trees", 
  "links": [
    "http://mbb.univ-montp2.fr/MBB/download_sources/16__TERA", 
    "http://compbio.mit.edu/treefix-dtl", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Joint amalgamation of most parsimonious reconciled gene trees", 
  "toolName": "Joint amalgamation of most parsimonious reconciled gene trees", 
  "abstract": "Motivation: Traditionally, gene phylogenies have been reconstructed solely on the basis of molecular sequences; this, however, often does not provide enough information to distinguish between statistically equivalent relationships. To address this problem, several recent methods have incorporated information on the species phylogeny in gene tree reconstruction, leading to dramatic improvements in accuracy. Although probabilistic methods are able to estimate all model parameters but are computationally expensive, parsimony methods\u2014generally computationally more ef-ficient\u2014require a prior estimate of parameters and of the statistical support. Results: Here, we present the Tree Estimation using Reconciliation (TERA) algorithm, a parsimony based, species tree aware method for gene tree reconstruction based on a scoring scheme combining duplication, transfer and loss costs with an estimate of the sequence likelihood. TERA explores all reconciled gene trees that can be amalgamated from a sample of gene trees. Using a large scale simulated dataset, we demonstrate that TERA achieves the same accuracy as the corresponding probabilistic method while being faster, and outperforms other parsimony-based methods in both accuracy and speed. Running TERA on a set of 1099 homologous gene families from complete cyanobacterial genomes, we find that incorporating knowledge of the species tree results in a two thirds reduction in the number of apparent transfer events. Availability and implementation: The algorithm is implemented in our program TERA, which is freely available from", 
  "summary": "To compare the accuracy of our method to that of others, we reconstructed gene trees using six different `species tree aware' methods: (i) the TERA algorithm described here, (ii) ALE (Szo llosi et al., 2013b), (iii) TreeFix-DTL (Bansal et al., 2014, submitted for publication, http://compbio.mit.edu/treefix-dtl/), (iv) MowgliNNI (Nguyen et al., 2013), (v) AnGST (David and Alm, 2010) and (vi) JPrIME-DLTRS (Sjo strand et al., 2014) as well as the species tree unaware method, PhyML (Guindon et al., 2010).", 
  "affiliations": [
    " ELTE-MTA 'Lend\u00fc let' Biophysics Research Group", 
    " ISEM CNRS-IRD "
  ], 
  "grants": [
    "We expect that relaxing the, in\n\nFunding\nC.S."
  ], 
  "acks": " The authors like to thank Eric Tannier, Bastien Boussau and Vincent Daubin for the constructive discussions. ", 
  "authors": [
    " Celine Scornavacca", 
    " Edwin Jacox", 
    " Gergely J Sz\u00f6 Llo \u2020si"
  ], 
  "keyWords": [
    "gene trees", 
    [
      "methods", 
      "genes", 
      "tree", 
      "tera", 
      "modeling", 
      "species"
    ]
  ], 
  "sourcelinks": [
    "http://mbb.univ-montp2.fr/MBB/download_sources/16__TERA"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-08T01:28:35Z"
}{
  "doi": "10.1093/bioinformatics/btu679", 
  "name": "jNMFMA a joint nonnegative matrix factorization metaanalysis of transcriptomics data", 
  "links": [
    "http://cancergenome.nih.gov", 
    "http://micblab.iim.ac.cn/Download", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.ncbi.nlm.nih.gov/geo"
  ], 
  "title": "Systems biology jNMFMA: a joint non-negative matrix factorization meta-analysis of transcriptomics data", 
  "toolName": "Systems biology jNMFMA: a joint non-negative matrix factorization meta-analysis of transcriptomics data", 
  "abstract": "Motivation: Tremendous amount of omics data being accumulated poses a pressing challenge of meta-analyzing the heterogeneous data for mining new biological knowledge. Most existing methods deal with each gene independently, thus often resulting in high false positive rates in detecting differentially expressed genes (DEG). To our knowledge , no or little effort has been devoted to methods that consider dependence structures underlying transcriptomics data for DEG identification in meta-analysis context. Results: This article proposes a new meta-analysis method for identification of DEGs based on joint non-negative matrix factorization (jNMFMA). We mathematically extend non-negative matrix factoriza-tion (NMF) to a joint version (jNMF), which is used to simultaneously decompose multiple transcriptomics data matrices into one common submatrix plus multiple individual submatrices. By the jNMF, the dependence structures underlying transcriptomics data can be interrogated and utilized, while the high-dimensional transcriptomics data are mapped into a low-dimensional space spanned by metagenes that represent hidden biological signals. jNMFMA finally identifies DEGs as genes that are associated with differentially expressed metagenes. The ability of extracting dependence structures makes jNMFMA more efficient and robust to identify DEGs in meta-analysis context. Furthermore, jNMFMA is also flexible to identify DEGs that are consistent among various types of omics data, e.g. gene expression and DNA methylation. Experimental results on both simulation data and real-world cancer data demonstrate the effectiveness of jNMFMA and its superior performance over other popular approaches. Availability and implementation: R code for jNMFMA is available for non-commercial use via http://micblab.iim.ac.cn/Download/.", 
  "summary": "jNMFMA can also meta-analyze transcriptomics data and DNA methylation data with common gene entities for identifying DEGs with negatively correlated expression and methylation patterns (mDEG).\nIt is also observed that true DEGs prefer a large d but non-DE genes not in all the five data scenarios (Supplementary Fig. S2), indicating the ability of jNMFMA to detect DEGs in meta-analysis context.\nBased on the Simulation Data II, we further compared jNMFMA with three popular methods, AW (Li and Tseng, 2011), REM (Choi et al., 2003), RankProd (Hong et al., 2006), as well as the two straightforward methods, Indiv-Uinon and Indiv-Inters, using the following measures: AUC; False positive rate (FPR), FPR = FP/(FP + TN); False negative rate (FNR), FNR = FN/(FN + TP); Positive predictive value (PPV),", 
  "affiliations": [
    " College of Electrical Engineering and Automation Anhui University ", 
    " Hefei Institutes of Physical Science Machine Intelligence and Computational Biology Lab Chinese Academy of Science ", 
    " Department of Computer Science School of Electronics and Information Engineering Tongji University "
  ], 
  "grants": [
    "Funding: This work was supported by the National Natural Science Foundation of China (61374181, 61300058, 61272339, 91130032, 61103075, 61402010); the Anhui Province Natural Science Foundation (1408085MF133); Research Grants Council, Hong Kong SAR, China (grant number 781511M), and HKU genomics SRT, Innovation Program of Shanghai Municipal Education Commission (13ZZ072); Shanghai Pujiang Program (13PJD032); K. C. Wong education foundation."
  ], 
  "acks": " ", 
  "authors": [
    " Hong-Qiang Wang", 
    " Chun-Hou Zheng", 
    " Xing-Ming Zhao"
  ], 
  "keyWords": [
    [
      "data", 
      "genes", 
      "cancers", 
      "methods", 
      "jnmfma"
    ]
  ], 
  "sourcelinks": [
    "http://cancergenome.nih.gov", 
    "http://micblab.iim.ac.cn/Download"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btu632", 
  "name": "KDDN an opensource Cytoscape app for constructing differential dependency networks with significant rewiring", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://apps.cytoscape.org/apps/kddn"
  ], 
  "title": "Data and text mining KDDN: an open-source Cytoscape app for constructing differential dependency networks with significant rewiring", 
  "toolName": "Data and text mining KDDN: an open-source Cytoscape app for constructing differential dependency networks with significant rewiring", 
  "abstract": "We have developed an integrated molecular network learning method, within a well-grounded mathematical framework, to construct differential dependency networks with significant rewiring. This knowledge-fused differential dependency networks (KDDN) method, implemented as a Java Cytoscape app, can be used to optimally integrate prior biological knowledge with measured data to simultaneously construct both common and differential networks, to quantitatively assign model parameters and significant rewiring p-values and to provide user-friendly graphical results. The KDDN algorithm is computationally efficient and provides users with parallel computing capability using ubiquitous multi-core machines. We demonstrate the performance of KDDN on various simulations and real gene expression datasets, and further compare the results with those obtained by the most relevant peer methods. The acquired biologically plausible results provide new insights into network rewiring as a mechanistic principle and illustrate KDDN's ability to detect them efficiently and correctly. Although the principal application here involves microarray gene expressions, our methodology can be readily applied to other types of quantitative molecular profiling data. Availability: Source code and compiled package are freely available for download at", 
  "summary": "This knowledge-fused differential dependency networks (KDDN) method, implemented as a Java Cytoscape app, can be used to optimally integrate prior biological knowledge with measured data to simultaneously construct both common and differential networks, to quantitatively assign model parameters and significant rewiring p-values and to provide user-friendly graphical results.\nDifferential dependency network (Zhang et al., 2009; Zhang and Wang, 2010) and its knowledge-fused extension (Tian et al., 2011, 2014) KDDN have been developed to infer biological networks with significant rewiring by integrating experimental data and biological knowledge.", 
  "affiliations": [
    " Research Center for Genetic Medicine Children's National Medical Center ", 
    " Lombardi Comprehensive Cancer Center Georgetown University ", 
    " Department of Medicine Wake Forest University ", 
    " Departments of Pathology and Oncology Johns Hopkins University ", 
    " Department of Electrical & Computer Engineering Virginia Tech "
  ], 
  "grants": [
    "Funding: National Institutes of Health, under Grants [CA160036, CA164384, NS29525, CA149147, HL111362]."
  ], 
  "acks": " ", 
  "authors": [
    " Ye Tian", 
    " Bai Zhang", 
    " Eric P Hoffman", 
    " Robert Clarke", 
    " Zhen Zhang", 
    " Ie-Ming Shih", 
    " Jianhua Xuan", 
    " David M Herrington", 
    " Yue Wang"
  ], 
  "keyWords": [
    [
      "stresses", 
      "differentially", 
      "biologically", 
      "kddn", 
      "networks", 
      "significance", 
      "data", 
      "university"
    ]
  ], 
  "sourcelinks": [
    "http://apps.cytoscape.org/apps/kddn"
  ], 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu438", 
  "name": "Journaled string treea scalable data structure for analyzing thousands of similar genomes on your laptop", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.seqan.de/projects/jst"
  ], 
  "title": "Journaled string tree\u2014a scalable data structure for analyzing thousands of similar genomes on your laptop", 
  "toolName": "Journaled string tree\u2014a scalable data structure for analyzing thousands of similar genomes on your laptop", 
  "abstract": "Motivation: Next-generation sequencing (NGS) has revolutionized biomedical research in the past decade and led to a continuous stream of developments in bioinformatics, addressing the need for fast and space-efficient solutions for analyzing NGS data. Often researchers need to analyze a set of genomic sequences that stem from closely related species or are indeed individuals of the same species. Hence, the analyzed sequences are similar. For analyses where local changes in the examined sequence induce only local changes in the results, it is obviously desirable to examine identical or similar regions not repeatedly. Results: In this work, we provide a datatype that exploits data paral-lelism inherent in a set of similar sequences by analyzing shared regions only once. In real-world experiments, we show that algorithms that otherwise would scan each reference sequentially can be speeded up by a factor of 115. Availability: The data structure and associated tools are publicly available at http://www.seqan.de/projects/jst and are part of SeqAn, the C++ template library for sequence analysis.", 
  "summary": "We can show that our approach exhibits speedups of 4100 times when analyzing a set of 2185 sequences of chromosome 1 [two haplotypes of 1092 sequences from the 1000 genomes project (Altshuler et al., 2010) and a reference sequence] compared with running the algorithms sequentially while using only $3.8 GB of space.\nA JST T is a data structure consisting of an array of branch-nodes u 2 VT sorted in ascending order according to their branch-position pos(u), a pointer to the common reference sequence r and a set of journal strings 1; .", 
  "affiliations": [
    " Department of Mathematics and Computer Science Freie Universit \u20ac at Berlin "
  ], 
  "grants": [
    "Funding: This work was supported by the Deutsche Forschungsgemeinschaft [1712/4-1, Algorithmic engineering for high throughput sequencing to R.R.]"
  ], 
  "acks": " ", 
  "authors": [
    " Ren E Rahn", 
    " David Weese", 
    " Knut Reinert"
  ], 
  "keyWords": [
    "genomic sequences", 
    [
      "sequencing", 
      "genomics", 
      "algorithmic", 
      "searching", 
      "strings"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-16T00:23:13Z"
}{
  "doi": "10.1093/bioinformatics/btu510", 
  "name": "Kotai Antibody Builder automated highresolution structural modeling of antibodies", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://kotaiab.org"
  ], 
  "title": "Structural bioinformatics Kotai Antibody Builder: automated high-resolution structural modeling of antibodies", 
  "toolName": "Structural bioinformatics Kotai Antibody Builder: automated high-resolution structural modeling of antibodies", 
  "abstract": "Motivation: Kotai Antibody Builder is a Web service for tertiary structural modeling of antibody variable regions. It consists of three main steps: hybrid template selection by sequence alignment and canonical rules, 3D rendering of alignments and CDR-H3 loop modeling. For the last step, in addition to rule-based heuristics used to build the initial model, a refinement option is available that uses fragment assembly followed by knowledge-based scoring. Using targets from the Second Antibody Modeling Assessment, we demonstrate that Kotai Antibody Builder generates models with an overall accuracy equal to that of the best-performing semi-automated predictors using expert knowledge. Availability and implementation: Kotai Antibody Builder is available at", 
  "summary": "Because CDR-H3 loops are well known to be more difficult to model than those of other CDRs, Kotai Antibody Builder provides a refinement option that includes sampling by fragment assembly followed by side-chain modeling and scoring by an empirical scoring function.\nKotai Antibody Builder uses the most important of these rules (rule i), which predicts the structural class of the `base' proximal to CDR-H3.\nThe refinement option was much more successful in modeling CDR-H3 loops than the protocol used to generate initial models or by the PIGS server (Marcatili et al., 2008).", 
  "affiliations": [
    " Institute for Protein Research Osaka University ", 
    " Molecular Medicine Research Laboratories Drug Discovery Research Astellas Pharma Inc ", 
    " National Institute of Biomedical Innovation", 
    " WPI Immunology Frontier Research Center (IFReC) Osaka University "
  ], 
  "grants": [
    "Funding: This work was supported by the Platform for Drug Discovery, Informatics and Structural Life Science, MEXT, Japanese Government."
  ], 
  "acks": " The authors would like to thank N. Sakiyama, H. Nakagawa, E. Kanamori, K. Tsuchida, N. Tanigawa and S. Soga of Astellas Pharma for helpful discussions. ", 
  "authors": [
    " Kazuo Yamashita", 
    " Kazuyoshi Ikeda", 
    " Karlou Amada", 
    " Shide Liang", 
    " Yuko Tsuchiya", 
    " Haruki Nakamura", 
    " Hiroki Shirai", 
    " Daron M Standley", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "proteins", 
      "antibodies", 
      "based", 
      "structures", 
      "modeling"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-27T00:29:24Z"
}{
  "doi": "10.1093/bioinformatics/btv022", 
  "name": "KMC 2 fast and resourcefrugal kmer counting", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://sun.aei.polsl.pl/kmc", 
    "http://cs.ucsb.edu"
  ], 
  "title": "Sequence analysis KMC 2: fast and resource-frugal k-mer counting", 
  "toolName": "Sequence analysis KMC 2: fast and resource-frugal k-mer counting", 
  "abstract": "Motivation: Building the histogram of occurrences of every k-symbol long substring of nucleotide data is a standard step in many bioinformatics applications, known under the name of k-mer counting. Its applications include developing de Bruijn graph genome assemblers, fast multiple sequence alignment and repeat detection. The tremendous amounts of NGS data require fast algorithms for k-mer counting, preferably using moderate amounts of memory. Results: We present a novel method for k-mer counting, on large datasets about twice faster than the strongest competitors (Jellyfish 2, KMC 1), using about 12 GB (or less) of RAM. Our disk-based method bears some resemblance to MSPKmerCounter, yet replacing the original minimizers with signatures (a carefully selected subset of all minimizers) and using (k, x)-mers allows to significantly reduce the I/O and a highly parallel overall architecture allows to achieve unprecedented processing speeds. For example, KMC 2 counts the 28-mers of a human reads collection with 44-fold coverage (106 GB of compressed size) in about 20 min, on a 6-core Intel i7 PC with an solid-state disk. Availability and implementation: KMC 2 is freely available at http://sun.aei.polsl.pl/kmc.", 
  "summary": "Results: We present a novel method for k-mer counting, on large datasets about twice faster than the strongest competitors (Jellyfish 2, KMC 1), using about 12 GB (or less) of RAM.\nThe minimizers were used for the first time for the k-mer counting in MSPKmerCounter, but our modification significantly reduces the main memory requirements (up to 35 times) and disk space (about 5 times) when compared with MSPKmerCounter.\nexperimental section 3 of the paper), the mentioned modification significantly reduces the size of the largest bin and also reduces the total number of super k-mers, therefore both the main memory and temporary disk use is much smaller compared with using just canonical minimizers.", 
  "affiliations": [
    " Institute of Informatics Silesian University of Technology ", 
    " Institute of Applied Computer Science Lodz University of Technology "
  ], 
  "grants": [
    "Funding\nThis work was supported by the Polish National Science Centre under the project DEC-2012/05/B/ST6/03148.", 
    "This work was performed using the infrastructure supported by POIG.02.03.01-24-099/13 grant: `GeCONiIUpper Silesian Center for Computational Science and Engineering'."
  ], 
  "acks": " ", 
  "authors": [
    " Sebastian Deorowicz", 
    " Marek Kokot", 
    " Szymon Grabowski", 
    " Agnieszka Debudaj-Grabysz"
  ], 
  "keyWords": [
    [
      "disks", 
      "bioinformatics", 
      "mers", 
      "timings", 
      "times", 
      "memory", 
      "bins"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-22T03:50:58Z"
}{
  "doi": "10.1093/bioinformatics/btu838", 
  "name": "Knowledgebased modeling of peptides at protein interfaces PiPreD", 
  "links": [
    "http://www.bioinsilico.org/PIPRED", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bioinsilico.org/PIPRED.2.2"
  ], 
  "title": "Knowledge-based modeling of peptides at protein interfaces: PiPreD", 
  "toolName": "Knowledge-based modeling of peptides at protein interfaces: PiPreD", 
  "abstract": "Motivation: Protein\u2013protein interactions (PPIs) underpin virtually all cellular processes both in health and disease. Modulating the interaction between proteins by means of small (chemical) agents is therefore a promising route for future novel therapeutic interventions. In this context, peptides are gaining momentum as emerging agents for the modulation of PPIs. Results: We reported a novel computational, structure and knowledge-based approach to model orthosteric peptides to target PPIs: PiPreD. PiPreD relies on a precompiled and bespoken library of structural motifs, iMotifs, extracted from protein complexes and a fast structural modeling algorithm driven by the location of native chemical groups on the interface of the protein target named anchor residues. PiPreD comprehensive and systematically samples the entire interface deriving peptide conformations best suited for the given region on the protein interface. PiPreD complements the existing technologies and provides new solutions for the disruption of selected interactions. Availability and implementation: Database and accessory scripts and programs are available upon request to the authors or at http://www.bioinsilico.org/PIPRED.", 
  "summary": "While useful, these technologies present a number of limitations: (i) the search is usually restricted to a small region of the interface given the time that will be required to fully explore the entire interface; (ii) the structure of peptides tend to be biased toward linear and extended conformations as its maximized the contacts with the target surface, however, peptides can adopt different conformations including a-helix (Kritzer et al., 2004); (iii) do not explicitly exploit the natural repertoire of peptide sequences, which has been proved to increase the chance to identify bioactive peptides (Watt, 2006); (iv) some methods require the sequence of the peptides, and thus neglecting the discovery of novel sequences; and (v) other methods are based on the optimization of main-chain stretches isolated from its cognate partner(s), and thus cannot be used to explore novel conformations.", 
  "affiliations": [
    " Departament de Structural Bioinformatics Lab (GRIB) Ciencies Experimental i de la Salut Universitat Pompeu Fabra "
  ], 
  "grants": [
    "Funding\nThe work has been supported by ACCIO through the TecnioSpring program under REA [600388]."
  ], 
  "acks": " The work has been supported by ACCIO through the TecnioSpring program under REA. Conflict of Interest: none declared. ", 
  "authors": [
    " Baldo Oliva", 
    " Narcis Fernandez-Fuentes"
  ], 
  "keyWords": [
    "peptide conformations", 
    [
      "conformational", 
      "peptidic", 
      "proteins", 
      "interfaces", 
      "structurally", 
      "modelling"
    ]
  ], 
  "sourcelinks": [
    "http://www.bioinsilico.org/PIPRED", 
    "http://www.bioinsilico.org/PIPRED.2.2"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-25T04:16:00Z"
}{
  "doi": "10.1093/bioinformatics/btu713", 
  "name": "KmerStream streaming algorithms for kmer abundance estimation", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/pmelsted/KmerStream"
  ], 
  "title": "KmerStream: streaming algorithms for k-mer abundance estimation", 
  "toolName": "KmerStream", 
  "abstract": "Motivation: Several applications in bioinformatics, such as genome assemblers and error corrections methods, rely on counting and keeping track of k-mers (substrings of length k). Histograms of k-mer frequencies can give valuable insight into the underlying distribution and indicate the error rate and genome size sampled in the sequencing experiment. Results: We present KmerStream, a streaming algorithm for estimating the number of distinct k-mers present in high-throughput sequen-cing data. The algorithm runs in time linear in the size of the input and the space requirement are logarithmic in the size of the input. We derive a simple model that allows us to estimate the error rate of the sequencing experiment, as well as the genome size, using only the aggregate statistics reported by KmerStream. As an application we show how KmerStream can be used to compute the error rate of a DNA sequencing experiment. We run KmerStream on a set of 2656 whole genome sequenced individuals and compare the error rate to quality values reported by the sequencing equipment. We discover that while the quality values alone are largely reliable as a predictor of error rate, there is considerable variability in the error rates between sequencing runs, even when accounting for reported quality values.", 
  "summary": "Some of the most commonly used algorithms for aligning reads to a reference genome start by finding short exact matches of a fixed length k (commonly referred to as a seed); an index of all k-mers is constructed and from this index an initial alignment of a part of the read is found.\nHence, if a read contains a basepair with a q-score less than the given threshold only k-mers to the left and to the right of the basepair were considered.\nNature biotechnology, 26, 11461153.\nSci., 31, 182209.\nProc Natl Acad Sci, 108, 15131518.\nProc.\nNatl Acad.\nSci., 109, 1327213277.\nNature, 497, 517520.", 
  "affiliations": [], 
  "grants": [], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/pmelsted/KmerStream"
  ], 
  "acks": " ", 
  "authors": [
    " P All Melsted", 
    " Bjarni V Halld Orsson"
  ], 
  "keyWords": [
    [
      "mers", 
      "genomics", 
      "errors", 
      "sequencers", 
      "estimators"
    ]
  ], 
  "github_data": {
    "name": "KmerStream", 
    "contributors": [
      {
        "contributions": 22, 
        "html_url": "https://github.com/pmelsted"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/pmelsted/KmerStream/zipball/v1.1", 
        "tarball_url": "https://api.github.com/repos/pmelsted/KmerStream/tarball/v1.1", 
        "name": "v1.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/pmelsted/KmerStream/zipball/v1.0", 
        "tarball_url": "https://api.github.com/repos/pmelsted/KmerStream/tarball/v1.0", 
        "name": "v1.0"
      }
    ], 
    "created_at": "2014-03-26T10:38:49Z", 
    "updated_at": "2016-08-01T10:31:52Z", 
    "languages": [
      "Python", 
      "Objective-C", 
      "Makefile", 
      "C", 
      "C++"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/pmelsted"
      }, 
      {
        "html_url": "https://github.com/yanlinlin82"
      }, 
      {
        "html_url": "https://github.com/cschin"
      }, 
      {
        "html_url": "https://github.com/bjarnivh"
      }, 
      {
        "html_url": "https://github.com/pimentel"
      }, 
      {
        "html_url": "https://github.com/druvus"
      }, 
      {
        "html_url": "https://github.com/jasper1918"
      }
    ], 
    "owner": "https://github.com/pmelsted", 
    "homepage": null
  }, 
  "technologies": [
    "C++"
  ], 
  "dateCreated": "2014-10-30T05:01:31Z"
}{
  "doi": "10.1093/bioinformatics/btu724", 
  "name": "Largescale binding ligand prediction by improved patchbased method PatchSurfer20", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://kiharalab.org/patchsurfer2.0"
  ], 
  "title": "Large-scale binding ligand prediction by improved patch-based method Patch-Surfer2.0", 
  "toolName": "Large-scale binding ligand prediction by improved patch-based method Patch-Surfer2.0", 
  "abstract": "Motivation: Ligand binding is a key aspect of the function of many proteins. Thus, binding ligand prediction provides important insight in understanding the biological function of proteins. Binding ligand prediction is also useful for drug design and examining potential drug side effects. Results: We present a computational method named Patch-Surfer2.0, which predicts binding ligands for a protein pocket. By representing and comparing pockets at the level of small local surface patches that characterize physicochemical properties of the local regions, the method can identify binding pockets of the same ligand even if they do not share globally similar shapes. Properties of local patches are represented by an efficient mathematical representation, 3D Zernike Descriptor. Patch-Surfer2.0 has significant technical improvements over our previous prototype, which includes a new feature that captures approximate patch position with a geodesic distance histogram. Moreover, we constructed a large comprehensive database of ligand binding pockets that will be searched against by a query. The benchmark shows better performance of Patch-Surfer2.0 over existing methods. Availability and implementation: http://kiharalab.org/patchsurfer2.0/", 
  "summary": "Results: We present a computational method named Patch-Surfer2.0, which predicts binding ligands for a protein pocket.\nBased on the ranked pocket list, predictions of binding ligands will be made using Pocket_Scorew, which is the score of a query pocket P for a ligand type F:\nThese examples illustrate that Patch-Surfer 2.0 identifies pockets of the same ligand type that have different overall shapes and locate in a protein of globally different structures.\n(2010) A new protein binding pocket similarity measure based on comparison of clouds of atoms in 3D: application to ligand prediction.", 
  "affiliations": [
    " Department of Biological Science Purdue University "
  ], 
  "grants": [
    "Funding\nThis work was supported by the National Institutes of Health [R01GM097528]; National Science Foundation [IIS1319551, DBI1262189, IOS1127027 to D.K."
  ], 
  "acks": " The authors are grateful to Lenna X. Peterson for proofreading the manuscript. ", 
  "authors": [
    " Xiaolei Zhu", 
    " Yi Xiong", 
    " Daisuke Kihara"
  ], 
  "keyWords": [
    "ligand prediction", 
    [
      "scoring", 
      "patches", 
      "proteins", 
      "predictions", 
      "pockets", 
      "ligands"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-31T00:44:56Z"
}{
  "doi": "10.1093/bioinformatics/btu784", 
  "name": "LigDig a web server for querying ligandprotein interactions", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://mcm.h-its.org/ligdig", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Data and text mining LigDig: a web server for querying ligand\u2013protein interactions", 
  "toolName": "Data and text mining LigDig: a web server for querying ligand\u2013protein interactions", 
  "abstract": "LigDig is a web server designed to answer questions that previously required several independent queries to diverse data sources. It also performs basic manipulations and analyses of the structures of protein\u2013ligand complexes. The LigDig webserver is modular in design and consists of seven tools, which can be used separately, or via linking the output from one tool to the next, in order to answer more complex questions. Currently, the tools allow a user to: (i) perform a free-text compound search, (ii) search for suitable ligands, particularly inhibitors, of a protein and query their interaction network, (iii) search for the likely function of a ligand, (iv) perform a batch search for compound identifiers, (v) find structures of protein\u2013ligand complexes, (vi) compare three-dimensional structures of ligand binding sites and (vii) prepare coordinate files of protein\u2013 ligand complexes for further calculations. Availability and implementation: LigDig makes use of freely available databases, including ChEMBL, PubChem and SABIO-RK, and software programs, including cytoscape.js, PDB2PQR, ProBiS and Fconv. LigDig can be used by non-experts in bio-and chemoinformatics. LigDig is available at: http://mcm.h-its.org/", 
  "summary": "Currently, the tools allow a user to: (i) perform a free-text compound search, (ii) search for suitable ligands, particularly inhibitors, of a protein and query their interaction network, (iii) search for the likely function of a ligand, (iv) perform a batch search for compound identifiers, (v) find structures of proteinligand complexes, (vi) compare three-dimensional structures of ligand binding sites and (vii) prepare coordinate files of protein ligand complexes for further calculations.\nLigDig also provides the number of structures in the Protein Database (PDB; Berman et al., 2002) which contain the compound of interest.", 
  "affiliations": [
    " Molecular and Cellular Modeling Group Heidelberg Institute for Theoretical Studies (HITS) "
  ], 
  "grants": [
    "Funding\nThis work was supported by the German Ministry of Education and Research (BMBF) Virtual Liver Network [0315749] and SysMO-LAB2 [0315788B] projects and the Klaus Tschira Foundation."
  ], 
  "acks": " We thank members of the Molecular and Cellular Modeling group and Nadine Veith for their help in testing LigDig. This work was supported by the German Ministry of Education and Research (BMBF) Virtual Liver Network and SysMO-LAB2 projects and the Klaus Tschira Foundation. Conflict of interests: none declared. ", 
  "authors": [
    " Jonathan C Fuller", 
    " Michael Martinez", 
    " Stefan Henrich", 
    " Antonia Stank", 
    " Stefan Richter", 
    " Rebecca C Wade"
  ], 
  "keyWords": [
    [
      "compounds", 
      "data", 
      "searching", 
      "ligdig", 
      "bioinformatics", 
      "tools"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "HTML", 
    "Java", 
    "Scala"
  ], 
  "dateCreated": "2014-11-30T01:39:32Z"
}{
  "doi": "10.1093/bioinformatics/btu408", 
  "name": "Learning proteinDNA interaction landscapes by integrating experimental data through computational models", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.cs.duke.edu/$amink"
  ], 
  "title": "Genome analysis Learning protein\u2013DNA interaction landscapes by integrating experimental data through computational models", 
  "toolName": "Genome analysis Learning protein\u2013DNA interaction landscapes by integrating experimental data through computational models", 
  "abstract": "Motivation: Transcriptional regulation is directly enacted by the interactions between DNA and many proteins, including transcription factors (TFs), nucleosomes and polymerases. A critical step in deciphering transcriptional regulation is to infer, and eventually predict, the precise locations of these interactions, along with their strength and frequency. While recent datasets yield great insight into these interactions, individual data sources often provide only partial information regarding one aspect of the complete interaction landscape. For example, chromatin immunoprecipitation (ChIP) reveals the binding positions of a protein, but only for one protein at a time. In contrast, nucleases like MNase and DNase can be used to reveal binding positions for many different proteins at once, but cannot easily determine the identities of those proteins. Currently, few statistical frameworks jointly model these different data sources to reveal an accurate, hol-istic view of the in vivo protein\u2013DNA interaction landscape. Results: Here, we develop a novel statistical framework that integrates different sources of experimental information within a thermo-dynamic model of competitive binding to jointly learn a holistic view of the in vivo protein\u2013DNA interaction landscape. We show that our framework learns an interaction landscape with increased accuracy, explaining multiple sets of data in accordance with thermodynamic principles of competitive DNA binding. The resulting model of genomic occupancy provides a precise mechanistic vantage point from which to explore the role of protein\u2013DNA interactions in transcriptional regulation. Availability and implementation: The C source code for COMPETE and Python source code for MCMC-based inference are available at", 
  "summary": "Our framework also integrates protein binding specificity information from PBM data and produces a more accurate and realistic proteinDNA interaction landscape than COMPETE alone, along with a mechanistic explanation of MNase-digested fragments of different sizes.\n2.2 Using paired-end MNase-seq data as a measure of genomic occupancy level of DNA-binding proteins\nWe show that integrating information from experimental data within a general framework built on a thermodynamic ensemble model of competitive factor binding can improve the accuracy of inferred proteinDNA interactions, providing a more biologically plausible view of the proteinDNA interaction landscape.", 
  "affiliations": [
    " Program in Computational Biology and Bioinformatics Duke University ", 
    " Knowledge Systems and Informatics Lawrence Livermore National Laboratory "
  ], 
  "grants": [
    "Funding: This work was funded in part by grants from NIH (P50 GM081883-01) and DARPA (HR0011-09-1-0040) to A.J.H."
  ], 
  "acks": " The authors would like to thank Jason Belsky, Kaixuan Luo, Yezhou Huang, and Michael Mayhew for helpful discussions and comments. ", 
  "authors": [
    " Jianling Zhong", 
    " Todd Wasson", 
    " Alexander J Hartemink", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "transcriptional", 
      "binding", 
      "proteins", 
      "genomes", 
      "modeling", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://www.cs.duke.edu/$amink"
  ], 
  "technologies": [], 
  "dateCreated": "2014-06-29T00:12:32Z"
}{
  "doi": "10.1093/bioinformatics/btv025", 
  "name": "Largescale extraction of brain connectivity from the neuroscientific literature", 
  "links": [
    "http://mallet.cs.umass.edu.Movshovitz-Attias,D", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Data and text mining Large-scale extraction of brain connectivity from the neuroscientific literature", 
  "toolName": "Data and text mining Large-scale extraction of brain connectivity from the neuroscientific literature", 
  "abstract": "Motivation: In neuroscience, as in many other scientific domains, the primary form of knowledge dissemination is through published articles. One challenge for modern neuroinformatics is finding methods to make the knowledge from the tremendous backlog of publications accessible for search, analysis and the integration of such data into computational models. A key example of this is metascale brain connectivity, where results are not reported in a normalized repository. Instead, these experimental results are published in natural language, scattered among individual scientific publications. This lack of normalization and centralization hinders the large-scale integration of brain connectivity results. In this article, we present text-mining models to extract and aggregate brain connectivity results from 13.2 million PubMed abstracts and 630 216 full-text publications related to neuroscience. The brain regions are identified with three different named entity recog-nizers (NERs) and then normalized against two atlases: the Allen Brain Atlas (ABA) and the atlas from the Brain Architecture Management System (BAMS). We then use three different extractors to assess inter-region connectivity. Results: NERs and connectivity extractors are evaluated against a manually annotated corpus. The complete in litero extraction models are also evaluated against in vivo connectivity data from ABA with an estimated precision of 78%. The resulting database contains over 4 million brain region mentions and over 100 000 (ABA) and 122 000 (BAMS) potential brain region connections. This database drastically accelerates connectivity literature review, by providing a centralized repository of connectivity data to neuroscientists. Availability and implementation: The resulting models are publicly available at github.com/ BlueBrain/bluima.", 
  "summary": "In this article, we present text-mining models to extract and aggregate brain connectivity results from 13.2 million PubMed abstracts and 630 216 full-text publications related to neuroscience.\nThe resulting database contains over 4 million brain region mentions and over 100 000 (ABA) and 122 000 (BAMS) potential brain region connections.\nIt facilitates the manual search of brain connectivity data by analyzing very large numbers of scientific articles and proposing to the neuroscientist a list of brain regions potentially connected.\n(2012) developed and evaluated several models to extract brain region connectivity.\nWe begin by quantitatively evaluating the performance of the brain region NERs and connectivity extractors against annotated corpora.", 
  "affiliations": [
    " Blue Brain Project Brain Mind Institute ", 
    " School of Computer and Communication Sciences Ecole Polytechnique F\u00e9d\u00e9 rale de Lausanne (EPFL) "
  ], 
  "grants": [
    "Funding\nThis research was supported by the European Union Seventh Framework Programme [FP7/2007-2013] under grant agreement no."
  ], 
  "acks": " The authors thank Phil\u00e9mon Favrod for helping developing the nearest neighbours filter, Catherine Zwahlen for the reviews and L\u00e9on French and his coworkers for developing the WhiteText annotated corpus. This research was supported by the European Union Seventh Framework Programme under grant agreement no. 604102 (HBP). Conflict of Interest: none declared. ", 
  "authors": [
    " Renaud Richardet", 
    " Jean-C\u00e9 Dric Chappelier", 
    " Martin Telefont", 
    " Sean Hill"
  ], 
  "keyWords": [
    "brain connectivity", 
    [
      "connections", 
      "articles", 
      "models", 
      "brains", 
      "ners", 
      "regions", 
      "extraction"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-22T03:50:58Z"
}{
  "doi": "10.1093/bioinformatics/btu582", 
  "name": "Limbform a functional ontologybased database of limb regeneration experiments", 
  "links": [
    "http://limbform.daniel-lobo.com", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Databases and ontologies Limbform: a functional ontology-based database of limb regeneration experiments", 
  "toolName": "Databases and ontologies Limbform: a functional ontology-based database of limb regeneration experiments", 
  "abstract": "The ability of certain organisms to completely regenerate lost limbs is a fascinating process, far from solved. Despite the extraordinary published efforts during the past centuries of scientists performing amputations, transplantations and molecular experiments, no mechanistic model exists yet that can completely explain patterning during the limb regeneration process. The lack of a centralized repository to enable the efficient mining of this huge dataset is hindering the discovery of comprehensive models of limb regeneration. Here, we introduce Limbform (Limb formalization), a centralized database of published limb regeneration experiments. In contrast to natural language or text-based ontologies, Limbform is based on a functional ontology using mathematical graphs to represent unambiguously limb phenotypes and manipulation procedures. The centralized database currently contains4800 published limb regeneration experiments comprising many model organisms, including salamanders, frogs, insects , crustaceans and arachnids. The database represents an extraordinary resource for mining the existing knowledge of functional data in this field; furthermore, its mathematical nature based on a functional ontology will pave the way for artificial intelligence tools applied to the discovery of the sought-after comprehensive limb regeneration models. Availability and implementaion: The Limbform database is freely available at http://limbform.daniel", 
  "summary": "In contrast, novel functional mathematical ontologies based on graphs have been recently proposed for the formalization of morphologies and manipulations for planarian worm (Lobo et al., 2013b) and limb (Lobo et al., 2014a) regenerative experiments.\nTo unambiguously describe the main characteristics of a limb morphology in the database, we used a novel functional ontology for limb regeneration experiments (Lobo et al., 2014a).\nWe created the Limbform database of limb regeneration experiments based on a functional ontology of mathematical graphs.\nWe curated hundreds of limb regeneration experiments, resultant morphologies and manipulations from the published literature into a centralized database.", 
  "affiliations": [
    " Center for Regenerative and Developmental Biology Department of Biology Tufts University "
  ], 
  "grants": [
    "Funding: National Science Foundation (EF-1124651), National Institutes of Health (GM078484), W. M. Keck Foundation and G. Harold and Leila Y. Mathers Charitable Foundation."
  ], 
  "acks": " The authors thank the Levin Lab members for valuable discussions and the four anonymous reviewers for useful suggestions. ", 
  "authors": [
    " Daniel Lobo", 
    " Erica B Feldman", 
    " Michelle Shah", 
    " Taylor J Malone", 
    " Michael Levin"
  ], 
  "keyWords": [
    "limb regeneration experiments", 
    [
      "limbs", 
      "models", 
      "morphological", 
      "lobo", 
      "graphs", 
      "experiment", 
      "databases", 
      "regenerate"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-29T00:22:23Z"
}{
  "doi": "10.1093/bioinformatics/btu859", 
  "name": "LINKPHASE3 an improved pedigreebased phasing algorithm robust to genotyping and map errors", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetics and population analysis LINKPHASE3: an improved pedigree-based phasing algorithm robust to genotyping and map errors", 
  "toolName": "Genetics and population analysis LINKPHASE3: an improved pedigree-based phasing algorithm robust to genotyping and map errors", 
  "abstract": "Many applications in genetics require haplotype reconstruction. We present a phasing program designed for large half-sibs families (as observed in plant and animals) that is robust to genotyping and map errors. We demonstrate that it is more efficient than previous versions and other programs, particularly in the presence of genotyping errors. Availability and implementation: The software LINKPHASE3 is included in the PHASEBOOK package and can be freely downloaded from www.giga.ulg.ac.be/jcms/prod_381171/software. The package is written in FORTRAN and contains source codes. A manual is provided with the package.", 
  "summary": "LinkPHASE has been successfully used in many applications including studies of the recombination process in cattle (Sandor, et al., 2012), imputation of missing marker genotypes (e.g. Zhang and Druet 2010), QTL mapping in half-sib families (Karim, et al., 2011) or routine genomic evaluation in dairy cattle (Boichard, et al., 2012).\n2.2.1 Haplotype reconstruction To test the program, we simulated datasets with and without genotyping errors and compared it with SHAPEIT2 (Delaneau, et al., 2013) combined with duoHMM (O'Connell, et al., 2014) which has been shown to perform well in pedigreed populations compared with other methods.", 
  "affiliations": [
    " Unit of Animal Genomics GIGA-R University of Li\u00e8 ge (B34) "
  ], 
  "grants": [
    "Funding\nThe project was funded by the European Research Council via the ERCAdG-GA323030 (DAMONA)."
  ], 
  "acks": " The authors are grateful to Livestock Improvement Company\u2014LIC (New Zealand) for providing data for this study. Tom Druet is a Research Associate from the FRS-FNRS. ", 
  "authors": [
    " Tom Druet", 
    " Michel Georges"
  ], 
  "keyWords": [
    "map errors", 
    [
      "genetics", 
      "genotyping", 
      "mapping", 
      "improvement", 
      "maps", 
      "haplotyping", 
      "error", 
      "linkphase"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-09T02:26:44Z"
}{
  "doi": "10.1093/bioinformatics/btu796", 
  "name": "LipidPro a computational lipid identification solution for untargeted lipidomics on dataindependent acquisition tandem mass spectrometry platforms", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.neurogenetics.biozentrum.uni-wuerzburg.de/en/project/services/lipidpro"
  ], 
  "title": "Data and text mining Lipid-Pro: a computational lipid identification solution for untargeted lipidomics on data-independent acquisition tandem mass spectrometry platforms", 
  "toolName": "Data and text mining Lipid-Pro: a computational lipid identification solution for untargeted lipidomics on data-independent acquisition tandem mass spectrometry platforms", 
  "abstract": "A major challenge for mass spectrometric-based lipidomics, aiming at describing all lipid species in a biological sample, lies in the computational and bioinformatic processing of the large amount of data that arises after data acquisition. Lipid-Pro is a software tool that supports the identification of lipids by interpreting large datasets generated by liquid chromatography\u2014 tandem mass spectrometry (LC\u2013MS/MS) using the advanced data-independent acquisition mode MS E. In the MS E mode, the instrument fragments all molecular ions generated from a sample and records time-resolved molecular ion data as well as fragment ion data for every detectable molecular ion. Lipid-Pro matches the retention time-aligned mass-to-charge ratio data of molecular-and fragment ions with a lipid database and generates a report on all identified lipid species. For generation of the lipid database, Lipid-Pro provides a module for construction of lipid species and their fragments using a flexible building block approach. Hence, Lipid-Pro is an easy to use analysis tool to interpret complex MS E lipidomics data and also offers a module to generate a user-specific lipid database. Availability and implementation: Lipid-Pro is freely available at:", 
  "summary": "Lipid-Pro matches the retention time-aligned mass-to-charge ratio data of molecular- and fragment ions with a lipid database and generates a report on all identified lipid species.\nWe developed the Lipid-Pro software solution for rapid identification of lipids in preprocessed DIA data acquired with LCMS/MS using rt-aligned monoisotopic masses of molecular ions (MS) and fragment ions (MS/MS).\nFor automatic generation of a lipid database, Lipid-Pro provides modules for the construction of complex lipid species and their fragments using a building block approach (Yang et al., 2009).\nFor lipid identification in LCMS/MS datasets, Lipid-Pro requires two preprocessed files in *.csv format that contain the rt-aligned m/z-values of molecular ions (MS data) and rt-aligned fragment ions (MS/MS data).", 
  "affiliations": [
    " Department of Pharmaceutical Biology University of Wuerzburg ", 
    " Department of Bioinformatics, Biocenter"
  ], 
  "grants": [
    "Acknowledgements\nFunding: The authors thank the Deutsche Forschungsgemeinschaft (DFG), collaborative research center SFB1047 \"Insect timing\", for funding this research (to ZA, MM, MJM, AF)."
  ], 
  "acks": " Funding: The authors thank the Deutsche Forschungsgemeinschaft (DFG), collaborative research center SFB1047 \" Insect timing \" , for funding this research (to ZA, MM, MJM, AF). The authors also thank TR34/Z1 for support (to SZ, TD). ", 
  "authors": [
    " Zeeshan Ahmed", 
    " Michel Mayr", 
    " Saman Zeeshan", 
    " Thomas Dandekar", 
    " Martin J Mueller", 
    " Agnes Fekete"
  ], 
  "keyWords": [
    [
      "fragmentation", 
      "masses", 
      "lipidomics", 
      "bioinformatics", 
      "molecular", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-30T01:39:32Z"
}{
  "doi": "10.1093/bioinformatics/btu692", 
  "name": "LIGSIFT an opensource tool for ligand structural alignment and virtual screening", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://cssb.biol"
  ], 
  "title": "Structural bioinformatics LIGSIFT: an open-source tool for ligand structural alignment and virtual screening", 
  "toolName": "Structural bioinformatics LIGSIFT: an open-source tool for ligand structural alignment and virtual screening", 
  "abstract": "Motivation: Shape-based alignment of small molecules is a widely used approach in computer-aided drug discovery. Most shape-based ligand structure alignment applications, both commercial and freely available ones, use the Tanimoto coefficient or similar functions for evaluating molecular similarity. Major drawbacks of using such functions are the size dependence of the score and the fact that the statistical significance of the molecular match using such metrics is not reported. Results: We describe a new open-source ligand structure alignment and virtual screening (VS) algorithm, LIGSIFT, that uses Gaussian molecular shape overlay for fast small molecule alignment and a size-independent scoring function for efficient VS based on the statistical significance of the score. LIGSIFT was tested against the compounds for 40 protein targets available in the Directory of Useful Decoys and the performance was evaluated using the area under the ROC curve (AUC), the Enrichment Factor (EF) and Hit Rate (HR). LIGSIFT-based VS shows an average AUC of 0.79, average EF values of 20.8 and a HR of 59% in the top 1% of the screened library. Availability and implementation: LIGSIFT software, including the source code, is freely available to academic users at http://cssb.biol ogy.gatech.edu/LIGSIFT.", 
  "summary": "Results: We describe a new open-source ligand structure alignment and virtual screening (VS) algorithm, LIGSIFT, that uses Gaussian molecular shape overlay for fast small molecule alignment and a sizeindependent scoring function for efficient VS based on the statistical significance of the score.\nIn this study, we present a new open-source, ligand-based VS algorithm that provides a size-independent scoring function to measure shape and chemical similarity and also reports the P-value to assess the statistical significance of the match between a pair of molecules.\n(EF1% 430) were observed for 15 proteins: ACHE, ADA, AMPC, AR, COMT, COX2, DHFR, GPB, HMGA, HSP90, INHA, MR, NA, PNP and RXR; while no enrichment (EF1% = 0) was observed for SRC (supplementary Table S8).", 
  "affiliations": [
    " Center for the Study of Systems Biology School of Biology Georgia Institute of Technology "
  ], 
  "grants": [
    "For example, Rapid Overlay of Chemical Structures (ROCS), a highly popular closed-source algorithm, uses a Gaussian description of the molecular shape and chemical nature of the ligand (Grant et al., 1996; Grant and Pickup, 1995) for ligand screening.", 
    "Funding: This work was supported by grants GM-48835 and GM-37408 from the Division of General Medical Sciences of the National Institutes of Health.", 
    "Grant,J.A."
  ], 
  "acks": " The authors thank Dr. Jianyi Yang and Dr. Yang Zhang of University of Michigan for providing us with the 3D-conformations of molecules included in the DUD database and Dr. Mu Gao for valuable comments and insightful suggestions. ", 
  "authors": [
    " Ambrish Roy", 
    " Jeffrey Skolnick", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "similarities", 
      "ligands", 
      "molecules", 
      "ligsift"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://cssb.biol"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-22T05:07:31Z"
}{
  "doi": "10.1093/bioinformatics/btu759", 
  "name": "Local statistics allow quantification of celltocell variability from highthroughput microscope images", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.moseslab.csb"
  ], 
  "title": "Local statistics allow quantification of cell-to-cell variability from high-throughput microscope images", 
  "toolName": "Local statistics allow quantification of cell-to-cell variability from high-throughput microscope images", 
  "abstract": "Motivation: Quantifying variability in protein expression is a major goal of systems biology and cell-to-cell variability in subcellular localization pattern has not been systematically quantified. Results: We define a local measure to quantify cell-to-cell variability in high-throughput microscope images and show that it allows comparable measures of variability for proteins with diverse subcellu-lar localizations. We systematically estimate cell-to-cell variability in the yeast GFP collection and identify examples of proteins that show cell-to-cell variability in their subcellular localization. Conclusions: Automated image analysis methods can be used to quantify cell-to-cell variability in microscope images. Contact: alan.moses@utoronto.ca Availability and Implementation: Software and data are available at", 
  "summary": "Time-lapse fluorescence microscopy experiments have also revealed examples of proteins that show cell-to-cell variability in subcellular localization (Cai et al.\nWe computed the subcellular spread (see Section 2) for each cell and plotted its CV and RV (see Section 2) for several large classes of proteins based on their subcellular localization as defined through inspection of the original GFP collection images (Huh et al.\nTo obtain a more objective measure of the power of our new measure (RV) to identify cell-to-cell variability in subcellular localization pattern, we examined images for the top 30 most spatially variable proteins according to the RV and CV.", 
  "affiliations": [
    " Department of Cell & Systems Biology", 
    " Department of Computer Science", 
    " Department of Molecular Genetics University of Toronto "
  ], 
  "grants": [
    "]; infrastructure grants from the Canada Foundation for Innovation (to A.M.M.", 
    "Funding\nThis work was supported by the National Sciences and Engineering Research Council of Canada (to L.F.H.", 
    "and Brenda Andrews) and by grants from the Canadian Institutes for Health Research (to Y.T.C.)."
  ], 
  "acks": " We thank an anonymous reviewer for pointing out the connection with kernel regression, Alex Nguyen Ba for comments on the manuscript and Dr. Gelila Tilahun for help with statistics. We also thank Dr. Brenda Andrews for access to data, supervisory support and invaluable help and guidance throughout the project. ", 
  "authors": [
    " Louis-Fran\u00e7 Ois Handfield", 
    " Bob Strome", 
    " Yolanda T Chong", 
    " Alan M Moses"
  ], 
  "keyWords": [
    "cell variability", 
    [
      "cells", 
      "proteins", 
      "imaging", 
      "variable", 
      "figs", 
      "localizations"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-15T04:10:48Z"
}{
  "doi": "10.1093/bioinformatics/btu826", 
  "name": "LocNES a computational tool for locating classical NESs in CRM1 cargo proteins", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://research.nki.nl", 
    "http://prodata.swmed.edu/LocNES"
  ], 
  "title": "LocNES: a computational tool for locating classical NESs in CRM1 cargo proteins", 
  "toolName": "LocNES: a computational tool for locating classical NESs in CRM1 cargo proteins", 
  "abstract": "Motivation: Classical nuclear export signals (NESs) are short cognate peptides that direct proteins out of the nucleus via the CRM1-mediated export pathway. CRM1 regulates the localization of hundreds of macromolecules involved in various cellular functions and diseases. Due to the diverse and complex nature of NESs, reliable prediction of the signal remains a challenge despite several attempts made in the last decade. Results: We present a new NES predictor, LocNES. LocNES scans query proteins for NES consensus-fitting peptides and assigns these peptides probability scores using Support Vector Machine model, whose feature set includes amino acid sequence, disorder propensity, and the rank of position-specific scoring matrix score. LocNES demonstrates both higher sensitivity and precision over existing NES prediction tools upon comparative analysis using experimentally identified NESs.", 
  "summary": "The feature set for SVM model is then constructed, which includes PSSM score rank, peptide sequence represented by a vector containing twenty-one indicator variables (one for each amino acid plus a blank position) for each residue, and the types of consensus (Classes 1a, 1b, 1c, 1d, 2, or 3) for the NES candidate.\nEnhanced performance of LocNES compared with NESsential and Wregex resulted from a more tolerant pre-filter, a more representative feature set for machine-learning models, a more accurate training dataset, and the combined use of machine-learning method, position specific scoring matrix and biophysical properties of NESs. As more CRM1 cargoes/NESs are discovered, increased size, diversity and accuracy of experimental NES databases will continue to improve training/testing datasets for future NES predictors.", 
  "affiliations": [
    " Southwestern Medical Center at Dallas Howard Hughes Medical Institute University of Texas ", 
    " Department of Pharmacology Southwestern Medical Center at Dallas University of Texas "
  ], 
  "grants": [
    "Funding\nThis work was funded by Cancer Prevention Research Institute of Texas (CPRIT) Grant RP120352 (to Y.M.C.", 
    "), National Institutes of Health Grants [F32GM093493 to D.X., GM094575 to N.V.G, and R01 GM069909 to Y.M.C.", 
    "), Welch Foundation Grant [I-1505 to N.V.G and I-1532 to Y.M.C."
  ], 
  "acks": " While this paper was under review, another NES predictor, NESmapper (), was published. ", 
  "authors": [
    " Darui Xu", 
    " Kara Marquis", 
    " Jimin Pei", 
    " Szu-Chin Fu", 
    " Tolga Cag", 
    " \u02d8 Atay", 
    " Nick V Grishin", 
    " Yuh Min Chook"
  ], 
  "keyWords": [
    [
      "locnes", 
      "proteins", 
      "cells", 
      "ness"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-12-17T01:32:50Z"
}{
  "doi": "10.1093/bioinformatics/btu652", 
  "name": "LocalAli an evolutionarybased local alignment approach to identify functionally conserved modules in multiple networks", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://code.google.com"
  ], 
  "title": "Systems biology LocalAli: an evolutionary-based local alignment approach to identify functionally conserved modules in multiple networks", 
  "toolName": "Systems biology LocalAli: an evolutionary-based local alignment approach to identify functionally conserved modules in multiple networks", 
  "abstract": "Motivation: Sequences and protein interaction data are of significance to understand the underlying molecular mechanism of organisms. Local network alignment is one of key systematic ways for predicting protein functions, identifying functional modules and understanding the phylogeny from these data. Most of currently existing tools, however, encounter their limitations, which are mainly concerned with scoring scheme, speed and scalability. Therefore, there are growing demands for sophisticated network evolution models and efficient local alignment algorithms. Results: We developed a fast and scalable local network alignment tool called LocalAli for the identification of functionally conserved modules in multiple networks. In this algorithm, we firstly proposed a new framework to reconstruct the evolution history of conserved modules based on a maximum-parsimony evolutionary model. By relying on this model, LocalAli facilitates interpretation of resulting local alignments in terms of conserved modules, which have been evolved from a common ancestral module through a series of evolutionary events. A meta-heuristic method simulated annealing was used to search for the optimal or near-optimal inner nodes (i.e. ancestral modules) of the evolutionary tree. To evaluate the performance and the statistical significance, LocalAli were tested on 26 real datasets and 1040 randomly generated datasets. The results suggest that LocalAli outperforms all existing algorithms in terms of coverage, consistency and scalability, meanwhile retains a high precision in the identification of functionally coherent subnetworks.", 
  "summary": "Global alignment approaches determine an optimal global node mapping table for the compared networks (Huang et al., 2013; Li et al., 2007; Milenkovic et al., 2013; Singh et al., 2007, 2008), each set of matched nodes (i.e. proteins) implying a putative function-oriented ortholog group.\nLocal network alignment aims to detect high-scoring d-subnets that imply putative functional modules of the compared species, such as protein complexes.\nTo identify functionally conserved modules from multiple networks, we proposed an evolutionary-based local alignment algorithm to heuristically search for high-scoring d-subnets with the information of interactions, homologous proteins and phylogenetic trees.", 
  "affiliations": [
    " Department of Mathematics and Computer Science Freie Universit\u00e4 t Berlin "
  ], 
  "grants": [
    "Funding: This work is financially supported by the China Scholarship Council."
  ], 
  "acks": " We are grateful to Xiao Liang and Rene\u00b4RahnRene\u00b4Rahn for proofreading the manuscript and their helpful suggestions. ", 
  "authors": [
    " Jialu Hu", 
    " Knut Reinert"
  ], 
  "keyWords": [
    [
      "alignments", 
      "proteins", 
      "networks", 
      "functionally", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://code.google.com"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-05T00:35:52Z"
}{
  "doi": "10.1093/bioinformatics/btu634", 
  "name": "Logodds sequence logos", 
  "links": [
    "http://www.ncbi.nlm.nih.gov/Structure/mmdb/mmdbsrv", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.ncbi.nlm.nih.gov/Structure/cdd/cddsrv", 
    "http://www.ncbi.nlm.nih.gov/CBBresearch/Yu", 
    "http://www.ncbi.nlm.nih"
  ], 
  "title": "Sequence analysis Log-odds sequence logos", 
  "toolName": "Sequence analysis Log-odds sequence logos", 
  "abstract": "Motivation: DNA and protein patterns are usefully represented by sequence logos. However, the methods for logo generation in common use lack a proper statistical basis, and are non-optimal for recognizing functionally relevant alignment columns. Results: We redefine the information at a logo position as a per-observation multiple alignment log-odds score. Such scores are positive or negative, depending on whether a column's observations are better explained as arising from relatedness or chance. Within this framework, we propose distinct normalized maximum likelihood and Bayesian measures of column information. We illustrate these measures on High Mobility Group B (HMGB) box proteins and a dataset of enzyme alignments. Particularly in the context of protein alignments, our measures improve the discrimination of biologically relevant positions. Availability and implementation: Our new measures are implemented in an open-source Web-based logo generation program, which is available at http://www.ncbi.nlm.nih.gov/CBBresearch/Yu/ logoddslogo/index.html. A stand-alone version of the program is also available from this site.", 
  "summary": "In this article, we show empirically that even within alignments of related protein sequence regions, the relative importance of alignment positions, which logos are designed to represent, is better captured using log-odds scores than other measures of column information in common use.\nBasing measures of column information on log-odds multiple alignment scores provides an automatic means for determining boundaries for a logo (Altschul et al., 2010).\nBecause the weighting system we use (Altschul et al., 2009; Sunyaev et al., 1999) yields estimates for N that vary by column, measures U and R yield true-positives and false-negatives that are not completely congruent, although nevertheless similar.", 
  "affiliations": [
    " National Center for Biotechnology Information National Library of Medicine National Institutes of Health "
  ], 
  "grants": [
    "Funding: Y.K.Y., A.S., D.L.", 
    "was supported by institutional funds from Vanderbilt University."
  ], 
  "acks": " ", 
  "authors": [
    " Yi-Kuo Yu", 
    " John A Capra", 
    " Aleksandar Stojmirovic\u00b41stojmirovic\u00b4stojmirovic\u00b41", 
    " David Landsman", 
    " Stephen F Altschul", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "measuring", 
      "scoring", 
      "proteins", 
      "positively", 
      "sequences"
    ]
  ], 
  "sourcelinks": [
    "http://www.ncbi.nlm.nih.gov/CBBresearch/Yu", 
    "http://www.ncbi.nlm.nih"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-08T00:30:13Z"
}{
  "doi": "10.1093/bioinformatics/btu643", 
  "name": "LongTarget a tool to predict lncRNA DNAbinding motifs and binding sites via Hoogsteen basepairing analysis", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis LongTarget: a tool to predict lncRNA DNA-binding motifs and binding sites via Hoogsteen base-pairing analysis", 
  "toolName": "Genome analysis LongTarget: a tool to predict lncRNA DNA-binding motifs and binding sites via Hoogsteen base-pairing analysis", 
  "abstract": "Motivation: In mammalian cells, many genes are silenced by genome methylation. DNA methyltransferases and polycomb repressive complexes , which both lack sequence-specific DNA-binding motifs, are recruited by long non-coding RNA (lncRNA) to specific genomic sites to methylate DNA and chromatin. Increasing evidence indicates that many lncRNAs contain DNA-binding motifs that can bind to DNA by forming RNA:DNA triplexes. The identification of lncRNA DNA-binding motifs and binding sites is essential for deciphering lncRNA functions and correct and erroneous genome methylation; however, such identification is challenging because lncRNAs may contain thousands of nucleotides. No computational analysis of typical lncRNAs has been reported. Here, we report a computational method and program (LongTarget) to predict lncRNA DNA-binding motifs and binding sites. We used this program to analyse multiple antisense lncRNAs, including those that control well-known imprinting clusters, and obtained results agreeing with experimental observations and epigen-etic marks. These results suggest that it is feasible to predict many lncRNA DNA-binding motifs and binding sites genome-wide. Availability and implementation: Website of LongTarget:", 
  "summary": "Because considerable TTSs are at transposable elements, we postulate that after targeting by lncRNAs, the methylation of these transposable elements may hinder the transcription of genes and that because many lncRNA genes contain multiple transposable elements (He et al., 2013; Kelley and Rinn, 2012), the TTSs at transposable elements in lncRNAs help control the highly tissue-specific expression of lncRNAs. Because most lncRNAs emerge in mammals, beyond the hypothesis that X-inactivation and gene imprinting may have co-evolved in mammals (Reik and Lewis, 2005), imprinting in somatic cells as a genome defence against transposable elements and as a mechanism regulating gene expression may also have an associated origin and evolution.", 
  "affiliations": [
    " Network Center Southern Medical University ", 
    " Bioinformatics Section School of Basic Medical Sciences "
  ], 
  "grants": [
    "Funding: This work was supported by the National SuperComputing Center-Guangzhou (2012Y2-00047, 2913Y200050)."
  ], 
  "acks": " We thank Xiaojia Yu, Tao Sun, and Yanfang Guo for their contribution during the early stages. ", 
  "authors": [
    " Sha He", 
    " Hai Zhang", 
    " Haihua Liu", 
    " Hao Zhu"
  ], 
  "keyWords": [
    "specific genomic", 
    [
      "lncrna", 
      "genes", 
      "specifically", 
      "triplexator", 
      "figs", 
      "genome"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-28T00:25:27Z"
}{
  "doi": "10.1093/bioinformatics/btu435", 
  "name": "lrgpr interactive linear mixed model analysis of genomewide association studies with composite hypothesis testing and regression diagnostics in R", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetics and population analysis lrgpr: interactive linear mixed model analysis of genome-wide association studies with composite hypothesis testing and regression diagnostics in R", 
  "toolName": "Genetics and population analysis lrgpr: interactive linear mixed model analysis of genome-wide association studies with composite hypothesis testing and regression diagnostics in R", 
  "abstract": "The linear mixed model is the state-of-the-art method to account for the confounding effects of kinship and population structure in genome-wide association studies (GWAS). Current implementations test the effect of one or more genetic markers while including prespecified covariates such as sex. Here we develop an efficient implementation of the linear mixed model that allows composite hypothesis tests to consider genotype interactions with variables such as other genotypes, environment, sex or ancestry. Our R package, lrgpr, allows interactive model fitting and examination of regression diagnos-tics to facilitate exploratory data analysis in the context of the linear mixed model. By leveraging parallel and out-of-core computing for datasets too large to fit in main memory, lrgpr is applicable to large GWAS datasets and next-generation sequencing data. Availability and implementation: lrgpr is an R package available from", 
  "summary": "Advance Access publication July 16, 2014\nReceived on March 3, 2014; revised on June 12, 2014; accepted on July 2, 2014\nFor genome-wide analysis, the function lrgprApply() allows time and memory-efficient fitting of the linear mixed model for millions of genetic markers and can apply a composite hypothesis test on a large scale.\nThe lrgpr package allows an analyst to apply multiple variations of existing methods to customize an analysis based on the empirical properties of a specific dataset.\n(2010) Variance component model to account for sample structure in genome-wide association studies.\n(2013) JAWAMix5: an out-of-core HDF5-based java implementation of whole-genome association studies using mixed models.\n(2012) Rapid variance components-based method for wholegenome association analysis.", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was supported by a fellowship from the Cornell Center for Comparative and Population Genomics, NSF grants IOS1026555 and DEB0922432 (Cornell University), and NIH grants R01AG046170 and R01MH095034, and a grant from The Leona M. and Harry B. Helmsley Charitable Trust (Icahn School of Medicine at Mount Sinai)."
  ], 
  "acks": " The authors thank Roman Kosoy, Sushila Shenoy, Sarah Brooks, Cris Van Hout and Monica Ramstetter for feedback on the software , and Michael Kane for help with the bigmemory R package. ", 
  "authors": [
    " Gabriel E Hoffman", 
    " Jason G Mezey", 
    " Eric E Schadt", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    [
      "models", 
      "lrgpr", 
      "genetics", 
      "linear", 
      "analysis"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-18T01:53:51Z"
}{
  "doi": "10.1093/bioinformatics/btu788", 
  "name": "LuciPHOr2 site localization of generic posttranslational modifications from tandem mass spectrometry data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://luciphor2.sourceforge.net"
  ], 
  "title": "LuciPHOr2: site localization of generic post-translational modifications from tandem mass spectrometry data", 
  "toolName": "luciphor2", 
  "abstract": "We present LuciPHOr2, a site localization tool for generic post-translational modifications (PTMs) using tandem mass spectrometry data. As an extension of the original LuciPHOr (version 1) for phosphorylation site localization, the new software provides a site-level localization score for generic PTMs and associated false discovery rate called the false localization rate. We describe several novel features such as operating system independence and reduced computation time through multiple threading. We also discuss optimal parameters for different types of data and illustrate the new tool on a human skeletal muscle dataset for lysine-acetylation. Availability and implementation: The software is freely available on the SourceForge website", 
  "summary": "We present LuciPHOr2, a site localization tool for generic post-translational modifications (PTMs) using tandem mass spectrometry data.\nHigh-throughput analysis of PTMs is typically achieved through mass spectrometry (MS) (Silva et al., 2013), where tandem (MS/MS) mass spectra are searched against a sequence database allowing for PTMs. A major challenge in the interpretation of MS/MS data for PTMs is that database search engines can correctly match a peptide sequence with incorrectly localized PTM(s).\nTo address this issue, we recently published a computational method called LuciPHOr for site localization analysis of phosphorylation events using MS/MS data (Fermin et al., 2013).", 
  "affiliations": [
    " Saw Swee Hock School of Public Health National University of Singapore National University Health System ", 
    " Department of Pathology University of Michigan Medical School "
  ], 
  "grants": [
    "Funding\nThis work was supported by Singapore MOE [grant R-608-000-088-112 to H.C.] and NIH [R01-GM-094231 to A.I.N.]."
  ], 
  "acks": " ", 
  "authors": [
    " Damian Fermin", 
    " Dmitry Avtonomov", 
    " Hyungwon Choi", 
    " Alexey I Nesvizhskii"
  ], 
  "keyWords": [
    "luciphor site localization", 
    [
      "scoring", 
      "proteomics", 
      "mass", 
      "sites", 
      "localizations"
    ]
  ], 
  "sourcelinks": [
    "http://luciphor2.sourceforge.net"
  ], 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-11-27T01:24:52Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/luciphor2/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/dfermin315/", 
        "name": "dfermin315"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu538", 
  "name": "LoRDEC accurate and efficient long read error correction", 
  "links": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://gatb-core.gforge.inria.fr", 
    "http://atgc.lirmm.fr/lordec"
  ], 
  "title": "Sequence analysis LoRDEC: accurate and efficient long read error correction", 
  "toolName": "Sequence analysis LoRDEC: accurate and efficient long read error correction", 
  "abstract": "Motivation: PacBio single molecule real-time sequencing is a third-generation sequencing technique producing long reads, with comparatively lower throughput and higher error rate. Errors include numerous indels and complicate downstream analysis like mapping or de novo assembly. A hybrid strategy that takes advantage of the high accuracy of second-generation short reads has been proposed for correcting long reads. Mapping of short reads on long reads provides sufficient coverage to eliminate up to 99% of errors, however, at the expense of prohibitive running times and considerable amounts of disk and memory space. Results: We present LoRDEC, a hybrid error correction method that builds a succinct de Bruijn graph representing the short reads, and seeks a corrective sequence for each erroneous region in the long reads by traversing chosen paths in the graph. In comparison, LoRDEC is at least six times faster and requires at least 93% less memory or disk space than available tools, while achieving comparable accuracy. Availability and implementaion: LoRDEC is written in C++, tested on Linux platforms and freely available at http://atgc.lirmm.fr/lordec.", 
  "summary": "Results: We present LoRDEC, a hybrid error correction method that builds a succinct de Bruijn graph representing the short reads, and seeks a corrective sequence for each erroneous region in the long reads by traversing chosen paths in the graph.\nMany current error correction algorithms for secondgeneration sequencing (Illumina, Roche, or Solid) adopt this counting strategy, also called spectral alignment (Chaisson et al., 2004; Pevzner et al., 2001): one computes the spectrum of solid k-mers and corrects each read by updating each weak k-mer with its closest solid k-mer.", 
  "affiliations": [
    " LIRMM and Institut de Biologie Computationelle CNRS and Universit e Montpellier ", 
    " Department of Computer Science Helsinki Institute for Information Technology HIIT University of Helsinki "
  ], 
  "grants": [
    "Funding: This work was supported by Academy of Finland (267591), by ANR Colib'read (ANR-12-BS02-0008) and Defi MASTODONS SePhHaDe from CNRS, and Labex NumEV, as well as the ATGC platform, which hosts the software."
  ], 
  "acks": " The authors wish to thank the GATB team for access to their library, and B. Cazaux for help with the figures. ", 
  "authors": [
    " Leena Salmela", 
    " Eric Rivals"
  ], 
  "keyWords": [
    [
      "genomics", 
      "corrective", 
      "paths", 
      "reads", 
      "lordec", 
      "sequencing", 
      "alignments"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://gatb-core.gforge.inria.fr"
  ], 
  "technologies": [
    "C++"
  ], 
  "dateCreated": "2014-08-28T02:52:42Z"
}{
  "doi": "10.1093/bioinformatics/btu835", 
  "name": "MACE mutationoriented profiling of chemical response and gene expression in cancers", 
  "links": [
    "http://dtp.nci.nih.gov", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.chemaxon.com", 
    "http://www.ncbi.nlm.nih", 
    "http://mace.sookmyung"
  ], 
  "title": "MACE: mutation-oriented profiling of chemical response and gene expression in cancers", 
  "toolName": "MACE: mutation-oriented profiling of chemical response and gene expression in cancers", 
  "abstract": "The mutational status of specific cancer lineages can affect the sensitivity to or resistance against cancer drugs. The MACE database provides web-based interactive tools for interpreting large chemical screening and gene expression datasets of cancer cell lines in terms of mutation and lineage categories. GI50 data of chemicals against individual NCI60 cell lines were normalized and organized to statistically identify mutation-or lineage-specific chemical responses. Similarly, DNA microarray data on NCI60 cell lines were processed to analyze mutation-or lineage-specific gene expression signatures. A combined analysis of GI50 and gene expression data to find potential associations between chemicals and genes is also a capability of this system. This database will provide extensive, systematic information to identify lineage-or mutation-specific anticancer agents and related gene targets. Availability and implementation: The MACE web database is available at http://mace.sookmyung. ac.kr/.", 
  "summary": "The MACE database provides web-based interactive tools for interpreting large chemical screening and gene expression datasets of cancer cell lines in terms of mutation and lineage categories.\nThus, we developed a web-based database, MACE (Mutation-oriented Analysis of Chemical response and gene Expression), to provide mutation- or lineage-oriented profiling of chemical response and gene expression on a wide variety of cancer cell lines from NCI60 datasets.\nMACE provides tools for 1) category-based analysis of chemical response or gene expression in cancer cell lines (Table 1), 2) finding a correlation between chemical response and gene expression on NCI60 cell lines, and 3) 2D- or 3D-based chemical structural searches in real-time.", 
  "affiliations": [
    " Department of Biological Sciences Sookmyung Women's University ", 
    " Center for Advanced Bioinformatics and Systems Medicine"
  ], 
  "grants": [
    "The NCI60 by the Developmental Therapeutics Program (DTP) of NCI/NIH provides anticancer compound screening data for\n\nVC The Author 2014.", 
    "Hovering over a cell line name will display the lineage information on the same window\n\nFunding\nThis work was supported by the National Research Foundation of Korea (KRF) grants, the Bio & Medical Technology Development Program (NRF2012M3A9B6055398) and the National Leading Research Lab (NLRL) program (NRF-2011-0028816), funded by the Korean government (MEST)."
  ], 
  "acks": " ", 
  "authors": [
    " Euna Jeong", 
    " Ningning He", 
    " Hyerin Park", 
    " Mee Song", 
    " Nayoung Kim", 
    " Seongjoon Lee", 
    " Sukjoon Yoon"
  ], 
  "keyWords": [
    [
      "cancers", 
      "mace", 
      "genes", 
      "mutational", 
      "chemicals"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-12-24T01:14:33Z"
}{
  "doi": "10.1093/bioinformatics/btu749", 
  "name": "M3D a kernelbased test for spatially correlated changes in methylation profiles", 
  "links": [
    "https://stat.ethz.ch/pipermail/bioconductor/2013-February", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Genome analysis M 3 D: a kernel-based test for spatially correlated changes in methylation profiles", 
  "toolName": "Genome analysis M 3 D: a kernel-based test for spatially correlated changes in methylation profiles", 
  "abstract": "Motivation: DNA methylation is an intensely studied epigenetic mark implicated in many biological processes of direct clinical relevance. Although sequencing-based technologies are increasingly allowing high-resolution measurements of DNA methylation, statistical modelling of such data is still challenging. In particular, statistical identification of differentially methylated regions across different conditions poses unresolved challenges in accounting for spatial correlations within the statistical testing procedure. Results: We propose a non-parametric, kernel-based method, M 3 D, to detect higher order changes in methylation profiles, such as shape, across pre-defined regions. The test statistic explicitly accounts for differences in coverage levels between samples, thus handling in a principled way a major confounder in the analysis of methylation data. Empirical tests on real and simulated data-sets show an increased power compared to established methods, as well as considerable robust-ness with respect to coverage and replication levels. Availability and implementation: R/Bioconductor", 
  "summary": "We then selectively altered the methylation profile of randomly chosen regions in the simulated replicates to create known methylation changes and used the M3D method to test for DMRs. To simulate methylation changes, we randomly selected 250 of the CpG clusters out of a possible 1000.\nOf the 250 differently methylated regions, the M3D method called 232, with no falsely called DMRs. Figures 2(ac) show scatterplots of coverage MMD on the y axis versus full MMD on the x axis for all 1000 regions, with colours denoting the results of the testing procedure using the different statistics.", 
  "affiliations": [
    " IANC School of Informatics University of Edinburgh "
  ], 
  "grants": [
    "]; and the European Research Council [grant number MLCS306999 to G.S.].", 
    "Funding\nThis work was supported by the UK Engineering and Physical Sciences Research Council, Biological Sciences Research Council, and the UK Medical Research Council [grant numbers EP/F500385/1 and BB/F529254/1 to T.M.]"
  ], 
  "acks": " We thank Prof. Rebecca Doerge for kindly sharing the MAGI code with us. ", 
  "authors": [
    " Tom R Mayo", 
    " Gabriele Schweikert", 
    " Guido Sanguinetti"
  ], 
  "keyWords": [
    "methylated regions", 
    [
      "changes", 
      "regional", 
      "data", 
      "testing", 
      "methylation"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-15T04:10:48Z"
}{
  "doi": "10.1093/bioinformatics/btu377", 
  "name": "MAGI a Nodejs web service for fast microRNASeq analysis in a GPU infrastructure", 
  "links": [
    "http://magi", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/3.0"
  ], 
  "title": "Gene expression MAGI: a Node.js web service for fast microRNA-Seq analysis in a GPU infrastructure", 
  "toolName": "Gene expression MAGI: a Node.js web service for fast microRNA-Seq analysis in a GPU infrastructure", 
  "abstract": "MAGI is a web service for fast MicroRNA-Seq data analysis in a graphics processing unit (GPU) infrastructure. Using just a browser, users have access to results as web reports in just a few hours\u20144600% end-to-end performance improvement over state of the art. MAGI's salient features are (i) transfer of large input files in native FASTA with Qualities (FASTQ) format through drag-and-drop operations, (ii) rapid prediction of microRNA target genes leveraging parallel computing with GPU devices, (iii) all-in-one analytics with novel feature extraction, statistical test for differential expression and diagnostic plot generation for quality control and (iv) interactive visu-alization and exploration of results in web reports that are readily available for publication.", 
  "summary": "MAGI's salient features are (i) transfer of large input files in native FASTA with Qualities (FASTQ) format through drag-and-drop operations, (ii) rapid prediction of microRNA target genes leveraging parallel computing with GPU devices, (iii) all-in-one analytics with novel feature extraction, statistical test for differential expression and diagnostic plot generation for quality control and (iv) interactive visualization and exploration of results in web reports that are readily available for publication.\nMAGI addresses these limitations by fully embracing the HTML5-technology with a Node.js-based web service backend for the analysis of miRNA sequencing data directly from FASTQ files.", 
  "affiliations": [
    " Division of Biomedical Informatics University of California at San Diego ", 
    " Department of Biostatistics and Biomedical Informatics Duke University ", 
    " Biomedical Informatics Program School of Informatics University of Applied Sciences Upper Austria "
  ], 
  "grants": [
    "NIH Grant U54HL108460.", 
    "Funding: This work was supported by NIH Grant U54HL 108460."
  ], 
  "acks": " ", 
  "authors": [
    " Jihoon Kim", 
    " Eric Levy", 
    " Alex Ferbrache", 
    " Petra Stepanowsky", 
    " Claudiu Farcas", 
    " Shuang Wang", 
    " Stefan Brunner", 
    " Tyler Bath", 
    " Yuan Wu", 
    " Lucila Ohno-Machado"
  ], 
  "keyWords": [
    [
      "files", 
      "mirnas", 
      "magi", 
      "sequencing", 
      "data", 
      "micrornas"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-06-07T06:24:55Z"
}{
  "doi": "10.1093/bioinformatics/btu702", 
  "name": "MACOED a multiobjective ant colony optimization algorithm for SNP epistasis detection in genomewide association studies", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://www.tgen.org"
  ], 
  "title": "MACOED: a multi-objective ant colony optimization algorithm for SNP epistasis detection in genome-wide association studies", 
  "toolName": "MACOED: a multi-objective ant colony optimization algorithm for SNP epistasis detection in genome-wide association studies", 
  "abstract": "Motivation: The existing methods for genetic-interaction detection in genome-wide association studies are designed from different paradigms, and their performances vary considerably for different disease models. One important reason for this variability is that their construction is based on a single-correlation model between SNPs and disease. Due to potential model preference and disease complexity, a single-objective method will therefore not work well in general, resulting in low power and a high false-positive rate. Method: In this work, we present a multi-objective heuristic optimization methodology named MACOED for detecting genetic interactions. In MACOED, we combine both logistical regression and Bayesian network methods, which are from opposing schools of statistics. The combination of these two evaluation objectives proved to be complementary, resulting in higher power with a lower false-positive rate than observed for optimizing either objective independently. To solve the space and time complexity for high-dimension problems, a memory-based multi-objective ant colony optimization algorithm is designed in MACOED that is able to retain non-dominated solutions found in past iterations. Results: We compared MACOED with other recent algorithms using both simulated and real data-sets. The experimental results demonstrate that our method outperforms others in both detection power and computational feasibility for large datasets.", 
  "summary": "MACOED: a multi-objective ant colony optimization algorithm for SNP epistasis detection in genome-wide association studies\nIn addition, in the comparison experiments among different methods, we compare the mean running time and power for one model (considering both the DME and DNME models) between MACOED, AntEpiSeeker, BEAM and BOOST on different sample sizes N and SNP sizes M.\nBased on the study by North et al., different logistic models used in the first stage of MACOED may partly affect the results of non-dominated solutions (North et al., 2005).", 
  "affiliations": [], 
  "grants": [
    "Funding\nThe National Natural Science Foundation of China [Nos."
  ], 
  "acks": " We are grateful to Miss. Sara Walker for reading through the manuscript. The National Natural Science Foundation of China, Shanghai Science and Technology Commission and a Foundation for the Author of National Excellent Doctoral Dissertation of PR China. Conflict of interest: none declared. ", 
  "authors": [
    " Peng-Jie Jing", 
    " Hong-Bin Shen"
  ], 
  "keyWords": [
    [
      "modeling", 
      "datasets", 
      "algorithms", 
      "methods", 
      "diseases"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-23T00:08:46Z"
}{
  "doi": "10.1093/bioinformatics/btu858", 
  "name": "MADGiC a modelbased approach for identifying driver genes in cancer", 
  "links": [
    "http://cancergenome.nih", 
    "http://cancergenome.nih.gov", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which", 
    "http://www.biostat.wisc.edu", 
    "http://www.sanger.ac.uk/research/projects/cancergenome/).A", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://tcga-data.nci.nih.gov/tcga"
  ], 
  "title": "MADGiC: a model-based approach for identifying driver genes in cancer", 
  "toolName": "MADGiC: a model-based approach for identifying driver genes in cancer", 
  "abstract": "Motivation: Identifying and prioritizing somatic mutations is an important and challenging area of cancer research that can provide new insights into gene function as well as new targets for drug development. Most methods for prioritizing mutations rely primarily on frequency-based criteria, where a gene is identified as having a driver mutation if it is altered in significantly more samples than expected according to a background model. Although useful, frequency-based methods are limited in that all mutations are treated equally. It is well known, however, that some mutations have no functional consequence, while others may have a major deleterious impact. The spatial pattern of mutations within a gene provides further insight into their functional consequence. Properly accounting for these factors improves both the power and accuracy of inference. Also important is an accurate background model. Results: Here, we develop a Model-based Approach for identifying Driver Genes in Cancer (termed MADGiC) that incorporates both frequency and functional impact criteria and accommodates a number of factors to improve the background model. Simulation studies demonstrate advantages of the approach, including a substantial increase in power over competing methods. Further advantages are illustrated in an analysis of ovarian and lung cancer data", 
  "summary": "In summary, the most important sources of information to consider when identifying driver genes include: mutation frequency, mutation type, gene-specific features such as replication timing and expression level that are known to affect background rates of mutation, mutationspecific scores that assess functional impact and the spatial patterning of mutations that only becomes apparent when thousands of samples are considered.", 
  "affiliations": [
    " Department of Statistics", 
    " Department of Biostatistics and Medical Informatics University of Wisconsin "
  ], 
  "grants": [
    "dbNSFP: A lightweight database of human nonsynonymous SNPs and their functional predictions.", 
    "The so-called COSMIC (Catalogue Of Somatic Mutations In Cancer) project was initiated by the NIH in 2004 and is ongoing, with new datasets being added several times per year.", 
    "Funding\nThis work was supported by NIH [GM102756]."
  ], 
  "acks": " The results published here are in whole or part based upon data generated by The Cancer Genome Atlas pilot project established by the NCI and NHGRI. Information about TCGA and the investigators and institutions who constitute the TCGA research network can be found at http://cancergenome.nih. gov/. We thank the anonymous reviewers for their comments and suggestions, which have served to improve the manuscript. This work was supported by NIH. Conflict of Interest: none declared. ", 
  "authors": [
    " Keegan D Korthauer", 
    " Christina Kendziorski"
  ], 
  "keyWords": [
    [
      "mutational", 
      "methods", 
      "genes", 
      "specifically", 
      "cancers", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://www.biostat.wisc.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://tcga-data.nci.nih.gov/tcga"
  ], 
  "technologies": [], 
  "dateCreated": "2015-01-09T02:26:44Z"
}{
  "doi": "10.1093/bioinformatics/btu488", 
  "name": "Masomenos a simple sign averaging method for discrimination in genomic data analysis", 
  "links": [
    "http://www.bitbucket.org/lwaldron/survhd", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://bitbucket.org/lwaldron/survhd.Bernau,C"
  ], 
  "title": "Gene expression M as-o-menos: a simple sign averaging method for discrimination in genomic data analysis", 
  "toolName": "Gene expression M as-o-menos: a simple sign averaging method for discrimination in genomic data analysis", 
  "abstract": "Motivation: The successful translation of genomic signatures into clinical settings relies on good discrimination between patient subgroups. Many sophisticated algorithms have been proposed in the statistics and machine learning literature, but in practice simpler algorithms are often used. However, few simple algorithms have been formally described or systematically investigated. Results: We give a precise definition of a popular simple method we refer to as m as-o-menos, which calculates prognostic scores for discrimination by summing standardized predictors, weighted by the signs of their marginal associations with the outcome. We study its behavior theoretically, in simulations and in an extensive analysis of 27 independent gene expression studies of bladder, breast and ovarian cancer, altogether totaling 3833 patients with survival outcomes. We find that despite its simplicity, m as-o-menos can achieve good discrimination performance. It performs no worse, and sometimes better, than popular and much more CPU-intensive methods for discrimination, including lasso and ridge regression. Availability and Implementation: M as-o-menos is implemented for survival analysis as an option in the survHD package, available from http://www.bitbucket.org/lwaldron/survhd and submitted to Bioconductor.", 
  "summary": "A systematic meta-analysis of prognostic models for late-stage ovarian cancer (Waldron et al., 2014) found that the most common methods in the field, and those used to generate the best-performing models on independent datasets, were of the `univariate ensemble' type, where results of univariate regressions are aggregated to formulate a risk score.\nWe applied mas-o-menos, lasso, ridge, marginal regression and the two negative control methods to an extensive compendium of real cancer gene expression data (Table 2).\nWhile we focused on microarray data and survival endpoints, mas-o-menos can be applied to any type of outcome variable, using any regression model, and has precedents for application in diverse settings outside of genomics (Davis-Stober et al., 2010; Wainer, 1976; Laughlin, 1978; Lovie and Lovie, 1986).", 
  "affiliations": [
    " City University of New York School of Public Health, Hunter College", 
    " Department of Biostatistics Harvard School of Public Health ", 
    " Department of Statistics University of Illinois at Urbana-Champaign "
  ], 
  "grants": [
    "Funding: This work was funded by the National Cancer Institute at the National Institutes of Health (1RC4CA156551-01 and 5P30 CA006516-46 to G.P.)"
  ], 
  "acks": " ", 
  "authors": [
    " Sihai Dave Zhao", 
    " Giovanni Parmigiani", 
    " Curtis Huttenhower", 
    " Levi Waldron"
  ], 
  "keyWords": [
    [
      "predictive", 
      "genes", 
      "data", 
      "methods", 
      "cancer"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-25T04:43:31Z"
}{
  "doi": "10.1093/bioinformatics/btu409", 
  "name": "MAGNA Maximizing Accuracy in Global Network Alignment", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://nd.edu/$cone/MAGNA"
  ], 
  "title": "Systems biology MAGNA: Maximizing Accuracy in Global Network Alignment", 
  "toolName": "Systems biology MAGNA: Maximizing Accuracy in Global Network Alignment", 
  "abstract": "Motivation: Biological network alignment aims to identify similar regions between networks of different species. Existing methods compute node similarities to rapidly identify from possible alignments the high-scoring alignments with respect to the overall node similarity. But, the accuracy of the alignments is then evaluated with some other measure that is different than the node similarity used to construct the alignments. Typically, one measures the amount of conserved edges. Thus, the existing methods align similar nodes between networks hoping to conserve many edges (after the alignment is constructed!). Results: Instead, we introduce MAGNA to directly 'optimize' edge conservation while the alignment is constructed, without decreasing the quality of node mapping. MAGNA uses a genetic algorithm and our novel function for 'crossover' of two 'parent' alignments into a superior 'child' alignment to simulate a 'population' of alignments that 'evolves' over time; the 'fittest' alignments survive and proceed to the next 'generation', until the alignment accuracy cannot be optimized further. While we optimize our new and superior measure of the amount of conserved edges, MAGNA can optimize any alignment accuracy measure, including a combined measure of both node and edge conservation. In systematic evaluations against state-of-the-art methods (IsoRank, MI-GRAAL and GHOST), on both synthetic networks and real-world biological data, MAGNA outperforms all of the existing methods, in terms of both node and edge conservation as well as both topological and biological alignment accuracy.", 
  "summary": "Running times (x-axis) and S3 scores (y-axis) of MAGNA (when S3 is optimized on random initial population) and of the existing methods (IsoRank, MI-GRAAL and GHOST), when aligning yeasthuman (YH), C.jejuniE.coli (CE) or MesorhizobiumSynechocystis (MS) networks\nAlso, over all alignments of real networks and all biological alignment quality analyses (Supplementary Figs S8, S9, S12, S13 and S15), in majority (70%) of cases, MAGNA's optimization of topological quality of the existing methods (including random alignments) actually improves the existing methods' biological quality as well, with respect to at least one of the optimization measures.", 
  "affiliations": [
    " Department of Computer Science Brown University ", 
    " Department of Computer Science and Engineering University of Notre Dame "
  ], 
  "grants": [
    "Funding: National Science Foundation (NSF) CCF-1319469 and NSF EAGER CCF-1243295 grants."
  ], 
  "acks": " The authors thank Dr H. Bunke for suggestions on MAGNA parameters, Drs R. Patro and C. Kingsford for assistance with GHOST and Drs O. Kuchaiev and N. Pr zulj for the data from MI-GRAAL study. ", 
  "authors": [
    " Vikram Saraph", 
    " Tijana Milenkovic\u00b41milenkovic\u00b4milenkovic\u00b41", 
    " "
  ], 
  "keyWords": [
    [
      "methods", 
      "biologically", 
      "optimization", 
      "magna", 
      "alignments", 
      "networks"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-12T03:16:15Z"
}{
  "doi": "10.1093/bioinformatics/btu717", 
  "name": "Mass spectrometrybased protein identification with accurate statistical significance assignment", 
  "links": [
    "http://www.matrixscience.com/help.html", 
    "http://www.thermofisher.com/en/home.html", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.proteomesoftware.com"
  ], 
  "title": "Mass spectrometry-based protein identification with accurate statistical significance assignment", 
  "toolName": "Mass spectrometry-based protein identification with accurate statistical significance assignment", 
  "abstract": "Motivation: Assigning statistical significance accurately has become increasingly important as metadata of many types, often assembled in hierarchies, are constructed and combined for further biological analyses. Statistical inaccuracy of metadata at any level may propagate to downstream analyses, undermining the validity of scientific conclusions thus drawn. From the perspective of mass spectrometry-based proteomics, even though accurate statistics for peptide identification can now be achieved, accurate protein level statistics remain challenging. Results: We have constructed a protein ID method that combines peptide evidences of a candidate protein based on a rigorous formula derived earlier; in this formula the database P-value of every peptide is weighted, prior to the final combination, according to the number of proteins it maps to. We have also shown that this protein ID method provides accurate protein level E-value, eliminating the need of using empirical post-processing methods for type-I error control. Using a known protein mixture, we find that this protein ID method, when combined with the Soric\u00b4formulaSoric\u00b4Soric\u00b4formula, yields accurate values for the proportion of false discoveries. In terms of retrieval efficacy, the results from our method are comparable with other methods tested. Availability and implementation: The source code, implemented in C\u00fe\u00fe on a linux system, is available for download at ftp://ftp.ncbi.nlm.nih.gov/pub/qmbp/qmbp_ms/RAId/RAId_Linux_64Bit.", 
  "summary": "Results: We have constructed a protein ID method that combines peptide evidences of a candidate protein based on a rigorous formula derived earlier; in this formula the database P-value of every peptide is weighted, prior to the final combination, according to the number of proteins it maps to.\nMethods belonging to the second group, capable of assigning per-spectrum perpeptide significance, can properly prioritize identified peptides when reported P-values/E-values are accurate; but the needed statistical accuracy is often unattainable due to improper heuristics or unjustifiable distribution assumptions (Alves et al., 2007; Segal, 2008; Spirin et al., 2011).", 
  "affiliations": [
    " National Center for Biotechnology Information National Library of Medicine National Institutes of Health "
  ], 
  "grants": [
    "Funding\nThis work was supported by the Intramural Research Program of the National Library of Medicine at the National Institutes of Health."
  ], 
  "acks": " ", 
  "authors": [
    " Gelio Alves", 
    " Yi-Kuo Yu"
  ], 
  "keyWords": [
    [
      "scoring", 
      "methods", 
      "proteins", 
      "peptides", 
      "values", 
      "statistical"
    ]
  ], 
  "sourcelinks": [
    "http://www.matrixscience.com/help.html", 
    "http://www.thermofisher.com/en/home.html", 
    "http://www.proteomesoftware.com", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-01T07:11:58Z"
}{
  "doi": "10.1093/bioinformatics/btu532", 
  "name": "MEGADOCK 40 an ultrahighperformance proteinprotein docking software for heterogeneous supercomputers", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bi.cs.titech.ac.jp", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Structural bioinformatics MEGADOCK 4.0: an ultra\u2013high-performance protein\u2013protein docking software for heterogeneous supercomputers", 
  "toolName": "Structural bioinformatics MEGADOCK 4.0: an ultra\u2013high-performance protein\u2013protein docking software for heterogeneous supercomputers", 
  "abstract": "The application of protein\u2013protein docking in large-scale interactome analysis is a major challenge in structural bioinformatics and requires huge computing resources. In this work, we present MEGADOCK 4.0, an FFT-based docking software that makes extensive use of recent heterogeneous supercomputers and shows powerful , scalable performance of 497% strong scaling. Availability and Implementation: MEGADOCK 4.0 is written in C+ + with OpenMPI and NVIDIA CUDA 5.0 (or later) and is freely available to all academic and non-profit users at:", 
  "summary": "et al., 2004; Kro l et al., 2007), cross-docking for identification of protein interaction partners (Lopes et al., 2013; Matsuzaki et al., 2009; Wass et al., 2011; Zhang et al., 2014) and multiple docking (Karaca and Bonvin, 2011).\nIts original scoring function, based on shape complementarity, electrostatics and desolvation free energy, is calculated by only one correlation function (Ohue et al., 2012, 2014).\nThis is advantageous for faster calculation because multiple correlation functions and thus multiple FFT calculations are used to evaluate multiple effects in previous methods (Kozakov et al., 2006; Pierce et al., 2011).\nSci. USA, 89, 21952199.\nSci., 7632, 178187.\n(2013) MEGADOCK-GPU: Acceleration of proteinprotein docking calculation on GPUs. In: Proceedings of the ACM-BCB'13.", 
  "affiliations": [
    " Education Academy of Computational Life Sciences (ACLS) Tokyo Institute of Technology ", 
    " Department of Computer Science Graduate School of Information Science and Engineering Tokyo Institute of Technology ", 
    " Japan Society for the Promotion of Science (JSPS)"
  ], 
  "grants": [
    "Funding: this work was supported by a Grant-in-Aid for JSPS Fellows (238750 and 2630002) from the Ministry of Education, Culture, Sports, Science and Technology of Japan (MEXT)."
  ], 
  "acks": " This research used computational resources of the TSUBAME 2.5 supercomputer provided by Tokyo Institute of Technology through the HPCI System Research project (Project ID: hp140173). ", 
  "authors": [
    " Masahito Ohue", 
    " Takehiro Shimoda", 
    " Shuji Suzuki", 
    " Yuri Matsuzaki", 
    " Takashi Ishida", 
    " Yutaka Akiyama", 
    " Alfonso Valencia", 
    " "
  ], 
  "keyWords": [
    [
      "computational", 
      "megadock", 
      "proteins", 
      "dockings", 
      "bioinformatics", 
      "nodes"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C++"
  ], 
  "dateCreated": "2014-08-07T03:03:22Z"
}{
  "doi": "10.1093/bioinformatics/btv033", 
  "name": "MEGAHIT an ultrafast singlenode solution for large and complex metagenomics assembly via succinct de Bruijn graph", 
  "links": [
    "https://github", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "MEGAHIT: an ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph", 
  "toolName": "github", 
  "abstract": "MEGAHIT is a NGS de novo assembler for assembling large and complex metagenomics data in a time-and cost-efficient manner. It finished assembling a soil metagenomics dataset with 252 Gbps in 44.1 and 99.6 h on a single computing node with and without a graphics processing unit, respectively. MEGAHIT assembles the data as a whole, i.e. no pre-processing like partitioning and normalization was needed. When compared with previous methods on assembling the soil data, MEGAHIT generated a three-time larger assembly, with longer contig N50 and average contig length; furthermore, 55.8% of the reads were aligned to the assembly, giving a fourfold improvement. Availability and implementation: The source code of MEGAHIT is freely available at https://github. com/voutcn/megahit under GPLv3", 
  "summary": "When compared with previous methods on assembling the soil data, MEGAHIT generated a three-time larger assembly, with longer contig N50 and average contig length; furthermore, 55.8% of the reads were aligned to the assembly, giving a fourfold improvement.\nTo evaluate the performance on large scale metagenomics data, we assembled an Iowa prairie soil metagenomics dataset that comprises 3.3 billion reads totaling 252 billion base-pairs (Howe et al., 2014) using MEGAHIT and Minia, another memory-efficient assembler (Chikhi and Rizk, 2012).", 
  "affiliations": [
    " L3 Bioinformatics Limited", 
    " Department of Computer Science HKU-BGI Bioinformatics Algorithms Research Laboratory University of Hong Kong ", 
    " National Institute of Informatics"
  ], 
  "grants": [
    "Funding\nThis work was funded by Hong Kong GRF (General Research Fund) HKU713512E and ITF (Innovation and Technology Fund) GHP/011/12."
  ], 
  "sourcelinks": [
    "https://github"
  ], 
  "acks": " The authors thank S.M. Yiu, C.M. Leung and Y. Peng for the detailed explanation about IDBA-UD. The authors also thank C. Titus Brown for providing the open evaluation with the E.coli data (Table 1). This work was funded by Hong Kong GRF (General Research Fund) HKU- 713512E and ITF (Innovation and Technology Fund) GHP/011/12. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Conflict of Interest: None declared. ", 
  "authors": [
    " Dinghua Li", 
    " Chi-Man Liu", 
    " Ruibang Luo", 
    " Kunihiko Sadakane", 
    " Tak-Wah Lam"
  ], 
  "keyWords": [
    "metagenomics assembly", 
    [
      "assembling", 
      "metagenomic", 
      "memory", 
      "bioinformatics", 
      "sequencing", 
      "megahit"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "BSD 3-clause \"New\" or \"Revised\" License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/michael/github/master/LICENSE"
      }
    ], 
    "name": "github", 
    "contributors": [
      {
        "contributions": 88, 
        "html_url": "https://github.com/AurelioDeRosa"
      }, 
      {
        "contributions": 44, 
        "html_url": "https://github.com/clayreimann"
      }, 
      {
        "contributions": 41, 
        "html_url": "https://github.com/ingalls"
      }, 
      {
        "contributions": 38, 
        "html_url": "https://github.com/aendrew"
      }, 
      {
        "contributions": 31, 
        "html_url": "https://github.com/mattpass"
      }, 
      {
        "contributions": 12, 
        "html_url": "https://github.com/darvin"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/iamdanfox"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/coderaiser"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/ctalau"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/STRd6"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/captn3m0"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/jlord"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/knsh14"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/mtscout6"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/maxogden"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/randalpinto"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/raphink"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/tricknotes"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/ele828"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/kpdecker"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/tristen"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/PeterDaveHello"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/arosenberg01"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/BernhardBezdek"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/mscdex"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/cassioscabral"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/dafortune"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/klcodanr"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/divergentdave"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/incrop"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.3.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.3.0", 
        "name": "v2.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.2.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.2.0", 
        "name": "v2.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.1.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.1.0", 
        "name": "v2.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.0.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.0.0", 
        "name": "v2.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.3.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.3.0", 
        "name": "v1.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.2.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.2.1", 
        "name": "v1.2.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.2.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.2.0", 
        "name": "v1.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.1.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.1.0", 
        "name": "v1.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.0.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.0.0", 
        "name": "v1.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.7", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.7", 
        "name": "v0.10.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.6", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.6", 
        "name": "v0.10.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.5", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.5", 
        "name": "v0.10.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.4", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.4", 
        "name": "v0.10.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.3", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.3", 
        "name": "v0.10.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.2", 
        "name": "v0.10.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.1", 
        "name": "v0.10.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.0", 
        "name": "v0.10.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.9.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.9.2", 
        "name": "v0.9.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.9.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.9.0", 
        "name": "v0.9.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.8.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.8.1", 
        "name": "v0.8.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.8.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.8.0", 
        "name": "v0.8.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.7.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.7.0", 
        "name": "v0.7.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.6.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.6.0", 
        "name": "v0.6.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.2", 
        "name": "v0.5.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.1", 
        "name": "v0.5.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.0", 
        "name": "v0.5.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.4.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.4.0", 
        "name": "v0.4.0"
      }
    ], 
    "created_at": "2012-03-06T18:08:53Z", 
    "updated_at": "2016-08-09T15:55:59Z", 
    "languages": [
      "Shell", 
      "JavaScript", 
      "HTML"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/GamesDev"
      }, 
      {
        "html_url": "https://github.com/fokusferit"
      }, 
      {
        "html_url": "https://github.com/rocel"
      }, 
      {
        "html_url": "https://github.com/thinkxl"
      }, 
      {
        "html_url": "https://github.com/schlagobers"
      }, 
      {
        "html_url": "https://github.com/jasonyandell"
      }, 
      {
        "html_url": "https://github.com/t0n1"
      }, 
      {
        "html_url": "https://github.com/timrchavez"
      }, 
      {
        "html_url": "https://github.com/apsrose"
      }, 
      {
        "html_url": "https://github.com/Bediako"
      }, 
      {
        "html_url": "https://github.com/kooyeed"
      }, 
      {
        "html_url": "https://github.com/PivotLogix"
      }, 
      {
        "html_url": "https://github.com/Timothee"
      }, 
      {
        "html_url": "https://github.com/esimionato"
      }, 
      {
        "html_url": "https://github.com/alixcan"
      }, 
      {
        "html_url": "https://github.com/FrediBach"
      }, 
      {
        "html_url": "https://github.com/antiface"
      }, 
      {
        "html_url": "https://github.com/ghostx2013"
      }, 
      {
        "html_url": "https://github.com/dnordstrom"
      }, 
      {
        "html_url": "https://github.com/bernardoantunes"
      }, 
      {
        "html_url": "https://github.com/petrosh"
      }, 
      {
        "html_url": "https://github.com/cloudtrends"
      }, 
      {
        "html_url": "https://github.com/cgkio"
      }, 
      {
        "html_url": "https://github.com/greenflag"
      }, 
      {
        "html_url": "https://github.com/loopByte"
      }, 
      {
        "html_url": "https://github.com/hasithaAlex"
      }, 
      {
        "html_url": "https://github.com/malei0311"
      }, 
      {
        "html_url": "https://github.com/majicmike"
      }, 
      {
        "html_url": "https://github.com/wuwenvogue"
      }, 
      {
        "html_url": "https://github.com/ingalls"
      }
    ], 
    "owner": "https://github.com/michael", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2015-01-22T03:50:58Z"
}{
  "doi": "10.1093/bioinformatics/btu742", 
  "name": "MeRP a highthroughput pipeline for Mendelian randomization analysis", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://py-merp", 
    "http://github.com/py-merp/py-merp"
  ], 
  "title": "MeRP: a high-throughput pipeline for Mendelian randomization analysis", 
  "toolName": "py-merp", 
  "abstract": "We present a Mendelian randomization (MR) pipeline (MeRP) to facilitate rapid, causal inference analysis through automating key steps in developing and analyzing genetic instruments obtained from publicly available data. Our tool uses the National Human Genome Research Institute catalog of associations to generate instrumental variable trait files and provides methods for filtering of potential confounding associations as well as linkage disequilibrium. MeRP generates estimated causal effect scores via a MR-score analysis using summary data for disease end-points typically found in the public domain. We utilize our pipeline to develop genetic instruments for seven traits and evaluate potential causal relationships with two disease endpoints, observing two putatively causal associations between blood pressure and bone-mineral density with type 2 diabetes. Our tool emphasizes the importance of careful but systematic screening of large datasets for discovery and systematic follow-up. Availability and implementation: MeRP is a free, open-source project and can be downloaded at", 
  "summary": "The surge in genome-wide association studies (GWAS) has generated an abundance of single-nucleotide polymorphisms (SNPs) associated with a wide range of phenotypes, resulting in a rapidly growing body of research employing a statistical method utilizing SNPs as instrumental variables to draw causal inference between traits and diseases (Ebrahim and Davey-Smith, 2008; Herna n and Robins, 2006).\nWe applied MeRP to generate IVFs corresponding to seven different traits and estimated the causal effect score with coronary heart disease (CHD) in up to 22 233 cases and 64 762 controls, and type 2 diabetes (T2D) in up to 34 840 cases and 114 981 controls in two stages, both from publicly available data (see Supplementary Note).", 
  "affiliations": [
    " Department of Biology College of Arts and Sciences University of Pennsylvania "
  ], 
  "grants": [], 
  "sourcelinks": [
    "http://github.com/py-merp/py-merp"
  ], 
  "acks": " ", 
  "authors": [
    " Peter Yin", 
    " Benjamin F Voight"
  ], 
  "keyWords": [
    "association studies", 
    [
      "associations", 
      "genetics", 
      "merp", 
      "study", 
      "traits", 
      "causality"
    ]
  ], 
  "github_data": {
    "name": "py-merp", 
    "contributors": [
      {
        "contributions": 49, 
        "html_url": "https://github.com/peteryin"
      }, 
      {
        "contributions": 13, 
        "html_url": "https://github.com/peteryin21"
      }
    ], 
    "versions": [], 
    "created_at": "2014-07-22T00:08:07Z", 
    "updated_at": "2015-10-14T14:56:36Z", 
    "languages": [
      "Python", 
      "R"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/peteryin21"
      }, 
      {
        "html_url": "https://github.com/bvoight"
      }
    ], 
    "owner": "https://github.com/peteryin21", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-11-12T04:44:24Z"
}{
  "doi": "10.1093/bioinformatics/btu509", 
  "name": "Metabomxtr an R package for mixturemodel analysis of nontargeted metabolomics data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0"
  ], 
  "title": "Genetics and population analysis Metabomxtr: an R package for mixture-model analysis of non-targeted metabolomics data", 
  "toolName": "Genetics and population analysis Metabomxtr: an R package for mixture-model analysis of non-targeted metabolomics data", 
  "abstract": "Non-targeted metabolomics technologies often yield data in which abundance for any given metabolite is observed and quantified for some samples and reported as missing for other samples. Apparent missingness can be due to true absence of the metabolite in the sample or presence at a level below detectability. Mixture-model analysis can formally account for metabolite 'missingness' due to absence or undetectability, but software for this type of analysis in the high-throughput setting is limited. The R package metabomxtr has been developed to facilitate mixture-model analysis of non-targeted metabolomics data in which only a portion of samples have quantifi-able abundance for certain metabolites.", 
  "summary": "The R package metabomxtr has been developed to facilitate mixture-model analysis of non-targeted metabolomics data in which only a portion of samples have quantifiable abundance for certain metabolites.\nThe R package metabomxtr facilitates mixture-model analysis of non-targeted metabolomics data.\nRe-analysis of the HAPO pilot metabolomics data indicates that mixture-model analysis detects metabolites identified by other common imputation approaches and additionally identifies associations that would otherwise be missed.\nA total of 49 non-targeted metabolites with at least five missing values were analyzed using mixture modeling as well as minimum imputation and five nearest neighbors.", 
  "affiliations": [
    " Department of Medicine Division of Endocrinology, Metabolism, and Molecular Medicine Feinberg School of Medicine Northwestern University ", 
    " Sarah W. Stedman Nutrition and Metabolism Center Duke Molecular Physiology Institute ", 
    " Department of Preventive Medicine Division of Biostatistics Feinberg School of Medicine Northwestern University "
  ], 
  "grants": [
    "Funding: (R01-HD34242 and R01-HD34243) from the National Institute of Child Health and Human Development and the National Institute of Diabetes, Digestive and Kidney Diseases, by the National Center for Research Resources (M01-RR00048, M01-RR00080) and by the American Diabetes Association and Friends of Prentice."
  ], 
  "acks": " ", 
  "authors": [
    " Michael Nodzenski", 
    " Michael J Muehlbauer", 
    " James R Bain", 
    " Anna C Reisetter", 
    " William L Lowe", 
    " Denise M Scholtens", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    "non metabolomics data", 
    [
      "modeling", 
      "metabolites", 
      "mixture", 
      "analysis"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C", 
    "R"
  ], 
  "dateCreated": "2014-07-30T00:14:11Z"
}{
  "doi": "10.1093/bioinformatics/btu715", 
  "name": "Measuring the wisdom of the crowds in networkbased gene function inference", 
  "links": [
    "http://gillislab.cshl.edu/supplements/.performance", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://gillislab.cshl.edu/supplements"
  ], 
  "title": "Data and text mining Measuring the wisdom of the crowds in network-based gene function inference", 
  "toolName": "Data and text mining Measuring the wisdom of the crowds in network-based gene function inference", 
  "abstract": "Motivation: Network-based gene function inference methods have proliferated in recent years, but measurable progress remains elusive. We wished to better explore performance trends by controlling data and algorithm implementation, with a particular focus on the performance of aggregate predictions. Results: Hypothesizing that popular methods would perform well without hand-tuning, we used well-characterized algorithms to produce verifiably 'untweaked' results. We find that most state-of-the-art machine learning methods obtain 'gold standard' performance as measured in critical assessments in defined tasks. Across a broad range of tests, we see close alignment in algorithm performances after controlling for the underlying data being used. We find that algorithm aggrega-tion provides only modest benefits, with a 17% increase in area under the ROC (AUROC) above the mean AUROC. In contrast, data aggregation gains are enormous with an 88% improvement in mean AUROC. Altogether, we find substantial evidence to support the view that additional algorithm development has little to offer for gene function prediction. Availability and implementation: The supplementary information contains a description of the algorithms, the network data parsed from different biological data resources and a guide to the source code (available at: http://gillislab.cshl.edu/supplements/).", 
  "summary": "To overcome this, we will first benchmark a set of machine learning algorithms based upon data resources available from MouseFunc to establish they are representative samples of high-performing methods.\nGene function prediction performance benefits strongly from data aggregation.\nSupported by these general findings, more focused assessments have also concluded that simply aggregating algorithms on the same data offers enormous benefits, again, on tasks closely related to but not identical to function prediction, e.g. mutation prioritization (ManChon et al., 2014).", 
  "affiliations": [
    " Stanley Institute for Cognitive Genomics Cold Spring Harbor Laboratory "
  ], 
  "grants": [
    "No funding source played any role in the design, in the collection, analysis and interpretation of data; in the writing of the manuscript; and in the decision to submit the manuscript for publication.", 
    "were supported by a grant from T. and V. Stanley.", 
    "Funding\nJ.G., W.V."
  ], 
  "acks": " the Matlab implementation of GeneMANIA. J.G., W.V. and S.B. were supported by a grant from T. and V. Stanley. No funding source played any role in the design, in the collection, analysis and interpretation of data; in the writing of the manuscript; and in the decision to submit the manuscript for publication. Conflict of Interest: none declared. ", 
  "authors": [
    " W Verleyen", 
    " S Ballouz", 
    " J Gillis"
  ], 
  "keyWords": [
    [
      "algorithmic", 
      "methods", 
      "aggregating", 
      "performances", 
      "genes", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://gillislab.cshl.edu/supplements/.performance", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://gillislab.cshl.edu/supplements"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-31T00:44:56Z"
}{
  "doi": "10.1093/bioinformatics/btu584", 
  "name": "Merging of multistring BWTs with applications", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://code.google.com/p"
  ], 
  "title": "Merging of multi-string BWTs with applications", 
  "toolName": "Merging of multi-string BWTs with applications", 
  "abstract": "Motivation: The throughput of genomic sequencing has increased to the point that is overrunning the rate of downstream analysis. This, along with the desire to revisit old data, has led to a situation where large quantities of raw, and nearly impenetrable, sequence data are rapidly filling the hard drives of modern biology labs. These datasets can be compressed via a multi-string variant of the Burrows\u2013Wheeler Transform (BWT), which provides the side benefit of searches for arbitrary k-mers within the raw data as well as the ability to reconstitute arbitrary reads as needed. We propose a method for merging such datasets for both increased compression and downstream analysis. Results: We present a novel algorithm that merges multi-string BWTs in O\u00f0LCS \u00c2 N\u00de time where LCS is the length of their longest common substring between any of the inputs, and N is the total length of all inputs combined (number of symbols) using O\u00f0N \u00c2 log 2 \u00f0F\u00de\u00de bits where F is the number of multi-string BWTs merged. This merged multi-string BWT is also shown to have a higher compressibility compared with the input multi-string BWTs separately. Additionally, we explore some uses of a merged multi-string BWT for bioinformatics applications. Availability and implementation: The MSBWT package is available through PyPI with source code located at https://code.google.com", 
  "summary": "Our algorithm merges two or more multi-string BWTs directly without any search index or the need to reconstitute any string or suffix of the input BWTs. The only auxiliary data structures required are two interleave arrays, which identify the input source of each symbol in the final result, so the only auxiliary data structures used by the algorithm are stored as part of the result.\nEach data point is a merge between two multi-string BWTs (CAST/EiJ and WSB/EiJ) where each BWT contains a randomly sampled collection of read sequences of length 100 bp aligned to the mouse mitochondria.", 
  "affiliations": [
    " Department of Computer Science UNC-CH "
  ], 
  "grants": [
    "Funding: The authors thank the Jackson Lab Center for Genome Dynamics: Evolution, Organization and Function (NIH P50GM076468) and the University of North Carolina Center for CISGen: Systems Genetics of Psychiatric Disorders (NIH P50HG006582, P50MH090338) for their funding."
  ], 
  "acks": " They thank Fernando Pardo Manuel de Villena for his advice and the University of North Carolina Bioinformatics group for their support in this work. They would also like to thank the three anonymous reviewers for their insightful questions and comments. Funding: The authors thank the Jackson Lab Center for Genome Dynamics: Evolution, Organization and Function (NIH P50GM076468) and the University of North Carolina Center for CISGen: Systems Genetics of Psychiatric Disorders (NIH P50HG006582, P50MH090338) for their funding. Conflict of interest: none declared. ", 
  "authors": [
    " James Holt", 
    " Leonard Mcmillan"
  ], 
  "keyWords": [
    [
      "genomics", 
      "bwts", 
      "merging", 
      "algorithms", 
      "compressibility", 
      "data", 
      "strings"
    ]
  ], 
  "sourcelinks": [
    "https://code.google.com/p"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-30T00:28:26Z"
}{
  "doi": "10.1093/bioinformatics/btu566", 
  "name": "MethylAid visual and interactive quality control of large Illumina 450k datasets", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetics and population analysis MethylAid: visual and interactive quality control of large Illumina 450k datasets", 
  "toolName": "Genetics and population analysis MethylAid: visual and interactive quality control of large Illumina 450k datasets", 
  "abstract": "The Illumina 450k array is a frequently used platform for large-scale genome-wide DNA methylation studies, i.e. epigenome-wide association studies. Currently, quality control of 450k data can be performed with Illumina's GenomeStudio and is part of a limited number 450k analysis pipelines. However, GenomeStudio cannot handle large-scale studies, and existing pipelines provide limited options for quality control and neither support interactive exploration by the user. To aid the detection of bad-quality samples in large-scale genome-wide DNA methylation studies as flexible and transparent as possible, we have developed MethylAid; a visual and interactive Web application using RStudio's shiny package. Bad-quality samples are detected using sample-dependent and sample-independent quality control probes present on the array and user-adjustable thresholds. In-depth exploration of bad-quality samples can be performed using several interactive diagnostic plots. Furthermore, plots can be annotated with user-provided metadata, for example, to identify outlying batches. Our new tool makes quality assessment of 450k array data interactive, flexible and efficient and is, therefore, expected to be useful for both data analysts and core facilities. Availability and implementation: MethylAid is implemented as an R/Bioconductor package (www.bioconductor.org/packages/3.0/bioc /html/MethylAid.html). A demo application is available from shiny. bioexp.nl/MethylAid. Contact: m.van_iterson@lumc.nl", 
  "summary": "To aid the detection of bad-quality samples in large-scale genomewide DNA methylation studies as flexible and transparent as possible, we have developed MethylAid; a visual and interactive Web application using RStudio's shiny package.\nThe Web application produces multiple interactive diagnostic plots on the basis of the quality control probes present on the 450k array that facilitate the detection of bad-quality samples irrespective of sample size.", 
  "affiliations": [
    " Department of Molecular Epidemiology Leiden University Medical Center "
  ], 
  "grants": [
    "Funding: This work was done within the framework of the Biobank-based Integrative Omics Study (BIOS) Consortium funded by BBMRI-NL, a Research Infrastructure financed by the Netherlands Organization for Scientific Research (NWO 184.021.007) and financially supported by the European Union's Seventh Framework Program (FP7/2007-2011) under grant agreement No."
  ], 
  "acks": " The authors thank Jenny van Dongen for beta testing. ", 
  "authors": [
    " Maarten Van Iterson", 
    " Elmar W Tobi", 
    " Roderick C Slieker", 
    " Ren E Luijk", 
    " P Eline Slagboom", 
    " Bastiaan T Heijmans", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    [
      "methylation", 
      "quality", 
      "data", 
      "samples", 
      "packages", 
      "studies", 
      "plotting"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C", 
    "R"
  ], 
  "dateCreated": "2014-08-22T03:08:27Z"
}{
  "doi": "10.1093/bioinformatics/btu705", 
  "name": "MetMSLine an automated and fully integrated pipeline for rapid processing of highresolution LCMS metabolomic datasets", 
  "links": [
    "http://creativecommons.org/licenses/by-nc/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://wmbedmands.github.io/MetMSLine"
  ], 
  "title": "MetMSLine: an automated and fully integrated pipeline for rapid processing of high-resolution LC\u2013MS metabolomic datasets", 
  "toolName": "MetMSLine", 
  "abstract": "MetMSLine represents a complete collection of functions in the R programming language as an accessible GUI for biomarker discovery in large-scale liquid-chromatography high-resolution mass spectral datasets from acquisition through to final metabolite identification forming a backend to output from any peak-picking software such as XCMS. MetMSLine automatically creates subdirectories, data tables and relevant figures at the following steps: (i) signal smoothing, normalization, filtration and noise transformation (PreProc.QC.LSC.R); (ii) PCA and automatic outlier removal (Auto.PCA.R); (iii) automatic regression, biomarker selection, hierarchical clustering and cluster ion/artefact identification (Auto.MV.Regress.R); (iv) Biomarker\u2014MS/MS fragmentation spectra matching and fragment/neutral loss annotation (Auto.MS.MS.match.R) and (v) semi-targeted metabolite identification based on a list of theoretical masses obtained from public databases (DBAnnotate.R). Availability and implementation: All source code and suggested parameters are available in an un-encapsulated layout on http://wmbedmands.github.io/MetMSLine/. Readme files and a synthetic dataset of both X-variables (simulated LC\u2013MS data), Y-variables (simulated continuous variables) and metabolite theoretical masses are also available on our GitHub repository.", 
  "summary": "MetMSLine automatically creates subdirectories, data tables and relevant figures at the following steps: (i) signal smoothing, normalization, filtration and noise transformation (PreProc.QC.LSC.R); (ii) PCA and automatic outlier removal (Auto.PCA.R); (iii) automatic regression, biomarker selection, hierarchical clustering and cluster ion/artefact identification (Auto.MV.Regress.R); (iv) Biomarker--MS/MS fragmentation spectra matching and fragment/neutral loss annotation (Auto.MS.MS.match.R) and (v) semi-targeted metabolite identification based on a list of theoretical masses obtained from public databases (DBAnnotate.R).", 
  "affiliations": [
    " Department of Biomarkers, Nutrition and Metabolism Section International Agency for Research on Cancer (IARC) "
  ], 
  "grants": [
    "urine dilution) is followed by untargeted MS and MS/MS data acquisition in sequence and peak picking softwares MetMSLine then performs sequentially: signal drift correction and pre-processing (Step 1), automatic PCA-based outlier removal (Step 2) (samples  black, QCs  red, outliers  green), automatic iterative regression based on continuous Y-variables supplied and cluster ion identification (Step 3) and final identification by data-dependent MS/MS and database matching (Step 4)\n\nFunding\nThis work was supported by the European Union (NutriTech and the European Cancer Platform (EUROCAN) [grant No."
  ], 
  "sourcelinks": [
    "http://wmbedmands.github.io/MetMSLine"
  ], 
  "acks": " ", 
  "authors": [
    " William M B Edmands", 
    " Dinesh K Barupal", 
    " Augustin Scalbert"
  ], 
  "keyWords": [
    [
      "masses", 
      "processing", 
      "metmsline", 
      "steps", 
      "biomarkers", 
      "data"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v2.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/WMBEdmands/MetMSLine/master/LICENSE"
      }
    ], 
    "name": "MetMSLine", 
    "contributors": [
      {
        "contributions": 9, 
        "html_url": "https://github.com/WMBEdmands"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/WMBEdmands/MetMSLine/zipball/v1.0.0", 
        "tarball_url": "https://api.github.com/repos/WMBEdmands/MetMSLine/tarball/v1.0.0", 
        "name": "v1.0.0"
      }
    ], 
    "created_at": "2013-11-27T10:16:58Z", 
    "updated_at": "2016-05-19T04:07:36Z", 
    "languages": [
      "R"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/WMBEdmands"
      }, 
      {
        "html_url": "https://github.com/wenbostar"
      }
    ], 
    "owner": "https://github.com/WMBEdmands", 
    "homepage": ""
  }, 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-28T04:40:37Z"
}{
  "doi": "10.1093/bioinformatics/btu791", 
  "name": "MetaPSICOV combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins", 
  "links": [
    "http://bioinf.cs.ucl.ac.uk/downloads/MetaPSICOV", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins", 
  "toolName": "MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins", 
  "abstract": "Motivation: Recent developments of statistical techniques to infer direct evolutionary couplings between residue pairs have rendered covariation-based contact prediction a viable means for accurate 3D modelling of proteins, with no information other than the sequence required. To extend the usefulness of contact prediction, we have designed a new meta-predictor (MetaPSICOV) which combines three distinct approaches for inferring covariation signals from multiple sequence alignments , considers a broad range of other sequence-derived features and, uniquely, a range of met-rics which describe both the local and global quality of the input multiple sequence alignment. Finally, we use a two-stage predictor, where the second stage filters the output of the first stage. This two-stage predictor is additionally evaluated on its ability to accurately predict the long range network of hydrogen bonds, including correctly assigning the donor and acceptor residues. Results: Using the original PSICOV benchmark set of 150 protein families, MetaPSICOV achieves a mean precision of 0.54 for top-L predicted long range contacts\u2014around 60% higher than PSICOV, and around 40% better than CCMpred. In de novo protein structure prediction using FRAGFOLD, MetaPSICOV is able to improve the TM-scores of models by a median of 0.05 compared with PSICOV. Lastly, for predicting long range hydrogen bonding, MetaPSICOV-HB achieves a precision of 0.69 for the top-L/10 hydrogen bonds compared with just 0.26 for the baseline MetaPSICOV. Availability and implementation: MetaPSICOV is available as a freely available web server at", 
  "summary": "Results: Using the original PSICOV benchmark set of 150 protein families, MetaPSICOV achieves a mean precision of 0.54 for top-L predicted long range contacts--around 60% higher than PSICOV, and around 40% better than CCMpred.\nand both stages of MetaPSICOV for the top-L/10, top-L/5, top-L/2 and top-L contacts where L is the length of the target protein, the thresholds used commonly in contact prediction assessment methods, for sequence separation !5 and !23 simultaneously (Ezkurdia et al., 2009).\nData are the top L/2 correct contact predictions from each method (for sequence separation >5 residues), for a set of 150 Pfam families (Jones et al., 2012)", 
  "affiliations": [
    " Department of Computer Science Bioinformatics Group University College London "
  ], 
  "grants": [
    "and grant number 096622/Z/11/ Z to S.T.].", 
    "Although benchmarks clearly show that more correct contacts are predicted by the second stage of the MetaPSICOV, the added bias in fact results in generally poorer 3D models when the contacts are used for de novo structure prediction using\n\nFunding\nThis study was funded by the Biotechnology and Biological Sciences Research Council [grant number BB/L018330/1; T.S.", 
    "and the Wellcome Trust [grant number 096624/Z/11/Z to T.K."
  ], 
  "acks": " ", 
  "authors": [
    " David T Jones", 
    " Tanya Singh", 
    " Tomasz Kosciolek", 
    " Stuart Tetchner"
  ], 
  "keyWords": [
    [
      "methods", 
      "proteins", 
      "predictions", 
      "metapsicov", 
      "sequencing", 
      "contacting"
    ]
  ], 
  "sourcelinks": [
    "http://bioinf.cs.ucl.ac.uk/downloads/MetaPSICOV"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-11-28T15:35:07Z"
}{
  "doi": "10.1093/bioinformatics/btu629", 
  "name": "METAINTER metaanalysis of multiple regression models in genomewide association studies", 
  "links": [
    "http://metainter.meb.uni-bonn.de", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.ncbi.nlm.nih.gov/gap"
  ], 
  "title": "Genome analysis METAINTER: meta-analysis of multiple regression models in genome-wide association studies", 
  "toolName": "Genome analysis METAINTER: meta-analysis of multiple regression models in genome-wide association studies", 
  "abstract": "Motivation: Meta-analysis of summary statistics is an essential approach to guarantee the success of genome-wide association studies (GWAS). Application of the fixed or random effects model to single-marker association tests is a standard practice. More complex methods of meta-analysis involving multiple parameters have not been used frequently, a gap that could be explained by the lack of a respective meta-analysis pipeline. Meta-analysis based on combining p-values can be applied to any association test. However, to be powerful, meta-analysis methods for high-dimensional models should incorporate additional information such as study-specific properties of parameter estimates, their effect directions, standard errors and covariance structure. Results: We modified 'method for the synthesis of linear regression slopes' recently proposed in the educational sciences to the case of multiple logistic regression, and implemented it in a meta-analysis tool called METAINTER. The software handles models with an arbitrary number of parameters, and can directly be applied to analyze the results of single-SNP tests, global haplotype tests, tests for and under gene\u2013 gene or gene\u2013environment interaction. Via simulations for two-single nucleotide polymorphisms (SNP) models we have shown that the proposed meta-analysis method has correct type I error rate. Moreover, power estimates come close to that of the joint analysis of the entire sample. We conducted a real data analysis of six GWAS of type 2 diabetes , available from dbGaP (http://www.ncbi.nlm.nih.gov/gap). For each study, a genome-wide interaction analysis of all SNP pairs was performed by logistic regression tests. The results were then meta-analyzed with METAINTER. Availability: The software is freely available and distributed under the conditions specified on http://metainter.", 
  "summary": "At first, we conducted a genome-wide interaction analysis (GWIA) with 8 and 4 df logistic regression tests in each study, and then meta-analyzed the results with METAINTER.\nAssume that k studies (sub-studies of the meta-analysis) were conducted to test a particular hypothesis H1 versus fixed H0: In two of three methods, the only information required to perform meta-analysis is p-values pj of each sub-study j and possibly sample size nj; j=1; .\nIn Table 4, among the top five results of the T2D meta-analysis for the 4 df logistic regression test, none of the SNP pairs reached a genome-wide significant p-value.", 
  "affiliations": [
    " Institute for Medical Biometry, Informatics and Epidemiology University of Bonn ", 
    " German Center for Neurodegenerative Diseases (DZNE)"
  ], 
  "grants": [
    "Funding: T. Vaitsiakhovich was supported by the German Research Foundation (DFG), projects BE 3828/3-2, BE 3828/3-3."
  ], 
  "acks": " ", 
  "authors": [
    " Tatsiana Vaitsiakhovich", 
    " Dmitriy Drichel", 
    " Christine Herold", 
    " Andr E Lacour", 
    " Tim Becker", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "modeling", 
      "testing", 
      "meta", 
      "studies", 
      "analysis"
    ]
  ], 
  "sourcelinks": [
    "http://metainter.meb.uni-bonn.de"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-25T00:36:59Z"
}{
  "doi": "10.1093/bioinformatics/btu681", 
  "name": "mimicMe a web server for prediction and analysis of hostlike proteins in microbial pathogens", 
  "links": [
    "http://www.drive5.com/muscle", 
    "https://github.com/tanghaibao/goatools", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://webglmol.source", 
    "http://www.pdb.org"
  ], 
  "title": "Genome analysis mimicMe: a web server for prediction and analysis of host-like proteins in microbial pathogens", 
  "toolName": "goatools", 
  "abstract": "mimicMe is a web server for prediction and analysis of host-like proteins (mimics) encoded by microbial pathogens. Users select a host species and any set of pathogen and control proteomes (bacterial, fungal, protozoan or viral) and mimicMe reports host-like proteins that are unique to or enriched among pathogens. Additional server features include visualization of structural similarities between pathogen and host proteins as well as function-enrichment analysis. Availability and implementation: mimicMe is available at http://", 
  "summary": "mimicMe: a web server for prediction and analysis of host-like proteins in microbial pathogens\nABSTRACT Summary: mimicMe is a web server for prediction and analysis of host-like proteins (mimics) encoded by microbial pathogens.\nUsers select a host species and any set of pathogen and control proteomes (bacterial, fungal, protozoan or viral) and mimicMe reports host-like proteins that are unique to or enriched among pathogens.\nmimicMe: analysis of host-like proteins in pathogens\n4 CONCLUSION mimicMe provides an automated pipeline for BLAST-based detection and exploration of host-like proteins (mimics) in microbial pathogens.", 
  "affiliations": [
    " Department of Biology University of Waterloo "
  ], 
  "grants": [
    "Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC), through a Discovery Grant to A.C.D."
  ], 
  "sourcelinks": [
    "https://github.com/tanghaibao/goatools", 
    "http://webglmol.source", 
    "http://www.pdb.org"
  ], 
  "acks": " ", 
  "authors": [
    " Pavel Petrenko", 
    " Andrew C Doxey"
  ], 
  "keyWords": [
    "host proteins", 
    [
      "pathogenic", 
      "predictions", 
      "hosts", 
      "bioinformatics", 
      "mimicme", 
      "protein"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "BSD 2-clause \"Simplified\" License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/tanghaibao/goatools/master/LICENSE"
      }
    ], 
    "name": "goatools", 
    "contributors": [
      {
        "contributions": 279, 
        "html_url": "https://github.com/dvklopfenstein"
      }, 
      {
        "contributions": 99, 
        "html_url": "https://github.com/tanghaibao"
      }, 
      {
        "contributions": 29, 
        "html_url": "https://github.com/fidelram"
      }, 
      {
        "contributions": 21, 
        "html_url": "https://github.com/olgabot"
      }, 
      {
        "contributions": 15, 
        "html_url": "https://github.com/brentp"
      }, 
      {
        "contributions": 6, 
        "html_url": "https://github.com/patflick"
      }, 
      {
        "contributions": 4, 
        "html_url": "https://github.com/bicycle1885"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/cmungall"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/deto"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/stuppie"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/yunesj"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/dmyersturnbull"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/mfiers"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/CFretter"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/tanghaibao/goatools/zipball/v0.6.5", 
        "tarball_url": "https://api.github.com/repos/tanghaibao/goatools/tarball/v0.6.5", 
        "name": "v0.6.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/tanghaibao/goatools/zipball/v0.6.4", 
        "tarball_url": "https://api.github.com/repos/tanghaibao/goatools/tarball/v0.6.4", 
        "name": "v0.6.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/tanghaibao/goatools/zipball/v0.5.9", 
        "tarball_url": "https://api.github.com/repos/tanghaibao/goatools/tarball/v0.5.9", 
        "name": "v0.5.9"
      }
    ], 
    "created_at": "2010-01-15T19:15:21Z", 
    "updated_at": "2016-07-03T19:03:35Z", 
    "languages": [
      "Python", 
      "Jupyter Notebook", 
      "Shell", 
      "Makefile"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/tanghaibao"
      }, 
      {
        "html_url": "https://github.com/aurelien-naldi"
      }, 
      {
        "html_url": "https://github.com/huanannd"
      }, 
      {
        "html_url": "https://github.com/patflick"
      }, 
      {
        "html_url": "https://github.com/ezequieljsosa"
      }, 
      {
        "html_url": "https://github.com/annabelhao"
      }, 
      {
        "html_url": "https://github.com/afsheenb"
      }, 
      {
        "html_url": "https://github.com/dvklopfenstein"
      }, 
      {
        "html_url": "https://github.com/alyamahmoud"
      }, 
      {
        "html_url": "https://github.com/mmendez12"
      }, 
      {
        "html_url": "https://github.com/danielsu0523"
      }, 
      {
        "html_url": "https://github.com/meono"
      }, 
      {
        "html_url": "https://github.com/lileiting"
      }, 
      {
        "html_url": "https://github.com/fidelram"
      }, 
      {
        "html_url": "https://github.com/BernardoBelloOrti"
      }, 
      {
        "html_url": "https://github.com/Huang-WeiQi"
      }, 
      {
        "html_url": "https://github.com/olgabot"
      }, 
      {
        "html_url": "https://github.com/dezzan"
      }
    ], 
    "owner": "https://github.com/tanghaibao", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btu704", 
  "name": "minimac2 faster genotype imputation", 
  "links": [
    "http://openmp.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://genome.sph.umich.edu/wiki/Minimac2"
  ], 
  "title": "Genetic and population analysis minimac2: faster genotype imputation", 
  "toolName": "Genetic and population analysis minimac2: faster genotype imputation", 
  "abstract": "Genotype imputation is a key step in the analysis of genome-wide association studies. Upcoming very large reference panels, such as those from The 1000 Genomes Project and the Haplotype Consortium, will improve imputation quality of rare and less common variants, but will also increase the computational burden. Here, we demonstrate how the application of software engineering techniques can help to keep imputation broadly accessible. Overall, these improvements speed up imputation by an order of magnitude compared with our previous implementation. Availability and implementation: minimac2, including source code, documentation, and examples is available at", 
  "summary": "Genotype imputation is routinely used to increase the power of genome-wide association studies (GWAS; Howie et al., 2012; Li et al., 2009; Marchini et al., 2007).\nWith standard methods (Browning and Browning, 2009; Li et al., 2009; Marchini et al., 2007) likely haplotypes for each sample are estimated every time imputation is repeated with a new or updated reference panel.\nSpeed-up for imputing 1000 FUSION (Scott et al., 2007) GWAS samples using The 1000 Genomes Project Phase 1 reference panel (1092 individuals, 37.4M SNPs) compared with minimac", 
  "affiliations": [
    " 23andMe, Inc"
  ], 
  "grants": [
    "Funding\nThis work was funded by National Institutes of Health research grants (to G.R.A.)."
  ], 
  "acks": " We acknowledge M. Zawistowski for assistance with simulations. We thank the GWAS community for testing the software and providing useful feedback. This work was funded by National Institutes of Health research grants (to G.R.A.). Conflict of interest: none declared. ", 
  "authors": [
    " Christian Fuchsberger", 
    " Gon\u00e7", 
    " R Abecasis", 
    " David A Hinds"
  ], 
  "keyWords": [
    "genotype imputation", 
    [
      "computationally", 
      "genotyped", 
      "minimac", 
      "genetic", 
      "imputing", 
      "haplotypes"
    ]
  ], 
  "sourcelinks": [
    "http://genome.sph.umich.edu/wiki/Minimac2"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-23T00:08:46Z"
}{
  "doi": "10.1093/bioinformatics/btv081", 
  "name": "MIRA mutual informationbased reporter algorithm for metabolic networks", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Corrigendum MIRA: mutual information-based reporter algorithm for metabolic networks", 
  "toolName": "Corrigendum MIRA: mutual information-based reporter algorithm for metabolic networks", 
  "abstract": "", 
  "summary": "MIRA: mutual information-based reporter algorithm for metabolic networks\nResults reported for the reporter algorithm (RA) in the Results section are raw z-scores per metabolite, normalized by subtracting the sample mean and dividing by the sample standard deviation.\nUsing the background correction as described in the original scoring scheme (Ideker et al., 2002; Patil and Nielsen, 2005) removes the hub metabpolffiiffitffie bias we claim for the RA (for both normalization by k and k).\nThe background correction improves the empirical significance of the RA as shown in Figure 5A and C.\nAfter the background correction, RA is among the top three permutations.", 
  "affiliations": [], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " A Ercument Cicek", 
    " Kathryn Roeder", 
    " Gultekin Ozsoyoglu"
  ], 
  "keyWords": [
    "results reported", 
    [
      "scoring", 
      "reporter", 
      "correction", 
      "mira", 
      "access", 
      "bioinformatics", 
      "networks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-03-12T00:32:06Z"
}{
  "doi": "10.1093/bioinformatics/btu783", 
  "name": "MGAS a powerful tool for multivariate genebased genomewide association analysis", 
  "links": [
    "http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1880831", 
    "http://www.ncbi.nlm.nih.gov/gap", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://www.genome.gov/gwastu", 
    "https://dbgap.ncbi.nlm.nih.gov", 
    "http://www", 
    "http://ctglab.nl/software", 
    "http://statgenpro.psych", 
    "http://hapmap", 
    "http://hapmap.ncbi.nlm.nih.gov/downloads/ld", 
    "http://statgenpro.psychiatry.hku.hk/limx/snptracker", 
    "http://statgenpro.psychiatry.hku.hk/limx/kgg", 
    "http://ctglab.nl/people", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetics and population analysis MGAS: a powerful tool for multivariate gene-based genome-wide association analysis Downloaded from", 
  "toolName": "Genetics and population analysis MGAS: a powerful tool for multivariate gene-based genome-wide association analysis Downloaded from", 
  "abstract": "Motivation: Standard genome-wide association studies, testing the association between one phenotype and a large number of single nucleotide polymorphisms (SNPs), are limited in two ways: (i) traits are often multivariate, and analysis of composite scores entails loss in statistical power and (ii) gene-based analyses may be preferred, e.g. to decrease the multiple testing problem. Results: Here we present a new method, multivariate gene-based association test by extended Simes procedure (MGAS), that allows gene-based testing of multivariate phenotypes in unrelated individuals. Through extensive simulation, we show that under most trait-generating genotype\u2013 phenotype models MGAS has superior statistical power to detect associated genes compared with gene-based analyses of univariate phenotypic composite scores (i.e. GATES, multiple regression), and multivariate analysis of variance (MANOVA). Re-analysis of metabolic data revealed 32 False Discovery Rate controlled genome-wide significant genes, and 12 regions harboring multiple genes; of these 44 regions, 30 were not reported in the original analysis. Conclusion: MGAS allows researchers to conduct their multivariate gene-based analyses efficiently , and without the loss of power that is often associated with an incorrectly specified geno-type\u2013phenotype models. Availability and implementation: MGAS is freely available in KGG v3.0 (http://statgenpro.psych iatry.hku.hk/limx/kgg/download.php). Access to the metabolic dataset can be requested at dbGaP (https://dbgap.ncbi.nlm.nih.gov/). The R-simulation code is available from http://ctglab.nl/people/ sophie_van_der_sluis.", 
  "summary": "Through extensive simulation, we show that under most trait-generating genotype phenotype models MGAS has superior statistical power to detect associated genes compared with gene-based analyses of univariate phenotypic composite scores (i.e. GATES, multiple regression), and multivariate analysis of variance (MANOVA).\nIn addition, we used real genome-wide genetic data of N  4763 subjects from the Northern Finland Birth Cohort (NFBC1966, Sabatti et al., 2009, see Section 2.6 later), simulated 10 sets of 9 phenotypes, of which the correlational structure mimicked the observed correlations of the NFBC1966 metabolic phenotypes (see Supplementary Table S8), and performed 10 whole-genome multivariate gene-based association analyses using MGAS.", 
  "affiliations": [
    " Department of Complex Trait Genetics Section Clinical Genetics Center for Neurogenomics and Cognitive Research (CNCR) VU Medical Center ", 
    " Department of Biochemistry", 
    " Department of Biological Psychology VU University Amsterdam "
  ], 
  "grants": [
    "Simulations were run on the Genetic Cluster Computer, which is financially supported by an NWO Medium Investment grant [480-05-003]; by the VU University Amsterdam, the Netherlands, and by the Dutch Brain Foundation.", 
    "Funding\nThis work was funded by the Netherlands Scientific Organization [NWO/ MaGW: VIDI-452-12-014]; the European Research Council [Genetics of Mental Illness: ERC-230374]; the Hong Kong Research Grants Council [GRF HKU 768610M, HKU 776412M and HKU 777511M]; the Hong Kong Research Grants Council Theme-Based Research Scheme [T12-705/11]; the European Community Seventh Framework Programme Grant on European Network of National Schizophrenia Networks Studying Gene-Environment Interactions (EU-GEI); the Hong Kong Health and Medical Research Fund [01121436, 01121726 and 02132236]; the HKU Seed Funding Programme for Basic Research [201302159006]; and The University of Hong Kong Strategic Research Theme on Genomics."
  ], 
  "acks": " The dataset used for the example analysis described in this manuscript was obtained in NFBC1966 study, found at http://www.ncbi.nlm.nih.gov/gap, controlled through dbGaP study accession number phs000276.v1.p1. The NFBC1966 Study is conducted and supported by the National Heart, Lung, and Blood Institute (NHLBI) in collaboration with the Broad Institute, UCLA, University of Oulu, and the National Institute for Health and Welfare in Finland. This manuscript was not prepared in collaboration with investigators of the NFBC1966 Study and does not necessarily reflect the opinions or views of the NFBC1966 Study Investigators, Broad Institute, UCLA, University of Oulu, National Institute for Health and Welfare in Finland, and the NHLBI. This work was funded by the Netherlands Scientific Organization; the HKU Seed Funding Programme for Basic Research; and The University of Hong Kong Strategic Research Theme on Genomics. Simulations were run on the Genetic Cluster Computer, which is financially supported by an NWO Medium Investment grant; by the VU University Amsterdam, the Netherlands, and by the Dutch Brain Foundation. The funders had no role in ", 
  "authors": [
    " Sophie Van Der Sluis", 
    " V Conor", 
    " Dolan", 
    " Jiang Li", 
    " Youqiang Song", 
    " Pak Sham", 
    " Danielle Posthuma", 
    " Miao-Xin Li"
  ], 
  "keyWords": [
    [
      "genetically", 
      "mgas", 
      "powerful", 
      "phenotypes", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://ctglab.nl/software", 
    "http://statgenpro.psych", 
    "http://hapmap", 
    "http://hapmap.ncbi.nlm.nih.gov/downloads/ld", 
    "http://statgenpro.psychiatry.hku.hk/limx/kgg", 
    "http://ctglab.nl/people"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-11-28T15:35:07Z"
}{
  "doi": "10.1093/bioinformatics/btu573", 
  "name": "MIRPIPE quantification of microRNAs in niche model organisms", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinformatics.mpi-bn.mpg.de", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Sequence analysis MIRPIPE: quantification of microRNAs in niche model organisms", 
  "toolName": "Sequence analysis MIRPIPE: quantification of microRNAs in niche model organisms", 
  "abstract": "MicroRNAs (miRNAs) represent an important class of small non-coding RNAs regulating gene expression in eukaryotes. Present algorithms typically rely on genomic data to identify miRNAs and require extensive installation procedures. Niche model organisms lacking genomic sequences cannot be analyzed by such tools. Here we introduce the MIRPIPE application enabling rapid and simple browser-based miRNA homology detection and quantification. MIRPIPE features automatic trimming of raw RNA-Seq reads originating from various sequencing instruments, processing of isomiRs and quantification of detected miRNAs versus public-or user-uploaded reference databases. Availability and implementation: The Web service is freely available at", 
  "summary": "MIRPIPE features automatic trimming of raw RNA-Seq reads originating from various sequencing instruments, processing of isomiRs and quantification of detected miRNAs versus public- or user-uploaded reference databases.\nSequence differences between taxa hamper quantification, especially if no genomic or miRNA data for the studied organism are available as in the case of niche model organisms.\nThe remaining read sequences are used for a sequence similarity search versus the chosen reference database of miRNAs. Mature reference miRNAs and their precursors are optionally collated by name on the family level to remove redundancy\nTo demonstrate congruent results for MIRPIPE, we compared the results with an miRNA analysis based on a genomic mapping of Illumina HiSeq reads (Lawless et al., 2013).", 
  "affiliations": [
    " Max Planck Institute for Heart and Lung Research Cardiac Development and Remodelling ", 
    " Group of Bioinformatics"
  ], 
  "grants": [
    "Funding: Excellence Cluster Cardio-Pulmonary System (ECCPS); MPI for Heart and Lung Research."
  ], 
  "acks": " ", 
  "authors": [
    " Carsten Kuenne", 
    " Jens Preussner", 
    " Mario Herzog", 
    " Thomas Braun", 
    " Mario Looso", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "mirnas", 
      "reads", 
      "references", 
      "bioinformatics", 
      "sequencing", 
      "mirpipe"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-28T02:52:42Z"
}{
  "doi": "10.1093/bioinformatics/btu380", 
  "name": "miRPREFeR an accurate fast and easytouse plant miRNA prediction tool using small RNASeq data", 
  "links": [
    "https://github.com/hangelwen/miRPREFeR", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining miR-PREFeR: an accurate, fast and easy-to-use plant miRNA prediction tool using small RNA-Seq data", 
  "toolName": "miRPREFeR", 
  "abstract": "Plant microRNA prediction tools that use small RNA-sequencing data are emerging quickly. These existing tools have at least one of the following problems: (i) high false-positive rate; (ii) long running time; (iii) work only for genomes in their databases; (iv) hard to install or use. We developed miR-PREFeR (miRNA PREdiction From small RNA-Seq data), which uses expression patterns of miRNA and follows the criteria for plant microRNA annotation to accurately predict plant miRNAs from one or more small RNA-Seq data samples of the same species. We tested miR-PREFeR on several plant species. The results show that miR-PREFeR is sensitive, accurate, fast and has low-memory footprint. Availability and implementation: https://github.com", 
  "summary": "We developed miR-PREFeR (miRNA PREdiction From small RNA-Seq data), which uses expression patterns of miRNA and follows the criteria for plant microRNA annotation to accurately predict plant miRNAs from one or more small RNA-Seq data samples of the same species.\nRecently developed genome-wide miRNA annotation tools all use small RNA-Seq data to quantify the expression of annotated miRNAs and to predict novel ones (Wang et al., 2009; Yang and Li, 2011; Hackenberg et al., 2011; Axtell, 2013).\nWe benchmarked miR-PREFeR and several popular or newly developed NGS-based miRNA prediction tools including ShortStack (Axtell, 2013), miRDeep-P (Yang and Li, 2011), miRanalyzer (Hackenberg et al., 2011), miRDeep2 (Friedlander et al., 2012), miRDeep* (An et al., 2013) and MIReNA (Mathelier and Carbone, 2010).", 
  "affiliations": [
    " Department of Computer Science and Engineering Associate Editor: Igor Jurisica Michigan State University "
  ], 
  "grants": [
    "Funding: This work was supported by NSF CAREER Grant DBI-0953738 and NSF IOS-1126998."
  ], 
  "acks": " ", 
  "authors": [
    " Jikai Lei", 
    " Yanni Sun"
  ], 
  "keyWords": [
    [
      "plants", 
      "mirnas", 
      "bioinformatics", 
      "tools", 
      "annotations", 
      "mirs"
    ]
  ], 
  "sourcelinks": [
    "https://github.com/hangelwen/miRPREFeR", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-06-16T20:35:17Z"
}{
  "doi": "10.1093/bioinformatics/btu676", 
  "name": "miRseqViewer multipanel visualization of sequence structure and expression for analysis of microRNA sequencing data", 
  "links": [
    "http://msv.kobic.re.kr", 
    "https://github.com/insoo078/mirseqviewer", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://picard.sourceforge.net"
  ], 
  "title": "Sequence analysis miRseqViewer: multi-panel visualization of sequence, structure and expression for analysis of microRNA sequencing data", 
  "toolName": "mirseqviewer", 
  "abstract": "Deep sequencing of small RNAs has become a routine process in recent years, but no dedicated viewer is as yet available to explore the sequence features simultaneously along with secondary structure and gene expression of microRNA (miRNA). We present a highly interactive application that visualizes the sequence alignment, secondary structure and normalized read counts in synchronous multi-panel windows. This helps users to easily examine the relationships between the structure of precursor and the sequences and abundance of final products and thereby will facilitate the studies on miRNA bio-genesis and regulation. The project manager handles multiple samples of multiple groups. The read alignment is imported in BAM file format.", 
  "summary": "Even though the sequence variations such as uridylation, iso-miRs and RNA editing are known to be important in biogenesis and function of miRNAs (Kim et al., 2009), we still lack a dedicated alignment viewer to examine the details of\nThe depth profile of read alignment, reference genome sequence, pre-miRNA sequence from miRBase, and secondary structure segments are shown in the alignment header.\nIn conclusion, miRseqViewer provides a unique visualization platform optimized for interactive analysis of miRNA deep sequencing data, supporting 84 organisms in the miRBase database.", 
  "affiliations": [
    " School of Biological Sciences Seoul National University ", 
    " Korean Bioinformation Center (KOBIC) KRIBB ", 
    " Department of Life Science and Ewha Research Center for Systems Biology (ERCSB) Ewha Womans University "
  ], 
  "grants": [
    "Funding: This work was supported by grants from the National Research Foundation of Korea (NRF-2012M3A9D1054744, NRF-2012M3A9B9036673 and NRF-2011-0019745), GIST\n\n597\n\n\fI.Jang et al.", 
    "Systems Biology Infrastructure Establishment Grant through ERCSB, and Korea Research Institute of Bioscience and Biotechnology (KRIBB) Research Initiative Program."
  ], 
  "sourcelinks": [
    "http://msv.kobic.re.kr", 
    "https://github.com/insoo078/mirseqviewer", 
    "http://picard.sourceforge.net"
  ], 
  "acks": " We appreciate critical comments and usage feedback from anonymous reviewers and A.T.M. Golam Bari for the program development. ", 
  "authors": [
    " Insu Jang", 
    " Hyeshik Chang", 
    " Yukyung Jun", 
    " Seongjin Park", 
    " Jin Ok Yang", 
    " Byungwook Lee", 
    " Wankyu Kim", 
    " V Narry Kim", 
    " Sanghyuk Lee"
  ], 
  "keyWords": [
    [
      "windows", 
      "mirseqviewer", 
      "mirnas", 
      "reads", 
      "bioinformatics", 
      "sequencing", 
      "alignment"
    ]
  ], 
  "github_data": {
    "name": "mirseqviewer", 
    "contributors": [
      {
        "contributions": 41, 
        "html_url": "https://github.com/insoo078"
      }
    ], 
    "versions": [], 
    "created_at": "2014-06-02T00:27:53Z", 
    "updated_at": "2014-07-18T01:49:14Z", 
    "languages": [
      "Java"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/insoo078"
      }
    ], 
    "owner": "https://github.com/insoo078", 
    "homepage": "msv.kobic.re.kr"
  }, 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-10-17T05:05:29Z"
}{
  "doi": "10.1093/bioinformatics/btu547", 
  "name": "mod_bio Apache modules for NextGeneration sequencing data", 
  "links": [
    "https://github.com", 
    "http://en.wikipedia.org/wiki/Apache_HTTP", 
    "http://cardioserve.nantes.inserm.fr/lindenb/mod", 
    "https://github.com/ga4gh/schemas).The", 
    "https://github.com/lindenb/mod", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Sequence analysis mod_bio: Apache modules for Next-Generation sequencing data", 
  "toolName": "mod", 
  "abstract": "We describe mod_bio, a set of modules for the Apache HTTP server that allows the users to access and query fastq, tabix, fasta and bam files through a Web browser. Those data are made available in plain text, HTML, XML, JSON and JSON-P. A javascript-based genome browser using the JSON-P communication technique is provided as an example of cross-domain Web service. Availability and implementation: https://github.com/lindenb/mod_ bio. Contact:", 
  "summary": "Summary: We describe mod_bio, a set of modules for the Apache HTTP server that allows the users to access and query fastq, tabix, fasta and bam files through a Web browser.\nCommon tasks such as getting the first lines of a file or accessing the data in a given genomic region are easily provided using standard linux tools or using the coordinate-sorted index of samtools (Li et al., 2009) and tabix (Li, 2011), but this index needs to be downloaded, and it remains difficult for the biologists to quickly get an overview of those data.\nTo answer those issues, we have developed mod_bio, a set of apache modules providing a user-friendly overview of the bioinformatics files through a Web browser.", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was supported by the Inserm (ATIP-Avenir program) and the French Regional Council of Pays-de-la-Loire (`VaCaRMe Project')."
  ], 
  "sourcelinks": [
    "http://en.wikipedia.org/wiki/Apache_HTTP", 
    "https://github.com/ga4gh/schemas).The", 
    "https://github.com/lindenb/mod", 
    "https://github.com"
  ], 
  "acks": " ", 
  "authors": [
    " Pierre Lindenbaum", 
    " Richard Redon"
  ], 
  "keyWords": [
    [
      "files", 
      "genomics", 
      "like", 
      "mod_bio", 
      "bioinformatics", 
      "formats", 
      "data"
    ]
  ], 
  "github_data": {
    "name": "mod", 
    "contributors": [
      {
        "contributions": 557, 
        "html_url": "https://github.com/yuanyan"
      }, 
      {
        "contributions": 8, 
        "html_url": "https://github.com/ibigbug"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/yanxyz"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/kud"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.4.7", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.4.7", 
        "name": "0.4.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.4.6", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.4.6", 
        "name": "0.4.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.4.5", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.4.5", 
        "name": "0.4.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.4.4", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.4.4", 
        "name": "0.4.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.4.3", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.4.3", 
        "name": "0.4.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.4.2", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.4.2", 
        "name": "0.4.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.4.1", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.4.1", 
        "name": "0.4.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.4.0", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.4.0", 
        "name": "0.4.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.36", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.36", 
        "name": "0.3.36"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.35", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.35", 
        "name": "0.3.35"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.34", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.34", 
        "name": "0.3.34"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.33", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.33", 
        "name": "0.3.33"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.32", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.32", 
        "name": "0.3.32"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.31", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.31", 
        "name": "0.3.31"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.30", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.30", 
        "name": "0.3.30"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.29", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.29", 
        "name": "0.3.29"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.28", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.28", 
        "name": "0.3.28"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.27", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.27", 
        "name": "0.3.27"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.26", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.26", 
        "name": "0.3.26"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.25", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.25", 
        "name": "0.3.25"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.24", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.24", 
        "name": "0.3.24"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.23", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.23", 
        "name": "0.3.23"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.22", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.22", 
        "name": "0.3.22"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.21", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.21", 
        "name": "0.3.21"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.20", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.20", 
        "name": "0.3.20"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/modjs/mod/zipball/0.3.19", 
        "tarball_url": "https://api.github.com/repos/modjs/mod/tarball/0.3.19", 
        "name": "0.3.19"
      }
    ], 
    "created_at": "2012-10-15T16:35:04Z", 
    "updated_at": "2016-08-04T11:15:34Z", 
    "languages": [
      "Batchfile", 
      "JavaScript", 
      "HTML", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/adamsxu"
      }, 
      {
        "html_url": "https://github.com/yuanyan"
      }, 
      {
        "html_url": "https://github.com/greenlieber"
      }, 
      {
        "html_url": "https://github.com/kylereicks"
      }, 
      {
        "html_url": "https://github.com/louiseliu"
      }, 
      {
        "html_url": "https://github.com/biqing"
      }, 
      {
        "html_url": "https://github.com/rehorn"
      }, 
      {
        "html_url": "https://github.com/wofeiwofeifeifei"
      }, 
      {
        "html_url": "https://github.com/qq286735628"
      }, 
      {
        "html_url": "https://github.com/edgardojimenez"
      }, 
      {
        "html_url": "https://github.com/johnwonder"
      }, 
      {
        "html_url": "https://github.com/hbdrawn"
      }, 
      {
        "html_url": "https://github.com/bondzhan"
      }, 
      {
        "html_url": "https://github.com/materliu"
      }, 
      {
        "html_url": "https://github.com/knightli"
      }, 
      {
        "html_url": "https://github.com/chinafather"
      }, 
      {
        "html_url": "https://github.com/amwjx"
      }, 
      {
        "html_url": "https://github.com/xhlv"
      }, 
      {
        "html_url": "https://github.com/wuhongjun"
      }, 
      {
        "html_url": "https://github.com/airyland"
      }, 
      {
        "html_url": "https://github.com/sonnyneko"
      }, 
      {
        "html_url": "https://github.com/jakschu"
      }, 
      {
        "html_url": "https://github.com/nuintun"
      }, 
      {
        "html_url": "https://github.com/youxiachai"
      }, 
      {
        "html_url": "https://github.com/dongkun"
      }, 
      {
        "html_url": "https://github.com/funjackyone"
      }, 
      {
        "html_url": "https://github.com/luxueyan2008"
      }, 
      {
        "html_url": "https://github.com/Noeek"
      }, 
      {
        "html_url": "https://github.com/JasonCole1980"
      }, 
      {
        "html_url": "https://github.com/peigong"
      }
    ], 
    "owner": "https://github.com/modjs", 
    "homepage": "http://madscript.com/modjs"
  }, 
  "technologies": [], 
  "dateCreated": "2014-09-06T04:48:38Z"
}{
  "doi": "10.1093/bioinformatics/btu545", 
  "name": "MindTheGap integrated detection and assembly of short and long insertions", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses", 
    "http://mindthegap.genouest.org"
  ], 
  "title": "Genome analysis MindTheGap: integrated detection and assembly of short and long insertions", 
  "toolName": "Genome analysis MindTheGap: integrated detection and assembly of short and long insertions", 
  "abstract": "Motivation: Insertions play an important role in genome evolution. However, such variants are difficult to detect from short-read sequen-cing data, especially when they exceed the paired-end insert size. Many approaches have been proposed to call short insertion variants based on paired-end mapping. However, there remains a lack of practical methods to detect and assemble long variants. Results: We propose here an original method, called MINDTHEGAP, for the integrated detection and assembly of insertion variants from re-sequencing data. Importantly, it is designed to call insertions of any size, whether they are novel or duplicated, homozygous or heterozy-gous in the donor genome. MINDTHEGAP uses an efficient k-mer-based method to detect insertion sites in a reference genome, and subsequently assemble them from the donor reads. MINDTHEGAP showed high recall and precision on simulated datasets of various genome complexities. When applied to real Caenorhabditis elegans and human NA12878 datasets, MINDTHEGAP detected and correctly assembled insertions 41 kb, using at most 14 GB of memory. Availability and implementation: http://mindthegap.", 
  "summary": "MINDTHEGAP uses an efficient k-mer-based method to detect insertion sites in a reference genome, and subsequently assemble them from the donor reads.\nFinally, several methods detect sites of mobile element insertions using collections of known transposable element sequences, by searching for read pairs where one mate is mapped to a known element and the other to a unique part of the reference genome (Ewing and Kazazian, 2011; Hormozdiari et al., 2010; Stewart et al., 2011).\nThe software performs three steps: (i) construction of the de Bruijn graph of the reads, (ii) detection of insertion breakpoints on the reference genome (find module) and (iii) local assembly of inserted sequences (fill module).", 
  "affiliations": [
    " Inria/IRISA GenScale"
  ], 
  "grants": [
    "Funding: This work was supported by the ANR (French National Research Agency), ANR-12-BS02-0008 Colib'read project, ANR-12-EMMA-0019-01 GATB project and ANR-11BSV7-005-01 SPECIAPHID."
  ], 
  "acks": " The authors are grateful to Raluca Uricaru for her help and advice. ", 
  "authors": [
    " Guillaume Rizk", 
    " Ana \u20ac Is Gouin", 
    " Rayan Chikhi", 
    " Claire Lemaitre"
  ], 
  "keyWords": [
    [
      "assembling", 
      "insertions", 
      "genomes", 
      "reads", 
      "sequencing", 
      "mindthegap"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-08-15T05:12:51Z"
}{
  "doi": "10.1093/bioinformatics/btu540", 
  "name": "Modeling genome coverage in singlecell sequencing", 
  "links": [
    "http://smith", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis Modeling genome coverage in single-cell sequencing", 
  "toolName": "Genome analysis Modeling genome coverage in single-cell sequencing", 
  "abstract": "Motivation: Single-cell DNA sequencing is necessary for examining genetic variation at the cellular level, which remains hidden in bulk sequencing experiments. But because they begin with such small amounts of starting material, the amount of information that is obtained from single-cell sequencing experiment is highly sensitive to the choice of protocol employed and variability in library preparation. In particular, the fraction of the genome represented in single-cell sequencing libraries exhibits extreme variability due to quantitative biases in amplification and loss of genetic material. Results: We propose a method to predict the genome coverage of a deep sequencing experiment using information from an initial shallow sequencing experiment mapped to a reference genome. The observed coverage statistics are used in a non-parametric empirical Bayes Poisson model to estimate the gain in coverage from deeper sequencing. This approach allows researchers to know statistical features of deep sequencing experiments without actually sequencing deeply, providing a basis for optimizing and comparing single-cell sequencing protocols or screening libraries. Availability and implementation: The method is available as part of the preseq software package. Source code is available at http://smith labresearch.org/preseq.", 
  "summary": "It is our goal here to investigate for a single cell DNA sequencing library the genome coverage from deep sequencing, which we define as the expected number of bases in the reference genome covered by sequencing using high-throughput short-read technology.\nA different approach would be to make estimates using only unique reads, and then to make predictions based on (i) the number of unique reads in a deeper experiment, using the method of Daley and Smith (2013), and (ii) to then predict genome coverage conditional on the number of unique reads sequenced in the full experiment (Supplementary Fig. S12).", 
  "affiliations": [
    " Department of Mathematics", 
    " Department of Molecular and Computational Biology University of Southern California "
  ], 
  "grants": [
    "Funding: This work was supported by US National Institute of Health National Heath Genome Research Institute grants (no."
  ], 
  "acks": " ", 
  "authors": [
    " Timothy Daley", 
    " Andrew D Smith", 
    " John Hancock"
  ], 
  "keyWords": [
    "single sequencing", 
    [
      "sequenced", 
      "genomics", 
      "cells", 
      "reads", 
      "coverages"
    ]
  ], 
  "sourcelinks": [
    "http://smith"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-09T01:36:21Z"
}{
  "doi": "10.1093/bioinformatics/btu850", 
  "name": "Modified screening and ranking algorithm for copy number variation detection", 
  "links": [
    "http://c2s2.yale.edu/software/modSaRa", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Modified screening and ranking algorithm for copy number variation detection", 
  "toolName": "Modified screening and ranking algorithm for copy number variation detection", 
  "abstract": "Motivation: Copy number variation (CNV) is a type of structural variation, usually defined as gen-omic segments that are 1 kb or larger, which present variable copy numbers when compared with a reference genome. The screening and ranking algorithm (SaRa) was recently proposed as an efficient approach for multiple change-points detection, which can be applied to CNV detection. However, some practical issues arise from application of SaRa to single nucleotide polymorphism data. Results: In this study, we propose a modified SaRa on CNV detection to address these issues. First, we use the quantile normalization on the original intensities to guarantee that the normal mean model-based SaRa is a robust method. Second, a novel normal mixture model coupled with a modified Bayesian information criterion is proposed for candidate change-point selection and further clustering the potential CNV segments to copy number states. Simulations revealed that the modified SaRa became a robust method for identifying change-points and achieved better performance than the circular binary segmentation (CBS) method. By applying the modified SaRa to real data from the HapMap project, we illustrated its performance on detecting CNV segments. In conclusion, our modified SaRa method improves SaRa theoretically and numerically, for identifying CNVs with high-throughput genotyping data. Availability and Implementation: The modSaRa package is implemented in R program and freely available at", 
  "summary": "PennCNV (Wang and Bucan, 2008) is probably the most popular software for CNV analysis, which implements hidden Markov model incorporating the Log R Ratio (LRR) values, B allele frequency (BAF) values and also the distances between neighbouring SNPs. Meanwhile, many change-point-based approaches have also been developed and extensively applied to the detection of CNVs. Existing change-point-based approaches use exhaustive searching, such as binary segmentation method (Sen and Srivastava, 1975), circular binary segmentation (CBS; Olshen et al., 2004) and penalized regression (Huang et al., 2005).\nPerformance of the modified SaRa and CBS on detecting change-points in the simulated datasets in Scenario 1 with deletion of a single copy", 
  "affiliations": [
    " Department of Biostatistics Yale School of Public Health New Haven "
  ], 
  "grants": [
    "Funding\nThis work was supported by National Institutes of Health grant, R01DA016750-10."
  ], 
  "acks": " The authors acknowledge The International HapMap Consortium for providing a dataset for our study. This work was supported by National Institutes of Health grant, R01DA016750-10. Conflict of Interest: none declared. ", 
  "authors": [
    " Feifei Xiao", 
    " Xiaoyi Min", 
    " Heping Zhang"
  ], 
  "keyWords": [
    [
      "changes", 
      "sara", 
      "copies", 
      "simulations", 
      "copy", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://c2s2.yale.edu/software/modSaRa", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-12-27T01:33:02Z"
}{
  "doi": "10.1093/bioinformatics/btu542", 
  "name": "Modeling dynamic functional relationship networks and application to ex vivo human erythroid differentiation", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://guanlab.ccmb.med.umich.edu", 
    "http://guanlab.ccmb.med.umich.edu/stageSpecific"
  ], 
  "title": "Gene expression Modeling dynamic functional relationship networks and application to ex vivo human erythroid differentiation", 
  "toolName": "Gene expression Modeling dynamic functional relationship networks and application to ex vivo human erythroid differentiation", 
  "abstract": "Motivation: Functional relationship networks, which summarize the probability of co-functionality between any two genes in the genome, could complement the reductionist focus of modern biology for understanding diverse biological processes in an organism. One major limitation of the current networks is that they are static, while one might expect functional relationships to consistently reprogram during the differentiation of a cell lineage. To address this potential limitation, we developed a novel algorithm that leverages both differentiation stage-specific expression data and large-scale heterogeneous functional genomic data to model such dynamic changes. We then applied this algorithm to the time-course RNA-Seq data we collected for ex vivo human erythroid cell differentiation. Results: Through computational cross-validation and literature validation, we show that the resulting networks correctly predict the (de)-activated functional connections between genes during erythro-poiesis. We identified known critical genes, such as HBD and GATA1, and functional connections during erythropoiesis using these dynamic networks, while the traditional static network was not able to provide such information. Furthermore, by comparing the static and the dynamic networks, we identified novel genes (such as OSBP2 and PDZK1IP1) that are potential drivers of erythroid cell differentiation. This novel method of modeling dynamic networks is applicable to other differentiation processes where time-course genome-scale expression data are available, and should assist in generating greater understanding of the functional dynamics at play across the genome during development. Availability and implementation: The network described in this article is available at http://guanlab.ccmb.", 
  "summary": "Our method of constructing `transitional' networks can be summarized diagrammatically in three steps (Fig. 1): (i) establish a `gold standard' describing functionally related gene pairs that become (de)activated between two time points using existing functional annotation databases and differentiation process-specific expression data; (ii) collect heterogeneous genomic datasets ($1000 in total) from public databases (Alfarano et al., 2005; Edgar et al., 2002; Ceol et al., 2010; Gu ldener et al., 2006; Kerrien et al., 2012; Ozier et al., 2003; Stark et al., 2011), including expression profiles, physical and genetic interactions (see complete data description in Supplementary Information S1), and (iii) weigh and integrate them using the transitional stage-specific `gold standard' pairs, and therefore generate networks that cover all genes.", 
  "affiliations": [
    " Department of Cell and Developmental Biology", 
    " Department of Computational Medicine and Bioinformatics"
  ], 
  "grants": [
    "Funding: This work is supported by NIH 1R21NS082212-01, NIH University of Michigan O'Brien Kidney Translational Core Center, AHA midwest postdoctoral fellowship N012616, NIH grants R21 HL114368 and U01 HL117658."
  ], 
  "acks": " ", 
  "authors": [
    " Fan Zhu", 
    " Lihong Shi", 
    " Hongdong Li", 
    " Ridvan Eksi", 
    " James Douglas Engel", 
    " Yuanfang Guan"
  ], 
  "keyWords": [
    "erythroid differentiation", 
    [
      "cells", 
      "genes", 
      "functionally", 
      "networks", 
      "differentiating"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-13T13:59:18Z"
}{
  "doi": "10.1093/bioinformatics/btu400", 
  "name": "Modeling timedependent transcription effects of HER2 oncogene and discovery of a role for E2F2 in breast cancer cellmatrix adhesion", 
  "links": [
    "http://www.cbmc.it/software/Software.php", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.itb.cnr.it", 
    "http://www.cbmc.it/software/Software.php.Promoter", 
    "http://www.ncbi.nlm.nih.gov/geo).2.2"
  ], 
  "title": "Gene expression Modeling time-dependent transcription effects of HER2 oncogene and discovery of a role for E2F2 in breast cancer cell-matrix adhesion", 
  "toolName": "Gene expression Modeling time-dependent transcription effects of HER2 oncogene and discovery of a role for E2F2 in breast cancer cell-matrix adhesion", 
  "abstract": "Motivation: Oncogenes are known drivers of cancer phenotypes and targets of molecular therapies; however, the complex and diverse sig-naling mechanisms regulated by oncogenes and potential routes to targeted therapy resistance remain to be fully understood. To this end, we present an approach to infer regulatory mechanisms downstream of the HER2 driver oncogene in SUM-225 metastatic breast cancer cells from dynamic gene expression patterns using a succession of analytical techniques, including a novel MP grammars method to mathematically model putative regulatory interactions among sets of clustered genes. Results: Our method highlighted regulatory interactions previously identified in the cell line and a novel finding that the HER2 oncogene, as opposed to the proto-oncogene, upregulates expression of the E2F2 transcription factor. By targeted gene knockdown we show the significance of this, demonstrating that cancer cell-matrix adhesion and outgrowth were markedly inhibited when E2F2 levels were reduced. Thus, validating in this context that upregulation of E2F2 represents a key intermediate event in a HER2 oncogene-directed gene expression-based signaling circuit. This work demonstrates how predictive modeling of longitudinal gene expression data combined with multiple systems-level analyses can be used to accurately predict downstream signaling pathways. Here, our integrated method was applied to reveal insights as to how the HER2 oncogene drives a specific cancer cell phenotype, but it is adaptable to investigate other oncogenes and model systems. Availability and implementation: Accessibility of various tools is listed in methods; the Log-Gain Stoichiometric Stepwise algorithm is accessible at http://www.cbmc.it/software/Software.php.", 
  "summary": "To this end, we present an approach to infer regulatory mechanisms downstream of the HER2 driver oncogene in SUM-225 metastatic breast cancer cells from dynamic gene expression patterns using a succession of analytical techniques, including a novel MP grammars method to mathematically model putative regulatory interactions among sets of clustered genes.\nWe started with a method to cluster genes displaying similar expression dynamics after HER2 oncogene signaling was inhibited in the HER2 amplified metastatic BC cell line SUM-225 (Forozan et al., 1999; Kuperwasser et al., 2005).", 
  "affiliations": [
    " Department of Obstetrics and Gynecology Wayne State University ", 
    " Department of Computer Science Wayne State University ", 
    " Department of Computer Science University of Verona "
  ], 
  "grants": [
    "Results from this analysis using the BioBase ExPlainTM 3.1 tool (Matys et al., 2003), which uses the TRANSFACVR database of empirical evidence, showed that the P38MAPK-associated cluster was significantly enriched for genes that are regulated by E2F transcription factor family members.", 
    "Additional funding support was provided by the\n\nNIH (RO1 DK089167 and STTR R42GM087013 to S.D.)", 
    "The results are based on analysis using the BioBase ExPlainTM 3.1 tool and TRANSFACVR database of empirical evidence.", 
    "(2003) TRANSFAC: transcriptional regulation, from patterns to profiles.", 
    "Promoter analysis of genes for experimentally verified transcription factor binding site over-representation used the BioBase ExPlain 3.1 tool and TRANSFACVR database (Beverly, MA).", 
    "Funding: United States Department of Defense (W81XWH-10-20068) and National Institutes of Health (NIH P30CA022453) awarded to the Karmanos Cancer Institute at Wayne State University."
  ], 
  "acks": " ", 
  "authors": [
    " Aliccia Bollig-Fischer", 
    " Luca Marchetti", 
    " Cristina Mitrea", 
    " Jiusheng Wu", 
    " Ad Ele Kruger", 
    " Vincenzo Manca", 
    " Sorin Dr Aghici", 
    " Janet Kelso"
  ], 
  "keyWords": [
    "gene expression", 
    [
      "clustering", 
      "oncogenes", 
      "cancer", 
      "cells", 
      "genes", 
      "expressing"
    ]
  ], 
  "sourcelinks": [
    "http://www.cbmc.it/software/Software.php", 
    "http://www.cbmc.it/software/Software.php.Promoter"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-16T00:23:13Z"
}{
  "doi": "10.1093/bioinformatics/btu685", 
  "name": "MOST a software environment for constraintbased metabolic modeling and strain design", 
  "links": [
    "http://most.ccib.rutgers.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://openopt.org/IPOPT", 
    "http://www.gnu"
  ], 
  "title": "Systems biology MOST: a software environment for constraint-based metabolic modeling and strain design", 
  "toolName": "Systems biology MOST: a software environment for constraint-based metabolic modeling and strain design", 
  "abstract": "MOST (metabolic optimization and simulation tool) is a software package that implements GDBB (genetic design through branch and bound) in an intuitive user-friendly interface with excel-like editing functionality, as well as implementing FBA (flux balance analysis), and supporting systems biology markup language and comma-separated values files. GDBB is currently the fastest algorithm for finding gene knockouts predicted by FBA to increase production of desired products, but GDBB has only been available on a command line interface, which is difficult to use for those without programming knowledge, until the release of MOST.", 
  "summary": "ABSTRACT Summary: MOST (metabolic optimization and simulation tool) is a software package that implements GDBB (genetic design through branch and bound) in an intuitive user-friendly interface with excellike editing functionality, as well as implementing FBA (flux balance analysis), and supporting systems biology markup language and comma-separated values files.\nMany software packages are currently available to create constraint-based models of metabolism, load existing models, export models and run analyses on these models (Lakshmanan et al., 2014) to predict the production of desired compounds by microbes under genetic manipulations (Egen and Lun, 2012).", 
  "affiliations": [
    " Center for Computational and Integrative Biology Department of Computer Science Rutgers University "
  ], 
  "grants": [], 
  "acks": " The authors would like to thank Trent Kroeger for his contributions in starting off the MOST project, and Zhiyuan Zhang for work on FBA. ", 
  "authors": [
    " James J Kelley", 
    " Anatoliy Lane", 
    " Xiaowei Li", 
    " Brahmaji Mutthoju", 
    " Shay Maor", 
    " Dennis Egen", 
    " Desmond S Lun"
  ], 
  "keyWords": [
    [
      "files", 
      "loading", 
      "bioinformatics", 
      "modeling", 
      "sbml", 
      "software"
    ]
  ], 
  "sourcelinks": [
    "http://most.ccib.rutgers.edu", 
    "http://openopt.org/IPOPT"
  ], 
  "technologies": [
    "MATLAB"
  ], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btu694", 
  "name": "MPGeneticSynth inferring biological network regulations from time series", 
  "links": [
    "http://www.jfree.org/jfreechart", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://code", 
    "http://mplab.sci.univr.it/plugins/mpgs/index", 
    "http://sourceforge.net/projects/jep"
  ], 
  "title": "MP-GeneticSynth: inferring biological network regulations from time series", 
  "toolName": "jep", 
  "abstract": "MP-GeneticSynth is a Java tool for discovering the logic and regulation mechanisms responsible for observed biological dynamics in terms of finite difference recurrent equations. The software makes use of: (i) metabolic P systems as a modeling framework, (ii) an evolutionary approach to discover flux regulation functions as linear combinations of given primitive functions, (iii) a suitable reformulation of the least squares method to estimate function parameters considering simultaneously all the reactions involved in complex dynamics. The tool is available as a plugin for the virtual laboratory MetaPlab. It has graphical and interactive interfaces for data preparation, a priori knowledge integration, and flux regulator analysis. Availability and implemention: Source code, binaries, documentation (including quick start guide and videos) and case studies are freely available at", 
  "summary": "The software makes use of: (i) metabolic P systems as a modeling framework, (ii) an evolutionary approach to discover flux regulation functions as linear combinations of given primitive functions, (iii) a suitable reformulation of the least squares method to estimate function parameters considering simultaneously all the reactions involved in complex dynamics.\nHere we present MP-GeneticSynth, a tool based on GA and multivariate regression for the synthesis and the analysis of flux regulation functions from observed time series within the modeling framework of Metabolic P systems (MP systems) (Manca, 2013).", 
  "affiliations": [
    " Department of Computer Science Verona University "
  ], 
  "grants": [
    "Funding\nThe first author was financially supported by CBMC (Center for Biomedical Computing), University of Verona."
  ], 
  "acks": " ", 
  "authors": [
    " Alberto Castellini", 
    " Daniele Paltrinieri", 
    " Vincenzo Manca"
  ], 
  "keyWords": [
    "systems biology", 
    [
      "biological", 
      "mainly", 
      "modelling", 
      "functionalities", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://www.jfree.org/jfreechart", 
    "http://sourceforge.net/projects/jep", 
    "http://mplab.sci.univr.it/plugins/mpgs/index", 
    "https://code"
  ], 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-10-25T03:54:31Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/jep/", 
    "languages": [
      "Java"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/pfafrich/", 
        "name": "Richard Morris"
      }, 
      {
        "url": "https://sourceforge.net/u/nathanfunk/", 
        "name": "Nathan Funk"
      }, 
      {
        "url": "https://sourceforge.net/u/xcheffo/", 
        "name": "Stephen Kolaroff"
      }, 
      {
        "url": "https://sourceforge.net/u/salixalba/", 
        "name": "Richard Morris"
      }
    ], 
    "Development Status": [
      {
        "status": "5 - Production/Stable"
      }
    ], 
    "license": [
      {
        "name": "GNU General Public License version 3.0 (GPLv3)"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu814", 
  "name": "MpTheory Java library a multiplatform Java library for systems biology based on the Metabolic P theory", 
  "links": [
    "http://mplab", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://mptheory.scienze.univr.it"
  ], 
  "title": "MpTheory Java library: a multi-platform Java library for systems biology based on the Metabolic P theory", 
  "toolName": "MpTheory Java library: a multi-platform Java library for systems biology based on the Metabolic P theory", 
  "abstract": "MpTheory Java library is an open-source project collecting a set of objects and algorithms for modeling observed dynamics by means of the Metabolic P (MP) theory, that is, a mathematical theory introduced in 2004 for modeling biological dynamics. By means of the library, it is possible to model biological systems both at continuous and at discrete time. Moreover, the library comprises a set of regression algorithms for inferring MP models starting from time series of observations. To enhance the modeling experience, beside a pure Java usage, the library can be directly used within the most popular computing environments, such as MATLAB, GNU Octave, Mathematica and R. Availability and implementation: The library is open-source and licensed under the GNU Lesser General Public License (LGPL) Version 3.0. Source code, binaries and complete documentation are available at http://mptheory.", 
  "summary": "Summary: MpTheory Java library is an open-source project collecting a set of objects and algorithms for modeling observed dynamics by means of the Metabolic P (MP) theory, that is, a mathematical theory introduced in 2004 for modeling biological dynamics.\nOnce a model has been created, the library is equipped with state of the art simulators for computing the system dynamics, which have been specifically developed within the MP theory (see Manca, 2013, Supplementary Material and online documentation of the software).\nWe presented a new open-source Java library for modeling biological dynamics by means of the MP theory.", 
  "affiliations": [], 
  "grants": [
    "The library can be directly used within the most popular computing environments, such as MATLAB, GNU Octave, Mathematica and R.\nFunding\nThis work was supported by the Center for Biomedical Computing (CBMC) of the University of Verona, Italy."
  ], 
  "acks": " ", 
  "authors": [
    " Luca Marchetti", 
    " Vincenzo Manca"
  ], 
  "keyWords": [
    "systems biology", 
    [
      "manca", 
      "computational", 
      "java", 
      "modelling", 
      "algorithms", 
      "biological", 
      "regression"
    ]
  ], 
  "sourcelinks": [
    "http://mptheory.scienze.univr.it", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://mplab"
  ], 
  "technologies": [
    "R", 
    "Mathematica", 
    "Java", 
    "MATLAB"
  ], 
  "dateCreated": "2014-12-12T02:23:08Z"
}{
  "doi": "10.1093/bioinformatics/btu564", 
  "name": "MR_predictor a simulation engine for Mendelian Randomization studies", 
  "links": [
    "http://coruscant.itmat.upenn.edu/mr", 
    "http://cran", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetics and population analysis MR_predictor: a simulation engine for Mendelian Randomization studies", 
  "toolName": "Genetics and population analysis MR_predictor: a simulation engine for Mendelian Randomization studies", 
  "abstract": "I present MR_predictor, a simulation engine designed to guide the development and interpretation of statistical tests of caus-ality between phenotypes using genetic instruments. MR_predictor provides a framework to model either individual traits or complex scenarios where multiple phenotypes are correlated or dependent on each other. Crucially, MR_predictor can incorporate the effects of multiple biallelic loci (linked or unlinked) contributing genotypic variability to one or more simulated phenotypes. The software has a range of options for sample generation, and output files generated by MR_predictor port into commonly used analysis tools (e.g. PLINK, R), facilitating analyses germane for Mendelian Randomization studies. Benchmarks for speed and power calculations for summary statistic-based Mendelian Randomization analyses are presented and compared with analytical expectation. Availability and implementation: The simulation engine is implemented in PERL, and the associated scripts can be downloaded from github.com, and online documentation, tutorial and example datasets are available at", 
  "summary": "Results presented in Table 1 demonstrate that simulation closely matches analytical expectations (Equation 1) for a single genetic instrument, for multiple genetic instrumental variables assembled into a single score-based MR analysis (Johnson, 2012) and, finally, when the effect of the instrument and/or sample sizes also vary.\nI simulated 10 000 individuals for three biomarker traits--high and low density lipoprotein cholesterol levels and triglyercides (HDL-C, LDL-C and TG, respectively)--with between-trait correlation determined from epidemiological observation (Emerging Risk Factors Collaboration et al., 2009), using 139 variants with 175 variant to trait relationships (Global Lipids Genetics Consortium et al., 2013).", 
  "affiliations": [
    " Department of Pharmacology", 
    " Department of Genetics University of Pennsylvania -Perelman School of Medicine "
  ], 
  "grants": [
    "Funding: The author is indebted to the Alfred P. Sloan Foundation (BR2012-087), the American Heart Association (13SDG14330006), and the W.W. Smith Charitable Trust (H1201) who provided support for the work."
  ], 
  "acks": " ", 
  "authors": [
    " Benjamin F Voight", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    [
      "associations", 
      "genetics", 
      "statistically", 
      "traits", 
      "bioinformatics", 
      "simulating"
    ]
  ], 
  "sourcelinks": [
    "http://coruscant.itmat.upenn.edu/mr"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-28T02:52:42Z"
}{
  "doi": "10.1093/bioinformatics/btu750", 
  "name": "Mpath a compass for navigating potential metabolic pathways", 
  "links": [
    "http://bp.scitec.kobe-u.ac.jp", 
    "http://cytoscapeweb.cytoscape.org", 
    "http://bp.scitec.kobe-u.ac.jp/m-path/aa/).We", 
    "http://www.mon", 
    "http://bp.scitec", 
    "http://www.gnu.org/software", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://openbabel.org/wiki/Tutorial:Fingerprints", 
    "http://bp.scitec.kobe-u.ac.jp/mpath/aa/).is"
  ], 
  "title": "M-path: a compass for navigating potential metabolic pathways", 
  "toolName": "M-path: a compass for navigating potential metabolic pathways", 
  "abstract": "Motivation: Construction of synthetic metabolic pathways promises sustainable production of diverse chemicals and materials. To design synthetic metabolic pathways of high value, computational methods are needed to expand present knowledge by mining comprehensive chemical and enzymatic information databases. Several computational methods have been already reported for the metabolic pathway design, but until now computation complexity has limited the diversity of chemical and enzymatic data used. Results: We introduce a computational platform, M-path, to explore synthetic metabolic pathways including putative enzymatic reactions and compounds. M-path is an iterative random algorithm, which makes efficient use of chemical and enzymatic databases to find potential synthetic metabolic pathways. M-path can readily control the search space and perform well compared with exhaustively enumerating possible pathways. A web-based pathway viewer is also developed to check extensive metabolic pathways with evaluation scores on the basis of chemical similarities. We further produce extensive synthetic metabolic pathways for a comprehensive set of alpha amino acids. The scalable nature of M-path enables us to calculate potential metabolic pathways for any given chemicals. Availability and implementation: The web tool and viewer are available for free at", 
  "summary": "Results: We introduce a computational platform, M-path, to explore synthetic metabolic pathways including putative enzymatic reactions and compounds.\nAs databases of enzymatic reactions and compounds have increased in size, computational methods have become necessary to identify the key enzymatic reaction steps for efficient synthetic pathway design (Kanehisa et al., 2008; Schomburg et al., 2013).\nHere, we introduce a computational platform, M-path, for synthetic pathway design, which makes efficient use of extensive enzymatic reaction and chemical compound databases.\nThe chemical similarities between assigned compounds in M-path calculations and substrates or products in known enzymatic reactions can thus be an index for evaluating the applicability of the enzymes and pathways.", 
  "affiliations": [
    " Department of Chemical Science and Engineering Graduate School of Engineering Kobe University ", 
    " MCHC R&D Synergy Center, Inc", 
    " Organization of Advanced Science and Technology Kobe University ", 
    " Mitsui Knowledge Industry (MKI) Co", 
    " Mitsubishi Chemical Group Science and Technology Research Center MCRC) Inc "
  ], 
  "grants": [
    "Funding\nThis work was supported by Grant-in-Aid for Scientific Research (KAKENHI) on Innovative Areas [No."
  ], 
  "acks": " ", 
  "authors": [
    " Michihiro Araki", 
    " Robert Sidney Cox Iii", 
    " Hiroki Makiguchi", 
    " Teppei Ogawa", 
    " Takeshi Taniguchi", 
    " Kohei Miyaoku", 
    " Masahiko Nakatsui", 
    " Kiyotaka Y Hara", 
    " Akihiko Kondo"
  ], 
  "keyWords": [
    "metabolic pathways", 
    [
      "reactions", 
      "compounds", 
      "enzymes", 
      "pathway", 
      "chemicals", 
      "metabolism"
    ]
  ], 
  "sourcelinks": [
    "http://www.gnu.org/software"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-15T04:10:48Z"
}{
  "doi": "10.1093/bioinformatics/btu633", 
  "name": "MTide an integrated tool for the identification of miRNAtarget interaction in plants", 
  "links": [
    "http://bis.zju.edu.cn", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bis.zju.edu.cn/MTide"
  ], 
  "title": "Data and text mining MTide: an integrated tool for the identification of miRNA\u2013target interaction in plants", 
  "toolName": "Data and text mining MTide: an integrated tool for the identification of miRNA\u2013target interaction in plants", 
  "abstract": "Motivation: Small RNA sequencing and degradome sequencing (also known as parallel analysis of RNA ends) have provided rich information on the microRNA (miRNA) and its cleaved mRNA targets on a genome-wide scale in plants, but no computational tools have been developed to effectively and conveniently deconvolute the miRNA\u2013target interaction (MTI). Results: A freely available package, MTide, was developed by combining modified miRDeep2 and CleaveLand4 with some other useful scripts to explore MTI in a comprehensive way. By searching for targets of a complete miRNAs, we can facilitate large-scale identification of miRNA targets, allowing us to discover regulatory interaction networks. Availability and implementation: http://bis.zju.edu.cn/MTide", 
  "summary": "ABSTRACT Motivation: Small RNA sequencing and degradome sequencing (also known as parallel analysis of RNA ends) have provided rich information on the microRNA (miRNA) and its cleaved mRNA targets on a genome-wide scale in plants, but no computational tools have been developed to effectively and conveniently deconvolute the miRNAtarget interaction (MTI).\nSmall RNA sequencing and degradome sequencing have been widely used in plants to identify miRNAs and target genes on a genome-wide scale.\nBased on deep sampling of small RNA libraries and degradome fragments of target mRNA by next-generation sequencing, MTide enables users to explore expression patterns of known miRNA genes, discover novel ones and identify target mRNA of these miRNAs. Figure 1 illustrates the workflow of MTide.", 
  "affiliations": [
    " Department of Control Science and Engineering Zhejiang University ", 
    " Department of Bioinformatics College of Life Sciences State Key Laboratory of Plant Physiology and Biochemistry Zhejiang University "
  ], 
  "grants": [
    "Funding: This work was supported by the National Natural Sciences Foundation of China [31371328, 30971743], National Science and Technology Project of China [2008AA10Z125]; the Fundamental Research Funds for the Central Universities, and the Program for Innovative Research Team in University."
  ], 
  "acks": " ", 
  "authors": [
    " Zhao Zhang", 
    " Li Jiang", 
    " Jingjing Wang", 
    " Peizhen Gu", 
    " Ming Chen"
  ], 
  "keyWords": [
    "rna sequencing", 
    [
      "sequenced", 
      "mirnas", 
      "micrornas", 
      "bioinformatics", 
      "mtide", 
      "targets"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R", 
    "Perl"
  ], 
  "dateCreated": "2014-09-26T00:20:55Z"
}{
  "doi": "10.1093/bioinformatics/btu485", 
  "name": "Monte Carlo algorithms for Brownian phylogenetic models", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Phylogenetics Monte Carlo algorithms for Brownian phylogenetic models", 
  "toolName": "Phylogenetics Monte Carlo algorithms for Brownian phylogenetic models", 
  "abstract": "Motivation: Brownian models have been introduced in phylogenetics for describing variation in substitution rates through time, with applications to molecular dating or to the comparative analysis of variation in substitution patterns among lineages. Thus far, however, the Monte Carlo implementations of these models have relied on crude approximations , in which the Brownian process is sampled only at the internal nodes of the phylogeny or at the midpoints along each branch, and the unknown trajectory between these sampled points is summarized by simple branchwise average substitution rates. Results: A more accurate Monte Carlo approach is introduced, explicitly sampling a fine-grained discretization of the trajectory of the (potentially multivariate) Brownian process along the phylogeny. Generic Monte Carlo resampling algorithms are proposed for updating the Brownian paths along and across branches. Specific computational strategies are developed for efficient integration of the finite-time substitution probabilities across branches induced by the Brownian trajectory. The mixing properties and the computational complexity of the resulting Markov chain Monte Carlo sampler scale reasonably with the discretization level, allowing practical applications with up to a few hundred discretization points along the entire depth of the tree. The method can be generalized to other Markovian stochas-tic processes, making it possible to implement a wide range of time-dependent substitution models with well-controlled computational precision. Availability: The program is freely available at www.phylobayes.org", 
  "summary": "This error incurred on the estimation of the covariance matrix can be explained by the fact that the variance contributed at the levels of branch-specific rates or substitution patterns by the randomness of the Brownian paths, which is ignored under the branchwise approximation, is absorbed by the values taken by the Brownian process at the nodes of the phylogeny.\nWhile confirming earlier observations that the classical branchwise approximation of Brownian substitution models gives qualitatively acceptable results (Lartillot and Poujol, 2011), the present simulations nevertheless suggest that a more fine-grained computational approach leads to increased accuracy in the estimation of those features of the model, ancestral rates and covariance matrix, that are of more direct relevance in a comparative perspective.", 
  "affiliations": [
    " UMR 5558 Laboratoire de Biom etrie Universit e de Lyon Universit e Lyon 1 CNRS "
  ], 
  "grants": [
    "Funding: Natural Sciences and Engineering Research Council of Canada (NSERC); French National Research Agency, Grant ANR-10-BINF-01-01 \"Ancestrome\"."
  ], 
  "acks": " The author wishes to thank two anonymous reviewers for their useful comments on this manuscript. ", 
  "authors": [
    " Benjamin Horvilleur", 
    " Nicolas Lartillot"
  ], 
  "keyWords": [
    "substitution rates", 
    [
      "processes", 
      "computationally", 
      "brownian", 
      "rate", 
      "times", 
      "substitutional", 
      "modeling"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-23T06:44:50Z"
}{
  "doi": "10.1093/bioinformatics/btu483", 
  "name": "MToolBox a highly automated pipeline for heteroplasmy annotation and prioritization analysis of human mitochondrial variants in highthroughput sequencing", 
  "links": [
    "https://sourceforge.net/projects/mtoolbox/files/1000Genomes", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/3.0", 
    "https://sourceforge.net/projects/mtoolbox"
  ], 
  "title": "Genome analysis MToolBox: a highly automated pipeline for heteroplasmy annotation and prioritization analysis of human mitochondrial variants in high-throughput sequencing", 
  "toolName": "mtoolbox", 
  "abstract": "Motivation: The increasing availability of mitochondria-targeted and off-target sequencing data in whole-exome and whole-genome sequencing studies (WXS and WGS) has risen the demand of effective pipelines to accurately measure heteroplasmy and to easily recognize the most functionally important mitochondrial variants among a huge number of candidates. To this purpose, we developed MToolBox, a highly automated pipeline to reconstruct and analyze human mito-chondrial DNA from high-throughput sequencing data. Results: MToolBox implements an effective computational strategy for mitochondrial genomes assembling and haplogroup assignment also including a prioritization analysis of detected variants. MToolBox provides a Variant Call Format file featuring, for the first time, allele-specific heteroplasmy and annotation files with prioritized variants. MToolBox was tested on simulated samples and applied on 1000 Genomes WXS datasets. Availability and implementation: MToolBox package is available at https://sourceforge.net/projects/mtoolbox/.", 
  "summary": "The MToolBox workflow includes a computational strategy to assemble mitochondrial genomes from whole-exome sequencing (WXS) and/or whole-genome sequencing (WGS) data (Picardi and Pesole, 2012), which was further updated to detect insertions and deletions (ins/dels) and to assess the heteroplasmic fraction (HF) of each variant allele with the related confidence interval (CI), reported as sample-specific meta-information in an enhanced version of the Variant Call Format (VCF) file (version 4.0).\nEach set of contigs is subjected to haplogroup prediction, relying on the RSRS-based Phylotree resource (van Oven and Kayser, 2009), by mt-classifier (Fig. 1l), an updated version of the fragment-classify tool (Rubino et al., 2012), which now includes a module to perform functional annotation and prioritization of mitochondrial variants (Fig. 1m and Supplementary Information).", 
  "affiliations": [
    " Department of Sciences and Technologies University of Sannio ", 
    " Department of Biosciences, Biotechnologies and Biopharmaceutics University of Bari ", 
    " Department of Biosciences University of Milan ", 
    " Department of Medical and Surgical Sciences University of Bologna "
  ], 
  "grants": [
    "Funding: This work was supported by Progetto Strategico `Invecchiamento' e `Medicina Personalizzata' (CNR, Italy) and the PRIN2009 fund assigned to M.A."
  ], 
  "acks": " ", 
  "authors": [
    " Claudia Calabrese", 
    " Domenico Simone", 
    " Maria Angela Diroma", 
    " Mariangela Santorsola", 
    " Cristiano Gutt\u00e0", 
    " Giuseppe Gasparre", 
    " Ernesto Picardi", 
    " Graziano Pesole", 
    " Marcella Attimonelli", 
    " "
  ], 
  "keyWords": [
    "mitochondrial variants", 
    [
      "files", 
      "genomics", 
      "mtoolbox", 
      "variant", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [
    "https://sourceforge.net/projects/mtoolbox", 
    "https://sourceforge.net/projects/mtoolbox/files/1000Genomes"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-16T00:23:13Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/mtoolbox/", 
    "languages": [
      "Unix Shell", 
      "Python"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/maridir/", 
        "name": "Maria Angela Diroma"
      }, 
      {
        "url": "https://sourceforge.net/u/mary09/", 
        "name": "Mariangela"
      }, 
      {
        "url": "https://sourceforge.net/u/claudia23/", 
        "name": "Claudia Calabrese"
      }, 
      {
        "url": "https://sourceforge.net/u/ros1985/", 
        "name": "Rosanna Clima"
      }, 
      {
        "url": "https://sourceforge.net/u/robertopreste/", 
        "name": "Roberto Preste"
      }, 
      {
        "url": "https://sourceforge.net/u/domesimone/", 
        "name": "Domenico Simone"
      }
    ], 
    "Development Status": [], 
    "license": [
      {
        "name": "GNU General Public License version 3.0 (GPLv3)"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu648", 
  "name": "MulRF a software package for phylogenetic analysis using multicopy gene trees", 
  "links": [
    "http://genome.cs.iastate.edu/CBL/MulRF", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/ruchiherself"
  ], 
  "title": "MulRF: a software package for phylogenetic analysis using multi-copy gene trees", 
  "toolName": "ruchiherself", 
  "abstract": "MulRF is a platform-independent software package for phylogenetic analysis using multi-copy gene trees. It seeks the species tree that minimizes the Robinson\u2013Foulds (RF) distance to the input trees using a generalization of the RF distance to multi-labeled trees. The underlying generic tree distance measure and fast running time make MulRF useful for inferring phylogenies from large collections of gene trees, in which multiple evolutionary processes as well as phylogenetic error may contribute to gene tree discord. MulRF implements several features for customizing the species tree search and assessing the results, and it provides a user-friendly graphical user interface (GUI) with tree visualization. The species tree search is implemented in C++ and the GUI in Java Swing. Availability: MulRF's executable as well as sample datasets and manual are available at", 
  "summary": "The MulRF distance between a species tree and an input gene tree can be computed in time linear in the number of leaves of the multicopy gene tree (Chaudhary et al., 2013).\nBoth studies demonstrated that MulRF can easily run on datasets with hundreds of taxa and a thousand gene trees, and it often provides more accurate phylogenetic estimates than iGTP, SPRSupertree and PHYLDOG (Chaudhary et al., 2013, 2014).\nWe executed MulRF on a set of 6966 gene trees from 36 mammalian species that were inferred using PhyML (Guindon et al., 2010) by Boussau et al.", 
  "affiliations": [
    " Department of Biology University of Florida ", 
    " Department of Computer Science Iowa State University "
  ], 
  "grants": [
    "Funding: The National Science Foundation (DEB-1208428 and CCF-1017189)."
  ], 
  "acks": " The authors thank the reviewers for their thoughtful comments. Funding: The National Science Foundation (DEB-1208428 and CCF-1017189). ", 
  "authors": [
    " Ruchi Chaudhary", 
    " David Fern Andez-Baca", 
    " John Gordon Burleigh"
  ], 
  "keyWords": [
    "species tree", 
    "gene trees", 
    [
      "chaudhary", 
      "phylogenetics", 
      "genes", 
      "mulrf"
    ]
  ], 
  "sourcelinks": [
    "http://genome.cs.iastate.edu/CBL/MulRF", 
    "https://github.com/ruchiherself"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu740", 
  "name": "MUSCLE automated multiobjective evolutionary optimization of targeted LCMSMS analysis", 
  "links": [
    "http://www.muscleproject.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "MUSCLE: automated multi-objective evolutionary optimization of targeted LC-MS/MS analysis", 
  "toolName": "MUSCLE: automated multi-objective evolutionary optimization of targeted LC-MS/MS analysis", 
  "abstract": "Developing liquid chromatography tandem mass spectrometry (LC-MS/MS) analyses of (bio)chemicals is both time consuming and challenging, largely because of the large number of LC and MS instrument parameters that need to be optimized. This bottleneck significantly impedes our ability to establish new (bio)analytical methods in fields such as pharmacology, metabolomics and pesticide research. We report the development of a multi-platform, user-friendly software tool MUSCLE (multi-platform unbiased optimization of spectrometry via closed-loop experimentation) for the robust and fully automated multi-objective optimization of targeted LC-MS/MS analysis. MUSCLE shortened the analysis times and increased the analytical sensitivities of targeted metab-olite analysis, which was demonstrated on two different manufacturer's LC-MS/MS instruments. Availability and implementation: Available at", 
  "summary": "A multi-objective genetic algorithm (GA) optimizes the values of the LC and MS parameters, based upon the fitness of user-defined objective functions that measure, e.g. analytical sensitivity and analysis time (Section 2.3).\nThis includes: (i) details of the target list of (bio)chemicals to be analysed, including m/z values of the parent and fragment ions (e.g. Supplementary Fig. S2 and Table S1), (ii) settings for the GA including the user-defined objective functions (see Section 2.3 and Supplementary Table S2) and (iii) user-defined list of LC and MS parameters to be optimized, where each parameter has an associated visual script and minimum, maximum and step size values (e.g. Supplementary Table S3).", 
  "affiliations": [
    " School of Computer Science The University of Manchester ", 
    " School of Computer Science", 
    " School of Biosciences University of Birmingham "
  ], 
  "grants": [
    "Funding\nThis work was supported by UK Biotechnology and Biological Sciences Research Council [BB/I024085/1]."
  ], 
  "acks": " We thank Steve O'Hagan for technical advice. ", 
  "authors": [
    " James Bradbury", 
    " Gr\u00e9 Gory Genta-Jouve", 
    " J William Allwood", 
    " Warwick B Dunn", 
    " Royston Goodacre", 
    " Joshua D Knowles", 
    " Shan He", 
    " Mark R Viant"
  ], 
  "keyWords": [
    [
      "instrumentation", 
      "optimizations", 
      "objectives", 
      "times", 
      "supplementary", 
      "analytical", 
      "muscle"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-11-12T04:44:24Z"
}{
  "doi": "10.1093/bioinformatics/btu612", 
  "name": "NAIL a software toolset for inferring analyzing and visualizing regulatory networks", 
  "links": [
    "https://sourceforge.net/projects/nailsystemsbiology", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Systems biology NAIL, a software toolset for inferring, analyzing and visualizing regulatory networks", 
  "toolName": "nailsystemsbiology", 
  "abstract": "The wide variety of published approaches for the problem of regulatory network inference makes using multiple inference algorithms complex and time-consuming. Network Analysis and Inference Library (NAIL) is a set of software tools to simplify the range of computational activities involved in regulatory network inference. It uses a modular approach to connect different network inference algorithms to the same visualization and network-based analyses. NAIL is technology-independent and includes an interface layer to allow easy integration of components into other applications. Availability and implementation: NAIL is implemented in MATLAB, runs on Windows, Linux and OSX, and is available from SourceForge at https://sourceforge.net/projects/nailsystemsbiology/ for all researchers to use.", 
  "summary": "Motivated by results from the DREAM reverse-engineering consortium suggesting that combinations of network inference algorithms can produce more biologically relevant networks than individual algorithms (Prill et al., 2011), we developed an extensible framework to simplify the use of multiple network inference methods simultaneously, and to visualize and compare their results.\nIn a previous study (Hurley et al., 2011), we described the initial development of this work and illustration of different biological meanings for regulatory networks inferred using different methods.\nIn this article, we describe a new framework developed from our previous methods, this time intended for researchers in systems biology and bioinformatics, called NAIL (Network Analysis and Inference Library).", 
  "affiliations": [
    " Maurice Wilkins Centre University of Auckland ", 
    " Melbourne School of Engineering Systems Biology Laboratory University of Melbourne ", 
    " Auckland Bioengineering Institute University of Auckland "
  ], 
  "grants": [
    "Funding: This work was supported by University of Auckland Doctoral Scholarship (to D.H, J.C, Y,W."
  ], 
  "acks": " The authors gratefully acknowledge the assistance of K. Wong and M. Pan with review, feedback and software testing. ", 
  "authors": [
    " Daniel G Hurley", 
    " Joseph Cursons", 
    " Yi Kan Wang", 
    " David M Budden", 
    " Cristin G Print", 
    " Edmund J Crampin", 
    " "
  ], 
  "keyWords": [
    "network inference", 
    [
      "methods", 
      "biologically", 
      "networks", 
      "bioinformatics", 
      "data", 
      "university", 
      "inferring"
    ]
  ], 
  "sourcelinks": [
    "https://sourceforge.net/projects/nailsystemsbiology"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-23T02:41:35Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/nailsystemsbiology/", 
    "languages": [
      "MATLAB"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/jcursons/", 
        "name": "Joe Cursons"
      }, 
      {
        "url": "https://sourceforge.net/u/buddend/", 
        "name": "David Budden"
      }, 
      {
        "url": "https://sourceforge.net/u/danielhurley/", 
        "name": "Daniel G Hurley"
      }
    ], 
    "Development Status": [
      {
        "status": "5 - Production/Stable"
      }
    ], 
    "license": [
      {
        "name": "Apache License V2.0"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu712", 
  "name": "Navigating protected genomics data with UCSC Genome Browser in a Box", 
  "links": [
    "http://genome.ucsc.edu/license", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://genome-store.ucsc.edu", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Navigating protected genomics data with UCSC Genome Browser in a Box", 
  "toolName": "Navigating protected genomics data with UCSC Genome Browser in a Box", 
  "abstract": "Genome Browser in a Box (GBiB) is a small virtual machine version of the popular University of California Santa Cruz (UCSC) Genome Browser that can be run on a researcher's own computer. Once GBiB is installed, a standard web browser is used to access the virtual server and add personal data files from the local hard disk. Annotation data are loaded on demand through the Internet from UCSC or can be downloaded to the local computer for faster access. Availability and implementation: Software downloads and installation instructions are freely available for non-commercial use at https://genome-store.ucsc.edu/. GBiB requires the installation of open-source software VirtualBox, available for all major operating systems, and the UCSC Genome Browser, which is open source and free for non-commercial use. Commercial use of GBiB and the Genome Browser requires a license", 
  "summary": "The University of California Santa Cruz (UCSC) Genome Browser (Karolchik et al., 2014; Kent et al., 2002) has supported user-generated custom annotation tracks since 2001, and recently added support for track and assembly data hubs (Nguyen et al., 2014; Raney et al., 2014) that allow the user to host collections of large data files on a local server and configure many details of the visualization.\nThe UCSC Genome Browser in a Box (GBiB) circumvents these issues, offering secure local use of private data files while still providing access to the full suite of UCSC Genome Browser tools and annotations.", 
  "affiliations": [
    " Center for Biomolecular Science and Engineering School of Engineering University of California Santa Cruz "
  ], 
  "grants": [
    "Funding\nThis work was funded by the National Human Genome Research Institute (grant numbers P41HG002371, U41HG004568 to M.H., B.J.R., A.H., H.C., A.S.Z., D.K., J.C., M.S., W.J.K.", 
    "); a fellowship by the European Molecular Biology Organization (grant number ALTF-2011-292 to M.H.)"
  ], 
  "acks": " We would like to thank the many Genome Browser users and collaborators who provided support, feedback, and suggestions during the development of Genome Browser in a Box. Thanks to Casey Callow for his assistance with Fig. 1. ", 
  "authors": [
    " Maximilian Haeussler", 
    " Brian J Raney", 
    " Angie S Hinrichs", 
    " Hiram Clawson", 
    " Ann S Zweig", 
    " Donna Karolchik", 
    " Jonathan Casper", 
    " Matthew L Speir", 
    " David Haussler", 
    " W James Kent"
  ], 
  "keyWords": [
    "genomics data", 
    [
      "gbib", 
      "accessing", 
      "database", 
      "browsers", 
      "bioinformatics", 
      "genomic"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://genome-store.ucsc.edu"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-28T04:40:37Z"
}{
  "doi": "10.1093/bioinformatics/btu778", 
  "name": "Multilevel regularized regression for simultaneous taxa selection and network construction with metagenomic count data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://biostatistics"
  ], 
  "title": "Multilevel regularized regression for simultaneous taxa selection and network construction with metagenomic count data", 
  "toolName": "Multilevel regularized regression for simultaneous taxa selection and network construction with metagenomic count data", 
  "abstract": "Motivation: Identifying disease associated taxa and constructing networks for bacteria interactions are two important tasks usually studied separately. In reality, differentiation of disease associated taxa and correlation among taxa may affect each other. One genus can be differentiated because it is highly correlated with another highly differentiated one. In addition, network structures may vary under different clinical conditions. Permutation tests are commonly used to detect differences between networks in distinct phenotypes, and they are time-consuming. Results: In this manuscript, we propose a multilevel regularized regression method to simultaneously identify taxa and construct networks. We also extend the framework to allow construction of a common network and differentiated network together. An efficient algorithm with dual formulation is developed to deal with the large-scale n (m problem with a large number of taxa (m) and a small number of samples (n) efficiently. The proposed method is regularized with a general L p (p 2 \u00bd0; 2\u008a) penalty and models the effects of taxa abundance differentiation and correlation jointly. We demonstrate that it can identify both true and biologically significant genera and network structures. Availability and implementation: Software MLRR in MATLAB is available at", 
  "summary": "L1-based approaches automatically identify a number of dependent genes and are computationally efficient for large-scale network construction, but these approaches construct the network without considering differentiation of the genes across different clinical conditions.\nWe build regression models with 100 different ks and 20 different ps, the optimal k and p are then selected with either the minimal mean squared error (MSE) of the test data or the most stable parameter (edge) estimation with k-fold cross-validation.\nComparing with available methods for network construction and feature selection in the literature, the proposed approach identifies true features and biologically important genera, and constructs common and differentiated networks jointly with high accuracy.", 
  "affiliations": [
    " Department of Pathology and Laboratory Medicine David Geffen School of Medicine at UCLA ", 
    " Molecular and Computational Biology Program Department of Biological Sciences USC ", 
    " Cedars-Sinai Medical Center Samuel Oschin Comprehensive Cancer Institute ", 
    " F. Widjaja Foundation -Inflammatory Bowel and Immunobiology Research Institute Cedars-Sinai Medical Center "
  ], 
  "grants": [
    "Funding\nThis work was partially supported by grants from National Science Foundation [DMAC: ADT-1220747 to Z.L."
  ], 
  "acks": " ", 
  "authors": [
    " Zhenqiu Liu", 
    " Fengzhu Sun", 
    " Jonathan Braun", 
    " Dermot P B Mcgovern", 
    " Steven Piantadosi"
  ], 
  "keyWords": [
    [
      "models", 
      "data", 
      "networks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-22T02:58:16Z"
}{
  "doi": "10.1093/bioinformatics/btu436", 
  "name": "Multifactor data normalization enables the detection of copy number aberrations in amplicon sequencing data", 
  "links": [
    "http://oncocnv.curie.fr/4", 
    "http://oncocnv.curie.fr", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/3.0", 
    "http://www.softgenetics.com/NextGENe_013.html"
  ], 
  "title": "Genome analysis Multi-factor data normalization enables the detection of copy number aberrations in amplicon sequencing data", 
  "toolName": "Genome analysis Multi-factor data normalization enables the detection of copy number aberrations in amplicon sequencing data", 
  "abstract": "Motivation: Because of its low cost, amplicon sequencing, also known as ultra-deep targeted sequencing, is now becoming widely used in oncology for detection of actionable mutations, i.e. mutations influencing cell sensitivity to targeted therapies. Amplicon sequencing is based on the polymerase chain reaction amplification of the regions of interest, a process that considerably distorts the information on copy numbers initially present in the tumor DNA. Therefore, additional experiments such as single nucleotide polymorphism (SNP) or comparative genomic hybridization (CGH) arrays often complement ampli-con sequencing in clinics to identify copy number status of genes whose amplification or deletion has direct consequences on the efficacy of a particular cancer treatment. So far, there has been no proven method to extract the information on gene copy number aberrations based solely on amplicon sequencing. Results: Here we present ONCOCNV, a method that includes a multi-factor normalization and annotation technique enabling the detection of large copy number changes from amplicon sequencing data. We validated our approach on high and low amplicon density datasets and demonstrated that ONCOCNV can achieve a precision comparable with that of array CGH techniques in detecting copy number aberrations. Thus, ONCOCNV applied on amplicon sequencing data would make the use of additional array CGH or SNP array experiments unnecessary. Availability and implementation:", 
  "summary": "Results: Here we present ONCOCNV, a method that includes a multifactor normalization and annotation technique enabling the detection of large copy number changes from amplicon sequencing data.\nAlthough we also compared our method with CONTRA (Li et al., 2012)--a method to detect one-exon copy number changes as well as large CNAs in targeted sequencing data--we do not report the results of this comparison in this article as, in our data, CONTRA identified 510% of the true CNAs (Supplementary Fig. S14).\nIn the SHIVA trial, a dual approach is used: amplicon sequencing is used to detect single nucleotide variants and indels, whereas SNP arrays provide information about the copy number status of the genes and contamination by normal cells.", 
  "affiliations": [
    " Institut de Pathologie et de G en etique", 
    " Institut Curie Centre de Recherche Genetics and Biology of Cancers ", 
    " Clinical Research Department"
  ], 
  "grants": [
    "Funding: The SHIVA trial is supported by the grant ANR-10EQPX-03 from the Agence Nationale de le Recherche (Investissements d'avenir) and SiRIC (Site de Recherche Integre sur le Cancer).", 
    "High-throughput sequencing was performed by the NGS platform of the Institut Curie, supported by grants ANR-10-EQPX-03 and ANR10-INBS-09-08 from the Agence Nationale de la Recherche (Investissements d'avenir) and by the Canceropo^ le Ile-de-France."
  ], 
  "acks": " ", 
  "authors": [
    " Valentina Boeva", 
    " Tatiana Popova", 
    " Maxime Lienard", 
    " Sebastien Toffoli", 
    " Maud Kamal", 
    " Christophe Le Tourneau", 
    " David Gentien", 
    " Nicolas Servant", 
    " Pierre Gestraud", 
    " Thomas Rio Frio", 
    " Philippe Hup", 
    " Emmanuel Barillot", 
    " Jean-Franc\u00b8ois Laes"
  ], 
  "keyWords": [
    "amplicon sequencing", 
    [
      "sequenced", 
      "genes", 
      "oncocnv", 
      "sampling", 
      "supplementary", 
      "cancers", 
      "amplicons", 
      "normalization"
    ]
  ], 
  "sourcelinks": [
    "http://www.softgenetics.com/NextGENe_013.html", 
    "http://oncocnv.curie.fr", 
    "http://oncocnv.curie.fr/4", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "C", 
    "R"
  ], 
  "dateCreated": "2014-07-13T00:13:40Z"
}{
  "doi": "10.1093/bioinformatics/btu536", 
  "name": "NetComm a network analysis tool based on communicability", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.shawlab.org/NetComm"
  ], 
  "title": "Genome analysis NetComm: a network analysis tool based on communicability", 
  "toolName": "Genome analysis NetComm: a network analysis tool based on communicability", 
  "abstract": "Motivation: Set-based network similarity metrics are increasingly used to productively analyze genome-wide data. Conventional approaches, such as mean shortest path and clique-based metrics, have been useful but are not well suited to all applications. Computational scientists in other disciplines have developed commu-nicability as a complementary metric. Network communicability considers all paths of all lengths between two network members. Given the success of previous network analyses of protein\u2013protein interactions , we applied the concepts of network communicability to this problem. Here we show that our communicability implementation has advantages over traditional approaches. Overall, analyses suggest network communicability has considerable utility in analysis of large-scale biological networks. Availability and implementation: We provide our method as an R package for use in both human protein\u2013protein interaction network analyses and analyses of arbitrary networks along with a tutorial at", 
  "summary": "NetComm: a network analysis tool based on communicability\nNetwork communicability considers all paths of all lengths between two network members.\nCommunicability considers paths of all lengths between two nodes scaled by the factorial of path length (Estrada and Hatano, 2008), retaining more information about connectedness compared with shortest path metrics.\nThus, we investigated a metric we deem finite network communicability Fi,j considering finite path lengths from 1 to l (Equation 1).\nNat. Protoc., 3, 16161629.\n(2008) Communicability in complex networks.\nNat. Biotechnol., 25, 309316.", 
  "affiliations": [
    " Department of Molecular and Human Genetics Baylor College of Medicine "
  ], 
  "grants": [
    "Funding: I.M.C."
  ], 
  "acks": " The authors thank the Lupski laboratory at BCM for valuable feedback. ", 
  "authors": [
    " Ian M Campbell", 
    " Regis A James", 
    " Edward S Chen", 
    " Chad A Shaw", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "associations", 
      "paths", 
      "diseases", 
      "communicabilities", 
      "proteins", 
      "analysis", 
      "metrics", 
      "networks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-15T05:12:51Z"
}{
  "doi": "10.1093/bioinformatics/btv005", 
  "name": "NLRparser rapid annotation of plant NLR complements", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "NLR-parser: rapid annotation of plant NLR complements", 
  "toolName": "NLR-parser: rapid annotation of plant NLR complements", 
  "abstract": "Motivation: The repetitive nature of plant disease resistance genes encoding for nucleotide-binding leucine-rich repeat (NLR) proteins hampers their prediction with standard gene annotation software. Motif alignment and search tool (MAST) has previously been reported as a tool to support annotation of NLR-encoding genes. However, the decision if a motif combination represents an NLR protein was entirely manual. Results: The NLR-parser pipeline is designed to use the MAST output from six-frame translated amino acid sequences and filters for predefined biologically curated motif compositions. Input reads can be derived from, for example, raw long-read sequencing data or contigs and scaffolds coming from plant genome projects. The output is a tab-separated file with information on start and frame of the first NLR specific motif, whether the identified sequence is a TNL or CNL, potentially full or fragmented. In addition, the output of the NB-ARC domain sequence can directly be used for phylogenetic analyses. In comparison to other prediction software, the highly complex NB-ARC domain is described in detail using several individual motifs. Availability and implementation: The NLR-parser tool can be downloaded from Git-Hub", 
  "summary": "Here, we present an NLR-MAST-parser, a java application for the identification of NLR-like sequences that uses the highly specific amino acid motif composition found in plant NLR gene products and parses this information into an easy-to-use tabular file.\nThe amino acid motif composition of NLR gene products is highly conserved amongst all plant species, sufficient to separate these from other protein sequences and sufficient to separate the two main types of NLRs (TNL and CNL).\nWe further tested the MEME motifs in our NLR-parser for their functionality in monocotyledonous plant genomes and screened the publicly available set of annotated genes from Brachypodium distachyon.", 
  "affiliations": [
    " Department of Crop Genetics John Innes Centre "
  ], 
  "grants": [
    "Funding\nThis work was supported by the 2Blades Foundation and by the Biotechnology and Biological Sciences Research Council (Grant numbers BB/ J003166/1 and BB/L011794/1)."
  ], 
  "acks": " ", 
  "authors": [
    " Burkhard Steuernagel", 
    " Florian Jupe", 
    " Kamil Witek", 
    " Jonathan D G Jones", 
    " Brande B H Wulff", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "plants", 
      "genomics", 
      "genes", 
      "proteins", 
      "motifs", 
      "sequencing", 
      "annotations"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-14T02:18:45Z"
}{
  "doi": "10.1093/bioinformatics/btu501", 
  "name": "NetPathMiner RBioconductor package for network path mining through gene expression", 
  "links": [
    "http://github.com/ahmohamed/NetPathMiner", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining NetPathMiner: R/Bioconductor package for network path mining through gene expression", 
  "toolName": "NetPathMiner", 
  "abstract": "NetPathMiner is a general framework for mining, from genome-scale networks, paths that are related to specific experimental conditions. NetPathMiner interfaces with various input formats including KGML, SBML and BioPAX files and allows for manipulation of networks in three different forms: metabolic, reaction and gene representations. NetPathMiner ranks the obtained paths and applies Markov model-based clustering and classification methods to the ranked paths for easy interpretation. NetPathMiner also provides static and interactive visualizations of networks and paths to aid manual investigation. Availability: The package is available through Bioconductor and from Github at", 
  "summary": "NetPathMiner interfaces with various input formats including KGML, SBML and BioPAX files and allows for manipulation of networks in three different forms: metabolic, reaction and gene representations.\n(Hancock et al., 2010), is limited to mining paths from metabolic networks constructed from KGML files, providing static visualization on one network representation only.\nTo alleviate these differences, NetPathMiner processes files into either one of the two outputs, regardless of input formats: (i) Metabolic networks, in metabolic representation, as a bipartite graph with metabolites and reactions parts, and edges representing production or consumption of metabolites.", 
  "affiliations": [
    " Bioinformatics Center Institute for Chemical Research Kyoto University "
  ], 
  "grants": [
    "Funding: This work is partially supported by KAKENHI (2430054) of MEXT, Japan."
  ], 
  "sourcelinks": [
    "http://github.com/ahmohamed/NetPathMiner"
  ], 
  "acks": " ", 
  "authors": [
    " Ahmed Mohamed", 
    " Timothy Hancock", 
    " Canh Hao Nguyen", 
    " Hiroshi Mamitsuka"
  ], 
  "keyWords": [
    "network path", 
    "networks paths", 
    "metabolic reaction", 
    [
      "reactions", 
      "netpathminer", 
      "differences", 
      "bioinformatics", 
      "visualizations", 
      "metabolism"
    ]
  ], 
  "github_data": {
    "name": "NetPathMiner", 
    "contributors": [
      {
        "contributions": 15, 
        "html_url": "https://github.com/ahmohamed"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/ahmohamed/NetPathMiner/zipball/v1.0.3", 
        "tarball_url": "https://api.github.com/repos/ahmohamed/NetPathMiner/tarball/v1.0.3", 
        "name": "v1.0.3"
      }
    ], 
    "created_at": "2014-03-10T12:31:58Z", 
    "updated_at": "2015-09-08T02:10:52Z", 
    "languages": [
      "Shell", 
      "C", 
      "R", 
      "C++"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/ahmohamed"
      }, 
      {
        "html_url": "https://github.com/kozo2"
      }
    ], 
    "owner": "https://github.com/ahmohamed", 
    "homepage": null
  }, 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-30T00:14:11Z"
}{
  "doi": "10.1093/bioinformatics/btu830", 
  "name": "NMRFAMSPARKY enhanced software for biomolecular NMR spectroscopy", 
  "links": [
    "https://www.cgl.ucsf.edu/home/sparky", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.nmrfam.wisc.edu/nmrfamsparky-distribution.htm", 
    "http://pine.nmrfam.wisc.edu/download_packages.html", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Structural bioinformatics NMRFAM-SPARKY: enhanced software for biomolecular NMR spectroscopy", 
  "toolName": "Structural bioinformatics NMRFAM-SPARKY: enhanced software for biomolecular NMR spectroscopy", 
  "abstract": "SPARKY (Goddard and Kneller, SPARKY 3) remains the most popular software program for NMR data analysis, despite the fact that development of the package by its originators ceased in 2001. We have taken over the development of this package and describe NMRFAM-SPARKY, which implements new functions reflecting advances in the biomolecular NMR field. NMRFAM-SPARKY has been repackaged with current versions of Python and Tcl/Tk, which support new tools for NMR peak simulation and graphical assignment determination. These tools, along with chemical shift predictions from the PACSY database, greatly accelerate protein side chain assignments. NMRFAM-SPARKY supports automated data format interconversion for interfacing with a variety of web servers including, PECAN , PINE, TALOS-N, CS-Rosetta, SHIFTX2 and PONDEROSA-C/S. Availability and implementation: The software package, along with binary and source codes, if desired, can be downloaded freely from http://pine.nmrfam.wisc.edu/download_packages.html. Instruction manuals and video tutorials can be found at http://www.nmrfam.wisc.edu/nmrfam-sparky-distribution.htm.", 
  "summary": "NMRFAM-SPARKY contains a structure predictions menu that includes PECAN (Eghbalnia et al., 2005), TALOS-N (Shen and Bax, 2013) and CS-Rosetta (Shen et al., 2009).\nIn favorable cases, the user can complete the chemical shift assignments and then use the NMRFAM-SPARKY interface to PONDEROSA-C/S (Lee et al., 2014) to carry out an NOE-based structure determination in seamless fashion.\nWe expect that use of NMRFAM-SPARKY will increase the success rate of automated assignment runs on the PINE server and automated structure determinations on the PONDEROSA-C/S server, because our analysis shows that most failures stem from incorrectly formatted input.", 
  "affiliations": [
    " National Magnetics Resonance Facility at Madison Biochemistry Department University of Wisconsin-Madison "
  ], 
  "grants": [
    "We used statistics from the PACSY database Lee et al., 2012), a relational database management system incorporating PDB (Berman et al., 2007), BMRB (Ulrich et al., 2008), SCOP (Murzin et al., 1995), STRIDE (Frishman and Argos, 1995) and MolProbity (Chen et al., 2010), to populate this Python module, which simulates resonance frequencies dynamically on the basis of conditions such as amino acid and atom type, preceding and following residues, secondary structure,\n\nFunding\nUnited States National Institutes of Health NIH [P41GM103399].", 
    ">>> Automated structure calculation: PONDEROSA\nRun Ponderosa Client Update Ponderosa Generate Distance Constraints\nfor PONDEROSA Cyana2Sparky format XEASY, DYANA format Extract phi-psi and accessible\nsurface info from PDB with STRIDE\n>>> Structural predictions Export to PECAN and go\nPECAN webserver 3D structure prediction with\nCS-Rosetta (BMRB) phi-psi prediction with\nTALOS-N (NIH) Secondary structure prediction\nwith PSIPRED (UCL)\nBackbone peak picking by APES PINE sequence formatting Easy pipe2ucsf Easy bruk2ucsf\n\nTwolettercode\nn1\nep ar n2\np2 ab pp pr se n3\ndg ta\nut cu E1\nn4\ncp up gd\ncy xe sr\nn5 n6\nce\ntl\nPP\nae fp Pu Bu\n\npH and temperature."
  ], 
  "acks": " We are grateful to Dr Thomas Goddard for making the source code availability for SPARKY3. We also thank the participants in the NMRFAM workshops who inspired us to develop many of the new functions described here. ", 
  "authors": [
    " Woonghee Lee", 
    " Marco Tonelli", 
    " John L Markley"
  ], 
  "keyWords": [
    [
      "assignments", 
      "sparky", 
      "proteins", 
      "pine", 
      "structures"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://pine.nmrfam.wisc.edu/download_packages.html"
  ], 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-12-14T01:33:54Z"
}{
  "doi": "10.1093/bioinformatics/btu623", 
  "name": "NOVA a software to analyze complexome profiling data", 
  "links": [
    "http://www.bioinformatik.unifrankfurt.de", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Systems biology NOVA: a software to analyze complexome profiling data", 
  "toolName": "Systems biology NOVA: a software to analyze complexome profiling data", 
  "abstract": "We introduce NOVA, a software for the analysis of complexome profiling data. NOVA supports the investigation of the composition of complexes, cluster analysis of the experimental data, visual inspection and comparison of experiments and many other features. Availability and implementation: NOVA is licensed under the Artistic License 2.0. It is freely available at http://www.bioinformatik.uni-frankfurt.de. NOVA requires at least Java 7 and runs under Linux, Microsoft Windows and Mac OS.", 
  "summary": "2012) uses blue-native gel electrophoresis to separate intact proteins and protein complexes up to a molecular weight of 10 MDa (Schagger and Jagow, 1991; Wittig et al., 2006) or even 60 MDa (Strecker et al., 2010) using special large pore gels (LP-BNE).\nThrough comparison of these profiles by hierarchical clustering, groups of co-migrating proteins are recognized, indicating the composition of quaternary structures and functional complexes (Andersen et al., 2003; Foster et al., 2006; Wessels et al., 2009).\nNOVA's graphical user interface displaying a clustered CP dataset from rat hearth mitochondria (Heide et al., 2012): (A) The heat map represents gel migration profiles.", 
  "affiliations": [
    " Molecular Bioenergetics Group Medical School Cluster of Excellence Frankfurt \" Macromolecular Complexes \" Goethe-University ", 
    " Molecular Bioinformatics Group Institute of Computer Science Faculty of Computer Science and Mathematics Cluster of Excellence Frankfurt \" Macromolecular Complexes \" Goethe-University "
  ], 
  "grants": [
    "Funding: This work was partly supported by the Excellence Initiative of the German Federal and State Governments (EXC 115)."
  ], 
  "acks": " The authors gratefully thank Valentina Strecker, Kim-Kristin Prior, Jens Einloft and J \u20ac org Kuharev for testing and many valuable suggestions. ", 
  "authors": [
    " Heiko Giese", 
    " J \u20ac Org Ackermann", 
    " Heinrich Heide", 
    " Lea Bleier", 
    " Stefan Dr", 
    " \u20ac Ose", 
    " Ilka Wittig", 
    " Ulrich Brandt", 
    " Ina Koch", 
    " Janet Kelso"
  ], 
  "keyWords": [
    "profiling data", 
    [
      "clustering", 
      "proteins", 
      "profiles", 
      "complexes", 
      "nova", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-10T03:01:25Z"
}{
  "doi": "10.1093/bioinformatics/btu774", 
  "name": "NucleusJ an ImageJ plugin for quantifying 3D images of interphase nuclei", 
  "links": [
    "https://github.com/ijpb/MorphoLibJ", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://www.gred-clermont.fr", 
    "http://imagejdocu.tudor.lu/doku.php?idplugin"
  ], 
  "title": "NucleusJ: an ImageJ plugin for quantifying 3D images of interphase nuclei", 
  "toolName": "MorphoLibJ", 
  "abstract": "NucleusJ is a simple and user-friendly ImageJ plugin dedicated to the characterization of nuclear morphology and chromatin organization in 3D. Starting from image stacks, the nuclear boundary is delimited by combining the Otsu segmentation method with optimization of nuclear sphericity. Chromatin domains are segmented by partitioning the nucleus using a 3D watershed algorithm and by thresholding a contrast measure over the resulting regions. As output, NucleusJ quantifies 15 parameters including shape and size of nuclei as well as intra-nuclear objects and their position within the nucleus. A step-by-step documentation is available for self-training, together with data sets of nuclei with different nuclear organization. Availability and implementation: Dataset of nuclei is available at https://www.gred-clermont.fr/ media/WorkDirectory.zip. NucleusJ is available at http://imagejdocu.tudor.lu/", 
  "summary": "Our NucleusJ plugin assembles a complete set of methods in a coherent strategy, quantifies 15 different nuclear and chromatin parameters and does not require expertise in image analysis.\nNucleusJ also computes chromatin organization parameters such as number, total volume and intensity of chromocentres relative to the entire nucleus (Tessadori et al., 2007) as well as individual chromocentre parameters including volume and distance to the nuclear periphery.\nAnalysis of a small set of 38 (wild type) and 39 (crwn1 crwn2 mutant) nuclei revealed reduced nuclear volume, increased sphericity and fewer chromocentres of increased size in the mutant compared with WT, in perfect agreement with Wang et al.", 
  "affiliations": [
    " UMR CNRS 6293 INSERM U 1103 GRED Universit\u00e9 ", 
    " UMR INRA-AgroParisTech 1318 ERL CNRS 3559 IJPB "
  ], 
  "grants": [
    "Funding\nThis work was supported by Auvergne and Oxford Brookes universities to AP, by the French National Research Agency [ANR-11 JSV2 009 01, ANR12-ISV6-0001 to AVP and Infrastructure TEFOR/ANR-11-INBS-0014 to IAC] and by the Region Auvergne [Life GRID to CT]."
  ], 
  "sourcelinks": [
    "https://github.com/ijpb/MorphoLibJ"
  ], 
  "acks": " ", 
  "authors": [
    " Axel Poulet", 
    " Ignacio Arganda-Carreras", 
    " David Legland", 
    " Aline V Probst", 
    " Philippe Andrey", 
    " Christophe Tatout"
  ], 
  "keyWords": [
    "d images", 
    [
      "chromocentres", 
      "nucleusj", 
      "imaging", 
      "analysis"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU Lesser General Public License v3.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/ijpb/MorphoLibJ/master/LICENSE"
      }
    ], 
    "name": "MorphoLibJ", 
    "contributors": [
      {
        "contributions": 488, 
        "html_url": "https://github.com/iarganda"
      }, 
      {
        "contributions": 358, 
        "html_url": "https://github.com/dlegland"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/dscho"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/stelfrich"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.2.2", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.2.2", 
        "name": "v1.2.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.2.1", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.2.1", 
        "name": "v1.2.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.2.0", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.2.0", 
        "name": "v1.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.1.1", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.1.1", 
        "name": "v1.1.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.1.0", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.1.0", 
        "name": "v1.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.0.7", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.0.7", 
        "name": "v1.0.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.0.6", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.0.6", 
        "name": "v1.0.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.0.5", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.0.5", 
        "name": "v1.0.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/zipball/v1.0.4", 
        "tarball_url": "https://api.github.com/repos/ijpb/MorphoLibJ/tarball/v1.0.4", 
        "name": "v1.0.4"
      }
    ], 
    "created_at": "2014-02-10T13:49:16Z", 
    "updated_at": "2016-08-01T07:59:14Z", 
    "languages": [
      "Java"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/iarganda"
      }, 
      {
        "html_url": "https://github.com/dlegland"
      }, 
      {
        "html_url": "https://github.com/kladtn"
      }, 
      {
        "html_url": "https://github.com/mingguichen"
      }
    ], 
    "owner": "https://github.com/ijpb", 
    "homepage": "http://imagej.net/MorphoLibJ"
  }, 
  "technologies": [], 
  "dateCreated": "2014-11-22T02:58:16Z"
}{
  "doi": "10.1093/bioinformatics/btu671", 
  "name": "Novel function discovery with GeneMANIA a new integrated resource for gene function prediction in Escherichia coli", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "BIOINFORMATICS DISCOVERY NOTE Systems biology Novel function discovery with GeneMANIA: a new integrated resource for gene function prediction in Escherichia coli", 
  "toolName": "BIOINFORMATICS DISCOVERY NOTE Systems biology Novel function discovery with GeneMANIA: a new integrated resource for gene function prediction in Escherichia coli", 
  "abstract": "Motivation: The model bacterium Escherichia coli is among the best studied prokaryotes, yet nearly half of its proteins are still of unknown biological function. This is despite a wealth of available large-scale physical and genetic interaction data. To address this, we extended the GeneMANIA function prediction web application developed for model eukaryotes to support E.coli. Results: We integrated 48 distinct E.coli functional interaction data-sets and used the GeneMANIA algorithm to produce thousands of novel functional predictions and prioritize genes for further functional assays. Our analysis achieved cross-validation performance comparable to that reported for eukaryotic model organisms, and revealed new functions for previously uncharacterized genes in specific biopro-cesses, including components required for cell adhesion, iron\u2013sulphur complex assembly and ribosome biogenesis. The GeneMANIA approach for network-based function prediction provides an innovative new tool for probing mechanisms underlying bacterial bioprocesses. Contact:", 
  "summary": "While many gene function prediction systems based on functional interaction networks exist (Alexeyenko and Sonnhammer, 2009), few are readily available for prokaryotes [e.g. eNet (Hu et al., 2009); EcID (Andres Leon et al., 2009); STRING (Franceschini et al., 2013)], and none consider the breadth of evidence supporting functional interactions available today, such as phenomics and epistatic interactions, which have only recently become available.\nHere, we extend the GeneMANIA resource to support E.coli (Mostafavi et al., 2008; Zuberi et al., 2013) for gene function prediction.", 
  "affiliations": [
    " Department of Biochemistry University of Regina ", 
    " Department of Biochemistry University of Toronto ", 
    " Department of Biology Carleton University ", 
    " Banting and Best Department of Medical Research Donnelly Centre for Cellular and Biomolecular Research University of Toronto "
  ], 
  "grants": [
    "Funding: This work was supported by grants from Global Leadership Round in Genomics and Life Sciences to Q.M."
  ], 
  "acks": " We thank Gabe Musso (Harvard Medical School, Massachusetts, USA) for helpful discussions and comments. J.V., K.Z., H.R., R.A. and L.C. analyzed data. S.P. and A.K. compiled all data sources. A.Ga., V.D., B.S., E.L. and K.R. conducted experiments. Q.M. and G.B. shared GeneMANIA resources. J.V and M.B wrote the article, with input from A.G., A.E., Q.M., J.F.G., W.H. and G.B. ", 
  "authors": [
    " James Vlasblom", 
    " Khalid Zuberi", 
    " Harold Rodriguez", 
    " Roland Arnold", 
    " Alla Gagarinova", 
    " Viktor Deineko", 
    " Ashwani Kumar", 
    " Elisa Leung", 
    " Kamran Rizzolo", 
    " Bahram Samanfar", 
    " Luke Chang", 
    " Sadhna Phanse", 
    " Ashkan Golshani", 
    " Jack F Greenblatt", 
    " Walid A Houry", 
    " Andrew Emili", 
    " Quaid Morris", 
    " Gary Bader", 
    " Mohan Babu"
  ], 
  "keyWords": [
    [
      "genemania", 
      "proteins", 
      "predictions", 
      "functionally", 
      "networks"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-15T02:56:07Z"
}{
  "doi": "10.1093/bioinformatics/btu482", 
  "name": "Networkbased analysis of genotypephenotype correlations between different inheritance modes", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Systems biology Network-based analysis of genotype\u2013phenotype correlations between different inheritance modes", 
  "toolName": "Systems biology Network-based analysis of genotype\u2013phenotype correlations between different inheritance modes", 
  "abstract": "Motivation: Recent studies on human disease have revealed that aberrant interaction between proteins probably underlies a substantial number of human genetic diseases. This suggests a need to investigate disease inheritance mode using interaction, and based on which to refresh our conceptual understanding of a series of properties regarding inheritance mode of human disease. Results: We observed a strong correlation between the number of protein interactions and the likelihood of a gene causing any dominant diseases or multiple dominant diseases, whereas no correlation was observed between protein interaction and the likelihood of a gene causing recessive diseases. We found that dominant diseases are more likely to be associated with disruption of important interactions. These suggest inheritance mode should be understood using protein interaction. We therefore reviewed the previous studies and refined an interaction model of inheritance mode, and then confirmed that this model is largely reasonable using new evidences. With these findings, we found that the inheritance mode of human genetic diseases can be predicted using protein interaction. By integrating the systems biology perspectives with the classical disease genetics paradigm, our study provides some new insights into genotype\u2013phenotype correlations.", 
  "summary": "Previous studies on human disease highlight the importance of bottleneck genes that mediate the communication between functional modules (Margadant and Sonnenberg, 2010; Shao et al., 2012; Taylor et al., 2009; Xu et al., 2011), for which the node degree might be small but with important protein interactions whose loss disrupts the cross talk between modules.\nTherefore, given that protein interaction underlies a substantial number of human genetic diseases, one can expect that AR diseases are more frequently associated with the complete gene loss than AD diseases.\nThen, we mapped disease mutations into the interacting interfaces of the associated genes following the previous studies (Das et al., 2014; Wang et al., 2012).", 
  "affiliations": [
    " College of Bioinformatics Science and Technology Harbin Medical University ", 
    " Institute for Systems Biology"
  ], 
  "grants": [
    "Funding: This work was supported by the Scientific Research Fund of Heilongjiang Provincial Education Department (NO:12541475)."
  ], 
  "acks": " ", 
  "authors": [
    " Dapeng Hao", 
    " Chuanxing Li", 
    " Shaojun Zhang", 
    " Jianping Lu", 
    " Yongshuai Jiang", 
    " Shiyuan Wang", 
    " Meng Zhou", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "protein interactions", 
    [
      "interaction", 
      "diseases", 
      "genes", 
      "proteins", 
      "mutations", 
      "networks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-31T03:29:43Z"
}{
  "doi": "10.1093/bioinformatics/btu395", 
  "name": "Omega an Overlapgraph de novo Assembler for Metagenomics", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com", 
    "https://sourceforge.net", 
    "http://sourceforge", 
    "http://omega.omicsbio.org"
  ], 
  "title": "Genome analysis Omega: an Overlap-graph de novo Assembler for Metagenomics", 
  "toolName": "github.com", 
  "abstract": "Motivation: Metagenomic sequencing allows reconstruction of microbial genomes directly from environmental samples. Omega (overlap-graph metagenome assembler) was developed for assembling and scaffolding Illumina sequencing data of microbial communities. Results: Omega found overlaps between reads using a prefix/suffix hash table. The overlap graph of reads was simplified by removing transitive edges and trimming short branches. Unitigs were generated based on minimum cost flow analysis of the overlap graph and then merged to contigs and scaffolds using mate-pair information. In comparison with three de Bruijn graph assemblers (SOAPdenovo, IDBA-UD and MetaVelvet), Omega provided comparable overall performance on a HiSeq 100-bp dataset and superior performance on a MiSeq 300-bp dataset. In comparison with Celera on the MiSeq data-set, Omega provided more continuous assemblies overall using a fraction of the computing time of existing overlap-layout-consensus assemblers. This indicates Omega can more efficiently assemble longer Illumina reads, and at deeper coverage, for metagenomic datasets. Availability and implementation: Implemented in C++ with source code and binaries freely available at http://omega.omicsbio.org.", 
  "summary": "Our benchmarking indicated the unique advantages of the five assemblers: Omega was generally more suitable for datasets with longer reads and higher coverage depth using larger overlaps; SOAPdenovo and MetaVelvet were efficient in memory and CPU usage, which is critical for large datasets; IDBA-UD automatically iterated through a k-mer range and provided better assembly for more genomes in the HiSeq 100-bp dataset; and Celera performed well for long reads but was computationally very expensive for Illumina datasets.", 
  "affiliations": [
    " U.S. Department of Energy Joint Genome Institute ", 
    " Computer Science and Mathematics Division Oak Ridge National Laboratory "
  ], 
  "grants": [
    "The contribution of J.C. was sponsored by the Office of Advanced Scientific Computing Research.", 
    "Funding: This work was supported by Laboratory Directed Research and Development (LDRD) funding from Oak Ridge National Laboratory and the Emerging Technologies Opportunity Program (ETOP) from DOE Joint Genome Institute."
  ], 
  "sourcelinks": [
    "http://omega.omicsbio.org", 
    "https://sourceforge.net", 
    "http://sourceforge", 
    "https://github.com"
  ], 
  "acks": " ", 
  "authors": [
    " Bahlul Haider", 
    " Tae-Hyuk Ahn", 
    " Brian Bushnell", 
    " Juanjuan Chai", 
    " Alex Copeland", 
    " Chongle Pan", 
    " John Hancock"
  ], 
  "keyWords": [
    "metagenomic sequencing", 
    [
      "genomics", 
      "assembling", 
      "overlapping", 
      "reads", 
      "edges", 
      "metagenomics", 
      "sequences", 
      "omega"
    ]
  ], 
  "github_data": {
    "name": "node-v0.x-archive", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/orangemocha"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/works", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/works", 
        "name": "works"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.7", 
        "name": "v0.12.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.6", 
        "name": "v0.12.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.5", 
        "name": "v0.12.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.4", 
        "name": "v0.12.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.3", 
        "name": "v0.12.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.2", 
        "name": "v0.12.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.1", 
        "name": "v0.12.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.0", 
        "name": "v0.12.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.16", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.16", 
        "name": "v0.11.16"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.15", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.15", 
        "name": "v0.11.15"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.14", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.14", 
        "name": "v0.11.14"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.13", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.13", 
        "name": "v0.11.13"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.12", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.12", 
        "name": "v0.11.12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.11", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.11", 
        "name": "v0.11.11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.10", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.10", 
        "name": "v0.11.10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.9", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.9", 
        "name": "v0.11.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.8", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.8", 
        "name": "v0.11.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.7", 
        "name": "v0.11.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.6", 
        "name": "v0.11.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.5", 
        "name": "v0.11.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.4", 
        "name": "v0.11.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.3", 
        "name": "v0.11.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.40", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.40", 
        "name": "v0.10.40"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.39", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.39", 
        "name": "v0.10.39"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.38", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.38", 
        "name": "v0.10.38"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.37", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.37", 
        "name": "v0.10.37"
      }
    ], 
    "created_at": "2009-05-27T16:29:46Z", 
    "updated_at": "2016-08-09T16:10:45Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/isaacs"
      }, 
      {
        "html_url": "https://github.com/koichik"
      }, 
      {
        "html_url": "https://github.com/igorzi"
      }, 
      {
        "html_url": "https://github.com/kunoichi-co"
      }, 
      {
        "html_url": "https://github.com/carloslemes"
      }, 
      {
        "html_url": "https://github.com/aboyon"
      }, 
      {
        "html_url": "https://github.com/arden"
      }, 
      {
        "html_url": "https://github.com/guyoun"
      }, 
      {
        "html_url": "https://github.com/vissul"
      }, 
      {
        "html_url": "https://github.com/bdtgzj"
      }, 
      {
        "html_url": "https://github.com/santigimeno"
      }, 
      {
        "html_url": "https://github.com/chrisbateskeegan"
      }, 
      {
        "html_url": "https://github.com/FlashSoft"
      }, 
      {
        "html_url": "https://github.com/charlesgan"
      }, 
      {
        "html_url": "https://github.com/BlaneCordes"
      }, 
      {
        "html_url": "https://github.com/mdgoody"
      }, 
      {
        "html_url": "https://github.com/gindis"
      }, 
      {
        "html_url": "https://github.com/chrisdickinson"
      }, 
      {
        "html_url": "https://github.com/jorgenio"
      }, 
      {
        "html_url": "https://github.com/joityui"
      }, 
      {
        "html_url": "https://github.com/wxnet2013"
      }, 
      {
        "html_url": "https://github.com/be5invis"
      }, 
      {
        "html_url": "https://github.com/ashleymcfarland"
      }, 
      {
        "html_url": "https://github.com/jimxl"
      }, 
      {
        "html_url": "https://github.com/bluefisher80"
      }, 
      {
        "html_url": "https://github.com/subhaze"
      }, 
      {
        "html_url": "https://github.com/yaoming6046"
      }, 
      {
        "html_url": "https://github.com/hirozhang"
      }, 
      {
        "html_url": "https://github.com/rhyw"
      }, 
      {
        "html_url": "https://github.com/rhaldkhein"
      }
    ], 
    "owner": "https://github.com/nodejs", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-06-20T06:59:59Z"
}{
  "doi": "10.1093/bioinformatics/btu642", 
  "name": "OrthoInspector 20 Software and database updates", 
  "links": [
    "http://lbgi", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Databases and ontologies OrthoInspector 2.0: Software and database updates", 
  "toolName": "Databases and ontologies OrthoInspector 2.0: Software and database updates", 
  "abstract": "We previously developed OrthoInspector, a package incorporating an original algorithm for the detection of orthology and inpar-alogy relations between different species. We have added new functionalities to the package. While its original algorithm was not modified, performing similar orthology predictions, we facilitated the prediction of very large databases (thousands of proteomes), refurbished its graphical interface, added new visualization tools for comparative genomics/protein family analysis and facilitated its deployment in a network environment. Finally, we have released three online databases of precomputed orthology relationships. Availability: Package and databases are freely available at http://lbgi. fr/orthoinspector with all major browsers supported.", 
  "summary": "While its original algorithm was not modified, performing similar orthology predictions, we facilitated the prediction of very large databases (thousands of proteomes), refurbished its graphical interface, added new visualization tools for comparative genomics/protein family analysis and facilitated its deployment in a network environment.\nWe previously developed an orthology inference algorithm also based on Blast and implemented it in the OrthoInspector (OI) package (Linard et al., 2011).\nContrary to most other packages, OI is not limited to predictions and provides tools for comprehensive mining of large orthology databases, nonspecialist use through a desktop graphical interface and can easily be deployed in a network environment.", 
  "affiliations": [
    " Computer Science Department LBGI UMR 7357 ICube University of Strasbourg CNRS F ed eration de m edecine translationnelle "
  ], 
  "grants": [
    "ACKNOWLEDGEMENTS\nFunding: This work was supported by the ANR [grant ANR-10INSB-05-01 FRISBI, grant ANR-10-BINF-03-02 BIPBIP] and Institute funds from the CNRS, the Faculte de Medecine de Strasbourg and the Universite de Strasbourg."
  ], 
  "acks": " ", 
  "authors": [
    " Benjamin Linard", 
    " Alexis Allot", 
    " Rapha \u20ac El Schneider", 
    " Can Morel", 
    " Raymond Ripp", 
    " Marc Bigler", 
    " Julie D Thompson", 
    " Olivier Poch", 
    " Odile Lecompte"
  ], 
  "keyWords": [
    [
      "databases", 
      "orthoinspector", 
      "blast", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    " SQL "
  ], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu720", 
  "name": "PALMsiever a tool to turn raw data into results for singlemolecule localization microscopy", 
  "links": [
    "http://code.google.com/p/palm-siever.Downloaded", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bigwww.epfl.ch", 
    "http://bigwww.epfl.ch/palm", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "PALMsiever: a tool to turn raw data into results for single-molecule localization microscopy", 
  "toolName": "PALMsiever: a tool to turn raw data into results for single-molecule localization microscopy", 
  "abstract": "During the past decade, localization microscopy (LM) has transformed into an accessible, commercially available technique for life sciences. However, data processing can be challenging to the non-specialist and care is still needed to produce meaningful results. PALMsiever has been developed to provide a user-friendly means of visualizing, filtering and analyzing LM data. It includes drift correction, clustering, intelligent line profiles, many rendering algorithms and 3D data visualization. It incorporates the main analysis and data processing modalities used by experts in the field, as well as several new features we developed, and makes them broadly accessible. It can easily be extended via plugins and is provided as free of charge open-source software.", 
  "summary": "Most recent developments in LM data processing have focused on extracting single molecule localizations from the raw data (ISBI LM Challenge 2012, http://bigwww.epfl.ch/palm/), or visualization of processed data (Baddeley et al., 2010).\nWhile other complementary tools have since been introduced (El Beheiry and Dahan, 2013; Ovesny et al., 2014), PALMsiever provides a single user-friendly extensible platform for LM data processing, combining visualization, post-processing, filtering and analysis features.\nCluster analysis of interacting biomolecules is enabled by a plugin for the density-based cluster analysis DBSCAN (Endesfelder et al., 2013; Ester, 1996), which also facilitates noise removal (Fig. 1d, showing clustering of GaG-mEos2 data).", 
  "affiliations": [
    " Institute for Physics of Biological Systems School of Basic Sciences Laboratory for Experimental Biophysics Swiss Federal School of Technology "
  ], 
  "grants": [
    "Funding\nTP was supported by the Brazilian Swiss Joint Research Programme of the EPFL and the SystemTB Collaborative Project (Ref.", 
    "SH and SM were supported by ERC Starting Grant 243016."
  ], 
  "acks": " The authors thank N. Olivier for providing the 3D microtubule data () and J. Gunzenh\u00e4 user for the Gag-mEos2 data (). ", 
  "authors": [
    " Thomas Pengo", 
    " Seamus J Holden", 
    " Suliana Manley"
  ], 
  "keyWords": [
    [
      "data", 
      "imaging", 
      "localizations"
    ]
  ], 
  "sourcelinks": [
    "http://code.google.com/p/palm-siever.Downloaded", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bigwww.epfl.ch"
  ], 
  "technologies": [
    "Rust"
  ], 
  "dateCreated": "2014-11-01T07:11:58Z"
}{
  "doi": "10.1093/bioinformatics/btu751", 
  "name": "Optimization method for obtaining nearestneighbour DNA entropies and enthalpies directly from melting temperatures", 
  "links": [
    "https://sites.google.com/site/geraldweberufmg/vargibbs", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://build.opensuse.org/package/show/home:drgweber/VarGibbs"
  ], 
  "title": "Optimization method for obtaining nearest-neighbour DNA entropies and enthalpies directly from melting temperatures", 
  "toolName": "Optimization method for obtaining nearest-neighbour DNA entropies and enthalpies directly from melting temperatures", 
  "abstract": "Motivation: Free energy nearest-neighbour (NN) thermodynamics is widely used in DNA biochemistry , ranging from the calculation of melting temperatures to the prediction of secondary structures. Methods to calculate NN parameters require the knowledge of total sequence entropies and enthalpies, which are not always available. Results: Here, we implement and test a new melting temperature optimization method where we obtain the NN parameters directly from the temperatures. In this way, we bypass the constraints imposed by total sequence entropies and enthalpies. This enabled us to calculate the missing NN entropies and enthalpies for some published datasets, including salt-dependent parameters. Also this allowed us to combine 281 sequences from different types of melting temperature data for which we derived a new set of NN parameters, which have a smaller uncertainty and an improved predictive power. Availability and implementation: C\u00fe\u00fe source code and compiled binaries for several Linux distributions are available from https://sites.google.com/site/geraldweberufmg/vargibbs and from OpenSuse build service at https://build.opensuse.org/package/show/home:drgweber/VarGibbs. The software package contains scripts and data files to reproduce all results presented here. Contact:", 
  "summary": "Also this allowed us to combine 281 sequences from different types of melting temperature data for which we derived a new set of NN parameters, which have a smaller uncertainty and an improved predictive power.\nDNA sequence and melting temperatures datasets are prefixed by D-, published parameters by P-, initiation parameters by iP- and averaged optimized parameters by AOP-.\nFigure 2c shows the self-predictions from the optimized parameter set AOP-CMB with very little dispersion along the diagonal line and a very low average temperature deviation hDTi. This clearly shows that the MTO method minimizes Equation (3) much more uniformly for all temperatures.", 
  "affiliations": [], 
  "grants": [
    "Funding\nThis work was supported by Fundaca~ o de Amparo a` Pesquisa do Estado de Minas Gerais (Fapemig); and Conselho Nacional de Desenvolvimento Cientifico e Tecnolo gico (CNPq)."
  ], 
  "acks": " ", 
  "authors": [], 
  "keyWords": [
    [
      "datasets", 
      "methods", 
      "parameters", 
      "differences", 
      "predictions", 
      "resulting", 
      "temperatures", 
      "sequences"
    ]
  ], 
  "sourcelinks": [
    "https://sites.google.com/site/geraldweberufmg/vargibbs", 
    "https://build.opensuse.org/package/show/home:drgweber/VarGibbs"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-11-13T05:02:10Z"
}{
  "doi": "10.1093/bioinformatics/btu548", 
  "name": "OptiType precision HLA typing from nextgeneration sequencing data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Sequence analysis OptiType: precision HLA typing from next-generation sequencing data", 
  "toolName": "Sequence analysis OptiType: precision HLA typing from next-generation sequencing data", 
  "abstract": "Motivation: The human leukocyte antigen (HLA) gene cluster plays a crucial role in adaptive immunity and is thus relevant in many bio-medical applications. While next-generation sequencing data are often available for a patient, deducing the HLA genotype is difficult because of substantial sequence similarity within the cluster and exceptionally high variability of the loci. Established approaches, therefore, rely on specific HLA enrichment and sequencing techniques, coming at an additional cost and extra turnaround time. Result: We present OptiType, a novel HLA genotyping algorithm based on integer linear programming, capable of producing accurate predictions from NGS data not specifically enriched for the HLA cluster. We also present a comprehensive benchmark dataset consisting of RNA, exome and whole-genome sequencing data. OptiType significantly outperformed previously published in silico approaches with an overall accuracy of 97% enabling its use in a broad range of applications.", 
  "summary": "HLA nucleotide coding DNA sequences (CDS), genomic nucleotide sequences and feature annotation for all HLA-I alleles have been obtained from the IMGT/HLA database [Release 3.14.0, July 2013 (Robinson et al., 2013)] for read mapping reference sequence construction.\nTherefore, OptiType has been optimized to yield correct HLA typing results on four-digit resolution (i.e. on the protein-coding level) for distinct read lengths and different types of sequencing technologies.\nTo determine the influence of coverage depth on HLA typing accuracy, reads of 253 exome sequencing runs of the 1000 Genomes Project were subsampled44000 times to simulate different coverage depth conditions.", 
  "affiliations": [
    " CeGaT GmbH", 
    " Institute of Medical Genetics and Applied Genomics University of T \u20ac ubingen ", 
    " Applied Bioinformatics Center for Bioinformatics Quantitative Biology Center Department of Computer Science University of T \u20ac ubingen "
  ], 
  "grants": [
    "Funding: This study was partially funded by Deutsche Forschungsgemeinschaft (SFB 685/B1, KO 2313/6-1) and Bundesministerium fu r Bildung und Forschung (01GU1106)."
  ], 
  "acks": " The authors would like to thank Michael Wittig (Institute of Clinical Molecular Biology, Christian-Albrechts-University of Kiel, Germany) and Agilent Technologies Deutschland GmbH (B\u20ac oblingen, Germany) for kindly providing the custom HLA enrichment kit. Funding: This study was partially funded by Deutsche Forschungsgemeinschaft (SFB 685/B1, KO 2313/6-1) and Bundesministerium f \u20ac ur Bildung und Forschung (01GU1106). Conflict of interest: none declared. ", 
  "authors": [
    " Andr As Szolek", 
    " Benjamin Schubert", 
    " Christopher Mohr", 
    " Marc Sturm", 
    " Magdalena Feldhahn", 
    " Oliver Kohlbacher"
  ], 
  "keyWords": [
    [
      "optitype", 
      "alleles", 
      "reads", 
      "bases", 
      "sequencers"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "Python", 
    "R"
  ], 
  "dateCreated": "2014-08-21T04:05:53Z"
}{
  "doi": "10.1093/bioinformatics/btv004", 
  "name": "PASPA a web server for mRNA polyA site predictions in plants and algae", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bmi.xmu.edu.cn/paspa"
  ], 
  "title": "PASPA: a web server for mRNA poly(A) site predictions in plants and algae", 
  "toolName": "PASPA: a web server for mRNA poly(A) site predictions in plants and algae", 
  "abstract": "Motivation: Polyadenylation is an essential process during eukaryotic gene expression. Prediction of poly(A) sites helps to define the 3 0 end of genes, which is important for gene annotation and elucidating gene regulation mechanisms. However, due to limited knowledge of poly(A) signals, it is still challenging to predict poly(A) sites in plants and algae. PASPA is a web server for poly(A) site prediction in plants and algae, which integrates many in-house tools as add-ons to facilitate poly(A) site prediction, visualization and mining. This server can predict poly(A) sites for ten species , including seven previously poly(A) signal non-characterized species, with sensitivity and spe-cificity in a range between 0.80 and 0.95. Availability and implementation: http://bmi.", 
  "summary": "A number of computational tools based on support vector machine or discriminant analysis have been developed to predict poly(A) sites or poly(A) signals in human, including Erpin (Gautheret and Lambert, 2001), POLYAR (Akhtar et al., 2010),\nOur group developed a poly(A) site prediction tool called Poly(A) Site Sleuth (PASS) (Ji et al., 2007) based on a generalized hidden Markov model (GHMM).\nThere are four authentic poly(A) site datasets from whole-genome studies currently available, including Arabidopsis, rice, M.truncatula, and C.reinhardtii (Shen et al., 2008; Wu et al., 2011, 2014; Zhao et al., 2014).", 
  "affiliations": [
    " Department of Automation"
  ], 
  "grants": [
    "Funding\nThe National Natural Science Foundation of China [61174161, 61201358 and 61375077], the Natural Science Foundation of Fujian Province of China [2012J01154], the specialized Research Fund for the Doctoral Program of Higher Education of China [20130121130004 and 20120121120038], the Key Research Project of Xiamen City of China [3502Z20123014], and the Fundamental Research Funds for the Central Universities in China Xiamen University [2013121025, 201412G009 and CBX2014007]."
  ], 
  "acks": " ", 
  "authors": [
    " Guoli Ji", 
    " Lei Li", 
    " Qingshun Q Li", 
    " Xiangdong Wu", 
    " Jingyi Fu", 
    " Gong Chen", 
    " Xiaohui Wu"
  ], 
  "keyWords": [
    "poly site predictions", 
    [
      "sequencing", 
      "predictive", 
      "genomics", 
      "sites", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-13T01:15:54Z"
}{
  "doi": "10.1093/bioinformatics/btu589", 
  "name": "Overrepresentation of correlation analysis ORCA a method for identifying associations between variable sets", 
  "links": [
    "https://github.com/ORCABioinfo/ORCAcode", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining Over-representation of correlation analysis (ORCA): a method for identifying associations between variable sets", 
  "toolName": "ORCAcode", 
  "abstract": "Motivation: Often during the analysis of biological data, it is of importance to interpret the correlation structure that exists between variables. Such correlations may reveal patterns of co-regulation that are indicative of biochemical pathways or common mechanisms of response to a related set of treatments. However, analyses of correlations are usually conducted by either subjective interpretation of the univariate covariance matrix or by applying multivariate modeling techniques , which do not take prior biological knowledge into account. Over-representation analysis (ORA) is a simple method for objectively deciding whether a set of variables of known or suspected biological relevance, such as a gene set or pathway, is more prevalent in a set of variables of interest than we expect by chance. However, ORA is usually applied to a set of variables differentiating a single experimental variable and does not take into account correlations. Results: Over-representation of correlation analysis (ORCA) is a novel combination of ORA and correlation analysis that provides a means to test whether more associations exist between two specific groups of variables than expected by chance. The method is exemplified by application to drug sensitivity and microRNA expression data from a panel of cancer cell lines (NCI60). ORCA highlighted a previously reported correlation between sensitivity to alkylating anticancer agents and topoisomerase inhibitors. We also used this approach to validate microRNA clusters predicted by mRNA correlations. These observations suggest that ORCA has the potential to reveal novel insights from these data, which are not readily apparent using classical ORA. Availability and implementation: The R code of the method is available at https://github.com/ORCABioinfo/ORCAcode", 
  "summary": "We introduce a novel method called Over-Representation of Correlation Analysis (ORCA) that seeks to combine both conventional approaches for analysis of coordinated biological responses and provide a new means to test for an unexpectedly high prevalence of informative associations between two sets of variables, which could be mRNA, microRNA or proteins, that comprise pathways/groups deemed of importance a priori.\nFor pathway or gene set analysis, ORCA can calculate the correlation coefficients between genes or metabolites and then find the pathway-pairs that have more correlation coefficients that pass a certain threshold (determined by the threshold selection method or by other means) than expected.", 
  "affiliations": [
    " Department of Surgery and Cancer Section of Computational and Systems Medicine Imperial College London "
  ], 
  "grants": [
    "Funding: Yotsawat Pomyen is funded by Ministry of Science and Technology, Royal Thai Government."
  ], 
  "sourcelinks": [
    "https://github.com/ORCABioinfo/ORCAcode"
  ], 
  "acks": " ", 
  "authors": [
    " Yotsawat Pomyen", 
    " Marcelo Segura", 
    " Timothy M D Ebbels", 
    " Hector C Keun"
  ], 
  "keyWords": [
    "correlation analysis", 
    [
      "biological", 
      "correlations", 
      "data", 
      "orca", 
      "groupings"
    ]
  ], 
  "github_data": {
    "name": "ORCAcode", 
    "contributors": [
      {
        "contributions": 2, 
        "html_url": "https://github.com/ORCABioinfo"
      }
    ], 
    "versions": [], 
    "created_at": "2014-06-10T09:52:47Z", 
    "updated_at": "2014-08-18T12:27:38Z", 
    "languages": [
      "R"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/ORCABioinfo"
      }, 
      {
        "html_url": "https://github.com/plijnzaad"
      }
    ], 
    "owner": "https://github.com/ORCABioinfo", 
    "homepage": ""
  }, 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-09-03T06:17:49Z"
}{
  "doi": "10.1093/bioinformatics/btu840", 
  "name": "PBOOST a GPUbased tool for parallel permutation tests in genomewide association studies", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinformatics.ust.hk/PBOOST.zip"
  ], 
  "title": "PBOOST: a GPU-based tool for parallel permutation tests in genome-wide association studies Downloaded from", 
  "toolName": "PBOOST: a GPU-based tool for parallel permutation tests in genome-wide association studies Downloaded from", 
  "abstract": "Motivation: The importance of testing associations allowing for interactions has been demonstrated by Marchini et al. (2005). A fast method detecting associations allowing for interactions has been proposed by Wan et al. (2010a). The method is based on likelihood ratio test with the assumption that the statistic follows the v 2 distribution. Many single nucleotide polymorphism (SNP) pairs with significant associations allowing for interactions have been detected using their method. However, the assumption of v 2 test requires the expected values in each cell of the contingency table to be at least five. This assumption is violated in some identified SNP pairs. In this case, likelihood ratio test may not be applicable any more. Permutation test is an ideal approach to checking the P-values calculated in likelihood ratio test because of its non-parametric nature. The P-values of SNP pairs having significant associations with disease are always extremely small. Thus, we need a huge number of permutations to achieve correspondingly high resolution for the P-values. In order to investigate whether the P-values from likelihood ratio tests are reliable, a fast permutation tool to accomplish large number of permutations is desirable. Results: We developed a permutation tool named PBOOST. It is based on GPU with highly reliable P-value estimation. By using simulation data, we found that the P-values from likelihood ratio tests will have relative error of >100% when 50% cells in the contingency table have expected count less than five or when there is zero expected count in any of the contingency table cells. In terms of speed, PBOOST completed 10 7 permutations for a single SNP pair from the Wellcome Trust Case Control Consortium (WTCCC) genome data (Wellcome Trust Case Control Consortium, 2007) within 1 min on a single Nvidia Tesla M2090 device, while it took 60 min in a single CPU Intel Xeon E5-2650 to finish the same task. More importantly, when simultaneously testing 256 SNP pairs for 10 7 permutations, our tool took only 5 min, while the CPU program took 10 h. By permuting on a GPU cluster consisting of 40 nodes, we completed 10 12 permutations for all 280 SNP pairs reported with P-values smaller than 1:6 \u00c2 10 \u00c012 in the WTCCC datasets in 1 week. Availability and implementation: The source code and sample data are available at http://bioinfor", 
  "summary": "By using simulation data, we found that the P-values from likelihood ratio tests will have relative error of >100% when 50% cells in the contingency table have expected count less than five or when there is zero expected count in any of the contingency table cells.\n(2010a) proposed an efficient likelihood ratio test method and reported 280 SNP pairs having significant associations allowing for interactions.\nWe also examined SNP pairs with associations allowing for interactions reported by likelihood ratio test.\nWe also did permutation tests on the SNP pairs with associations", 
  "affiliations": [
    " Department of Computer Science and Engineering The Hong Kong University of Science and Technology ", 
    " Department of Electronic and Computer Engineering Laboratory of Bioinformatics and Computational Biology "
  ], 
  "grants": [
    "Acknowledgement\nThis work is partially supported by theme-based research project T12-402/ 13N from the Research Grant Council of the Hong Kong S.A.R."
  ], 
  "acks": " This work is partially supported by theme-based research project T12-402/ 13N from the Research Grant Council of the Hong Kong S.A.R. Government of China. Conflict of interest: none declared. ", 
  "authors": [
    " Guangyuan Yang", 
    " Wei Jiang", 
    " Qiang Yang", 
    " Weichuan Yu"
  ], 
  "keyWords": [
    "permutation tests", 
    "testing associations", 
    [
      "significance", 
      "permutations", 
      "pboost", 
      "bioinformatics", 
      "test", 
      "data", 
      "association"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.ust.hk/PBOOST.zip"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-23T03:29:09Z"
}{
  "doi": "10.1093/bioinformatics/btu790", 
  "name": "PEASE predicting Bcell epitopes utilizing antibody sequence", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "PEASE: predicting B-cell epitopes utilizing antibody sequence", 
  "toolName": "PEASE: predicting B-cell epitopes utilizing antibody sequence", 
  "abstract": "Antibody epitope mapping is a key step in understanding antibody\u2013antigen recognition and is of particular interest for drug development, diagnostics and vaccine design. Most computational methods for epitope prediction are based on properties of the antigen sequence and/or structure , not taking into account the antibody for which the epitope is predicted. Here, we introduce PEASE, a web server predicting antibody-specific epitopes, utilizing the sequence of the antibody. The predictions are provided both at the residue level and as patches on the antigen structure. The tradeoff between recall and precision can be tuned by the user, by changing the default parameters. The results are provided as text and HTML files as well as a graph, and can be viewed on the antigen 3D structure.", 
  "summary": "Several strategies that take into account the Ab sequence or structure were proposed and introduced, in the last few years: Antibody Specific Epitope Prediction (ASEP) (Soga et al., 2010) is an index, computed based on Ab-Ag residue preferences, used to narrow down candidate residues predicted by conventional methods.\nBepar (Zhao and Li, 2010) and ABepar (Zhao et al., 2011) are sequence-based methods predicting Ab-specific epitopes based on association patterns of Ab-Ag residues (Bepar) and on AbAg preferences of individual residues and residue pairs (ABepar).", 
  "affiliations": [
    " Division of Vaccine Discovery Institute for Allergy and Immunology ", 
    " The Goodman Faculty of Life Sciences Bar-Ilan University "
  ], 
  "grants": [
    "Funding\nThis project has been funded in whole or in part with federal funds from the National Institutes of Allergy and Infectious Diseases [contract no: HHSN272200900048C]."
  ], 
  "acks": " We thank Anat Burkovitz, Vered Kunik, Ariel Feiglin and Guy Nimrod for their help in designing the server. This ", 
  "authors": [
    " Inbal Sela-Culang", 
    " Shaul Ashkenazi", 
    " Bjoern Peters", 
    " Yanay Ofran"
  ], 
  "keyWords": [
    "pease predicting", 
    [
      "epitopic", 
      "antibodies", 
      "structures", 
      "predictions", 
      "residues"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-29T02:15:47Z"
}{
  "doi": "10.1093/bioinformatics/btu517", 
  "name": "PEPPER cytoscape app for protein complex expansion using proteinprotein interaction networks", 
  "links": [
    "http://apps.cytoscape.org/apps/pepper", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Gene expression PEPPER: cytoscape app for protein complex expansion using protein\u2013protein interaction networks", 
  "toolName": "Gene expression PEPPER: cytoscape app for protein complex expansion using protein\u2013protein interaction networks", 
  "abstract": "We introduce PEPPER (Protein complex Expansion using Protein\u2013 Protein intERactions), a Cytoscape app designed to identify protein complexes as densely connected subnetworks from seed lists of proteins derived from proteomic studies. PEPPER identifies connected sub-graph by using multi-objective optimization involving two functions: (i) the coverage, a solution must contain as many proteins from the seed as possible, (ii) the density, the proteins of a solution must be as connected as possible, using only interactions from a proteome-wide interaction network. Comparisons based on gold standard yeast and human datasets showed Pepper's integrative approach as superior to standard protein complex discovery methods. The visualization and interpretation of the results are facilitated by an automated post-processing pipeline based on topological analysis and data integration about the predicted complex proteins. PEPPER is a user-friendly tool that can be used to analyse any list of proteins.", 
  "summary": "Based on this hypothesis, we developed PEPPER, which addresses the problem of finding protein complexes by combining the experimental results of a single AP-MS assay with the available information from protein interactions in a global PPI network.\nWe assessed the performance of PEPPER and two network clustering algorithms for protein complex discovery--MCODE (Bader and Hogue, 2003) and ClusterONE (Nepusz et al., 2012)--on a benchmark dataset of 135 yeast and 9 human single-bait AP-MS experiments and using a set of hand-curated protein complexes as gold standards.\nOverall, these results demonstrate the feasibility of expanding the protein complexes identified in an AP-MS experiment through the use of PPI networks and the value of PEPPER for this purpose.", 
  "affiliations": [
    " University of Evry", 
    " UMR 144 CNRS/Institut Curie ", 
    " School of Computing Science Newcastle University ", 
    " CNRS"
  ], 
  "grants": [
    "Funding: This work was supported by the French National Cancer Institute (INCa_2960: PLBIO10) and the European Union/Framework Programme 7/2009 (\"SYSCILIA\" consortium, grant 241955).", 
    "Funding for open access charge: SYSCILIA."
  ], 
  "acks": " ", 
  "authors": [
    " C Winterhalter", 
    " R Nicolle", 
    " A Louis", 
    " C To", 
    " F Radvanyi", 
    " M Elati", 
    " ", 
    " "
  ], 
  "keyWords": [
    "protein complex", 
    [
      "scoring", 
      "pepper", 
      "methods", 
      "proteins", 
      "complexes", 
      "networks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-20T05:11:29Z"
}{
  "doi": "10.1093/bioinformatics/btu851", 
  "name": "PANNZER highthroughput functional annotation of uncharacterized proteins in an errorprone environment", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://ekhidna.biocenter.helsinki.fi/pannzer"
  ], 
  "title": "PANNZER: high-throughput functional annotation of uncharacterized proteins in an error-prone environment", 
  "toolName": "PANNZER: high-throughput functional annotation of uncharacterized proteins in an error-prone environment", 
  "abstract": "Motivation: The last decade has seen a remarkable growth in protein databases. This growth comes at a price: a growing number of submitted protein sequences lack functional annotation. Approximately 32% of sequences submitted to the most comprehensive protein database UniProtKB are labelled as 'Unknown protein' or alike. Also the functionally annotated parts are reported to contain 30\u201340% of errors. Here, we introduce a high-throughput tool for more reliable functional annotation called Protein ANNotation with Z-score (PANNZER). PANNZER predicts Gene Ontology (GO) classes and free text descriptions about protein functionality. PANNZER uses weighted k-nearest neighbour methods with statistical testing to maximize the reliability of a functional annotation. Results: Our results in free text description line prediction show that we outperformed all competing methods with a clear margin. In GO prediction we show clear improvement to our older method that performed well in CAFA 2011 challenge. Availability and implementation: The PANNZER program was developed using the Python programming language (Version 2.6). The stand-alone installation of the PANNZER requires MySQL database for data storage and the BLAST (BLASTALL v.2.2.21) tools for the sequence similarity search. The tutorial, evaluation test sets and results are available on the PANNZER web site. PANNZER is freely available at http://ekhidna.biocenter.helsinki.fi/pannzer.", 
  "summary": "PANNZER predicts Gene Ontology (GO) classes and free text descriptions about protein functionality.\nOur results show an improved performance over alternative scoring methods and we also show improvement to our earlier version of PANNZER that was ranked third in Critical Assessment of protein Function Annotation algorithms (CAFA) 2011 challenge (Radivojac et al., 2013).\nWe did the prediction for the prokaryote test set with BLAST-based methods: Best BLAST Hit (BBH), Best Informative BLAST Hit (BIBH), Blannotator (Kankainen et al., 2012) and PANNZER using NOSELF and NOCLOUD databases.\nThe evaluation of the PANNZER method performance was conducted using description prediction and also prediction of GO classes.", 
  "affiliations": [
    " Department of Biosciences University of Helsinki ", 
    " Institute of Biotechnology University of Helsinki "
  ], 
  "grants": [
    "Funding\nThis work was supported by Biocenter Finland, University of Helsinki and Institute of Biotechnology."
  ], 
  "acks": " We would like to thank Petri Auvinen for critical reading of the manuscript and Teija Ojala for the artistic eye. We would also like to thank the anonymous reviewers for their constructive comments. ", 
  "authors": [
    " Patrik Koskinen", 
    " Petri T\u00f6 R\u00f6 Nen", 
    " Jussi Nokso-Koivisto", 
    " Liisa Holm"
  ], 
  "keyWords": [
    "functional annotation", 
    "functionally annotated", 
    [
      "scoring", 
      "methods", 
      "functionality", 
      "descriptions", 
      "pannzer", 
      "sequencing", 
      "annotations"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Python", 
    "R", 
    "Lasso"
  ], 
  "dateCreated": "2015-01-10T04:10:59Z"
}{
  "doi": "10.1093/bioinformatics/btu646", 
  "name": "PFPESG automated protein function prediction servers enhanced with Gene Ontology visualization tool", 
  "links": [
    "http://kiharalab.org/web/esg.php", 
    "http://kiharalab.org/pfp_esg.php", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://kiharalab"
  ], 
  "title": "Sequence analysis PFP/ESG: automated protein function prediction servers enhanced with Gene Ontology visualization tool", 
  "toolName": "Sequence analysis PFP/ESG: automated protein function prediction servers enhanced with Gene Ontology visualization tool", 
  "abstract": "Protein function prediction (PFP) is an automated function prediction method that predicts Gene Ontology (GO) annotations for a protein sequence using distantly related sequences and contextual associations of GO terms. Extended similarity group (ESG) is another GO prediction algorithm that makes predictions based on iterative sequence database searches. Here, we provide interactive web servers for the PFP and ESG algorithms that are equipped with an effective visualization of the GO predictions in a hierarchical topology. Availability: PFP/ESG servers are freely available at", 
  "summary": "ABSTRACT Summary: Protein function prediction (PFP) is an automated function prediction method that predicts Gene Ontology (GO) annotations for a protein sequence using distantly related sequences and contextual associations of GO terms.\nHere we developed web servers for our two sequence-based function prediction algorithms, protein function prediction (PFP) and extended similarity group (ESG).\nPFP extends traditional PSI-BLAST (Altschul et al., 1997) search by extracting and scoring Gene Ontology (GO) annotations from distantly similar sequences and by applying contextual associations of GO terms observed in the annotation database to the scoring scheme (Hawkins et al., 2006, 2009).\nBoth PFP and ESG algorithms predict GO terms for a given protein sequence.", 
  "affiliations": [
    " Department of Biological Science Purdue University ", 
    " Department of Computer Science"
  ], 
  "grants": [
    "Funding: This work was supported partly by the National Institutes of Health (R01GM097528), the National Science Foundation (IIS1319551, DBI1262189, IOS1127027) and National Research Foundation of Korea (NRF-2011-220-C00004)."
  ], 
  "acks": " ", 
  "authors": [
    " Ishita K Khan", 
    " Qing Wei", 
    " Meghana Chitale", 
    " Daisuke Kihara", 
    " John Hancock"
  ], 
  "keyWords": [
    "protein function prediction", 
    "proteins annotated", 
    [
      "sequenced", 
      "functional", 
      "annotations", 
      "predictions"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu628", 
  "name": "PhaseTank genomewide computational identification of phasiRNAs and their regulatory cascades", 
  "links": [
    "http://phasetank.sourceforge.net", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://phasetank"
  ], 
  "title": "Data and text mining PhaseTank: genome-wide computational identification of phasiRNAs and their regulatory cascades", 
  "toolName": "phasetank", 
  "abstract": "Emerging evidence has revealed phased siRNAs (phasiRNAs) as important endogenous regulators in plants. However, the integrated prediction tools for phasiRNAs are still limited. In this article, we introduce a stand-alone package PhaseTank for systematically characterizing phasiRNAs and their regulatory networks. (i) It can identify phasiRNAs/tasiRNAs functional cascades (miRNA/phasiRNA!PHAS loci!phasiRNA!target) with high sensitivity and specificity. (ii) By one command analysis, it generates comprehensive annotation and quantification of the predicted PHAS genes from any given sequences. (iii) PhaseTank has no restriction with regards to prior information of sequence homology of unrestricted organism origins. Availability and implementation: PhaseTank is a free and open-source tool. The package is available at http://phasetank.source", 
  "summary": "Well-characterized trans-acting siRNAs, as a special subgroup of phasiRNAs, are initiated by miRNA-mediated cleavage and converted to dsRNA, yielding siRNAs in a 21-nt phase (Allen and Howell, 2010; Axtell, 2013a; Fei et al., 2013).\nNext-generation sequencing technology provides a powerful tool for genome-wide screening of phasiRNAs (Wang et al., 2009), and several methods were proposed correspondingly (Axtell, 2010, 2013b; Chen et al., 2007; Gupta et al., 2012; Li et al., 2012).\n(A) PhaseTank shows higher sensitivities compared with ShortStack in different Arabidopsis tissues using cDNA data; (B) PhaseTank detects a greater number of true PHAS genes than that predicted by ShortStack using Arabidopsis genome data; (C) A rarely reported regulatory cascade is discovered by PhaseTank", 
  "affiliations": [
    " School of Biomedical Engineering Shanghai Jiao Tong University "
  ], 
  "grants": [
    "Funding: This work was supported by the grants from Natural Science Foundation of China [No."
  ], 
  "acks": " ", 
  "authors": [
    " Qingli Guo", 
    " Xiongfei Qu", 
    " Weibo Jin"
  ], 
  "keyWords": [
    [
      "phasetank", 
      "plants", 
      "sequencing", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://phasetank.sourceforge.net", 
    "http://phasetank"
  ], 
  "technologies": [
    "Perl"
  ], 
  "dateCreated": "2014-09-23T02:41:35Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/phasetank/", 
    "languages": [
      "Perl"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/guoql-karen/", 
        "name": "Karen Guo"
      }
    ], 
    "Development Status": [
      {
        "status": "3 - Alpha"
      }
    ], 
    "license": [
      {
        "name": "GNU General Public License version 3.0 (GPLv3)"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu626", 
  "name": "PDBwide collection of binding data current status of the PDBbind database", 
  "links": [
    "http://www.pdbbind-cn.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.pdbbind.org.cn", 
    "http://www"
  ], 
  "title": "Databases and ontologies PDB-wide collection of binding data: current status of the PDBbind database", 
  "toolName": "Databases and ontologies PDB-wide collection of binding data: current status of the PDBbind database", 
  "abstract": "", 
  "summary": "The PDBbind database was created to collect experimentally measured binding data for the biomolecular complexes throughout the Protein Data Bank (PDB).\nThe latest release of PDBbind, i.e. version 2013, provides binding data for 410 700 molecular complexes in the PDB, making it the largest collection of its kind to date.\nThe latest release of PDBbind is version 2013, which provides the experimental binding data and structures of 10 776 molecular complexes in the PDB.\nThe PDBbind database aims to provide a PDB-wide collection of binding data for various biomolecular complexes.\n(2004) The PDBbind database: collection of binding affinities for protein-ligand complexes with known three-dimensional structures.", 
  "affiliations": [
    " State Key Laboratory of Bioorganic and Natural Products Chemistry Shanghai Institute of Organic Chemistry Chinese Academy of Sciences "
  ], 
  "grants": [
    "Funding: This work was supported by the Chinese National Natural Science Foundation [Grant 81172984, 21072213 to Prof. R. Wang and Grant 21102168 to Dr. Y. Li ], the Chinese Ministry of Science and Technology [863 High-Tech Grant 2012AA020308 to Prof. R. Wang], and the Science and Technology Development Fund of Macao SAR [Grant 055/ 2013/A2 to Prof. R. Wang]."
  ], 
  "acks": " The authors are grateful to Dr. Ying-Duo Gao and Dr. M. Katharine Holloway from Merck for their careful proofreading. ", 
  "authors": [
    " Zhihai Liu", 
    " Yan Li", 
    " Han Li", 
    " Jie Li", 
    " Jie Liu", 
    " Zhixiong Zhao", 
    " Wei Nie", 
    " Yuchen Liu", 
    " Renxiao Wang"
  ], 
  "keyWords": [
    [
      "binding", 
      "databases", 
      "proteins", 
      "structures", 
      "complexes"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-10T03:01:25Z"
}{
  "doi": "10.1093/bioinformatics/btu617", 
  "name": "PHIDAC protein homology database through dihedral angle conservation", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Databases and ontologies Advance Access publication PHI-DAC: protein homology database through dihedral angle conservation", 
  "toolName": "Databases and ontologies Advance Access publication PHI-DAC: protein homology database through dihedral angle conservation", 
  "abstract": "Finding related conformations in the Protein Data Bank is essential in many areas of bioscience. To assist this task, we designed a dihedral angle database for searching protein segment homologs. The search engine relies on encoding of the protein coordinates into text characters representing amino acid sequence, ' and dihedral angles. The search engine is advantageous owing to its high speed and interactive nature and is expected to assist scientists in discovering conformation homologs and evolutionary kinship. The search engine is fast, with query times lasting a few seconds, and freely available at http://", 
  "summary": "PHI-DAC: protein homology database through dihedral angle\nThe search engine relies on encoding of the protein coordinates into text characters representing amino acid sequence, ' and dihedral angles.\nAn additional search engine named Fragment Finder was designed to identify similar structural motifs based on backbone dihedral angles (Ananthalakshmi et al., 2005).\nHere we describe an online search engine that rapidly finds protein segments based on amino acid sequence and ' and dihedral angles.\nThe search engine finds protein segment homologs based on ' and dihedral angles and amino acid sequence.", 
  "affiliations": [
    " Faculty of Medicine in the Galilee Bar Ilan University "
  ], 
  "grants": [
    "Funding: This research was supported by the Katz Foundation and a Marie-Curie CIG grant 322113 (to A.O.S.)."
  ], 
  "acks": " ", 
  "authors": [
    " Noa Maatuk", 
    " Yitav Glantz-Gashai", 
    " Maya Rotman", 
    " Meirav Baydany", 
    " Gennadiy Fonar", 
    " Amir Shechvitz", 
    " Karin Shemer", 
    " Aviva Peleg", 
    " Eli Reuveni", 
    " Abraham O Samson"
  ], 
  "keyWords": [
    [
      "querying", 
      "segments", 
      "searching", 
      "angles", 
      "bioinformatics", 
      "protein", 
      "structures"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-25T00:36:59Z"
}{
  "doi": "10.1093/bioinformatics/btu572", 
  "name": "PHOXTRACKa tool for interpreting comprehensive datasets of posttranslational modifications of proteins", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://groups.google.com"
  ], 
  "title": "PHOXTRACK\u2013a tool for interpreting comprehensive datasets of post-translational modifications of proteins", 
  "toolName": "PHOXTRACK\u2013a tool for interpreting comprehensive datasets of post-translational modifications of proteins", 
  "abstract": "We introduce PHOXTRACK (PHOsphosite-X-TRacing Analysis of Causal Kinases), a user-friendly freely available software tool for analyzing large datasets of post-translational modifications of proteins, such as phosphorylation, which are commonly gained by mass spectrometry detection. In contrast to other currently applied data analysis approaches, PHOXTRACK uses full sets of quantitative proteomics data and applies non-parametric statistics to calculate whether defined kinase-specific sets of phosphosite sequences indicate statistically significant concordant differences between various biological conditions. PHOXTRACK is an efficient tool for extracting post-translational information of comprehensive proteomics datasets to decipher key regulatory proteins and to infer biologically relevant molecular pathways. Availability: PHOXTRACK will be maintained over the next years and is freely available as an online tool for non-commercial use at http:// phoxtrack.molgen.mpg.de. Users will also find a tutorial at this Web site and can additionally give feedback at https://groups.google.com/ d/forum/phoxtrack-discuss.", 
  "summary": "In contrast to other currently applied data analysis approaches, PHOXTRACK uses full sets of quantitative proteomics data and applies non-parametric statistics to calculate whether defined kinase-specific sets of phosphosite sequences indicate statistically significant concordant differences between various biological conditions.\nWe initially defined experimentally validated kinase-specific phosphosite sets using data from the public databases PhosphoSitePlus (Hornbeck et al., 2012), Swiss-Prot, the Human Protein Reference Database (Keshava Prasad et al., 2009) and Phospho.ELM (Dinkel et al., 2011), containing thousands of substrate sequence/kinase interactions derived from thousands of human and murine studies (Fig. 1 and Supplementary Table S1).", 
  "affiliations": [
    " Max Planck Institute for Molecular Genetics Associate Editor: Igor Jurisica Otto Warburg Laboratory "
  ], 
  "grants": [
    "Funding: The study was supported by the BMBF (0315082/ 01EA1303)."
  ], 
  "acks": " The manuscript was written through contributions of all authors. All authors have given approval to the final version of the manuscript. Funding: The study was supported by the BMBF (0315082/ 01EA1303). ", 
  "authors": [
    " Christopher Weidner", 
    " Cornelius Fischer", 
    " Sascha Sauer"
  ], 
  "keyWords": [
    "sequence analysis", 
    [
      "phoxtrack", 
      "proteins", 
      "kinases", 
      "sequences", 
      "phosphosites", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-25T00:08:47Z"
}{
  "doi": "10.1093/bioinformatics/btu396", 
  "name": "PGS a tool for association study of highdimensional microRNA expression data with repeated measures", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/feizhe/PGS", 
    "http://www.microrna.gr/miRPathv2"
  ], 
  "title": "Data and text mining PGS: a tool for association study of high-dimensional microRNA expression data with repeated measures", 
  "toolName": "PGS", 
  "abstract": "Motivation: MicroRNAs (miRNAs) are short single-stranded non-coding molecules that usually function as negative regulators to silence or suppress gene expression. Owning to the dynamic nature of miRNA and reduced microarray and sequencing costs, a growing number of researchers are now measuring high-dimensional miRNA expression data using repeated or multiple measures in which each individual has more than one sample collected and measured over time. However, the commonly used univariate association testing or the site-by-site (SBS) testing may underutilize the longitudinal feature of the data, leading to underpowered results and less biologically meaningful results. Results: We propose a penalized regression model incorporating grid search method (PGS), for analyzing associations of high-dimensional miRNA expression data with repeated measures. The development of this analytical framework was motivated by a real-world miRNA dataset. Comparisons between PGS and the SBS testing revealed that PGS provided smaller phenotype prediction errors and higher enrichment of phenotype-related biological pathways than the SBS testing. Our extensive simulations showed that PGS provided more accurate estimates and higher sensitivity than the SBS testing with comparable specificities. Availability and implementation: R source code for PGS algorithm, implementation example and simulation study are available for down-load at https://github.com/feizhe/PGS.", 
  "summary": "Results: We propose a penalized regression model incorporating grid search method (PGS), for analyzing associations of high-dimensional miRNA expression data with repeated measures.\nThe popular site-by-site (SBS) testing using generalized estimation equation (GEE) (Zeger et al., 1988) or linear mixed model (LMM) (Henderson et al., 1959) represents a feasible approach to accommodate the presence of high-dimensional miRNA expression data measured at different time points.\nWe found LMM-based PGS were more stable, as six of eight repeats yielded the same 10 selected influential miRNAs related to lung function with Pm = 110 and  = 0.14, while GEE-based PGS selection results showed less consistency across the eight repeats (Supplementary Table S2).", 
  "affiliations": [
    " Institute for Public Health and Medicine Northwestern University Feinberg School of Medicine ", 
    " Department of Preventive Medicine Northwestern University Feinberg School of Medicine ", 
    " Department of Biostatistics University of Michigan ", 
    " Division of Health and Biomedical Informatics Departments of Preventive Medicine and Medical Social Sciences Northwestern University Feinberg School of Medicine ", 
    " Institute of Human Genetics University of Illinois at Chicago "
  ], 
  "grants": [
    "Funding: This work was supported by R21 ES020010 and R21 ES020984-01 from the National Institute of Environmental Health Sciences (NIEHS)."
  ], 
  "sourcelinks": [
    "https://github.com/feizhe/PGS"
  ], 
  "acks": " Many thanks to Dr. Lan Wang and Jianhui Zhou for kindly providing the R source code of PGEE and for their many thoughtful suggestions in developing PGS. ", 
  "authors": [
    " Yinan Zheng", 
    " Zhe Fei", 
    " Wei Zhang", 
    " Justin B Starren", 
    " Lei Liu", 
    " Andrea A Baccarelli", 
    " Yi Li", 
    " Lifang Hou", 
    " Robert H Lurie"
  ], 
  "keyWords": [
    [
      "data", 
      "testing", 
      "mirnas", 
      "studied"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "BSD 2-clause \"Simplified\" License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/semiocast/pgsql/master/LICENSE"
      }
    ], 
    "name": "pgsql", 
    "contributors": [
      {
        "contributions": 65, 
        "html_url": "https://github.com/pguyot"
      }, 
      {
        "contributions": 6, 
        "html_url": "https://github.com/sebastian"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/Licenser"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/sasa1977"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/tsloughter"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/semiocast/pgsql/zipball/25", 
        "tarball_url": "https://api.github.com/repos/semiocast/pgsql/tarball/25", 
        "name": "25"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/semiocast/pgsql/zipball/22", 
        "tarball_url": "https://api.github.com/repos/semiocast/pgsql/tarball/22", 
        "name": "22"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/semiocast/pgsql/zipball/21", 
        "tarball_url": "https://api.github.com/repos/semiocast/pgsql/tarball/21", 
        "name": "21"
      }
    ], 
    "created_at": "2013-03-07T15:02:20Z", 
    "updated_at": "2016-08-02T15:29:27Z", 
    "languages": [
      "Erlang"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/kpy3"
      }, 
      {
        "html_url": "https://github.com/rambocoder"
      }, 
      {
        "html_url": "https://github.com/0x1997"
      }, 
      {
        "html_url": "https://github.com/ates"
      }, 
      {
        "html_url": "https://github.com/pguyot"
      }, 
      {
        "html_url": "https://github.com/tsloughter"
      }, 
      {
        "html_url": "https://github.com/evgl"
      }, 
      {
        "html_url": "https://github.com/edwardt"
      }, 
      {
        "html_url": "https://github.com/liwooood"
      }, 
      {
        "html_url": "https://github.com/rschiavi"
      }, 
      {
        "html_url": "https://github.com/linearregression"
      }, 
      {
        "html_url": "https://github.com/nilskassube"
      }, 
      {
        "html_url": "https://github.com/odo"
      }, 
      {
        "html_url": "https://github.com/bullno1"
      }, 
      {
        "html_url": "https://github.com/TohaUA"
      }, 
      {
        "html_url": "https://github.com/waisbrot"
      }, 
      {
        "html_url": "https://github.com/lywaterman"
      }, 
      {
        "html_url": "https://github.com/quanta"
      }, 
      {
        "html_url": "https://github.com/marcelloceschia"
      }, 
      {
        "html_url": "https://github.com/pichi"
      }
    ], 
    "owner": "https://github.com/semiocast", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-06-20T06:59:59Z"
}{
  "doi": "10.1093/bioinformatics/btu825", 
  "name": "PhyMer a novel alignmentfree and referenceindependent mitochondrial haplogroup classifier", 
  "links": [
    "http://dna.jameslick.com/mthap", 
    "https://mseqdr.org", 
    "http://sourceforge.net/projects/h-mito", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/danielnavarrogomez/phy-mer", 
    "http://www.phylotree.org"
  ], 
  "title": "Phy-Mer: a novel alignment-free and reference-independent mitochondrial haplogroup classifier", 
  "toolName": "phy-mer", 
  "abstract": "Motivation: All current mitochondrial haplogroup classification tools require variants to be detected from an alignment with the reference sequence and to be properly named according to the canonical nomenclature standards for describing mitochondrial variants, before they can be compared with the haplogroup determining polymorphisms. With the emergence of high-throughput sequencing technologies and hence greater availability of mitochondrial genome sequences, there is a strong need for an automated haplogroup classification tool that is alignment-free and agnostic to reference sequence. Results: We have developed a novel mitochondrial genome haplogroup-defining algorithm using a k-mer approach namely Phy-Mer. Phy-Mer performs equally well as the leading haplogroup clas-sifier, HaploGrep, while avoiding the errors that may occur when preparing variants to required formats and notations. We have further expanded Phy-Mer functionality such that next-generation sequencing data can be used directly as input.", 
  "summary": "Results: We have developed a novel mitochondrial genome haplogroup-defining algorithm using a k-mer approach namely Phy-Mer. Phy-Mer performs equally well as the leading haplogroup classifier, HaploGrep, while avoiding the errors that may occur when preparing variants to required formats and notations.\nFor performance evaluation, we ran Phy-Mer, as well as HaploGrep [via MitoMaster (Lott et al., 2013)], on 6924 full-length mtDNA genome sequences that were manually assigned to haplogroups\nPhy-Mer made slightly different haplogroup calls for 54 mtDNA genome sequences compared with their original PhyloTree assignments (Supplementary Table S1).", 
  "affiliations": [
    " Center for Biomedical Informaticsand", 
    " Department of Clinical Genetics Maastricht University Medical Centre ", 
    " Department of Forensic Molecular Biology University Medical Center Rotterdam ", 
    " Center for Mitochondrial and Epigenomic Medicine The Children's Hospital of Philadelphia ", 
    " Department of Ophthalmology Harvard Medical School "
  ], 
  "grants": [
    "were supported in part by the National Institutes of Health grant NS021328 (D.C.W.).", 
    "MvO was supported in part by a grant from the Netherlands Genomic Initiative (NGI)/ Netherlands Organization for Scientific Research (NWO) within the framework of the Forensic Genomics Consortium Netherlands (FGCN).", 
    "Funding\nThis work was largely supported by institutional fund provided by Massachusetts Eye and Ear Infirmary (MEEI) for MEEI Bioinformatics Center (X.G.)."
  ], 
  "sourcelinks": [
    "https://github.com/danielnavarrogomez/phy-mer", 
    "http://sourceforge.net/projects/h-mito", 
    "http://dna.jameslick.com/mthap"
  ], 
  "acks": " ", 
  "authors": [
    " Daniel Navarro-Gomez", 
    " Jeremy Leipzig", 
    " Lishuang Shen", 
    " Marie Lott", 
    " Alphons P M Stassen", 
    " Douglas C Wallace", 
    " Janey L Wiggs", 
    " Marni J Falk", 
    " Mannis Van Oven", 
    " Xiaowu Gai"
  ], 
  "keyWords": [
    "mitochondrial haplogroup", 
    [
      "sequencing", 
      "haplogrouping", 
      "mtdna"
    ]
  ], 
  "github_data": {
    "name": "phy-mer", 
    "contributors": [
      {
        "contributions": 69, 
        "html_url": "https://github.com/danielnavarrogomez"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/fmerinocasallo"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/MatthewMaher"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/MEEIBioinformaticsCenter/phy-mer/zipball/1.0", 
        "tarball_url": "https://api.github.com/repos/MEEIBioinformaticsCenter/phy-mer/tarball/1.0", 
        "name": "1.0"
      }
    ], 
    "created_at": "2014-06-05T19:27:35Z", 
    "updated_at": "2016-01-14T14:39:30Z", 
    "languages": [
      "Python"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/danielnavarrogomez"
      }, 
      {
        "html_url": "https://github.com/leipzig"
      }, 
      {
        "html_url": "https://github.com/xgai"
      }, 
      {
        "html_url": "https://github.com/jdeligt"
      }, 
      {
        "html_url": "https://github.com/fmerinocasallo"
      }
    ], 
    "owner": "https://github.com/MEEIBioinformaticsCenter", 
    "homepage": ""
  }, 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-12-14T01:33:54Z"
}{
  "doi": "10.1093/bioinformatics/btu644", 
  "name": "PINBPA Cytoscape app for network analysis of GWAS data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis PINBPA: Cytoscape app for network analysis of GWAS data", 
  "toolName": "Genome analysis PINBPA: Cytoscape app for network analysis of GWAS data", 
  "abstract": "Protein interaction network-based pathway analysis (PINBPA) for genome-wide association studies (GWAS) has been developed as a Cytoscape app, to enable analysis of GWAS data in a network fashion. Users can easily import GWAS summary-level data, draw Manhattan plots, define blocks, prioritize genes with random walk with restart, detect enriched subnetworks and test the significance of subnetworks via a user-friendly interface.", 
  "summary": "Summary: Protein interaction network-based pathway analysis (PINBPA) for genome-wide association studies (GWAS) has been developed as a Cytoscape app, to enable analysis of GWAS data in a network fashion.\nThe original protein interaction network-based pathway analysis (PINBPA) method was first developed in the context of MS research (Baranzini et al., 2009) and most recently used to successfully identify novel associations by the International MS Genetics Consortium (International Multiple Sclerosis Genetics Consortium, 2013).", 
  "affiliations": [
    " School of Computing Queen's University ", 
    " Department of Neurology University of California San Francisco "
  ], 
  "grants": [
    "Funding: S.E.B."
  ], 
  "acks": " ", 
  "authors": [
    " Lili Wang", 
    " Takuya Matsushita", 
    " Lohith Madireddy", 
    " Parvin Mousavi", 
    " Sergio E Baranzini", 
    " John Hancock"
  ], 
  "keyWords": [
    "genetic associations", 
    "genome analysis", 
    [
      "genomic", 
      "genetics", 
      "networks", 
      "association", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-27T03:04:09Z"
}{
  "doi": "10.1093/bioinformatics/btu663", 
  "name": "PhosphoPICK modelling cellular context to map kinasesubstrate phosphorylation events", 
  "links": [
    "http://bioinf.scmb.uq.edu.au", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Systems biology PhosphoPICK: modelling cellular context to map kinase-substrate phosphorylation events", 
  "toolName": "Systems biology PhosphoPICK: modelling cellular context to map kinase-substrate phosphorylation events", 
  "abstract": "Motivation: The determinants of kinase-substrate phosphorylation can be found both in the substrate sequence and the surrounding cellular context. Cell cycle progression, interactions with mediating proteins and even prior phosphorylation events are necessary for kinases to maintain substrate specificity. While much work has focussed on the use of sequence-based methods to predict phosphorylation sites, there has been very little work invested into the application of systems biology to understand phosphorylation. Lack of specificity in many kinase sub-strate binding motifs means that sequence methods for predicting kinase binding sites are susceptible to high false-positive rates. Results: We present here a model that takes into account protein\u2013 protein interaction information, and protein abundance data across the cell cycle to predict kinase substrates for 59 human kinases that are representative of important biological pathways. The model shows high accuracy for substrate prediction (with an average AUC of 0.86) across the 59 kinases tested. When using the model to complement sequence-based kinase-specific phosphorylation site prediction, we found that the additional information increased prediction performance for most comparisons made, particularly on kinases from the CMGC family. We then used our model to identify functional overlaps between predicted CDK2 substrates and targets from the E2F family of transcription factors. Our results demonstrate that a model harnessing context data can account for the shortfalls in sequence information and provide a robust description of the cellular events that regulate protein phosphorylation. Availability and implementation: The method is freely available online as a web server at the website", 
  "summary": "Results: We present here a model that takes into account protein protein interaction information, and protein abundance data across the cell cycle to predict kinase substrates for 59 human kinases that are representative of important biological pathways.\nThe model we present here, named PhosphoPICK (Phosphorylation in a Protein Interaction Context for Kinases), integrates known kinasesubstrate relationships, PPI, and cell-cycle data to predict kinase substrates for 59 human kinases.\nThe model represents observations about a phosphorylation substrate--the kinases that bind to it, protein interactions, and whether it is upregulated during the cell-cycle phases.", 
  "affiliations": [
    " School of Chemistry and Molecular Biosciences"
  ], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " Ralph Patrick", 
    " Kim-Anh L ^ E Cao", 
    " Bostjan Kobe", 
    " Mikael Bod En", 
    " Alfonso Valencia"
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-11T03:33:58Z"
}{
  "doi": "10.1093/bioinformatics/btu637", 
  "name": "PIPDB the Protein Isoelectric Point database", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.pip-db.org"
  ], 
  "title": "Databases and ontologies Advance Access publication PIP-DB: the Protein Isoelectric Point database", 
  "toolName": "Databases and ontologies Advance Access publication PIP-DB: the Protein Isoelectric Point database", 
  "abstract": "A protein's isoelectric point or pI corresponds to the solution pH at which its net surface charge is zero. Since the early days of solution biochemistry, the pI has been recorded and reported, and thus literature reports of pI abound. The Protein Isoelectric Point database (PIP-DB) has collected and collated these data to provide an increasingly comprehensive database for comparison and bench-marking purposes. A web application has been developed to warehouse this database and provide public access to this unique resource. PIP-DB is a web-enabled SQL database with an HTML GUI front-end. PIP-DB is fully searchable across a range of properties. Availability and implementation: The PIP-DB database and documentation are available at http://www.pip", 
  "summary": "PIP-DB: the Protein Isoelectric Point database\nThe Protein Isoelectric Point database (PIP-DB) has collected and collated these data to provide an increasingly comprehensive database for comparison and benchmarking purposes.\nFor a macromolecular polyprotic system--such as protein, DNA or RNA--the isoelectric or isoionic point--commonly referred to as the isoelectric point (pI)--can be defined by the point of singularity in a titration curve, corresponding to the solution pH value at which the net overall surface charge, and thus, the mobility, of the ampholyte sums to zero (Maldonado et al., 2010).\nThus, we describe here the Protein pI Database (PIP-DB), our ongoing attempt to catalogue comprehensively the pIs of proteins, as reported in the literature.", 
  "affiliations": [
    " School of Life and Health Sciences", 
    " School of Engineering and Applied Science University of Aston Aston Triangle "
  ], 
  "grants": [
    "Funding: This work was supported by Aston University."
  ], 
  "acks": " The authors thank Sukhvir Johal, Anees Ul-haq, Saleh Ahmed and Mohammad Abdullah for their help. ", 
  "authors": [
    " Egle Bunkute", 
    " Christopher Cummins", 
    " Fraser J Crofts", 
    " Gareth Bunce", 
    " Ian T Nabney", 
    " Darren R Flower"
  ], 
  "keyWords": [
    [
      "proteins", 
      "databases"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "Python", 
    " SQL ", 
    "Clojure", 
    "JavaScript"
  ], 
  "dateCreated": "2014-09-25T00:36:59Z"
}{
  "doi": "10.1093/bioinformatics/btu647", 
  "name": "piPipes a set of pipelines for piRNA and transposon analysis via small RNAseq RNAseq degradome and CAGEseq ChIPseq and genomic DNA sequencing", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Sequence analysis piPipes: a set of pipelines for piRNA and transposon analysis via small RNA-seq, RNA-seq, degradome-and CAGE-seq, ChIP-seq and genomic DNA sequencing", 
  "toolName": "Sequence analysis piPipes: a set of pipelines for piRNA and transposon analysis via small RNA-seq, RNA-seq, degradome-and CAGE-seq, ChIP-seq and genomic DNA sequencing", 
  "abstract": "Motivation: PIWI-interacting RNAs (piRNAs), 23\u201336 nt small silencing RNAs, repress transposon expression in the metazoan germ line, thereby protecting the genome. Although high-throughput sequencing has made it possible to examine the genome and transcriptome at unprecedented resolution, extracting useful information from giga-bytes of sequencing data still requires substantial computational skills. Additionally, researchers may analyze and interpret the same data differently, generating results that are difficult to reconcile. To address these issues, we developed a coordinated set of pipelines, 'piPipes', to analyze piRNA and transposon-derived RNAs from a variety of high-throughput sequencing libraries, including small RNA, RNA, degradome or 7-methyl guanosine cap analysis of gene expression (CAGE), chromatin immunoprecipitation (ChIP) and genomic DNA-seq. piPipes can also produce figures and tables suitable for publication. By facilitating data analysis, piPipes provides an opportunity to standardize computational methods in the piRNA field.", 
  "summary": "To address these issues, we developed a coordinated set of pipelines, `piPipes', to analyze piRNA and transposon-derived RNAs from a variety of high-throughput sequencing libraries, including small RNA, RNA, degradome or 7-methyl guanosine cap analysis of gene expression (CAGE), chromatin immunoprecipitation (ChIP) and genomic DNA-seq.", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was funded by National Institutes of Health grants [GM62862 and GM65236] to P.D.Z."
  ], 
  "acks": " The authors thank the members of the Zamore and Weng laboratories for helpful discussions, and Jia Xu, Jui-Hung Hung and Soo Lee for their initial work. ", 
  "authors": [
    " Bo W Han", 
    " Wei Wang", 
    " Phillip D Zamore", 
    " Zhiping Weng"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "transposons", 
      "pipipes", 
      "pirnas"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btu590", 
  "name": "poRe an R package for the visualization and analysis of nanopore sequencing data", 
  "links": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://www.R-project.org", 
    "http://sourceforge.net/projects/rpore", 
    "https://sourceforge.net/p/rpore/wiki/Home", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.hdfgroup.org/HDF5"
  ], 
  "title": "Sequence analysis poRe: an R package for the visualization and analysis of nanopore sequencing data", 
  "toolName": "rpore", 
  "abstract": "Motivation: The Oxford Nanopore MinION device represents a unique sequencing technology. As a mobile sequencing device powered by the USB port of a laptop, the MinION has huge potential applications. To enable these applications, the bioinformatics community will need to design and build a suite of tools specifically for MinION data. Results: Here we present poRe, a package for R that enables users to manipulate, organize, summarize and visualize MinION nanopore sequ-encing data. As a package for R, poRe has been tested on Windows , Linux and MacOSX. Crucially, the Windows version allows users to analyse MinION data on the Windows laptop attached to the device. Availability and implementation: poRe is released as a package for R at http://sourceforge.net/projects/rpore/. A tutorial and further information are available at https://sourceforge.", 
  "summary": "Results: Here we present poRe, a package for R that enables users to manipulate, organize, summarize and visualize MinION nanopore sequencing data.\nWe have developed poRe, a package for the statistical package R (http://www.R-project.org/; R Core Team, 2014), which enables users to manipulate MinION FAST5 files into run folders, extract FASTQ, gather statistics on each run and plot a number of key graphs, such as read-length histograms and yield-over-time.\npoRe: an R package for the visualization and analysis of nanopore sequencing data\nAs a package for R, poRe is available for both Windows and Linux, and crucially the Windows version will allow data analysis on the mandatory Windows laptop on which the MinION depends.", 
  "affiliations": [
    " Edinburgh Genomics The Roslin Institute and R(D)SVS University of Edinburgh "
  ], 
  "grants": [
    "Funding: Edinburgh Genomics is partly supported through core grants from the National Environmental Research Council (NERC R8/H10/56), Medical Research Council (MRC MR/ K001744/1) and The Biotechnology and Biological Sciences Research Council (BBSRC BB/J004243/1).", 
    "ACKNOWLEDGEMENT\nThe authors would like to thank Oxford Nanopore for granting Edinburgh Genomics access to the MinION Access Programme (MAP)."
  ], 
  "acks": " The authors would like to thank Oxford Nanopore for granting Edinburgh Genomics access to the MinION Access Programme (MAP). ", 
  "authors": [
    " Mick Watson", 
    " Marian Thomson", 
    " Judith Risse", 
    " Richard Talbot", 
    " Javier Santoyo-Lopez", 
    " Karim Gharbi", 
    " Mark Blaxter"
  ], 
  "keyWords": [
    "sequencing data", 
    [
      "users", 
      "minion", 
      "pore", 
      "functionality", 
      "sequencer", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "https://sourceforge.net/p/rpore/wiki/Home", 
    "http://sourceforge.net/projects/rpore"
  ], 
  "technologies": [
    "Python", 
    "R"
  ], 
  "dateCreated": "2014-08-31T00:26:27Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/rpore/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/mw55309/", 
        "name": "Mick Watson"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu555", 
  "name": "Poretools a toolkit for analyzing nanopore sequence data", 
  "links": [
    "http://www.google.com/patents/US20120160687.Li,H", 
    "https://www.github.com/arq5x/poretools", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://poretools.readthedocs.org.Received", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Sequence analysis Poretools: a toolkit for analyzing nanopore sequence data", 
  "toolName": "poretools", 
  "abstract": "Motivation: Nanopore sequencing may be the next disruptive technology in genomics, owing to its ability to detect single DNA molecules without prior amplification, lack of reliance on expensive optical components , and the ability to sequence long fragments. The MinION TM from Oxford Nanopore Technologies (ONT) is the first nanopore se-quencer to be commercialized and is now available to early-access users. The MinION TM is a USB-connected, portable nanopore sequen-cer that permits real-time analysis of streaming event data. Currently, the research community lacks a standardized toolkit for the analysis of nanopore datasets. Results: We introduce poretools, a flexible toolkit for exploring data-sets generated by nanopore sequencing devices from MinION TM for the purposes of quality control and downstream analysis. Poretools operates directly on the native FAST5 (an application of the HDF5 standard) file format produced by ONT and provides a wealth of format conversion utilities and data exploration and visualization tools. Availability and implementation: Poretools is an open-source software and is written in Python as both a suite of command line utilities and a Python application programming interface.", 
  "summary": "2 FEATURES AND METHODS We have developed poretools, an open-source software toolkit that addresses the pressing need for methods to manipulate the FAST5 format and permit explorations of the raw nanopore event data and the resulting DNA sequences.\nPoretools provides an extensive set of data analysis methods that operate directly on either a single FAST5 file or a set of files from one or more sequencing runs.\nRecognizing this, we have developed a Python programming interface that researchers can use to directly access the sequence data, the raw nanopore event data and other metadata (e.g. the flowcell and run identifiers) contained in one or more FAST5 files.", 
  "affiliations": [
    " Department of Public Health Sciences University of Virginia ", 
    " Institute of Microbiology and Infection University of Birmingham "
  ], 
  "grants": [
    "Funding: N.J.L.", 
    "was supported by the NIH (NGHRI; 1R01HG006693-01)."
  ], 
  "sourcelinks": [
    "https://www.github.com/arq5x/poretools"
  ], 
  "acks": " The authors would like to express their gratitude to the employees of ONT for assistance during the MinION Access Programme TM , with particular thanks to Zoe McDougall, Clive Brown, Daniel Turner, Stephanie Brooking, Roger Pettett and Stuart Reid. We would like to thank members of the MinION user community who have tested poretools and continue to provide feedback and bug reports. ", 
  "authors": [
    " Nicholas J Loman", 
    " Aaron R Quinlan"
  ], 
  "keyWords": [
    "file format", 
    [
      "files", 
      "runs", 
      "poretools", 
      "fast", 
      "running", 
      "nanopores", 
      "bioinformatics", 
      "sequencing"
    ]
  ], 
  "github_data": {
    "name": "poretools", 
    "contributors": [
      {
        "contributions": 71, 
        "html_url": "https://github.com/nickloman"
      }, 
      {
        "contributions": 33, 
        "html_url": "https://github.com/arq5x"
      }, 
      {
        "contributions": 10, 
        "html_url": "https://github.com/brentp"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/JohnUrban"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/lonelyjoeparker"
      }, 
      {
        "contributions": 6, 
        "html_url": "https://github.com/misha-at-genestack"
      }, 
      {
        "contributions": 6, 
        "html_url": "https://github.com/ml4wc"
      }, 
      {
        "contributions": 6, 
        "html_url": "https://github.com/Phelimb"
      }, 
      {
        "contributions": 5, 
        "html_url": "https://github.com/stephenturner"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/neurons"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/bitdeli-chef"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/joshquick"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/arq5x/poretools/zipball/v0.5.1", 
        "tarball_url": "https://api.github.com/repos/arq5x/poretools/tarball/v0.5.1", 
        "name": "v0.5.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/arq5x/poretools/zipball/v0.5.0", 
        "tarball_url": "https://api.github.com/repos/arq5x/poretools/tarball/v0.5.0", 
        "name": "v0.5.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/arq5x/poretools/zipball/v0.4.0", 
        "tarball_url": "https://api.github.com/repos/arq5x/poretools/tarball/v0.4.0", 
        "name": "v0.4.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/arq5x/poretools/zipball/v0.3.0", 
        "tarball_url": "https://api.github.com/repos/arq5x/poretools/tarball/v0.3.0", 
        "name": "v0.3.0"
      }
    ], 
    "created_at": "2014-06-25T20:18:21Z", 
    "updated_at": "2016-07-28T15:36:33Z", 
    "languages": [
      "Python", 
      "Jupyter Notebook", 
      "Batchfile", 
      "PowerShell"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/arq5x"
      }, 
      {
        "html_url": "https://github.com/nickloman"
      }, 
      {
        "html_url": "https://github.com/druvus"
      }, 
      {
        "html_url": "https://github.com/suryasaha"
      }, 
      {
        "html_url": "https://github.com/iosefward"
      }, 
      {
        "html_url": "https://github.com/gringer"
      }, 
      {
        "html_url": "https://github.com/iiSeymour"
      }, 
      {
        "html_url": "https://github.com/Albapore"
      }, 
      {
        "html_url": "https://github.com/ywetzel"
      }, 
      {
        "html_url": "https://github.com/timp0"
      }, 
      {
        "html_url": "https://github.com/timini"
      }, 
      {
        "html_url": "https://github.com/mastergnome"
      }, 
      {
        "html_url": "https://github.com/bstamps"
      }, 
      {
        "html_url": "https://github.com/lmmx"
      }, 
      {
        "html_url": "https://github.com/kapsakcj"
      }, 
      {
        "html_url": "https://github.com/gatoravi"
      }, 
      {
        "html_url": "https://github.com/isc-"
      }, 
      {
        "html_url": "https://github.com/gschofl"
      }, 
      {
        "html_url": "https://github.com/ScottBGI"
      }, 
      {
        "html_url": "https://github.com/shyamrallapalli"
      }, 
      {
        "html_url": "https://github.com/knausb"
      }, 
      {
        "html_url": "https://github.com/misha-at-genestack"
      }, 
      {
        "html_url": "https://github.com/JohnUrban"
      }, 
      {
        "html_url": "https://github.com/your-highness"
      }, 
      {
        "html_url": "https://github.com/idarman"
      }, 
      {
        "html_url": "https://github.com/nweisenfeld"
      }, 
      {
        "html_url": "https://github.com/rfinkers"
      }, 
      {
        "html_url": "https://github.com/melferink"
      }, 
      {
        "html_url": "https://github.com/EvdH0"
      }, 
      {
        "html_url": "https://github.com/jdeligt"
      }
    ], 
    "owner": "https://github.com/arq5x", 
    "homepage": ""
  }, 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-08-21T04:05:53Z"
}{
  "doi": "10.1093/bioinformatics/btu711", 
  "name": "PPDMsa resource for mapping small molecule bioactivities from ChEMBL to PfamA protein domains", 
  "links": [
    "https://github.com/chembl/pfam_maps", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "PPDMs\u2014a resource for mapping small molecule bioactivities from ChEMBL to Pfam-A protein domains", 
  "toolName": "pfam_maps", 
  "abstract": "PPDMs is a resource that maps small molecule bioactivities to protein domains from the Pfam-A collection of protein families. Small molecule bioactivities mapped to protein domains add important precision to approaches that use protein sequence searches alignments to assist applications in computational drug discovery and systems and chemical biology. We have previously proposed a mapping heuristic for a subset of bioactivities stored in ChEMBL with the Pfam-A domain most likely to mediate small molecule binding. We have since refined this mapping using a manual procedure. Here, we present a resource that provides up-to-date mappings and the possibility to review assigned mappings as well as to participate in their assignment and curation. We also describe how mappings provided through the PPDMs resource are made accessible through the main schema of the ChEMBL database. Availability and implementation: The PPDMs resource and curation interface is available at https://", 
  "summary": "PPDMs--a resource for mapping small molecule bioactivities from ChEMBL to Pfam-A protein domains\nSummary: PPDMs is a resource that maps small molecule bioactivities to protein domains from the Pfam-A collection of protein families.\nTo add precision to these methods, we have previously proposed a simple mapping heuristic of small molecule bioactivities to protein domains (Kruger et al., 2012).\nThe objective of the mapping heuristic is to annotate biological assays reported in ChEMBL with the protein domain that mediates small molecule binding.", 
  "affiliations": [
    " Wellcome Trust Genome Campus", 
    " EMBL-EBI"
  ], 
  "grants": [
    "Funding\nEMBL (F.A.K."
  ], 
  "sourcelinks": [
    "https://github.com/chembl/pfam_maps", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "acks": " ", 
  "authors": [
    " Felix A Kruger", 
    " Anna Gaulton", 
    " Michal Nowotka", 
    " John P Overington", 
    " ", 
    " "
  ], 
  "keyWords": [
    "protein domains", 
    "mapping small", 
    [
      "domain", 
      "mappings", 
      "bioinformatics", 
      "proteins", 
      "maps", 
      "ppdms", 
      "chembl"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "Apache License 2.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/chembl/pfam_maps/master/LICENSE"
      }
    ], 
    "name": "pfam_maps", 
    "contributors": [
      {
        "contributions": 83, 
        "html_url": "https://github.com/fak"
      }, 
      {
        "contributions": 30, 
        "html_url": "https://github.com/MoSander"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/mnowotka"
      }
    ], 
    "versions": [], 
    "created_at": "2013-11-15T12:13:57Z", 
    "updated_at": "2015-01-12T13:30:39Z", 
    "languages": [
      "Python", 
      "JavaScript", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/fak"
      }, 
      {
        "html_url": "https://github.com/MoSander"
      }, 
      {
        "html_url": "https://github.com/mnowotka"
      }, 
      {
        "html_url": "https://github.com/loopasam"
      }, 
      {
        "html_url": "https://github.com/mark-davies"
      }, 
      {
        "html_url": "https://github.com/flatkinson"
      }, 
      {
        "html_url": "https://github.com/zhilongjia"
      }, 
      {
        "html_url": "https://github.com/revajk"
      }, 
      {
        "html_url": "https://github.com/agaulton"
      }, 
      {
        "html_url": "https://github.com/PruMM"
      }, 
      {
        "html_url": "https://github.com/nclopezo"
      }, 
      {
        "html_url": "https://github.com/pureza"
      }
    ], 
    "owner": "https://github.com/chembl", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-28T04:40:37Z"
}{
  "doi": "10.1093/bioinformatics/btu771", 
  "name": "Populationbased structural variation discovery with HydraMulti", 
  "links": [
    "https://github", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/lh3/misc/tree/master/sys/runit", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Population-based structural variation discovery with Hydra-Multi", 
  "toolName": "github", 
  "abstract": "Current strategies for SNP and INDEL discovery incorporate sequence alignments from multiple individuals to maximize sensitivity and specificity. It is widely accepted that this approach also improves structural variant (SV) detection. However, multisample SV analysis has been sty-mied by the fundamental difficulties of SV calling, e.g. library insert size variability, SV alignment signal integration and detecting long-range genomic rearrangements involving disjoint loci. Extant tools suffer from poor scalability, which limits the number of genomes that can be co-analyzed and complicates analysis workflows. We have developed an approach that enables multisample SV analysis in hundreds to thousands of human genomes using commodity hardware. Here, we describe Hydra-Multi and measure its accuracy, speed and scalability using publicly available data-sets provided by", 
  "summary": "We present an extension of Hydra (Quinlan et al., 2010), our structural variant (SV) discovery software that, like many extant tools, was designed to detect SV in a single genome using discordant paired-end alignment signals.\nBy assuming that all variants private to a normal genome are false and that the absolute number of false positive somatic calls is similar between tumor and normal datasets, we inferred the somatic false discovery rate (FDR) to be 5.2% (323/6179) (Malhotra et al., 2013).", 
  "affiliations": [
    " Department of Biochemistry and Molecular Genetics"
  ], 
  "grants": [
    "and an NIH New Innovator Award [DP2OD006493-01] and a Burroughs Wellcome Fund Career Award [to I.M.H.]", 
    "Funding\nThis work was supported by an NIH/NHGRI [1R01HG006693-01 to A.R.Q.]"
  ], 
  "sourcelinks": [
    "https://github", 
    "https://github.com/lh3/misc/tree/master/sys/runit"
  ], 
  "acks": " ", 
  "authors": [
    " Michael R Lindberg", 
    " Ira M Hall", 
    " Aaron R Quinlan"
  ], 
  "keyWords": [
    [
      "genomics", 
      "datasets", 
      "somatic", 
      "hydra", 
      "samples", 
      "discovery"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "BSD 3-clause \"New\" or \"Revised\" License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/michael/github/master/LICENSE"
      }
    ], 
    "name": "github", 
    "contributors": [
      {
        "contributions": 88, 
        "html_url": "https://github.com/AurelioDeRosa"
      }, 
      {
        "contributions": 44, 
        "html_url": "https://github.com/clayreimann"
      }, 
      {
        "contributions": 41, 
        "html_url": "https://github.com/ingalls"
      }, 
      {
        "contributions": 38, 
        "html_url": "https://github.com/aendrew"
      }, 
      {
        "contributions": 31, 
        "html_url": "https://github.com/mattpass"
      }, 
      {
        "contributions": 12, 
        "html_url": "https://github.com/darvin"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/iamdanfox"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/coderaiser"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/ctalau"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/STRd6"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/captn3m0"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/jlord"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/knsh14"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/mtscout6"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/maxogden"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/randalpinto"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/raphink"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/tricknotes"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/ele828"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/kpdecker"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/tristen"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/PeterDaveHello"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/arosenberg01"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/BernhardBezdek"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/mscdex"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/cassioscabral"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/dafortune"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/klcodanr"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/divergentdave"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/incrop"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.3.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.3.0", 
        "name": "v2.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.2.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.2.0", 
        "name": "v2.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.1.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.1.0", 
        "name": "v2.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.0.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.0.0", 
        "name": "v2.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.3.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.3.0", 
        "name": "v1.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.2.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.2.1", 
        "name": "v1.2.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.2.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.2.0", 
        "name": "v1.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.1.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.1.0", 
        "name": "v1.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.0.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.0.0", 
        "name": "v1.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.7", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.7", 
        "name": "v0.10.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.6", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.6", 
        "name": "v0.10.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.5", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.5", 
        "name": "v0.10.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.4", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.4", 
        "name": "v0.10.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.3", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.3", 
        "name": "v0.10.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.2", 
        "name": "v0.10.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.1", 
        "name": "v0.10.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.0", 
        "name": "v0.10.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.9.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.9.2", 
        "name": "v0.9.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.9.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.9.0", 
        "name": "v0.9.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.8.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.8.1", 
        "name": "v0.8.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.8.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.8.0", 
        "name": "v0.8.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.7.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.7.0", 
        "name": "v0.7.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.6.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.6.0", 
        "name": "v0.6.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.2", 
        "name": "v0.5.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.1", 
        "name": "v0.5.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.0", 
        "name": "v0.5.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.4.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.4.0", 
        "name": "v0.4.0"
      }
    ], 
    "created_at": "2012-03-06T18:08:53Z", 
    "updated_at": "2016-08-09T15:55:59Z", 
    "languages": [
      "Shell", 
      "JavaScript", 
      "HTML"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/GamesDev"
      }, 
      {
        "html_url": "https://github.com/fokusferit"
      }, 
      {
        "html_url": "https://github.com/rocel"
      }, 
      {
        "html_url": "https://github.com/thinkxl"
      }, 
      {
        "html_url": "https://github.com/schlagobers"
      }, 
      {
        "html_url": "https://github.com/jasonyandell"
      }, 
      {
        "html_url": "https://github.com/t0n1"
      }, 
      {
        "html_url": "https://github.com/timrchavez"
      }, 
      {
        "html_url": "https://github.com/apsrose"
      }, 
      {
        "html_url": "https://github.com/Bediako"
      }, 
      {
        "html_url": "https://github.com/kooyeed"
      }, 
      {
        "html_url": "https://github.com/PivotLogix"
      }, 
      {
        "html_url": "https://github.com/Timothee"
      }, 
      {
        "html_url": "https://github.com/esimionato"
      }, 
      {
        "html_url": "https://github.com/alixcan"
      }, 
      {
        "html_url": "https://github.com/FrediBach"
      }, 
      {
        "html_url": "https://github.com/antiface"
      }, 
      {
        "html_url": "https://github.com/ghostx2013"
      }, 
      {
        "html_url": "https://github.com/dnordstrom"
      }, 
      {
        "html_url": "https://github.com/bernardoantunes"
      }, 
      {
        "html_url": "https://github.com/petrosh"
      }, 
      {
        "html_url": "https://github.com/cloudtrends"
      }, 
      {
        "html_url": "https://github.com/cgkio"
      }, 
      {
        "html_url": "https://github.com/greenflag"
      }, 
      {
        "html_url": "https://github.com/loopByte"
      }, 
      {
        "html_url": "https://github.com/hasithaAlex"
      }, 
      {
        "html_url": "https://github.com/malei0311"
      }, 
      {
        "html_url": "https://github.com/majicmike"
      }, 
      {
        "html_url": "https://github.com/wuwenvogue"
      }, 
      {
        "html_url": "https://github.com/ingalls"
      }
    ], 
    "owner": "https://github.com/michael", 
    "homepage": ""
  }, 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-12-04T02:51:43Z"
}{
  "doi": "10.1093/bioinformatics/btu490", 
  "name": "Predicting protein phosphorylation from gene expression top methods from the IMPROVER Species Translation Challenge", 
  "links": [
    "http://bioinformaticsprb.med.wayne.edu", 
    "http://people.cs.clemson.edu/$luofeng/sbv.rar.3", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinformaticsprb.med.wayne.edu.2.3"
  ], 
  "title": "Systems biology Predicting protein phosphorylation from gene expression: top methods from the IMPROVER Species Translation Challenge", 
  "toolName": "Systems biology Predicting protein phosphorylation from gene expression: top methods from the IMPROVER Species Translation Challenge", 
  "abstract": "Motivation: Using gene expression to infer changes in protein phos-phorylation levels induced in cells by various stimuli is an outstanding problem. The intra-species protein phosphorylation challenge organized by the IMPROVER consortium provided the framework to identify the best approaches to address this issue. Results: Rat lung epithelial cells were treated with 52 stimuli, and gene expression and phosphorylation levels were measured. Competing teams used gene expression data from 26 stimuli to develop protein phosphorylation prediction models and were ranked based on prediction performance for the remaining 26 stimuli. Three teams were tied in first place in this challenge achieving a balanced accuracy of about 70%, indicating that gene expression is only moderately predictive of protein phosphorylation. In spite of the similar performance, the approaches used by these three teams, described in detail in this article, were different , with the average number of predictor genes per phosphoprotein used by the teams ranging from 3 to 124. However, a significant overlap of gene signatures between teams was observed for the majority of the proteins considered, while Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways were enriched in the union of the predictor genes of the three teams for multiple proteins. Availability and implementation: Gene expression and protein phos-phorylation data are available from ArrayExpress (E-MTAB-2091). Software implementation of the approach of Teams 49 and 75 are available at", 
  "summary": "Competing teams used gene expression data from 26 stimuli to develop protein phosphorylation prediction models and were ranked based on prediction performance for the remaining 26 stimuli.\nThe approach of Team 49 was based on the expectation that some of the gene expression changes between stimuli-treated rat cells and control (DME) should be informative/predictive of the phosphorylation status of a given protein.\nAn average sensitivity and specificity of 72% (BAC = 0.72) was the best performance recorded among the 21 participating teams, pointing to a moderate level of predictive information available in a one snapshot of gene expression data to infer protein phosphorylation status.", 
  "affiliations": [
    " Perinatology Research Branch Eunice Kennedy Shriver National Institute of Child Health and Human Development NIH ", 
    " School of Computing Clemson University ", 
    " Department of Molecular Biology and Biochemistry Department of Physics and BioMaPS Institute Rutgers University ", 
    " Johann Bernoulli Institute for Mathematics and Computer Science University of Groningen ", 
    " Kohn Hall University of California ", 
    " Computational Biology Center IBM T.J. Watson Research Center "
  ], 
  "grants": [
    "A.T. was also supported by the DSC best overall performer grant from Philip Morris International.", 
    "were supported, in part, by the Perinatology Research Branch, Division of Intramural Research, Eunice Kennedy Shriver National Institute of Child Health and Human Development, National Institutes of Health, Department of Health and Human Services (NICHD/NIH); and, in part, with Federal funds from NICHD, NIH under Contract No.", 
    "Funding: A.D., S.H.", 
    "NSF PHY1125915.", 
    "were supported in part by the National Science Foundation under Grant No.", 
    "4 2015, pages 462470 doi:10.1093/bioinformatics/btu490\n\nSystems biology\n\nAdvance Access publication July 23, 2014\n\nPredicting protein phosphorylation from gene expression: top\nmethods from the IMPROVER Species Translation Challenge\nAdel Dayarian1,y, Roberto Romero2,y, Zhiming Wang3,4,y, Michael Biehl5, Erhan Bilal6, Sahand Hormoz1, Pablo Meyer6, Raquel Norel6, Kahn Rhrissorrakrai6, Gyan Bhanot7,*, Feng Luo4,* and Adi L. Tarca2,8,*\n1Kavli Institute for Theoretical Physics, Kohn Hall, University of California, Santa Barbara, CA 93106, 2Perinatology\nResearch Branch, Eunice Kennedy Shriver National Institute of Child Health and Human Development, NIH, Bethesda, MD, and Detroit, MI 48201, USA, 3College of Plant Protection and College of Science, Hunan Agricultural University, Changsha, 410128, China, 4School of Computing, Clemson University, Clemson, SC 29634, USA, 5Johann Bernoulli Institute for Mathematics and Computer Science, University of Groningen, 9700 AK Groningen, The Netherlands, 6IBM T.J. Watson Research Center, Computational Biology Center, Yorktown Heights, NY 10003, 7Department of Molecular\nBiology and Biochemistry, Department of Physics and BioMaPS Institute, Rutgers University, Piscataway, NJ 08854 and 8Department of Computer Science, Wayne State University, Detroit, MI 48202, USA\nAssociate Editor: Igor Jurisica\n\nABSTRACT Motivation: Using gene expression to infer changes in protein phosphorylation levels induced in cells by various stimuli is an outstanding problem.", 
    "was also supported in part by grant 1066293 from the Aspen Center for Physics, respectively."
  ], 
  "acks": " A.D. and S.H. thank the Kavli Institute for Theoretical Physics (KITP) at UCSB, and in particular Boris Shraiman, for support. G.B. thanks the KITP for their support during the early stages of this project, the Juelich Supercomputing Centre at the Forschungszentrum, Juelich for their support when this research was being completed and Tata Institute of Fundamental Research, Mumbai, India for support and hospitality during the writing of the manuscript. A.T. was also supported by the DSC best overall performer grant from Philip Morris International. Z.W. and F.L. were supported in part by Hunan Agricultural University and by the Citrus Research and Development Foundation. P.M., K.R. and R.N. helped to edit the manuscript and generate tables and figures. E.B., R.N., P.M. and K.R. helped to develop and organize the challenge. The data and organization of challenge was performed under a joint research collaboration between I.B.M. and P.M.I., and was funded by P.M.I. ", 
  "authors": [
    " Adel Dayarian", 
    " Roberto Romero", 
    " Zhiming Wang", 
    " Michael Biehl", 
    " Erhan Bilal", 
    " Sahand Hormoz", 
    " Pablo Meyer", 
    " Raquel Norel", 
    " Kahn Rhrissorrakrai", 
    " Gyan Bhanot", 
    " Feng Luo", 
    " Adi L Tarca"
  ], 
  "keyWords": [
    "predicting protein phosphorylation", 
    [
      "proteins", 
      "genes", 
      "phosphorylated", 
      "predictions", 
      "teams", 
      "performance", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://people.cs.clemson.edu/$luofeng/sbv.rar.3", 
    "http://bioinformaticsprb.med.wayne.edu", 
    "http://bioinformaticsprb.med.wayne.edu.2.3"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-25T04:43:31Z"
}{
  "doi": "10.1093/bioinformatics/btu583", 
  "name": "PrEMeRCG inferring nucleotide level DNA methylation values from MethylCapseq data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioserv.mps.ohio-state.edu/premer"
  ], 
  "title": "Genome analysis PrEMeR-CG: inferring nucleotide level DNA methylation values from MethylCap-seq data", 
  "toolName": "Genome analysis PrEMeR-CG: inferring nucleotide level DNA methylation values from MethylCap-seq data", 
  "abstract": "Motivation: DNA methylation is an epigenetic change occurring in genomic CpG sequences that contribute to the regulation of gene transcription both in normal and malignant cells. Next-generation sequencing has been used to characterize DNA methylation status at the genome scale, but suffers from high sequencing cost in the case of whole-genome bisulfite sequencing, or from reduced resolution (inability to precisely define which of the CpGs are methylated) with capture-based techniques. Results: Here we present a computational method that computes nucleotide-resolution methylation values from capture-based data by incorporating fragment length profiles into a model of methylation analysis. We demonstrate that it compares favorably with nucleotide-resolution bisulfite sequencing and has better predictive power with respect to a reference than window-based methods, often used for enrichment data. The described method was used to produce the methylation data used in tandem with gene expression to produce a novel and clinically significant gene signature in acute myeloid leuke-mia. In addition, we introduce a complementary statistical method that uses this nucleotide-resolution methylation data for detection of differentially methylated features. Availability: Software in the form of Python and R scripts is available at http://bioserv.mps.ohio-state.edu/premer and is free for", 
  "summary": "PrEMeR-CG is a computational approach that harnesses the implicit information associated with library fragment profiles to infer nucleotide-resolution methylation values in addition to read counts data, and Methylation Modeling Analysis using GEEs (MethMAGE), a complementary statistical method that uses the nucleotide-resolution methylation data in the detection of differentially methylated features (DiMeFs), previously annotated genomic regions in which the methylation profile differs between different groups (e.g. those treated with a hypomethylating agent versus those that are not).", 
  "affiliations": [
    " Department of Statistics", 
    " College of Medicine Biomedical Sciences Graduate Program ", 
    " Department of Internal Medicine Division of Hematology "
  ], 
  "grants": [
    "Funding: This work was supported by the National Institutes of Health (CA016058, CA140158, CA101140 and CA102031); National Science Foundation (DMS-1042946 and agreement No."
  ], 
  "acks": " The authors gratefully acknowledge Benjamin AT Rodriguez for his help in sample preparation. ", 
  "authors": [
    " David E Frankhouser", 
    " Mark Murphy", 
    " James S Blachly", 
    " Jincheol Park", 
    " Mike W Zoller", 
    " Javkhlan-Ochir Ganbat", 
    " John Curfman", 
    " John C Byrd", 
    " Shili Lin", 
    " Guido Marcucci", 
    " Pearlly Yan", 
    " Ralf Bundschuh"
  ], 
  "keyWords": [
    "dna methylation", 
    [
      "genomics", 
      "methylated", 
      "based", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Python", 
    "R"
  ], 
  "dateCreated": "2014-09-02T00:17:48Z"
}{
  "doi": "10.1093/bioinformatics/btu392", 
  "name": "proovread largescale highaccuracy PacBio correction through iterative short read consensus", 
  "links": [
    "http://www.univa.com/products", 
    "http://arxiv.org/abs/1203.4802).Carneiro,M.O", 
    "http://www.stanford.edu/$kinfai/human_cerebel", 
    "http://www.pacb.com/devnet/).As", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://proovread.bioapps.biozentrum.uni-wuerzburg.de", 
    "https://github.com/najoshi/sickle"
  ], 
  "title": "Sequence analysis proovread: large-scale high-accuracy PacBio correction through iterative short read consensus", 
  "toolName": "sickle", 
  "abstract": "Motivation: Today, the base code of DNA is mostly determined through sequencing by synthesis as provided by the Illumina sequen-cers. Although highly accurate, resulting reads are short, making their analyses challenging. Recently, a new technology, single molecule real-time (SMRT) sequencing, was developed that could address these challenges, as it generates reads of several thousand bases. But, their broad application has been hampered by a high error rate. Therefore, hybrid approaches that use high-quality short reads to correct erroneous SMRT long reads have been developed. Still, current implementations have great demands on hardware, work only in well-defined computing infrastructures and reject a substantial amount of reads. This limits their usability considerably, especially in the case of large sequencing projects. Results: Here we present proovread, a hybrid correction pipeline for SMRT reads, which can be flexibly adapted on existing hardware and infrastructurefrom a laptop to a high-performance computing cluster.On genomic and transcriptomic test cases covering Escherichia coli, Arabidopsis thaliana and human, proovread achieved accuracies up to 99.9% and outperformed the existing hybrid correction programs. Furthermore, proovread-corrected sequences were longer and the throughput was higher. Thus, proovread combines the most accurate correctionresultswithan excellentadaptabilitytotheavailablehardware. It will therefore increase the applicability and value of SMRT sequencing. Availability and implementation: proovread is available at the following URL: http://proovread.", 
  "summary": "Results: Here we present proovread, a hybrid correction pipeline for SMRT reads, which can be flexibly adapted on existing hardware and infrastructurefrom a laptop to a high-performance computing cluster.On genomic and transcriptomic test cases covering Escherichia coli, Arabidopsis thaliana and human, proovread achieved accuracies up to 99.9% and outperformed the existing hybrid correction programs.\nMapping of the corrected reads onto the E.coli reference genome revealed accuracies of 99.98% for proovread, followed by 99.93% (PacBioToCA) and 88.79% (LSC).\nproovread is designed to correct erroneous LRs sequenced by SMRT with high-quality SR data as generated by Illumina sequencers.", 
  "affiliations": [
    " Department for Molecular Plant Physiology and Biophysics University of W \u20ac urzburg ", 
    " Department of Bioinformatics University of W \u20ac urzburg "
  ], 
  "grants": [
    "Funding: The research leading to these results has received funding from the European Research Council under the European\n\nUnion's Seventh Framework Programme (FP/20010-2015)/ERC Grant Agreement number 250194-Carnivorom."
  ], 
  "sourcelinks": [
    "https://github.com/najoshi/sickle"
  ], 
  "acks": " We would like to thank the 'Leibniz Rechenzentrum' and the 'Data Intensive Academic Grid' for providing us with the infrastructure for evaluation and benchmarking of the programs, Henri van de Geest, Wageningen UR, Plant Research International, Netherlands for testing of proovread and Jessika D'Lorge-Harnisch for proofreading the manuscript. Funding: The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP/20010-2015)/ERC Grant Agreement number 250194-Carnivorom. ", 
  "authors": [
    " Thomas Hackl", 
    " Rainer Hedrich", 
    " J \u20ac Org Schultz", 
    " Frank F \u20ac Orster"
  ], 
  "keyWords": [
    [
      "genomics", 
      "proovread", 
      "corrections", 
      "reads", 
      "sequencing", 
      "alignments"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/najoshi/sickle/master/LICENSE"
      }
    ], 
    "name": "sickle", 
    "contributors": [
      {
        "contributions": 82, 
        "html_url": "https://github.com/najoshi"
      }, 
      {
        "contributions": 4, 
        "html_url": "https://github.com/vsbuffalo"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/najoshi/sickle/zipball/v1.33", 
        "tarball_url": "https://api.github.com/repos/najoshi/sickle/tarball/v1.33", 
        "name": "v1.33"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/najoshi/sickle/zipball/v1.31", 
        "tarball_url": "https://api.github.com/repos/najoshi/sickle/tarball/v1.31", 
        "name": "v1.31"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/najoshi/sickle/zipball/v.1.29", 
        "tarball_url": "https://api.github.com/repos/najoshi/sickle/tarball/v.1.29", 
        "name": "v.1.29"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/najoshi/sickle/zipball/v1.2", 
        "tarball_url": "https://api.github.com/repos/najoshi/sickle/tarball/v1.2", 
        "name": "v1.2"
      }
    ], 
    "created_at": "2011-02-09T01:18:45Z", 
    "updated_at": "2016-08-04T21:09:09Z", 
    "languages": [
      "C", 
      "Makefile"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/najoshi"
      }, 
      {
        "html_url": "https://github.com/biowei"
      }, 
      {
        "html_url": "https://github.com/tanglingfung"
      }, 
      {
        "html_url": "https://github.com/jmw86069"
      }, 
      {
        "html_url": "https://github.com/biomystery"
      }, 
      {
        "html_url": "https://github.com/tsibley"
      }, 
      {
        "html_url": "https://github.com/bioinfonm"
      }, 
      {
        "html_url": "https://github.com/mteague"
      }, 
      {
        "html_url": "https://github.com/gawbul"
      }, 
      {
        "html_url": "https://github.com/kchambers58178"
      }, 
      {
        "html_url": "https://github.com/cornfly"
      }, 
      {
        "html_url": "https://github.com/ShujiaHuang"
      }, 
      {
        "html_url": "https://github.com/lingdudefeiteng"
      }
    ], 
    "owner": "https://github.com/najoshi", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-07-12T03:16:15Z"
}{
  "doi": "10.1093/bioinformatics/btu640", 
  "name": "PROPER comprehensive power evaluation for differential expression using RNAseq", 
  "links": [
    "http://web1.sph.emory.edu/users/hwu30/PROPER.html", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Gene expression PROPER: comprehensive power evaluation for differential expression using RNA-seq", 
  "toolName": "Gene expression PROPER: comprehensive power evaluation for differential expression using RNA-seq", 
  "abstract": "Motivation: RNA-seq has become a routine technique in differential expression (DE) identification. Scientists face a number of experimental design decisions, including the sample size. The power for detecting differential expression is affected by several factors, including the fraction of DE genes, distribution of the magnitude of DE, distribution of gene expression level, sequencing coverage and the choice of type I error control. The complexity and flexibility of RNA-seq experiments, the high-throughput nature of transcriptome-wide expression measurements and the unique characteristics of RNA-seq data make the power assessment particularly challenging. Results: We propose prospective power assessment instead of a direct sample size calculation by making assumptions on all of these factors. Our power assessment tool includes two components: (i) a semi-parametric simulation that generates data based on actual RNA-seq experiments with flexible choices on baseline expressions, biological variations and patterns of DE; and (ii) a power assessment component that provides a comprehensive view of power. We introduce the concepts of stratified power and false discovery cost, and demonstrate the usefulness of our method in experimental design (such as sample size and sequencing depth), as well as analysis plan (gene filtering).", 
  "summary": "We demonstrate that, in addition to the sample size and the other usual suspects in power analysis (namely, effect size and withingroup variance), there are other factors (such as the distribution of mean expression level) and other choices (such as sequencing depth and gene filtering) that influence the power of DE detection.\nMarginal targeted power versus sample sizes, with and without filtering out genes with average counts lower than 10, averaged from 100 simulations based on Cheung data\nThe user visualizes the relationship between various types of power and sample size, expression level and biological variation, and understands the cost of false discovery in different strata of genes.", 
  "affiliations": [
    " Department of Biostatistics Brown University ", 
    " Department of Biostatistics and Markey Cancer Center University of Kentucky ", 
    " Department of Biostatistics and Bioinformatics Emory University "
  ], 
  "grants": [
    "A vignette is distributed with the package, which contains detailed instruction and examples of using the package, interpreting the results and an example of sample size justification for grant proposal.", 
    "Funding: H.W."
  ], 
  "acks": " ", 
  "authors": [
    " Hao Wu", 
    " Chi Wang", 
    " Zhijin Wu"
  ], 
  "keyWords": [
    "gene expression", 
    "expressions biological", 
    [
      "powers", 
      "biologically", 
      "genes", 
      "expressed", 
      "simulations", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://web1.sph.emory.edu/users/hwu30/PROPER.html", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btv039", 
  "name": "Prediction of potential diseaseassociated microRNAs based on random walk", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.nlm.nih.gov/mesh", 
    "http://bioinfolab.stx.hk/midp", 
    "http://www"
  ], 
  "title": "Data and text mining Prediction of potential disease-associated microRNAs based on random walk", 
  "toolName": "Data and text mining Prediction of potential disease-associated microRNAs based on random walk", 
  "abstract": "Motivation: Identifying microRNAs associated with diseases (disease miRNAs) is helpful for exploring the pathogenesis of diseases. Because miRNAs fulfill function via the regulation of their target genes and because the current number of experimentally validated targets is insufficient, some existing methods have inferred potential disease miRNAs based on the predicted targets. It is difficult for these methods to achieve excellent performance due to the high false-positive and false-negative rates for the target prediction results. Alternatively, several methods have constructed a network composed of miRNAs based on their associated diseases and have exploited the information within the network to predict the disease miRNAs. However, these methods have failed to take into account the prior information regarding the network nodes and the respective local topological structures of the different categories of nodes. Therefore, it is essential to develop a method that exploits the more useful information to predict reliable disease miRNA candidates. Results: miRNAs with similar functions are normally associated with similar diseases and vice versa. Therefore, the functional similarity between a pair of miRNAs is calculated based on their associated diseases to construct a miRNA network. We present a new prediction method based on random walk on the network. For the diseases with some known related miRNAs, the network nodes are divided into labeled nodes and unlabeled nodes, and the transition matrices are established for the two categories of nodes. Furthermore, different categories of nodes have different transition weights. In this way, the prior information of nodes can be completely exploited. Simultaneously, the various ranges of topologies around the different categories of nodes are integrated. In addition, how far the walker can go away from the labeled nodes is controlled by restarting the walking. This is helpful for relieving the negative effect of noisy data. For the diseases without any known related miRNAs, we extend the walking on a miRNA-disease bilayer network. During the prediction process, the similarity between diseases, the similarity between miRNAs, the known miRNA-disease associations and the topology information of the bilayer network are exploited. Moreover, the importance of information from different layers of network is considered. Our method achieves superior performance for 18 human diseases with AUC values ranging from 0.786 to 0.945. Moreover, case studies on breast neoplasms, lung neoplasms, prostatic neo-plasms and 32 diseases further confirm the ability of our method to discover potential disease miRNAs.", 
  "summary": "HDMP integrated the functional similarities with the characteristics of miRNAs to predict candidates associated with a given disease (Xuan et al., 2013).\nBy integrating known miRNA-disease associations, the similarity between diseases and the miRNA network, RLSMDA (Chen and Yan, 2014) developed the prediction method based on regularized least squares to uncover potential miRNA candidates for a specific disease.\nBecause the unlabeled nodes are probably associated with d, the prediction goal is to rank all the unlabeled nodes and obtain the potential disease miRNAs. In our method, we correlate an unlabeled node ui with a relevance score RScore(ui).", 
  "affiliations": [
    " School of Computer and Information Engineering Harbin University of Commerce ", 
    " School of Information Science and Technology Heilongjiang University ", 
    " School of Computer Science and Technology Heilongjiang University ", 
    " College of Bioinformatics Science and Technology Harbin Medical University "
  ], 
  "grants": [], 
  "acks": " The work was supported by the Natural Science Foundation of China (61302139, 61402138), China Postdoctoral Science Foundation (2014M550200, 2014M561350), the Science and Technology Innovation Team Construction Project of Heilongjiang Province College (2013TD012), the Natural Science Foundation of Heilongjiang Province (F201324, E201452), the Postdoctoral Foundation of Heilongjiang Province (LBH- Z14152), the Young Innovative Talent Research Foundation of Harbin Science and Technology Bureau (2012RFQXS094), the Support Program for Young Academic Key Teacher of Higher Education of Heilongjiang Province (1254G030), and the Distinguished Youth Foundation of Heilongjiang University (JCL201405). Conflict of Interest: none declared. ", 
  "authors": [
    " Ping Xuan", 
    " Ke Han", 
    " Yahong Guo", 
    " Jin Li", 
    " Xia Li", 
    " Yingli Zhong", 
    " Zhaogong Zhang", 
    " Jian Ding"
  ], 
  "keyWords": [
    [
      "associations", 
      "methods", 
      "predicting", 
      "mirnas", 
      "cancers", 
      "diseases"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-25T01:20:12Z"
}{
  "doi": "10.1093/bioinformatics/btu554", 
  "name": "ProtocolNavigator emulationbased software for the design documentation and reproduction biological experiments", 
  "links": [
    "http://protocolnavigator.org/index.html", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Databases and ontologies ProtocolNavigator: emulation-based software for the design, documentation and reproduction biological experiments", 
  "toolName": "Databases and ontologies ProtocolNavigator: emulation-based software for the design, documentation and reproduction biological experiments", 
  "abstract": "Motivation: Experimental reproducibility is fundamental to the progress of science. Irreproducible research decreases the efficiency of basic biological research and drug discovery and impedes experimental data reuse. A major contributing factor to irreproducibility is difficulty in interpreting complex experimental methodologies and designs from written text and in assessing variations among different experiments. Current bioinformatics initiatives either are focused on computational research reproducibility (i.e. data analysis) or laboratory information management systems. Here, we present a software tool, ProtocolNavigator, which addresses the largely overlooked challenges of interpretation and assessment. It provides a biologist-friendly open-source emulation-based tool for designing, documenting and reproducing biological experiments. Availability and implementation: ProtocolNavigator was implemented in Python 2.7, using the wx module to build the graphical user interface. It is a platform-independent software and freely available from http://protocolnavigator.org/index.html under the GPL v2 license.", 
  "summary": "Prominent reproducibility initiatives in bioinformatics are focused either on Taverna-like (Wolstencroft et al., 2013) `data analysis' centric computational research or laboratory information management systems like `data curation-management' infrastructures (Rocca-Serra et al., 2010).\nIn design terms, the first approach (Fig. 1A) achieves a time course for the tumour cultures achieved by seeding the cells at staggered time points (on sequential days), and labelling each population with nanoparticles 24 h post seeding.\nThis experience confirmed that for the computational researchers to easily interpret and use these experiment-derived data, visual perception and understanding of the experimental design and provenance information was critical.", 
  "affiliations": [
    " Imaging Platform Broad Institute of MIT and Harvard ", 
    " School of Medicine Cardiff University ", 
    " School of Optometry and Vision Sciences Cardiff University "
  ], 
  "grants": [
    "Funding: This work was supported by European Union Marie Curie International Outgoing Fellowship (FP7-PEOPLE-2009IOF-254046) and Union for International Cancer Control Fellowship (YY2/08/011) to I.A.K.", 
    "(A.E.C) and UK Engineering and Physical Sciences Research Council grant (EP/G037841/1 to R.J.E).", 
    "This work was further supported by the US National Institute of Health (R01 grant GM089652 to A.E.C), Human Frontiers in Science Program\n\n3441\n\n\fI.A.Khan et al."
  ], 
  "acks": " ", 
  "authors": [
    " Imtiaz A Khan", 
    " Adam Fraser", 
    " Mark-Anthony Bray", 
    " Paul J Smith", 
    " Nick S White", 
    " Anne E Carpenter", 
    " Rachel J Errington"
  ], 
  "keyWords": [
    [
      "designing", 
      "protocolnavigator", 
      "researchers", 
      "bioinformatics", 
      "data", 
      "experimental"
    ]
  ], 
  "sourcelinks": [
    "http://protocolnavigator.org/index.html"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-23T04:14:10Z"
}{
  "doi": "10.1093/bioinformatics/btv042", 
  "name": "protrProtrWeb R package and web server for generating various numerical representation schemes of protein sequences", 
  "links": [
    "http://cran", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://protrweb.scbdd.com"
  ], 
  "title": "Systems biology protr/ProtrWeb: R package and web server for generating various numerical representation schemes of protein sequences", 
  "toolName": "Systems biology protr/ProtrWeb: R package and web server for generating various numerical representation schemes of protein sequences", 
  "abstract": "Amino acid sequence-derived structural and physiochemical descriptors are extensively utilized for the research of structural, functional, expression and interaction profiles of proteins and peptides. We developed protr, a comprehensive R package for generating various numerical representation schemes of proteins and peptides from amino acid sequence. The package calculates eight descriptor groups composed of 22 types of commonly used descriptors that include about 22 700 descriptor values. It allows users to select amino acid properties from the AAindex database, and use self-defined properties to construct customized descriptors. For proteochemometric mod-eling, it calculates six types of scales-based descriptors derived by various dimensionality reduction methods. The protr package also integrates the functionality of similarity score computation derived by protein sequence alignment and Gene Ontology semantic similarity measures within a list of proteins, and calculates profile-based protein features based on position-specific scoring matrix. We also developed ProtrWeb, a user-friendly web server for calculating descriptors presented in the protr package. Availability and implementation: The protr package is freely available from CRAN: http://cran. r-project.org/package\u00bcprotr, ProtrWeb, is freely available at http://protrweb.", 
  "summary": "Moreover, for protein and peptides, amino acid sequence and annotation-based similarity scores derived from sequence alignments and Gene Ontology (GO) annotation comparison are also useful representation schemes, which are widely used in modeling, such as genome-wide inference of proteinprotein interactions (Zhang et al., 2012).\nHere, we introduce protr and ProtrWeb, the R package and web server for calculating various numerical representation schemes of protein and peptides from amino acid sequence.\nThe protr package calculates various commonly used structural and physicochemical descriptors and PCMs modeling descriptors for amino acid sequences.", 
  "affiliations": [
    " School of Mathematics and Statistics", 
    " School of Pharmaceutical Sciences Central South University "
  ], 
  "grants": [
    "Grant,B.J.", 
    "In future development of protr, it is a potential direction to incorporate 3D structural information of proteins (Grant et al., 2006), which would be beneficial in several analysis and modeling scenerios.", 
    "Funding\nThis study was supported by the National Key Basic Research Program [2015CB910700] and the National Natural Science Foundation of China [Grants Nos."
  ], 
  "acks": " ", 
  "authors": [
    " Nan Xiao", 
    " Dong-Sheng Cao", 
    " Min-Feng Zhu", 
    " Qing-Song Xu"
  ], 
  "keyWords": [
    "descriptor groups", 
    "protein sequences", 
    [
      "group", 
      "sequence", 
      "descriptors", 
      "proteins", 
      "modelling", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "Python", 
    "R", 
    "Factor"
  ], 
  "dateCreated": "2015-01-26T01:18:20Z"
}{
  "doi": "10.1093/bioinformatics/btu580", 
  "name": "Proteinprotein binding affinity prediction from amino acid sequence", 
  "links": [
    "http://www.iitm.ac.in/bioinfo/PPA_Pred", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.iitm.ac.in/bioinfo/PPA_Pred/.4"
  ], 
  "title": "Protein\u2013protein binding affinity prediction from amino acid sequence", 
  "toolName": "Protein\u2013protein binding affinity prediction from amino acid sequence", 
  "abstract": "Motivation: Protein\u2013protein interactions play crucial roles in many biological processes and are responsible for smooth functioning of the machinery in living organisms. Predicting the binding affinity of pro-tein\u2013protein complexes provides deep insights to understand the recognition mechanism and identify the strong binding partners in protein\u2013protein interaction networks. Results: In this work, we have collected the experimental binding affinity data for a set of 135 protein\u2013protein complexes and analyzed the relationship between binding affinity and 642 properties obtained from amino acid sequence. We noticed that the overall correlation is poor, and the factors influencing affinity depends on the type of the complex based on their function, molecular weight and binding site residues. Based on the results, we have developed a novel methodology for predicting the binding affinity of protein\u2013protein complexes using sequence based features by classifying the complexes with respect to their function and predicted percentage of binding site residues. We have developed regression models for the complexes belonging to different classes with three to five properties, which showed a correlation in the range of 0.739\u20130.992 using jack-knife test. We suggest that our approach adds a new aspect of biological significance in terms of classifying the protein\u2013protein complexes for affinity prediction.", 
  "summary": "Based on the results, we have developed a novel methodology for predicting the binding affinity of proteinprotein complexes using sequence-based features by classifying the complexes with respect to their function and predicted percentage of binding site residues.\nWe have classified the proteinprotein complexes based on their function, molecular weight and percentage of binding site residues and analyzed the relationship between sequence and structural properties of interacting free proteins and binding affinity.\nWe have classified the complexes based on the molecular weights of free proteins, function and predicted number of binding site residues, and computed the correlation between binding affinity and sequence/structure-based properties.", 
  "affiliations": [
    " Department of Biotechnology, Bhupat and Jyoti Mehta School of BioSciences Indian Institute of Technology Madras "
  ], 
  "grants": [
    "Funding: The work was partly supported by the Department of Science and Technology, Government of India to M.M.G.", 
    "thanks Paruchuri Anoosha for the help in developing the Web server and the University Grants Commission (UGC), Government of India for providing research fellowship."
  ], 
  "acks": " ", 
  "authors": [
    " K Yugandhar", 
    " M Michael Gromiha", 
    " Alfonso Valencia"
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-30T00:28:26Z"
}{
  "doi": "10.1093/bioinformatics/btu848", 
  "name": "PRSice Polygenic Risk Score software", 
  "links": [
    "http://PRSice.info.3", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://PRSice.info", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "PRSice: Polygenic Risk Score software", 
  "toolName": "PRSice: Polygenic Risk Score software", 
  "abstract": "A polygenic risk score (PRS) is a sum of trait-associated alleles across many genetic loci, typically weighted by effect sizes estimated from a genome-wide association study. The application of PRS has grown in recent years as their utility for detecting shared genetic aetiology among traits has become appreciated; PRS can also be used to establish the presence of a genetic signal in under-powered studies, to infer the genetic architecture of a trait, for screening in clinical trials, and can act as a biomarker for a phenotype. Here we present the first dedicated PRS software, PRSice ('precise'), for calculating, applying, evaluating and plotting the results of PRS. PRSice can calculate PRS at a large number of thresholds (\" high resolution \") to provide the best-fit PRS, as well as provide results calculated at broad P-value thresholds, can thin Single Nucleotide Polymorphisms (SNPs) according to linkage disequilibrium and P-value or use all SNPs, handles genotyped and imputed data, can calculate and incorporate ancestry-informative variables, and can apply PRS across multiple traits in a single run. We exemplify the use of PRSice via application to data on schizophrenia, major depres-sive disorder and smoking, illustrate the importance of identifying the best-fit PRS and estimate a P-value significance threshold for high-resolution PRS studies. Availability and implementation: PRSice is written in R, including wrappers for bash data management scripts and PLINK-1.9 to minimize computational time. PRSice runs as a command-line program with a variety of user-options, and is freely available for download from http://PRSice.info Contact:", 
  "summary": "PRSice can calculate PRS at a large number of thresholds (\"high resolution\") to provide the best-fit PRS, as well as provide results calculated at broad P-value thresholds, can thin Single Nucleotide Polymorphisms (SNPs) according to linkage disequilibrium and P-value or use all SNPs, handles genotyped and imputed data, can calculate and incorporate ancestry-informative variables, and can apply PRS across multiple traits in a single run.\nWe exemplify the use of PRSice via application to data on schizophrenia, major depressive disorder and smoking, illustrate the importance of identifying the best-fit PRS and estimate a P-value significance threshold for high-resolution PRS studies.", 
  "affiliations": [
    " Genetic and Developmental Psychiatry Centre Institute of Psychiatry, Psychology and Neuroscience MRC Social King's College London "
  ], 
  "grants": [
    "Funding\nMRC studentship (to JE), EU FP7 no.", 
    "279227(PsychDPC), and the NIHR Biomedical Research Centre at SLaM and KCL."
  ], 
  "acks": " ", 
  "authors": [
    " Jack Euesden", 
    " Cathryn M Lewis", 
    " Paul F O 'reilly"
  ], 
  "keyWords": [
    [
      "prsice", 
      "genetics", 
      "studies", 
      "including", 
      "significance", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://PRSice.info"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-12-31T01:24:57Z"
}{
  "doi": "10.1093/bioinformatics/btu602", 
  "name": "PseKNCGeneral a crossplatform package for generating various modes of pseudo nucleotide compositions", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://lin.uestc.edu.cn/server/pseknc", 
    "http://lin"
  ], 
  "title": "PseKNC-General: a cross-platform package for generating various modes of pseudo nucleotide compositions", 
  "toolName": "PseKNC-General: a cross-platform package for generating various modes of pseudo nucleotide compositions", 
  "abstract": "The avalanche of genomic sequences generated in the post-genomic age requires efficient computational methods for rapidly and accurately identifying biological features from sequence information. Towards this goal, we developed a freely available and open-source package, called PseKNC-General (the general form of pseudo k-tuple nucleotide composition), that allows for fast and accurate computation of all the widely used nucleotide structural and physicochemical properties of both DNA and RNA sequences. PseKNC-General can generate several modes of pseudo nucleotide compositions, including conventional k-tuple nucleotide compositions, Moreau\u2013Broto autocorrelation coefficient, Moran autocorrelation coefficient , Geary autocorrelation coefficient, Type I PseKNC and Type II PseKNC. In every mode, 4100 physicochemical properties are available for choosing. Moreover, it is flexible enough to allow the users to calculate PseKNC with user-defined properties. The package can be run on Linux, Mac and Windows systems and also provides a graph-ical user interface. Availability and implementation: The package is freely available at: http://", 
  "summary": "Towards this goal, we developed a freely available and opensource package, called PseKNC-General (the general form of pseudo k-tuple nucleotide composition), that allows for fast and accurate computation of all the widely used nucleotide structural and physicochemical properties of both DNA and RNA sequences.\nIn the present work, we provide a cross-platform stand-alone and open-source package, called PseKNC-General, which could convert large-scale sequence datasets to pseudo nucleotide compositions with numerous choices of physicochemical property combinations.\n(2014) iNuc-PseKNC: a sequence-based predictor for predicting nucleosome positioning in genomes with pseudo k-tuple nucleotide composition.", 
  "affiliations": [
    " Department of Computer Science Vassar College ", 
    " Department of Computer Science Virginia Tech ", 
    " Department of Electrical and Computer Engineering University of Virginia "
  ], 
  "grants": [
    "Funding: This work was supported by the National Nature Science Foundation of China [61100092 to W.C., 61202256 to H.L.", 
    "], the Nature Scientific Foundation of Hebei Province [C2013209105 to W.C.] and the National Science Foundation [NSF OCI-1124123 to L.Z.]."
  ], 
  "acks": " The authors wish to thank the three anonymous reviewers for their constructive comments, which were helpful for strengthening the presentation of this study. ", 
  "authors": [
    " Wei Chen", 
    " Xitong Zhang", 
    " Jordan Brooker", 
    " Hao Lin", 
    " Liqing Zhang", 
    " Kuo-Chen Chou", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "computational", 
      "university", 
      "bioinformatics", 
      "pseknc", 
      "sciences", 
      "sequences", 
      "chen"
    ]
  ], 
  "sourcelinks": [
    "http://lin"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-18T04:36:19Z"
}{
  "doi": "10.1093/bioinformatics/btu598", 
  "name": "Proteomic analysis and prediction of human phosphorylation sites in subcellular level reveal subcellular specificity", 
  "links": [
    "http://bioinfo.ncu.edu.cn/SubPhos.aspx.2", 
    "http://bioinfo.ncu.edu.cn/SubPhos.aspx", 
    "http://string-db.org", 
    "http://david.abcc.ncifcrf.gov/home.jsp", 
    "http://www.biocompare.com/Editorial-Articles", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.csie.ntu.edu.tw/cjlin/libsvm", 
    "http://motifx.med.harvard.edu"
  ], 
  "title": "Sequence analysis Proteomic analysis and prediction of human phosphorylation sites in subcellular level reveal subcellular specificity", 
  "toolName": "Sequence analysis Proteomic analysis and prediction of human phosphorylation sites in subcellular level reveal subcellular specificity", 
  "abstract": "Motivation: Protein phosphorylation is the most common post-translational modification (PTM) regulating major cellular processes through highly dynamic and complex signaling pathways. Large-scale comparative phosphoproteomic studies have frequently been done on whole cells or organs by conventional bottom-up mass spec-trometry approaches, i.e at the phosphopeptide level. Using this approach , there is no way to know from where the phosphopeptide signal originated. Also, as a consequence of the scale of these studies, important information on the localization of phosphorylation sites in subcellular compartments (SCs) is not surveyed. Results: Here, we present a first account of the emerging field of subcellular phosphoproteomics where a support vector machine (SVM) approach was combined with a novel algorithm of discrete wavelet transform (DWT) to facilitate the identification of compartment specific phosphorylation sites and to unravel the intricate regulation of protein phosphorylation. Our data reveal that the subcellular phosphorylation distribution is compartment type dependent and that the phosphorylation displays site-specific sequence motifs that diverge between SCs. Availability and implementation: The method and database both are available as a web server at: http://bioinfo.", 
  "summary": "But as evident from Figure 1F, the phosphorylated proteins have higher specialization than the corresponding kinases for phosphorylation signaling pathways in specific SCs. Additionally, for two different levels, all phosphorylated proteins in compartments and unique phosphorylated proteins in one compartment, we computed the average number of phosphosites observed in eight SCs and the global cell (Fig. 1G).\nAlthough SCs share a partly independent phosphorylation network according to our analysis, the protein composition of a specific compartment is not static and undergoes dynamic changes following interactions with other SCs. Statistical results from kinase and phosphorylated protein data have exhibited coincident results that the kinases and the phosphorylated proteins concurrently resided in different SCs (Supplementary Fig. S5, for the detailssee Supplementary Tables S8 and S9).", 
  "affiliations": [
    " Department of Chemistry Nanchang University "
  ], 
  "grants": [
    "Funding: This work was supported by Program for New Century Excellent Talents in University (NCET-11-1002); and the National Natural Science Foundation of China (21305062, 21175064)."
  ], 
  "acks": " The authors thank A. Burlingame for suggestions for this article. ", 
  "authors": [
    " Xiang Chen", 
    " Shao-Ping Shi", 
    " Sheng-Bao Suo", 
    " Hao-Dong Xu", 
    " Jian-Ding Qiu", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "phosphorylations", 
      "subcellular", 
      "cells", 
      "proteins", 
      "specifically", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://bioinfo.ncu.edu.cn/SubPhos.aspx.2", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinfo.ncu.edu.cn/SubPhos.aspx", 
    "http://david.abcc.ncifcrf.gov/home.jsp", 
    "http://motifx.med.harvard.edu"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-19T04:50:41Z"
}{
  "doi": "10.1093/bioinformatics/btv001", 
  "name": "PVAAS identify variants associated with aberrant splicing from RNAseq", 
  "links": [
    "http://pvaas.sourceforge.net", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "PVAAS: identify variants associated with aberrant splicing from RNA-seq", 
  "toolName": "pvaas", 
  "abstract": "Motivation: RNA-seq has been widely used to study the transcriptome. Comparing to microarray, sequencing-based RNA-seq is able to identify splicing variants and single nucleotide variants in one experiment simultaneously. This provides unique opportunity to detect variants that associated with aberrant splicing. Despite the popularity of RNA-seq, no bioinformatics tool has been developed to leverage this advantage to identify variants associated with aberrant splicing. Results: We have developed PVAAS, a tool to identify single nucleotide variants that associated with aberrant alternative splicing from RNA-seq data. PVAAS works in three steps: (i) identify aberrant splicings; (ii) use user-provided variants or perform variant calling; (iii) assess the significance of association between variants and aberrant splicing events. Availability and implementation: PVAAS is written in Python and C. Source code and a comprehensive user's manual are freely available at: http://pvaas.sourceforge.net/.", 
  "summary": "PVAAS: identify variants associated with aberrant splicing from RNA-seq\nDespite the popularity of RNA-seq, no bioinformatics tool has been developed to leverage this advantage to identify variants associated with aberrant splicing.\nResults: We have developed PVAAS, a tool to identify single nucleotide variants that associated with aberrant alternative splicing from RNA-seq data.\nWe have developed PVAAS, a program to detect SNVs that are associated with aberrant alternative splicing from RNA-seq data directly.\nIdentify variants associated with aberrant splicing from RNA-seq\n2.2 Identify variants associated with aberrant splicing", 
  "affiliations": [
    " Division of Biomedical Statistics and Informatics Mayo Clinic College of Medicine "
  ], 
  "grants": [
    "Funding\nThis work was supported by the Center for Individualized Medicine at Mayo Clinic (CA15083-40C19 to J.-P.A.K.)."
  ], 
  "acks": " ", 
  "authors": [
    " Liguo Wang", 
    " Jinfu J Nie", 
    " Jean-Pierre A Kocher"
  ], 
  "keyWords": [
    [
      "splicings", 
      "mutations", 
      "exonic", 
      "bioinformatics", 
      "variants", 
      "pvaas"
    ]
  ], 
  "sourcelinks": [
    "http://pvaas.sourceforge.net"
  ], 
  "technologies": [
    "Python", 
    "C"
  ], 
  "dateCreated": "2015-01-09T02:26:44Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/pvaas/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/zbfish/", 
        "name": "Liguo Wang"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu735", 
  "name": "PyFDAP automated analysis of fluorescence decay after photoconversion FDAP experiments", 
  "links": [
    "http://people.tuebingen.mpg.de/mueller-lab", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "PyFDAP: automated analysis of fluorescence decay after photoconversion (FDAP) experiments", 
  "toolName": "PyFDAP: automated analysis of fluorescence decay after photoconversion (FDAP) experiments", 
  "abstract": "We developed the graphical user interface PyFDAP for the fitting of linear and non-linear decay functions to data from fluorescence decay after photoconversion (FDAP) experiments. PyFDAP structures and analyses large FDAP datasets and features multiple fitting and plotting options. Availability and implementation: PyFDAP was written in Python and runs on Ubuntu Linux, Mac OS X and Microsoft Windows operating systems. The software, a user guide and a test FDAP data-set are freely available for download from http://people.", 
  "summary": "Summary: We developed the graphical user interface PyFDAP for the fitting of linear and non-linear decay functions to data from fluorescence decay after photoconversion (FDAP) experiments.\nOur software PyFDAP features (i) a comprehensive data format for handling, sorting and annotating large FDAP datasets, (ii) the ability to separate FDAP datasets into their intra- and extracellular components based on counter-labeling, (iii) established fitting algorithms and (iv) a user-friendly environment that allows researchers from a non-computational background to easily evaluate FDAP datasets.", 
  "affiliations": [
    " Systems Biology of Development Group Friedrich Miescher Laboratory of the Max Planck Society "
  ], 
  "grants": [], 
  "acks": " We thank Katherine Rogers, Luciano Marcon, David M\u00f6 rsdorf and Gary Soh for useful suggestions and discussions. This work was supported by the Emmy Noether Program of the Deutsche Forschungsgemeinschaft, the Max Planck Society and a Career Development Award from the Human Frontier Science Program. ", 
  "authors": [
    " Alexander Bl\u00e4", 
    " Patrick M\u00fc"
  ], 
  "keyWords": [
    [
      "datasets", 
      "users", 
      "decaying", 
      "proteins", 
      "extracellular", 
      "fdap", 
      "pyfdap"
    ]
  ], 
  "sourcelinks": [
    "http://people.tuebingen.mpg.de/mueller-lab"
  ], 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-11-08T01:28:35Z"
}{
  "doi": "10.1093/bioinformatics/btu565", 
  "name": "PyBamView a browserbased application for viewing short read alignments", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://mgymrek.github.io/pybamview"
  ], 
  "title": "Sequence analysis PyBamView: a browser-based application for viewing short read alignments", 
  "toolName": "pybamview", 
  "abstract": "Current sequence alignment browsers allow visualization of large and complex next-generation sequencing datasets. However, most of these tools provide inadequate display of insertions and can be cumbersome to use on large datasets. I implemented PyBamView, a lightweight Web application for visualizing short read alignments. It provides an easy-to-use Web interface for viewing alignments across multiple samples, with a focus on accurate visualization of insertions. Availability and Implementation: PyBamView is available as a standard python package. The source code is freely available under the MIT license at https://mgymrek.github.io/pybamview.", 
  "summary": "Current genome browsers, such as the UCSC Genome Browser (Kent et al., 2002) and the Integrative Genomics Viewer (IGV) (Robinson et al., 2011), offer visualization of alignments from SAM/BAM files across multiple samples and integration of many layers of genomics datasets.\nSeveral alignment browsers, such as Bambino (Edmonson et al., 2011), Consed (Gordon and Green, 2013) and the text-based SAMtools (Li et al., 2009) tview, overcome these limitations: they display the sequence of insertions even when using the standard ungapped reference, and are run locally with relatively low system requirements.\nThis allows easy visualization of the sequence and size of inserted bases, which is not currently possible with most alignment browsers (Supplementary Fig. S1).", 
  "affiliations": [], 
  "grants": [
    "Funding: This work was supported by a National Defense Science and Engineering Graduate Fellowship (32 CFR 168a)."
  ], 
  "sourcelinks": [
    "https://mgymrek.github.io/pybamview"
  ], 
  "acks": " The author would like to acknowledge members of the Erlich lab, Alon Goren, and Roy Ronen for helpful feedback, and Assaf Gordon for valuable programming guidance. ", 
  "authors": [], 
  "keyWords": [
    [
      "genomics", 
      "insertions", 
      "pybamview", 
      "bioinformatics", 
      "sequencing", 
      "alignments"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/mgymrek/pybamview/master/LICENSE.txt"
      }
    ], 
    "name": "pybamview", 
    "contributors": [
      {
        "contributions": 180, 
        "html_url": "https://github.com/mgymrek"
      }, 
      {
        "contributions": 14, 
        "html_url": "https://github.com/robinandeer"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/moonso"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/mdshw5"
      }, 
      {
        "contributions": 6, 
        "html_url": "https://github.com/ihaque"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/andynu"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/laserson"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/fomightez"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/mgymrek/pybamview/zipball/v1.0.5", 
        "tarball_url": "https://api.github.com/repos/mgymrek/pybamview/tarball/v1.0.5", 
        "name": "v1.0.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mgymrek/pybamview/zipball/v1.0.4", 
        "tarball_url": "https://api.github.com/repos/mgymrek/pybamview/tarball/v1.0.4", 
        "name": "v1.0.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mgymrek/pybamview/zipball/v1.0.3", 
        "tarball_url": "https://api.github.com/repos/mgymrek/pybamview/tarball/v1.0.3", 
        "name": "v1.0.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mgymrek/pybamview/zipball/v1.0.2", 
        "tarball_url": "https://api.github.com/repos/mgymrek/pybamview/tarball/v1.0.2", 
        "name": "v1.0.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mgymrek/pybamview/zipball/1.0.1", 
        "tarball_url": "https://api.github.com/repos/mgymrek/pybamview/tarball/1.0.1", 
        "name": "1.0.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/mgymrek/pybamview/zipball/1.0.0", 
        "tarball_url": "https://api.github.com/repos/mgymrek/pybamview/tarball/1.0.0", 
        "name": "1.0.0"
      }
    ], 
    "created_at": "2014-01-23T01:47:14Z", 
    "updated_at": "2016-06-27T11:50:17Z", 
    "languages": [
      "Python", 
      "JavaScript", 
      "HTML", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/mgymrek"
      }, 
      {
        "html_url": "https://github.com/gatoravi"
      }, 
      {
        "html_url": "https://github.com/kchennen"
      }
    ], 
    "owner": "https://github.com/mgymrek", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-08-22T03:08:27Z"
}{
  "doi": "10.1093/bioinformatics/btv016", 
  "name": "QTLMiner QTL database curation by mining tables in literature", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining QTLMiner: QTL database curation by mining tables in literature", 
  "toolName": "Data and text mining QTLMiner: QTL database curation by mining tables in literature", 
  "abstract": "Motivation: Figures and tables in biomedical literature record vast amounts of important experiment results. In scientific papers, for example, quantitative trait locus (QTL) information is usually presented in tables. However, most of the popular text-mining methods focus on extracting knowledge from unstructured free text. As far as we know, there are no published works on mining tables in biomedical literature. In this article, we propose a method to extract QTL information from tables and plain text found in literature. Heterogeneous and complex tables were converted into a structured database, combined with information extracted from plain text. Our method could greatly reduce labor burdens involved with database curation. Results: We applied our method on a soybean QTL database curation, from which 2278 records were extracted from 228 papers with a precision rate of 96.9% and a recall rate of 83.3%, F value for the method is 89.6%.", 
  "summary": "QTLMiner: QTL database curation by mining tables in literature\nIn this article, we propose a method to extract QTL information from tables and plain text found in literature.\nResults: We applied our method on a soybean QTL database curation, from which 2278 records were extracted from 228 papers with a precision rate of 96.9% and a recall rate of 83.3%, F value for the method is 89.6%.\nTo allow researchers to query QTL information easily, several databases have been developed by manually curating the information scattered in the literature (Grant et al.", 
  "affiliations": [
    " The Key Lab of Soybean Molecular Design Breeding Northeast Institute of Geography and Agroecology Chinese Academy of Sciences ", 
    " College of Electronic and Information Northeast Agricultural University ", 
    " School of Computer Science and Technology Changchun University of Science and Technology "
  ], 
  "grants": [
    "QTLMiner\n\n1691\n\nReferences\nGrant,D.", 
    "To allow researchers to query QTL information easily, several databases have been developed by manually curating the information scattered in the literature (Grant et al.", 
    "Compared with the records in the SoyBase QTL database (Grant et al.", 
    "Funding\nThis research was funded by `Hundred Talents Program' of the Chinese Academy of Sciences and Postdoctoral Science Foundation of Heilongjiang Province (LBH-Z13018)."
  ], 
  "acks": " ", 
  "authors": [
    " Jing Peng", 
    " Xinyi Shi", 
    " Yiming Sun", 
    " Dongye Li", 
    " Baohui Liu", 
    " Fanjiang Kong", 
    " Xiaohui Yuan"
  ], 
  "keyWords": [
    "qtl database", 
    "text mining", 
    [
      "recording", 
      "tables", 
      "information", 
      "databases", 
      "texts"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-14T02:18:45Z"
}{
  "doi": "10.1093/bioinformatics/btu424", 
  "name": "PyWATER a PyMOL plugin to find conserved water molecules in proteins by clustering", 
  "links": [
    "https://github.com/hiteshpatel379/PyWATER", 
    "http://www.pymol.org", 
    "http://sourceforge.net/projects/pymol", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://sourceforge.net/projects"
  ], 
  "title": "Structural bioinformatics PyWATER: a PyMOL plug-in to find conserved water molecules in proteins by clustering", 
  "toolName": "PyWATER", 
  "abstract": "Conserved water molecules play a crucial role in protein structure, stabilization of secondary structure, protein activity, flexibility and ligand binding. Clustering of water molecules in superimposed protein structures, obtained by X-ray crystallography at high resolution , is an established method to identify consensus water molecules in all known protein structures of the same family. PyWATER is an easy-to-use PyMOL plug-in and identifies conserved water molecules in the protein structure of interest. PyWATER can be installed via the user interface of PyMOL. No programming or command-line knowledge is required for its use. Availability and Implementation: PyWATER and a tutorial are available at https://github.com/hiteshpatel379/PyWATER. PyMOL is available at http://www.pymol.org/ or http://sourceforge.net/projects/ pymol/.", 
  "summary": "PyWATER is an easy-to-use PyMOL plug-in and identifies conserved water molecules in the protein structure of interest.\nIf many crystal structures are available for a protein, conserved water molecules can be also determined via cluster analysis and electron density of existing crystal structures.\nThe method WaterScore uses a regression analysis to establish a statistical correlation between the structural properties of water molecules of a free protein crystal structure compared with the ligand complexed form (Garcia-Sosa et al., 2003).\nAs the importance of conserved water molecules is generally accepted, a simple and rapid tool for their identification in protein structures is desirable before structural bioinformatics studies are performed.", 
  "affiliations": [
    " Pharmaceutical Bioinformatics Institute of Pharmaceutical Sciences Albert-Ludwigs-University ", 
    " Pharmaceutical Biology and Biotechnology Institute of Pharmaceutical Sciences Albert-Ludwigs-University "
  ], 
  "grants": [], 
  "sourcelinks": [
    "https://github.com/hiteshpatel379/PyWATER", 
    "http://www.pymol.org", 
    "http://sourceforge.net/projects", 
    "http://sourceforge.net/projects/pymol"
  ], 
  "acks": " ", 
  "authors": [
    " Hitesh Patel", 
    " Bj", 
    " A Gr \u20ac Uning", 
    " Stefan G \u20ac Unther", 
    " Irmgard Merfort", 
    " Janet Kelso"
  ], 
  "keyWords": [
    "conserved water molecules", 
    [
      "clustering", 
      "proteins", 
      "waters", 
      "molecule", 
      "conservation", 
      "pywater", 
      "structurally"
    ]
  ], 
  "github_data": {
    "name": "PyWATER", 
    "contributors": [
      {
        "contributions": 61, 
        "html_url": "https://github.com/hiteshpatel379"
      }, 
      {
        "contributions": 35, 
        "html_url": "https://github.com/bgruening"
      }, 
      {
        "contributions": 4, 
        "html_url": "https://github.com/rasbt"
      }
    ], 
    "versions": [], 
    "created_at": "2014-01-08T10:10:10Z", 
    "updated_at": "2015-05-06T16:26:26Z", 
    "languages": [
      "Python"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/hiteshpatel379"
      }, 
      {
        "html_url": "https://github.com/bgruening"
      }, 
      {
        "html_url": "https://github.com/rrchaudhari"
      }
    ], 
    "owner": "https://github.com/hiteshpatel379", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-07-03T00:27:22Z"
}{
  "doi": "10.1093/bioinformatics/btu781", 
  "name": "QuasR quantification and annotation of short reads in R", 
  "links": [
    "http://www.bioconductor.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "QuasR: quantification and annotation of short reads in R", 
  "toolName": "QuasR: quantification and annotation of short reads in R", 
  "abstract": "QuasR is a package for the integrated analysis of high-throughput sequencing data in R, covering all steps from read preprocessing, alignment and quality control to quantification. QuasR supports different experiment types (including RNA-seq, ChIP-seq and Bis-seq) and analysis variants (e.g. paired-end, stranded, spliced and allele-specific), and is integrated in Bioconductor so that its output can be directly processed for statistical analysis and visualization. Availability and implementation: QuasR is implemented in R and C/C\u00fe\u00fe. Source code and binaries for major platforms (Linux, OS X and MS Windows) are available from Bioconductor (www.biocon-ductor.org/packages/release/bioc/html/QuasR.html). The package includes a 'vignette' with step-by-step examples for typical work flows.", 
  "summary": "Summary: QuasR is a package for the integrated analysis of high-throughput sequencing data in R, covering all steps from read preprocessing, alignment and quality control to quantification.\nIn the last years, there have been many efforts to provide software in R/ Bioconductor (Gentleman et al., 2004) to simplify data processing and biological interpretation, such as an efficient framework for working with genomic ranges (Lawrence et al., 2013), or tools for read alignment (Liao et al., 2013), quality control (Morgan et al., 2009) and statistical analysis (Anders and Huber 2010; Robinson et al., 2010).", 
  "affiliations": [
    " Novartis Institute for Biomedical Research"
  ], 
  "grants": [
    "Funding\nThis work was supported by Sybit, the IT support project for SystemsX.ch (A.L."
  ], 
  "acks": " We acknowledge the Bioconductor core team and community for providing a great software environment. We thank Lukas Burger, Robert Ivanek, Hans- Rudolf Hotz and Christian Hundsrucker for discussions and feedback. This work was supported by Sybit, the IT support project for SystemsX.ch (A.L.), the Novartis Research Foundation (D.G. and M.B.S.) and Novartis (F.H.). ", 
  "authors": [
    " Dimos Gaidatzis", 
    " Anita Lerch", 
    " Florian Hahne", 
    " Michael B Stadler"
  ], 
  "keyWords": [
    "sequence analysis", 
    [
      "sequencing", 
      "quasr", 
      "alignments", 
      "genomes", 
      "researchers"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-11-23T01:09:29Z"
}{
  "doi": "10.1093/bioinformatics/btu610", 
  "name": "RAMONA a Web application for gene set analysis on multilevel omics data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://icb.helm"
  ], 
  "title": "Gene expression RAMONA: a Web application for gene set analysis on multilevel omics data", 
  "toolName": "Gene expression RAMONA: a Web application for gene set analysis on multilevel omics data", 
  "abstract": "Decreasing costs of modern high-throughput experiments allow for the simultaneous analysis of altered gene activity on various molecular levels. However, these multi-omics approaches lead to a large amount of data, which is hard to interpret for a non-bioinforma-tician. Here, we present the remotely accessible multilevel ontology analysis (RAMONA). It offers an easy-to-use interface for the simultaneous gene set analysis of combined omics datasets and is an extension of the previously introduced MONA approach. RAMONA is based on a Bayesian enrichment method for the inference of overrepresented biological processes among given gene sets. Overrepresentation is quantified by interpretable term probabilities. It is able to handle data from various molecular levels, while in parallel coping with redundancies arising from gene set overlaps and related multiple testing problems. The comprehensive output of RAMONA is easy to interpret and thus allows for functional insight into the affected biological processes. With RAMONA, we provide an efficient implementation of the Bayesian inference problem such that ontologies consisting of thousands of terms can be processed in the order of seconds. Availability and implementation: RAMONA is implemented as ASP.NET Web application and publicly available at http://icb.helm holtz-muenchen.de/", 
  "summary": "To provide a powerful method to integrate multilevel gene response data for the determination of altered biological processes, we recently introduced the multilevel ontology analysis (MONA) (Sass et al., 2013).\n(B) If the cooperative model is chosen, a scatterplot can be displayed that shows the P-values for each term determined by the traditional gene set analysis (Fisher's exact test) on the two input gene lists individually.\nThis information includes the set of altered genes for each level as well as the decision whether a term is active (red) or not (black) for RAMONA or Fisher's exact test", 
  "affiliations": [
    " Institute of Computational Biology Ingolst \u20ac adter Landstra\u00dfe 1 Helmholtz Zentrum M \u20ac unchen "
  ], 
  "grants": [
    "Funding: This work was supported by The European Research Council [Latent Causes: 259294]; the Deutsche\n\nForschungsgemeinschaft [InKoMBio: SPP 1395]; and the Federal Ministry of Education and Research [GerontoSys: FKZ 0315576C; LungSys: FKZ 0316042I; Virtual Liver: FKZ 0315752]."
  ], 
  "acks": " The authors thank Benedikt Rauscher and Michael Schollerer for preliminary work on the Web interface. Funding: This work was supported by The European Research Council; the Deutsche Forschungsgemeinschaft; and the Federal Ministry of Education and Research [GerontoSys: FKZ 0315576C; LungSys: FKZ 0316042I; Virtual Liver: FKZ \n0315752]. ", 
  "authors": [
    " Steffen Sass", 
    " Florian Buettner", 
    " Nikola S Mueller", 
    " Fabian J Theis"
  ], 
  "keyWords": [
    "set analysis", 
    [
      "terms", 
      "ramona", 
      "mona", 
      "genes", 
      "sets", 
      "settings", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-19T04:50:41Z"
}{
  "doi": "10.1093/bioinformatics/btu419", 
  "name": "RAPIDR an analysis package for noninvasive prenatal testing of aneuploidy", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://cran.r-project", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Genome analysis RAPIDR: an analysis package for non-invasive prenatal testing of aneuploidy", 
  "toolName": "Genome analysis RAPIDR: an analysis package for non-invasive prenatal testing of aneuploidy", 
  "abstract": "Non-invasive prenatal testing (NIPT) of fetal aneuploidy using cell-free fetal DNA is becoming part of routine clinical practice. RAPIDR (Reliable Accurate Prenatal non-Invasive Diagnosis R package) is an easy-to-use open-source R package that implements several published NIPT analysis methods. The input to RAPIDR is a set of sequence alignment files in the BAM format, and the outputs are calls for aneuploidy, including trisomies 13, 18, 21 and monosomy X as well as fetal sex. RAPIDR has been extensively tested with a large sample set as part of the RAPID project in the UK. The package contains quality control steps to make it robust for use in the clinical setting. Availability and implementation: RAPIDR is implemented in R and can be freely downloaded via CRAN from here: http://cran.r-project. org/web/packages/RAPIDR/index.html.", 
  "summary": "RAPIDR implements two methods from the literature: the normalized chromosome value (NCV) method (Sehnert et al., 2011) and weighting of counts by GC content bin (Fan and Quake, 2010).\n(a) Workflow of NIPT; (b) The z-scores across all the autosomes for a T21 sample, the triangles highlight chromosomes 13, 18 and 21 for which we test for aneuploidy; (c) Visual display of the criteria used to call fetal sex and chromosome aneuploidy\nOnce artifacts have been removed from the read counts, the next step is for RAPIDR to create a baseline using a set of reference samples with known outcomes and fetal sex.", 
  "affiliations": [
    " North East Thames Regional Genetics Service Great Ormond Street Hospital NHS Foundation Trust ", 
    " University College London Genetics Institute University College London "
  ], 
  "grants": [
    "RAPIDR combines several analytical techniques that have been proposed for NIPT analysis and has been tested with a large sample set from the RAPID project (NIHR funded project to evaluate the use of NIPT).", 
    "The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health.", 
    "2966\n\n\fR package for NIPT\n\nFunding: This manuscript presents independent research funded by the National Institute for Health Research (NIHR) under the Program Grants for Applied Research Program (RP-PG-070710107) (the RAPID project) and the NIHR Biomedical Research Centre at Great Ormond Street Hospital."
  ], 
  "acks": " ", 
  "authors": [
    " Kitty K Lo", 
    " Christopher Boustred", 
    " Lyn S Chitty", 
    " Vincent Plagnol", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "rapidr", 
      "aneuploidies", 
      "supplementary", 
      "reads", 
      "samples", 
      "nipt", 
      "fetal"
    ]
  ], 
  "sourcelinks": [
    "http://cran.r-project"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-03T00:27:22Z"
}{
  "doi": "10.1093/bioinformatics/btu846", 
  "name": "rbamtools an R interface to samtools enabling fast accumulative tabulation of splicing events over multiple RNAseq samples", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://en.wikipedia.org/wiki/Bus_error", 
    "http://samtools.github.io/hts-specs/SAMv1.pdf2.2"
  ], 
  "title": "Genome analysis rbamtools: an R interface to samtools enabling fast accumulative tabulation of splicing events over multiple RNA-seq samples", 
  "toolName": "Genome analysis rbamtools: an R interface to samtools enabling fast accumulative tabulation of splicing events over multiple RNA-seq samples", 
  "abstract": "The open source environment R isf the most widely used software to statistically explore biological data sets including sequence alignments. BAM is the de facto standard file format for sequence alignment. With rbamtools, we provide now a full spectrum of accessibility to BAM for R users such as reading, writing, extraction of subsets and plotting of alignment depth where the script syntax closely follows the SAM/BAM format. Additionally, rbamtools enables fast accumulative tabulation of splicing events over multiple BAM files. Availability and implementation: rbamtools is available on CRAN and on R-Forge.", 
  "summary": "With rbamtools, we provide now a full spectrum of accessibility to BAM for R users such as reading, writing, extraction of subsets and plotting of alignment depth where the script syntax closely follows the SAM/BAM format.\nAdditionally rbamtools contains a framework for sequential and fast extraction of alignment gap positions (see Table 1) on RNA-seq data which are candidate sites for true splicing events.\nThe package consists of three layers: the samtools C library, C based containers for alignments and alignment gaps as well as an S4 class library in R providing the user interface.\nGap site positions and numbers of crossing read alignments can be obtained from multiple BAM files as data.frame by executing:", 
  "affiliations": [
    " Center for Bioinformatics and Biostatistics BMFZ Heinrich Heine University D\u00fc sseldorf "
  ], 
  "grants": [
    "Funding\nRNA-seq datasets were obtained from RNA deep sequencing projects which were partly funded by the German Ministry of Research and Education (Network Gerontosys), DFG [SCHA 909/3-1], the Heinz Ansmann Foundation for AIDS Research, Du sseldorf, Germany (He.S), and the Ju rgen Manchot Stiftung (H.S.)."
  ], 
  "acks": " We thank the R Core Team (R Core), and in particular Profs. Brian Ripley and Kurt Hornik, very much for their fruitful comments and for their valuable help in improving rbamtools. RNA-seq datasets were obtained from RNA deep sequencing projects which were partly funded by the German Ministry of Research and Education (Network Gerontosys), DFG, the Heinz Ansmann Foundation for AIDS Research, D\u00fc sseldorf, Germany (He.S), and the J\u00fc rgen Manchot Stiftung (H.S.). The financial support of the Deutsche Forschungsgemeinschaft ", 
  "authors": [
    " Wolfgang Kaisers", 
    " Heiner Schaal", 
    " Holger Schwender"
  ], 
  "keyWords": [
    [
      "files", 
      "splicing", 
      "rbamtools", 
      "positions", 
      "sites", 
      "data", 
      "alignments"
    ]
  ], 
  "sourcelinks": [
    "http://samtools.github.io/hts-specs/SAMv1.pdf2.2"
  ], 
  "technologies": [
    "C", 
    "R"
  ], 
  "dateCreated": "2015-01-07T03:18:32Z"
}{
  "doi": "10.1093/bioinformatics/btu752", 
  "name": "Rclick a web server for comparison of RNA 3D structures", 
  "links": [
    "http://mspc.bii.a-star", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Structural bioinformatics Rclick: a web server for comparison of RNA 3D structures", 
  "toolName": "Structural bioinformatics Rclick: a web server for comparison of RNA 3D structures", 
  "abstract": "RNA molecules play important roles in key biological processes in the cell and are becoming attractive for developing therapeutic applications. Since the function of RNA depends on its structure and dynamics, comparing and classifying the RNA 3D structures is of crucial importance to molecular biology. In this study, we have developed Rclick, a web server that is capable of superimposing RNA 3D structures by using clique matching and 3D least-squares fitting. Our ser-ver Rclick has been benchmarked and compared with other popular servers and methods for RNA structural alignments. In most cases, Rclick alignments were better in terms of structure overlap. Our server also recognizes conformational changes between structures. For this purpose, the ser-ver produces complementary alignments to maximize the extent of detectable similarity. Various examples showcase the utility of our web server for comparison of RNA, RNA\u2013protein complexes and RNA-ligand structures. Availability and implementation: The Rclick web server is freely accessible at", 
  "summary": "Here, the Rclick web server is designed to provide a user-friendly interface to compare RNA 3D structures and produces accurate alignments when the structures are similar.\nThe performance of Rclick was compared to other popular servers and methods that align RNA 3D structures, including ARTS\nWhile Rclick produced accurate alignments of RNAprotein interactions between two Ribonuclease III (PDB codes 2LUP and 1RC7, Fig. 1), when compared to other web servers such as SETTER (Cech, et al., 2012) and ARTS (Dror et al., 2006) which could not align correctly RNA-binding sites.\nOur server Rclick aligns RNA 3D structures by enhancing the CLICK algorithm that matches cliques of points (Nguyen and Madhusudhan, 2011; Nguyen et al.,2011).", 
  "affiliations": [
    " Biomolecular Modeling and Design Division"
  ], 
  "grants": [
    "Funding\nBiomedical Research Council (A*STAR), Singapore."
  ], 
  "acks": " The authors thank Dr. M. S. Madhusudhan for valuable comments and insights . The authors also offer special thanks to Yong Taipang for his help in setting up, maintaining and improving the server. Biomedical Research Council (A*STAR), Singapore. Conflict of interest: none declared. ", 
  "authors": [
    " Minh N Nguyen", 
    " Chandra Verma"
  ], 
  "keyWords": [
    "d structures", 
    [
      "rclick", 
      "alignments", 
      "structurally", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-13T05:02:10Z"
}{
  "doi": "10.1093/bioinformatics/btu530", 
  "name": "Quartet Inference from SNP Data Under the Coalescent Model", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.stat.osu.edu/$lkubatko", 
    "http://arxiv.org/abs/1406.4811.Degnan,J"
  ], 
  "title": "Quartet Inference from SNP Data Under the Coalescent Model", 
  "toolName": "Quartet Inference from SNP Data Under the Coalescent Model", 
  "abstract": "Motivation: Increasing attention has been devoted to estimation of species-level phylogenetic relationships under the coalescent model. However, existing methods either use summary statistics (gene trees) to carry out estimation, ignoring an important source of variability in the estimates, or involve computationally intensive Bayesian Markov chain Monte Carlo algorithms that do not scale well to whole-genome datasets. Results: We develop a method to infer relationships among quartets of taxa under the coalescent model using techniques from algebraic statistics. Uncertainty in the estimated relationships is quantified using the nonparametric bootstrap. The performance of our method is assessed with simulated data. We then describe how our method could be used for species tree inference in larger taxon samples, and demonstrate its utility using datasets for Sistrurus rattlesnakes and for soybeans. Availability and implementation: The method to infer the phylogen-etic relationship among quartets is implemented in the software", 
  "summary": "Before describing the simulation procedure, we first point out that while much of the currently available methodology for inferring species trees assumes that multi-locus data (e.g. aligned DNA sequences from many independent loci) are available for inference, our method is actually designed for unlinked sites, for example, for a sample of unlinked SNPs. This is because in computing the probability distribution of site patterns at the tips of the species tree, we integrate over the probability distribution of gene trees under the coalescent model, with the implicit assumption that sequence data evolve along these gene trees.", 
  "affiliations": [
    " Department of Cancer Biology Wake Forest School of Medicine "
  ], 
  "grants": [
    "Funding: The authors are supported in part by National Science Foundation award DMS-1106706 and in part by NIH Cancer Biology Training Grant (T32-CA079448 to J.C.)."
  ], 
  "acks": " The authors thank David Bryant and Remco Bouckaert for assistance in running the SNAPP analysis. We thank three anonymous reviewers and Associate Editor David Posada for helpful comments on earlier versions of this manuscript. ", 
  "authors": [
    " Julia Chifman", 
    " Laura Kubatko"
  ], 
  "keyWords": [
    [
      "methods", 
      "modeled", 
      "trees", 
      "times", 
      "resulting", 
      "timing", 
      "data", 
      "species"
    ]
  ], 
  "sourcelinks": [
    "http://www.stat.osu.edu/$lkubatko"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-08-08T05:05:20Z"
}{
  "doi": "10.1093/bioinformatics/btu651", 
  "name": "Quantifying tumor heterogeneity in wholegenome and wholeexome sequencing data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://compbio.cs.brown.edu/software"
  ], 
  "title": "Sequence analysis Quantifying tumor heterogeneity in whole-genome and whole-exome sequencing data", 
  "toolName": "Sequence analysis Quantifying tumor heterogeneity in whole-genome and whole-exome sequencing data", 
  "abstract": "Motivation: Most tumor samples are a heterogeneous mixture of cells, including admixture by normal (non-cancerous) cells and sub-populations of cancerous cells with different complements of somatic aberrations. This intra-tumor heterogeneity complicates the analysis of somatic aberrations in DNA sequencing data from tumor samples. Results: We describe an algorithm called THetA2 that infers the composition of a tumor sample\u2014including not only tumor purity but also the number and content of tumor subpopulations\u2014directly from both whole-genome (WGS) and whole-exome (WXS) high-throughput DNA sequencing data. This algorithm builds on our earlier Tumor Heterogeneity Analysis (THetA) algorithm in several important directions. These include improved ability to analyze highly rearranged gen-omes using a variety of data types: both WGS sequencing (including low $7\u00c2 coverage) and WXS sequencing. We apply our improved THetA2 algorithm to WGS (including low-pass) and WXS sequence data from 18 samples from The Cancer Genome Atlas (TCGA). We find that the improved algorithm is substantially faster and identifies numerous tumor samples containing subclonal populations in the TCGA data, including in one highly rearranged sample for which other tumor purity estimation algorithms were unable to estimate tumor purity. Availability and implementation: An implementation of THetA2 is available at", 
  "summary": "Results: We describe an algorithm called THetA2 that infers the composition of a tumor sample--including not only tumor purity but also the number and content of tumor subpopulations--directly from both whole-genome (WGS) and whole-exome (WXS) high-throughput DNA sequencing data.\nHowever, although the purity estimates are similar, THetA2 is additionally able to identify two subpopulations of tumor cells, in 46.4 and 20.1% of cells in sample (Fig. 4b) and determine which copy number aberrations are part of each subpopulation.", 
  "affiliations": [
    " Department of Computer Science"
  ], 
  "grants": [
    "Funding: This work is supported by a National Science Foundation (NSF) graduate research fellowship DGE0228243 (to L.O.", 
    "); National Science Foundation (NSF) career award CCF-1053753 (to B.J.R.", 
    "); and grant RO1HG005690 from the National Institutes of Health to B.J.R."
  ], 
  "acks": " The results published here are in whole or part based on data generated by TCGA research network established by the National Cancer Institute and the National Human Genome Research Institute. ", 
  "authors": [
    " Layla Oesper", 
    " Gryte Satas", 
    " Benjamin J Raphael"
  ], 
  "keyWords": [
    [
      "tumors", 
      "genomes", 
      "sampling", 
      "intervals", 
      "theta", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://compbio.cs.brown.edu/software"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-10-09T01:49:44Z"
}{
  "doi": "10.1093/bioinformatics/btu680", 
  "name": "Rcount simple and flexible RNASeq read counting", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Gene expression Rcount: simple and flexible RNA-Seq read counting", 
  "toolName": "Gene expression Rcount: simple and flexible RNA-Seq read counting", 
  "abstract": "Analysis of differential gene expression by RNA sequen-cing (RNA-Seq) is frequently done using feature counts, i.e. the number of reads mapping to a gene. However, commonly used count algorithms (e.g. HTSeq) do not address the problem of reads aligning with multiple locations in the genome (multireads) or reads aligning with positions where two or more genes overlap (ambiguous reads). Rcount specifically addresses these issues. Furthermore, Rcount allows the user to assign priorities to certain feature types (e.g. higher priority for protein-coding genes compared to rRNA-coding genes) or to add flanking regions. Availability and implementation: Rcount provides a fast and easy-to-use graphical user interface requiring no command line or programming skills. It is implemented in C++ using the SeqAn (www.seqan. de) and the Qt libraries (qt-project.org). Source code and 64 bit binaries for (Ubuntu) Linux, Windows (7) and MacOSX are released under the GPLv3 license and are freely available on github.com/ MWSchmid/Rcount. Contact: marcschmid@gmx.ch Supplementary information: Test data, genome annotation files, useful Python and R scripts and a step-by-step user guide (including run-time and memory usage tests) are available on github.com/ MWSchmid/Rcount.", 
  "summary": "ABSTRACT Summary: Analysis of differential gene expression by RNA sequencing (RNA-Seq) is frequently done using feature counts, i.e. the number of reads mapping to a gene.\nHowever, commonly used count algorithms (e.g. HTSeq) do not address the problem of reads aligning with multiple locations in the genome (multireads) or reads aligning with positions where two or more genes overlap (ambiguous reads).\nCurrent work flows for DE analysis generally involve the (i) alignment of the short reads to a reference genome, (ii) quantification of expression levels and (iii) comparison between different treatments, tissue/cell types and time-points (Anders et al., 2013).", 
  "affiliations": [
    " Institute of Plant Biology and Z\u20ac urich-Basel Plant Science Center University of Zurich "
  ], 
  "grants": [
    "Funding: This work was supported by the University of Zurich, and grants from the Swiss National Science Foundation to U.G."
  ], 
  "acks": " ", 
  "authors": [
    " Marc W Schmid", 
    " Ueli Grossniklaus"
  ], 
  "keyWords": [
    [
      "genes", 
      "genomes", 
      "rcount", 
      "reads", 
      "bioinformatics", 
      "alignments"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-17T05:05:29Z"
}{
  "doi": "10.1093/bioinformatics/btu624", 
  "name": "Rcpi RBioconductor package to generate various descriptors of proteins compounds and their interactions", 
  "links": [
    "http://bioconductor.org/packages/release/bioc", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioconductor.org/biocLite.R"
  ], 
  "title": "Systems biology Rcpi: R/Bioconductor package to generate various descriptors of proteins, compounds and their interactions", 
  "toolName": "Systems biology Rcpi: R/Bioconductor package to generate various descriptors of proteins, compounds and their interactions", 
  "abstract": "In chemoinformatics and bioinformatics fields, one of the main computational challenges in various predictive modeling is to find a suitable way to effectively represent the molecules under investigation , such as small molecules, proteins and even complex interactions. To solve this problem, we developed a freely available R/Bioconductor package, called Compound\u2013Protein Interaction with R (Rcpi), for complex molecular representation from drugs, proteins and more complex interactions, including protein\u2013protein and compound\u2013 protein interactions. Rcpi could calculate a large number of structural and physicochemical features of proteins and peptides from amino acid sequences, molecular descriptors of small molecules from their topology and protein\u2013protein interaction and compound\u2013protein interaction descriptors. In addition to main functionalities, Rcpi could also provide a number of useful auxiliary utilities to facilitate the user's need. With the descriptors calculated by this package, the users could conveniently apply various statistical machine learning methods in R to solve various biological and drug research questions in computational biology and drug discovery. Availability and implementation: Rcpi is freely available from the Bioconductor site", 
  "summary": "To solve this problem, we developed a freely available R/Bioconductor package, called CompoundProtein Interaction with R (Rcpi), for complex molecular representation from drugs, proteins and more complex interactions, including proteinprotein and compound protein interactions.\nRcpi could calculate a large number of structural and physicochemical features of proteins and peptides from amino acid sequences, molecular descriptors of small molecules from their topology and proteinprotein interaction and compoundprotein interaction descriptors.\nIn the field of chemoinformatics, molecular descriptors for small molecules have frequently been used in quantitative structure-activity/property relationship (QSAR/QSPR), virtual screening, database search, ranking, drug ADME/T prediction and other drug discovery processes (Cao et al., 2011; Cherkasov et al., 2014; Gola et al., 2006, Willett, 2014).", 
  "affiliations": [
    " School of Mathematics and Statistics Central South University ", 
    " School of Pharmaceutical Sciences"
  ], 
  "grants": [
    "Funding: This study was supported by the National key basic research program (2015CB910700) and the National Natural Science Foundation of China (Grant No."
  ], 
  "acks": " ", 
  "authors": [
    " Dong-Sheng Cao", 
    " Nan Xiao", 
    " Qing-Song Xu", 
    " Alex F Chen"
  ], 
  "keyWords": [
    [
      "rcpi", 
      "interactions", 
      "descriptors", 
      "fingerprints", 
      "bioinformatics", 
      "molecular"
    ]
  ], 
  "sourcelinks": [
    "http://bioconductor.org/biocLite.R"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-09-23T02:41:35Z"
}{
  "doi": "10.1093/bioinformatics/btu596", 
  "name": "RCSB PDB Mobile iOS and Android mobile apps to provide data access and visualization to the RCSB Protein Data Bank", 
  "links": [
    "http://www.rcsb.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://webglmol.sourceforge.jp", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Structural bioinformatics RCSB PDB Mobile: iOS and Android mobile apps to provide data access and visualization to the RCSB Protein Data Bank", 
  "toolName": "webglmol", 
  "abstract": "The Research Collaboratory for Structural Bioinformatics Protein Data Bank (RCSB PDB) resource provides tools for query, analysis and visualization of the 3D structures in the PDB archive. As the mobile Web is starting to surpass desktop and laptop usage, scientists and educators are beginning to integrate mobile devices into their research and teaching. In response, we have developed the RCSB PDB Mobile app for the iOS and Android mobile platforms to enable fast and convenient access to RCSB PDB data and services. Using the app, users from the general public to expert researchers can quickly search and visualize biomolecules, and add personal annotations via the RCSB PDB's integrated MyPDB service. Availability and implementation: RCSB PDB Mobile is freely available from the Apple App Store and Google Play (http://www.rcsb.org).", 
  "summary": "Access to data- and content-rich life science resources such as the Protein Data Bank (PDB) (Berman et al., 2003) through the Research Collaboratory for Structural Bioinformatics (RCSB) PDB Web site (Berman et al., 2000, Rose et al., 2011, 2013) can, however, prove problematic on tablets, and more so on hand-held devices, such as smart phones.\nOur goal was to produce an intuitive app with the following core capabilities: (i) simple search interface, (ii) quick browsing of search results, (iii) view of basic data about a structure entry and its PubMed abstract, (iv) high-performance molecular visualization and (v) access to the archive of Molecule of the Month educational articles by David Goodsell (Dutta et al., 2010).", 
  "affiliations": [
    " San Diego Supercomputer Center RCSB Protein Data Bank University of California San Diego "
  ], 
  "grants": [
    "Funding: The RCSB PDB is a member of the Worldwide Protein Data Bank and is funded by grant [NSF DBI-1338415] provided by the National Science Foundation, the National Institutes of Health, and the Department of Energy."
  ], 
  "acks": " ", 
  "authors": [
    " Gregory B Quinn", 
    " Chunxiao Bi", 
    " Cole H Christie", 
    " Kyle Pang", 
    " Andreas Prlic\u00b41prlic\u00b4prlic\u00b41", 
    " Takanori Nakane", 
    " Christine Zardecki", 
    " Maria Voigt", 
    " Helen M Berman", 
    " Philip E Bourne", 
    " Peter W Rose", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "provide data", 
    [
      "mobile", 
      "provided", 
      "android", 
      "access", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://webglmol.sourceforge.jp"
  ], 
  "technologies": [
    "Java"
  ], 
  "dateCreated": "2014-09-03T06:17:49Z"
}{
  "doi": "10.1093/bioinformatics/btu533", 
  "name": "READemptiona tool for the computational analysis of deepsequencingbased transcriptome data", 
  "links": [
    "http://cran.r-project.org", 
    "http://pypi.python.org/pypi/pysam", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://pythonhosted.org", 
    "http://python"
  ], 
  "title": "Gene expression READemption\u2014a tool for the computational analysis of deep-sequencing\u2013based transcriptome data", 
  "toolName": "Gene expression READemption\u2014a tool for the computational analysis of deep-sequencing\u2013based transcriptome data", 
  "abstract": "RNA-Seq has become a potent and widely used method to qualitatively and quantitatively study transcriptomes. To draw biological conclusions based on RNA-Seq data, several steps, some of which are computationally intensive, have to be taken. Our READemption pipeline takes care of these individual tasks and integrates them into an easy-to-use tool with a command line interface. To leverage the full power of modern computers, most subcommands of READemption offer parallel data processing. While READemption was mainly developed for the analysis of bacterial primary transcriptomes, we have successfully applied it to analyze RNA-Seq reads from other sample types, including whole transcriptomes and RNA immunopre-cipitated with proteins, not only from bacteria but also from eukaryotes and archaea. Availability and implementation: READemption is implemented in Python and is published under the ISC open source license. The tool and documentation is hosted at http://pythonhosted.org/ READemption (", 
  "summary": "We have created an automated RNA-Seq processing pipeline named READemption with the initial purpose to handle differential RNA-Seq (dRNA-Seq) data for the determination of transcriptional start sites in bacteria (Sharma et al., 2010, Sharma and Vogel, 2014).\nAdditionally, while most available RNA-Seq pipelines put priority on fast mapping, we have chosen segemehl as short read aligner (Hoffmann et al., 2009).\nDifferential gene expression analysis: For pairwise expression comparison, the subcommand deseq offers statistical analysis based on the approach implemented in DESeq2 (Anders and Huber, 2010, Love et al., 2014), which builds on the raw read counting and is a widely adapted and intensively tested library (Guo et al., 2013, Rapaport et al., 2013, Reeb and Steibel, 2013).", 
  "affiliations": [
    " Institute for Molecular Infection Biology", 
    " Research Centre for Infectious Diseases (ZINF) University of W \u20ac urzburg "
  ], 
  "grants": [
    "The JV laboratory received financial support from a BMBF eBio grant RNAsys and DFG project VO 875/4-2.", 
    "Funding: Work in the Sharma and Vogel laboratories is supported by the Bavarian Research Network for Molecular Biosystems (BioSysNet)."
  ], 
  "acks": " The authors thank members of the Sharma and the Vogel groups, especially Thorsten Bischler and Lei Li for testing and constructive feedback. ", 
  "authors": [
    " Konrad U F \u20ac Orstner", 
    " J \u20ac Org Vogel", 
    " Cynthia M Sharma"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "maps", 
      "reademption", 
      "mappings", 
      "transcriptomics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-15T05:12:51Z"
}{
  "doi": "10.1093/bioinformatics/btu820", 
  "name": "repDNA a Python package to generate various modes of feature vectors for DNA sequences by incorporating userdefined physicochemical properties and sequenceorder effects", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioinformatics.hitsz.edu.cn/repDNA"
  ], 
  "title": "repDNA: a Python package to generate various modes of feature vectors for DNA sequences by incorporating user-defined physicochemical properties and sequence-order effects", 
  "toolName": "repDNA: a Python package to generate various modes of feature vectors for DNA sequences by incorporating user-defined physicochemical properties and sequence-order effects", 
  "abstract": "In order to develop powerful computational predictors for identifying the biological features or attributes of DNAs, one of the most challenging problems is to find a suitable approach to effectively represent the DNA sequences. To facilitate the studies of DNAs and nucleotides, we developed a Python package called representations of DNAs (repDNA) for generating the widely used features reflecting the physicochemical properties and sequence-order effects of DNAs and nucleotides. There are three feature groups composed of 15 features. The first group calculates three nucleic acid composition features describing the local sequence information by means of kmers; the second group calculates six autocorrelation features describing the level of correlation between two oligonucleotides along a DNA sequence in terms of their specific physicochemical properties; the third group calculates six pseudo nucleotide composition features, which can be used to represent a DNA sequence with a discrete model or vector yet still keep considerable sequence-order information via the physicochemical properties of its constituent oligonucleotides. In addition, these features can be easily calculated based on both the built-in and user-defined properties via using repDNA. Availability and implementation: The repDNA Python package is freely accessible to the public at", 
  "summary": "The first group calculates three nucleic acid composition features describing the local sequence information by means of kmers; the second group calculates six autocorrelation features describing the level of correlation between two oligonucleotides along a DNA sequence in terms of their specific physicochemical properties; the third group calculates six pseudo nucleotide composition features, which can be used to represent a DNA sequence with a discrete model or vector yet still keep considerable sequence-order information via the physicochemical properties of its constituent oligonucleotides.", 
  "affiliations": [
    " School of Computer Science and Technology"
  ], 
  "grants": [
    "Funding\nThis work was supported by the National Natural Science Foundation of China [grant no 61300112, 61370010 and 61272383] and the Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry."
  ], 
  "acks": " ", 
  "authors": [
    " Bin Liu", 
    " Fule Liu", 
    " Longyun Fang", 
    " Xiaolong Wang", 
    " Kuo-Chen Chou"
  ], 
  "keyWords": [
    [
      "compositions", 
      "bioinformatics", 
      "features", 
      "repdna", 
      "sequences"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-12-12T02:23:08Z"
}{
  "doi": "10.1093/bioinformatics/btu434", 
  "name": "repfdr a tool for replicability analysis for genomewide association studies", 
  "links": [
    "http://www.math.tau.ac.il/$ruheller/App.html].3", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis repfdr: a tool for replicability analysis for genome-wide association studies", 
  "toolName": "Genome analysis repfdr: a tool for replicability analysis for genome-wide association studies", 
  "abstract": "Motivation: Identification of single nucleotide polymorphisms that are associated with a phenotype in more than one study is of great scientific interest in the genome-wide association studies (GWAS) research. The empirical Bayes approach for discovering whether results have been replicated across studies was shown to be a reliable method, and close to optimal in terms of power. Results: The R package repfdr provides a flexible implementation of the empirical Bayes approach for replicability analysis and meta-analysis, to be used when several studies examine the same set of null hypotheses. The usefulness of the package for the GWAS community is discussed. Availability and implementation: The R package repfdr can be downloaded from CRAN. Contact:", 
  "summary": "The three main analysis steps are--(i) in each study, bin the z-scores and estimate the non-null probabilities for each bin; (ii) estimate the probabilities of association status by an expectation-maximization (EM) algorithm; (iii) estimate the Bayes FDR and select which hypotheses have replicated findings or meta-analysis findings, depending on whether the aim is replicability analysis or meta-analysis.\nSpecifically, the replicability analysis using the P-values of several published GWAS that examine the same complex disease can shed light on the genetic architecture of the disease by identifying the SNPs that have replicated associations across studies as well as the SNPs associated with the disease that show no evidence (or inconsistent evidence) of replicability of associations.", 
  "affiliations": [
    " Department of Statistics and Operations Research Tel-Aviv University "
  ], 
  "grants": [
    "Funding: The work of Ruth Heller and Shay Yaacoby was supported by grant no."
  ], 
  "acks": " ", 
  "authors": [
    " Ruth Heller", 
    " Shay Yaacoby", 
    " Daniel Yekutieli", 
    " John Hancock"
  ], 
  "keyWords": [
    "association studies", 
    [
      "associations", 
      "snps", 
      "study", 
      "analysis", 
      "estimating", 
      "replicability", 
      "bioinformatics", 
      "null"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-11T01:11:54Z"
}{
  "doi": "10.1093/bioinformatics/btv002", 
  "name": "RNARocket an RNASeq analysis resource for infectious disease research", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bioinformatics.babraham.ac.uk", 
    "https://github.com/najoshi/sickle", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "RNA-Rocket: an RNA-Seq analysis resource for infectious disease research Downloaded from", 
  "toolName": "sickle", 
  "abstract": "Motivation: RNA-Seq is a method for profiling transcription using high-throughput sequencing and is an important component of many research projects that wish to study transcript isoforms, condition specific expression and transcriptional structure. The methods, tools and technologies used to perform RNA-Seq analysis continue to change, creating a bioinformatics challenge for researchers who wish to exploit these data. Resources that bring together genomic data, analysis tools, educational material and computational infrastructure can minimize the overhead required of life science researchers. Results: RNA-Rocket is a free service that provides access to RNA-Seq and ChIP-Seq analysis tools for studying infectious diseases. The site makes available thousands of pre-indexed genomes, their annotations and the ability to stream results to the bioinformatics resources VectorBase, EuPathDB and PATRIC. The site also provides a combination of experimental data and metadata, examples of pre-computed analysis, step-by-step guides and a user interface designed to enable both novice and experienced users of RNA-Seq data. Availability and implementation: RNA-Rocket is available at rnaseq.pathogenportal.org. Source code for this project can be found at github.com/cidvbi/PathogenPortal.", 
  "summary": "The primary workflow for RNA-Seq analysis aligns short read data to a reference genome using Bowtie2 (Langmead and Salzberg, 2012) or TopHat2 (Kim et al., 2013), assembles transcripts using Cufflinks, and generates coverage bedGraph and BigWig files using BEDTools (Quinlan and Hall, 2010) and UCSC tools (Kuhn et al., 2013), respectively.\nTo help users account for this, RNA-Rocket provides access to the FastQC tool (http://www.bioinformatics.babraham.ac.uk/ projects/fastqc) for determining the quality profile of sample reads, as well as Sickle (https://github.com/najoshi/sickle) and Trimmomatic (Bolger et al., 2014), which can be used to automatically trim off low quality base calls from the ends of sequencing reads.", 
  "affiliations": [
    " Department of Microbiology and Molecular Genetics University of California ", 
    " European Bioinformatics Institute (EMBL-EBI) Wellcome Trust Genome Campus ", 
    " Center for Tropical & Emerging Global Diseases University of Georgia ", 
    " Virginia Bioinformatics Institute Virginia Tech ", 
    " Department of Biological Sciences University of Notre Dame Notre Dame "
  ], 
  "grants": [
    "Funding\nThis project has been funded with Federal funds from the National Institute of Allergy and Infectious Diseases, National Institutes of Health, Department of Health and Human Services [HHSN272200900040C awarded to B.W.S."
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.bioinformatics.babraham.ac.uk", 
    "https://github.com/najoshi/sickle"
  ], 
  "acks": " The Pathogen Portal team acknowledge the efforts of Alison Yao and Yan Zhang for helping to make this project a reality. This ", 
  "authors": [
    " Andrew S Warren", 
    " Cristina Aurrecoechea", 
    " Brian Brunk", 
    " Prerak Desai", 
    " Scott Emrich", 
    " Gloria I Giraldo-Calder\u00f3", 
    " Omar Harb", 
    " Deborah Hix", 
    " Daniel Lawson", 
    " Dustin Machi", 
    " Chunhong Mao", 
    " Michael Mcclelland", 
    " Eric Nordberg", 
    " Maulik Shukla", 
    " Leslie B Vosshall", 
    " Alice R Wattam", 
    " Rebecca Will", 
    " Hyun Seung Yoo", 
    " Bruno Sobral"
  ], 
  "keyWords": [
    [
      "genomics", 
      "users", 
      "tools", 
      "analysis", 
      "bioinformatics", 
      "data"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/najoshi/sickle/master/LICENSE"
      }
    ], 
    "name": "sickle", 
    "contributors": [
      {
        "contributions": 82, 
        "html_url": "https://github.com/najoshi"
      }, 
      {
        "contributions": 4, 
        "html_url": "https://github.com/vsbuffalo"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/najoshi/sickle/zipball/v1.33", 
        "tarball_url": "https://api.github.com/repos/najoshi/sickle/tarball/v1.33", 
        "name": "v1.33"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/najoshi/sickle/zipball/v1.31", 
        "tarball_url": "https://api.github.com/repos/najoshi/sickle/tarball/v1.31", 
        "name": "v1.31"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/najoshi/sickle/zipball/v.1.29", 
        "tarball_url": "https://api.github.com/repos/najoshi/sickle/tarball/v.1.29", 
        "name": "v.1.29"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/najoshi/sickle/zipball/v1.2", 
        "tarball_url": "https://api.github.com/repos/najoshi/sickle/tarball/v1.2", 
        "name": "v1.2"
      }
    ], 
    "created_at": "2011-02-09T01:18:45Z", 
    "updated_at": "2016-08-04T21:09:09Z", 
    "languages": [
      "C", 
      "Makefile"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/najoshi"
      }, 
      {
        "html_url": "https://github.com/biowei"
      }, 
      {
        "html_url": "https://github.com/tanglingfung"
      }, 
      {
        "html_url": "https://github.com/jmw86069"
      }, 
      {
        "html_url": "https://github.com/biomystery"
      }, 
      {
        "html_url": "https://github.com/tsibley"
      }, 
      {
        "html_url": "https://github.com/bioinfonm"
      }, 
      {
        "html_url": "https://github.com/mteague"
      }, 
      {
        "html_url": "https://github.com/gawbul"
      }, 
      {
        "html_url": "https://github.com/kchambers58178"
      }, 
      {
        "html_url": "https://github.com/cornfly"
      }, 
      {
        "html_url": "https://github.com/ShujiaHuang"
      }, 
      {
        "html_url": "https://github.com/lingdudefeiteng"
      }
    ], 
    "owner": "https://github.com/najoshi", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2015-01-09T02:26:44Z"
}{
  "doi": "10.1093/bioinformatics/btv017", 
  "name": "Repulsive parallel MCMC algorithm for discovering diverse motifs from large sequence sets", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://sites.google.com/site/anshulkundaje/projects/blacklists.In", 
    "http://daweb.ism.ac.jp/yoshidalab", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Repulsive parallel MCMC algorithm for discovering diverse motifs from large sequence sets", 
  "toolName": "Repulsive parallel MCMC algorithm for discovering diverse motifs from large sequence sets", 
  "abstract": "Motivation: The motif discovery problem consists of finding recurring patterns of short strings in a set of nucleotide sequences. This classical problem is receiving renewed attention as most early motif discovery methods lack the ability to handle large data of recent genome-wide ChIP studies. New ChIP-tailored methods focus on reducing computation time and pay little regard to the accuracy of motif detection. Unlike such methods, our method focuses on increasing the detection accuracy while maintaining the computation efficiency at an acceptable level. The major advantage of our method is that it can mine diverse multiple motifs undetectable by current methods. Results: The repulsive parallel Markov chain Monte Carlo (RPMCMC) algorithm that we propose is a parallel version of the widely used Gibbs motif sampler. RPMCMC is run on parallel interacting motif samplers. A repulsive force is generated when different motifs produced by different sam-plers near each other. Thus, different samplers explore different motifs. In this way, we can detect much more diverse motifs than conventional methods can. Through application to 228 transcription factor ChIP-seq datasets of the ENCODE project, we show that the RPMCMC algorithm can find many reliable cofactor interacting motifs that existing methods are unable to discover. Availability and implementation: A C\u00fe\u00fe implementation of RPMCMC and discovered cofactor motifs for the 228 ENCODE ChIP-seq datasets are available from", 
  "summary": "Through application to 228 transcription factor ChIP-seq datasets of the ENCODE project, we show that the RPMCMC algorithm can find many reliable cofactor interacting motifs that existing methods are unable to discover.\nWe report the performance of several motif discovery algorithms on two types of data: (i) promoter sequences into which strings generated from PPMs in the JASPAR CORE database are planted and (ii) 228 TF ChIP-seq datasets of the ENCODE project.\n(a) The number of motifs in JASPAR CORE that were matched to outputs of each algorithm for each of the 228 datasets (blue: RPMCMC; magenta: Hegma; green: DREME).", 
  "affiliations": [
    " Department of Statistical Science The Graduate University for Advanced Studies (Sokendai) "
  ], 
  "grants": [
    "Funding\nThis work was supported by a Grant-in-Aid for Scientific Research on Innovative Areas \"Systems Cancer\" (No.", 
    "(1995) TRANSFAC: a database on transcription factors and their DNA binding sites.", 
    "JASPAR (Sandelin et al., 2004), TRANSFAC\n\nVC The Author 2015."
  ], 
  "acks": " We thank the ENCODE project consortium for making its data publicly available, specifically to the groups who provided the SYDH TFBS ChIP-seq datasets. This work was supported by a Grant-in-Aid for Scientific Research on Innovative Areas \" Systems Cancer \" (No. 4201) of The Ministry of Education, Culture, Sports, Science, and Technology, Japan. Conflict of Interest: none declared. ", 
  "authors": [
    " Hisaki Ikebata", 
    " Ryo Yoshida"
  ], 
  "keyWords": [
    [
      "rpmcmc", 
      "sequences", 
      "methods", 
      "motifs", 
      "sampling"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    " REST "
  ], 
  "dateCreated": "2015-01-13T01:15:54Z"
}{
  "doi": "10.1093/bioinformatics/btu437", 
  "name": "Resolving complex tandem repeats with long reads", 
  "links": [
    "http://blog.pacificbiosciences.com/2013/10/data-releaselong-read-shotgun.html", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/alibashir"
  ], 
  "title": "Resolving complex tandem repeats with long reads", 
  "toolName": "alibashir", 
  "abstract": "Motivation: Resolving tandemly repeated genomic sequences is a necessary step in improving our understanding of the human genome. Short tandem repeats (TRs), or microsatellites, are often used as molecular markers in genetics, and clinically, variation in microsatellites can lead to genetic disorders like Huntington's diseases. Accurately resolving repeats, and in particular TRs, remains a challenging task in genome alignment, assembly and variation calling. Though tools have been developed for detecting microsatellites in short-read sequencing data, these are limited in the size and types of events they can resolve. Single-molecule sequencing technologies may potentially resolve a broader spectrum of TRs given their increased length, but require new approaches given their significantly higher raw error profiles. However, due to inherent error profiles of the single-molecule technologies, these reads presents a unique challenge in terms of accurately identifying and estimating the TRs. Results: Here we present PACMONSTR, a reference-based probabilistic approach, to identify the TR region and estimate the number of these TR elements in long DNA reads. We present a multistep approach that requires as input, a reference region and the reference TR element. Initially, the TR region is identified from the long DNA reads via a 3-stage modified Smith\u2013Waterman approach and then, expected number of TR elements is calculated using a pair-Hidden Markov Models\u2013based method. Finally, TR-based genotype selection (or clustering: homozygous/heterozygous) is performed with Gaussian mixture models, using the Akaike information criteria, and coverage expectations. Availability and implementation: https://github.com/alibashir/", 
  "summary": "Doi et al.'s study uses depth of coverage information on tagged TRs to estimate TR multiplicity in repeats longer than the size of a single short read.\nSupplementary Figure S2 details the model structure of the pairHMM, which takes as input the predicted TR interval in Qka  e; b+e (labeled as q) and the consensus TR element sequence, r, and models the error modes with three hidden states: M = match, X = deletion and Y = insertion.", 
  "affiliations": [
    " Department of Genetics and Genomic Science and Institute for Genomics and Multiscale Biology Icahn School of Medicine at Mount Sinai "
  ], 
  "grants": [
    "Funding: This research was funded by the Icahn School of Medicine at Mount Sinai through seed funding to A.B."
  ], 
  "acks": " ", 
  "authors": [
    " Ajay Ummat", 
    " Ali Bashir"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "reads", 
      "multiplicities", 
      "repeating"
    ]
  ], 
  "sourcelinks": [
    "https://github.com/alibashir"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-16T00:23:13Z"
}{
  "doi": "10.1093/bioinformatics/btu841", 
  "name": "RRDistMaps a UCSF Chimera tool for viewing and comparing protein distance maps", 
  "links": [
    "http://rbvi.ucsf.edu/chimera/download.html", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Structural bioinformatics RRDistMaps: a UCSF Chimera tool for viewing and comparing protein distance maps", 
  "toolName": "Structural bioinformatics RRDistMaps: a UCSF Chimera tool for viewing and comparing protein distance maps", 
  "abstract": "Motivation: Contact maps are a convenient method for the structural biologists to identify structural features through two-dimensional simplification. Binary (yes/no) contact maps with a single cutoff distance can be generalized to show continuous distance ranges. We have developed a UCSF Chimera tool, RRDistMaps, to compute such generalized maps in order to analyze pairwise variations in intramolecular contacts. An interactive utility, RRDistMaps, visualizes conformational changes, both local (e.g. binding-site residues) and global (e.g. hinge motion), between unbound and bound proteins through distance patterns. Users can target residue pairs in RRDistMaps for further navigation in Chimera. The interface contains the unique features of identifying long-range residue motion and aligning sequences to simultaneously compare distance maps.", 
  "summary": "RR contact maps are powerful two-dimensional (2D) representations of protein 3D structure that plot patterns of spatial interactions, e.g. pairs of amino acids with a-carbons <8 A apart (Wu et al., 2008).\nAs an extension to the molecular visualization application UCSF Chimera (Pettersen et al., 2004), we developed RRDistMaps, a tool to interactively compute and display distance maps for individual proteins and to compare the distance maps of pairs of similar proteins.\nRRDistMaps is invoked from the Tools/Structure Comparison menu in UCSF Chimera and displays a list of the molecular chains displayed in the 3D visualization window and an area for showing a distance map.", 
  "affiliations": [
    " Lowell High School", 
    " Resource for Biocomputing, Visualization, and Informatics University of California "
  ], 
  "grants": [
    "Funding\nThis work was supported by National Institutes of Health [grant NIGMS P41-GM103311] and the UCSF-Lowell Summer Internship Program."
  ], 
  "acks": " We thank Elaine Meng for careful proofreading of the manuscript. This work was supported by National Institutes of Health ", 
  "authors": [
    " Jonathan E Chen", 
    " Conrad C Huang", 
    " Thomas E Ferrin"
  ], 
  "keyWords": [
    "protein distance maps", 
    [
      "distances", 
      "visualization", 
      "rrdistmaps", 
      "proteins", 
      "contacts", 
      "residues", 
      "structures"
    ]
  ], 
  "sourcelinks": [
    "http://rbvi.ucsf.edu/chimera/download.html", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-25T04:16:00Z"
}{
  "doi": "10.1093/bioinformatics/btu514", 
  "name": "Retro conceptbased clustering of biomedical topical sets", 
  "links": [
    "http://www.ncbi.nlm.nih.gov/IRET", 
    "http://www.ncbi.nlm.nih.gov/CBBresearch/Wilbur", 
    "http://www.ncbi.nlm.nih", 
    "http://www.nlm.nih.gov/research/umls", 
    "http://www.omim.org", 
    "http://mallet.cs.umass.edu", 
    "http://search", 
    "http://people.csail.mit.edu/jrennie/20Newsgroups", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.nlm.nih.gov/mesh"
  ], 
  "title": "Data and text mining Retro: concept-based clustering of biomedical topical sets", 
  "toolName": "Data and text mining Retro: concept-based clustering of biomedical topical sets", 
  "abstract": "Motivation: Clustering methods can be useful for automatically grouping documents into meaningful clusters, improving human comprehension of a document collection. Although there are clustering algorithms that can achieve the goal for relatively large document collections, they do not always work well for small and homogenous datasets. Methods: In this article, we present Retro\u2014a novel clustering algorithm that extracts meaningful clusters along with concise and descriptive titles from small and homogenous document collections. Unlike common clustering approaches, our algorithm predicts cluster titles before clustering. It relies on the hypergeometric distribution model to discover key phrases, and generates candidate clusters by assigning documents to these phrases. Further, the statistical significance of candidate clusters is tested using supervised learning methods, and a multiple testing correction technique is used to control the overall quality of clustering. Results: We test our system on five disease datasets from OMIM V R and evaluate the results based on MeSH V R term assignments. We further compare our method with several baseline and state-of-the-art methods, including K-means, expectation maximization, latent Dirichlet allocation-based clustering, Lingo, OPTIMSRC and adapted GK-means. The experimental results on the 20-Newsgroup and ODP-239 collections demonstrate that our method is successful at extracting significant clusters and is superior to existing methods in terms of quality of clusters. Finally, we apply our system to a collection of 6248 topical sets from the HomoloGene V R database, a resource in PubMed V R. Empirical evaluation confirms the method is useful for small homogenous datasets in producing meaningful clusters with descriptive titles. Availability and implementation: A web-based demonstration of the algorithm applied to a collection of sets from the HomoloGene database is available at http://www.ncbi.nlm.nih.gov/CBBresearch/Wilbur/ IRET/CLUSTERING_HOMOLOGENE/index.html.", 
  "summary": "Methods: In this article, we present Retro--a novel clustering algorithm that extracts meaningful clusters along with concise and descriptive titles from small and homogenous document collections.\nUnlike traditional clustering algorithms, which first group the documents and then identify cluster titles, we first identify central topics (defined by key phrases) and then assign documents to these topics.\nWe use machine learning to evaluate a key phrase as a candidate cluster topic based on a group of documents that relate to that phrase.\nMeSH term assignments to evaluate how well a set of documents is grouped by topic into clusters.", 
  "affiliations": [
    " National Center for Biotechnology Information Associate Editor: Igor Jurisica National Library of Medicine NIH "
  ], 
  "grants": [
    "22 2014, pages 32403248 doi:10.1093/bioinformatics/btu514\n\nData and text mining\n\nAdvance Access publication July 29, 2014\n\nRetro: concept-based clustering of biomedical topical sets\nLana Yeganova*, Won Kim, Sun Kim and W. John Wilbur\nNational Center for Biotechnology Information, National Library of Medicine, NIH, 8600 Rockville Pike, Bethesda, MD 20894, USA\nAssociate Editor: Igor Jurisica\n\nABSTRACT Motivation: Clustering methods can be useful for automatically grouping documents into meaningful clusters, improving human comprehension of a document collection.", 
    "Funding: This research was supported by the Intramural Research Program of the NIH, National Library of Medicine."
  ], 
  "acks": " The authors thank Natalie Xie for her help in preparing the demonstration Web site. They also thank Dawid Weiss and Stanislaw Osinski for providing clustering results of Lingo on the ODP-239 collection, and Jose Moreno for providing recall and precision values of their clustering of the ODP-239 collection . The authors further thank the anonymous reviewers for valuable suggestions. ", 
  "authors": [
    " Lana Yeganova", 
    " Won Kim", 
    " Sun Kim", 
    " W John Wilbur"
  ], 
  "keyWords": [
    "topical sets", 
    [
      "clustering", 
      "documents", 
      "based", 
      "methods", 
      "settings", 
      "phrases", 
      "titles", 
      "topically"
    ]
  ], 
  "sourcelinks": [
    "http://www.ncbi.nlm.nih"
  ], 
  "technologies": [
    "Lingo"
  ], 
  "dateCreated": "2014-07-30T00:14:11Z"
}{
  "doi": "10.1093/bioinformatics/btu748", 
  "name": "SAMNetWeb identifying conditionspecific networks linking signaling and transcription", 
  "links": [
    "http://fraenkel.mit.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://fraenkel"
  ], 
  "title": "SAMNetWeb: identifying condition-specific networks linking signaling and transcription", 
  "toolName": "SAMNetWeb: identifying condition-specific networks linking signaling and transcription", 
  "abstract": "Motivation: High-throughput datasets such as genetic screens, mRNA expression assays and global phospho-proteomic experiments are often difficult to interpret due to inherent noise in each experimental system. Computational tools have improved interpretation of these datasets by enabling the identification of biological processes and pathways that are most likely to explain the measured results. These tools are primarily designed to analyse data from a single experiment (e.g. drug treatment versus control), creating a need for computational algorithms that can handle heterogeneous datasets across multiple experimental conditions at once. Summary: We introduce SAMNetWeb, a web-based tool that enables functional enrichment analysis and visualization of high-throughput datasets. SAMNetWeb can analyse two distinct data types (e.g. mRNA expression and global proteomics) simultaneously across multiple experimental systems to identify pathways activated in these experiments and then visualize the pathways in a single interaction network. Through the use of a multi-commodity flow based algorithm that requires each experiment 'share' underlying protein interactions, SAMNetWeb can identify distinct and common pathways across experiments. Availability and implementation: SAMNetWeb is freely available at http://fraenkel.mit.edu/ samnetweb.", 
  "summary": "SAMNetWeb can analyse two distinct data types (e.g. mRNA expression and global proteomics) simultaneously across multiple experimental systems to identify pathways activated in these experiments and then visualize the pathways in a single interaction network.\nThere are numerous network-based analysis tools (Tuncbag et al., 2013; Yeger-Lotem et al., 2009), including their web-server counterparts (Lan et al., 2011; Tuncbag et al., 2012) that perform visualization and functional enrichment of high-throughput datasets within a single experiment.\nSAMNetWeb is able to analyse multiple experiments by mapping both signaling and transcriptomic datasets to the same underlying protein-protein interaction network.", 
  "affiliations": [
    " Department of Biological Engineering Massachusetts Institute of Technology "
  ], 
  "grants": [
    "Funding\nThis work is supported by NIH grants [U54CA112967 and R01GM089903] and used computing resources funded by the National Science Foundation [DB1-0821391].", 
    "The provided transcriptional regulatory networks (Step 4) are derived from DNase I hypersensitive clusters from the ENCODE consortium to predict proteinDNA interactions using clusters of TRANSFAC motifs (MacIsaac et al.,\n\n2010)."
  ], 
  "acks": " The authors acknowledge Dr. Nurcan Tuncbag for her technical guidance and Scott Moskrin for technical support. This work is supported by NIH grants and used computing resources funded by the National Science Foundation. Conflict of Interest: none declared. ", 
  "authors": [
    " Sara J C Gosline", 
    " Coyin Oh", 
    " Ernest Fraenkel"
  ], 
  "keyWords": [
    [
      "networks", 
      "data", 
      "samnetweb", 
      "analysis", 
      "interactions"
    ]
  ], 
  "sourcelinks": [
    "http://fraenkel"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-21T01:18:58Z"
}{
  "doi": "10.1093/bioinformatics/btu577", 
  "name": "RVboost RNAseq variants prioritization using a boosting method", 
  "links": [
    "http://bioinformaticstools.mayo", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Sequence analysis RVboost: RNA-seq variants prioritization using a boosting method", 
  "toolName": "Sequence analysis RVboost: RNA-seq variants prioritization using a boosting method", 
  "abstract": "Motivation: RNA-seq has become the method of choice to quantify genes and exons, discover novel transcripts and detect fusion genes. However, reliable variant identification from RNA-seq data remains challenging because of the complexities of the transcriptome, the challenges of accurately mapping exon boundary spanning reads and the bias introduced during the sequencing library preparation. Method: We developed RVboost, a novel method specific for RNA variant prioritization. RVboost uses several attributes unique in the process of RNA library preparation, sequencing and RNA-seq data analyses. It uses a boosting method to train a model of 'good quality' variants using common variants from HapMap, and prioritizes and calls the RNA variants based on the trained model. We packaged RVboost in a comprehensive workflow, which integrates tools of variant calling, annotation and filtering. Results: RVboost consistently outperforms the variant quality score recalibration from the Genome Analysis Tool Kit and the RNA-seq variant-calling pipeline SNPiR in 12 RNA-seq samples using ground-truth variants from paired exome sequencing data. Several RNA-seq\u2013specific attributes were identified as critical to differentiate true and false variants, including the distance of the variant positions to exon boundaries, and the percent of the reads supporting the variant in the first six base pairs. The latter identifies false variants introduced by the random hexamer priming during the library construction. Availability and implementation: The RVboost package is implemented to readily run in Mac or Linux environments. The software and user manual are available at http://bioinformaticstools.mayo. edu/research/rvboost/.", 
  "summary": "RVboost takes an aligned RNA-seq BAM file [e.g. the BAM file generated by TopHat (Trapnell et al., 2009)] and processes it through three major components (Fig. 1): (i) Unified Genotyper from GATK for raw variant calling in the target region and generation of the annotations including all GATK classic annotations and the three novel attributes described above in Section 2.1; (ii) annotation of each variant with\nadditional attributes, including all functional annotations from SnpEff (Cingolani et al., 2012), and whether the variant position is a known RNA-editing site according to a RNA-editing database (Ramaswami and Li, 2014); (iii) Variant prioritization and ranking using RVboost.", 
  "affiliations": [
    " Division of Epidemiology Department of Health Sciences Research Mayo Clinic ", 
    " Department of Health Sciences Research Mayo Clinic ", 
    " Department of Cancer Biology Mayo Clinic ", 
    " Department of Laboratory Medicine and Pathology", 
    " Division of Hematology Department of Internal Medicine ", 
    " Division of Biomedical Statistics and Informatics Mayo Clinic "
  ], 
  "grants": [
    "Funding: Support for this work was provided by gift from Everett and Jane Hauck to the Center for Individualized Medicine at Mayo Clinic Jacksonville Florida, funds from the 26.2 with Donna Foundation and the proceeds of the National Marathon to Fight Breast Cancer and NIH P50 CA097274."
  ], 
  "acks": " ", 
  "authors": [
    " Chen Wang", 
    " Jaime I Davila", 
    " Saurabh Baheti", 
    " Aditya V Bhagwate", 
    " Xue Wang", 
    " Jean-Pierre A Kocher", 
    " Susan L Slager", 
    " Andrew L Feldman", 
    " Anne J Novak", 
    " James R Cerhan", 
    " E Aubrey Thompson", 
    " Yan W Asmann"
  ], 
  "keyWords": [
    "rna variants", 
    [
      "attribution", 
      "scores", 
      "variant", 
      "functional", 
      "boosting", 
      "rvboost"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformaticstools.mayo"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-29T00:22:23Z"
}{
  "doi": "10.1093/bioinformatics/btu397", 
  "name": "sapFinder an RBioconductor package for detection of variant peptides in shotgun proteomics experiments", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioconductor.org/packages/devel/bioc/html"
  ], 
  "title": "Data and text mining sapFinder: an R/Bioconductor package for detection of variant peptides in shotgun proteomics experiments", 
  "toolName": "Data and text mining sapFinder: an R/Bioconductor package for detection of variant peptides in shotgun proteomics experiments", 
  "abstract": "Single nucleotide variations (SNVs) located within a reading frame can result in single amino acid polymorphisms (SAPs), leading to alteration of the corresponding amino acid sequence as well as function of a protein. Accurate detection of SAPs is an important issue in proteomic analysis at the experimental and bioinformatic level. Herein, we present sapFinder, an R software package, for detection of the variant peptides based on tandem mass spectrometry (MS/MS)-based proteomics data. This package automates the construction of variation-associated databases from public SNV repositories or sample-specific next-generation sequencing (NGS) data and the identification of SAPs through database searching, post-processing and generation of HTML-based report with visualized interface. Availability and implementation: sapFinder is implemented as a Bioconductor package in R. The package and the vignette can be downloaded at http://bioconductor.org/packages/devel/bioc/html/ sapFinder.html and are provided under a GPL-2 license.", 
  "summary": "Herein, we present sapFinder, an R software package, for detection of the variant peptides based on tandem mass spectrometry (MS/MS)-based proteomics data.\nThis package automates the construction of variation-associated databases from public SNV repositories or sample-specific next-generation sequencing (NGS) data and the identification of SAPs through database searching, post-processing and generation of HTML-based report with visualized interface.\nSysPIMP collects human disease-related mutant sequences from the Online Mendelian Inheritance in Man (Hamosh et al., 2005), Protein Mutant Database (Kawabata et al., 1999) and SwissProt database (Boeckmann et al., 2003); however, it does not incorporate sample-specific NGS data,", 
  "affiliations": [
    " BGI-Shenzhen", 
    " Department of Chemistry University of Wisconsin-Madison "
  ], 
  "grants": [
    "Funding: We acknowledge the support from the State Key Development Program for Basic Research of China973 Program (NO."
  ], 
  "acks": " ", 
  "authors": [
    " Bo Wen", 
    " Shaohang Xu", 
    " Gloria M Sheynkman", 
    " Qiang Feng", 
    " Liang Lin", 
    " Quanhui Wang", 
    " Xun Xu", 
    " Jun Wang", 
    " Siqi Liu"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "proteomics", 
      "databases", 
      "searching", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://bioconductor.org/packages/devel/bioc/html"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-07-23T06:44:50Z"
}{
  "doi": "10.1093/bioinformatics/btu551", 
  "name": "SCDFinder a Webbased tool for the identification of putative novel ATM and ATR targets", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://ustbioinfo.webfactional.com/scd"
  ], 
  "title": "Sequence analysis SCDFinder, a Web-based tool for the identification of putative novel ATM and ATR targets", 
  "toolName": "Sequence analysis SCDFinder, a Web-based tool for the identification of putative novel ATM and ATR targets", 
  "abstract": "Motivation: The S/TQ cluster domain (SCD) constitutes a new type of protein domain that is not defined by sequence similarity but by the presence of multiple S/TQ motifs within a variable stretch of amino acids. SCDs are recognized targets for DNA damage response (DDR) kinases like ATM and ATR. Characterizing DDR targets is of significant interest. The aim of this work was to develop a web-based tool to allow for easy identification and visualization of SCDs within specific proteins or in whole proteome sets, a feature not supported by current domain and motif search tools. Results: We have developed an algorithm that (i) generates a list of all proteins in an organism containing at least one user-defined SCD within their sequence, or (ii) identifies and renders a visual representation of all user-defined SCDs present in a single sequence or batch of sequences. Availability and implementation: The application was developed using Pearl and Python, and is available at the following", 
  "summary": "This domain is present in 43 of 81 of the better-characterized ATM targets in mammalian cells, and it can also be found in more than half of 686 human proteins identified in a highthroughput analysis as containing phosphorylated S/TQ motifs (Matsuoka et al., 2007).\nStudies using a more stringent SCD definition (three S/TQ in 50 amino acids) found this domain present four times more abundantly in the yeast proteome than expected by random generation and concentrated in proteins belonging to pathways known to be under ATM/ATR control (Cheung et al., 2012).", 
  "affiliations": [
    " Bioinformatics Program Department of Biology University of St ", 
    " Department of Mathematics, Computer Science and Cooperative Engineering University of St. Thomas "
  ], 
  "grants": [
    "Funding: This project was generously supported by funds from the Smith Cullen Chair of Biology and from the Committee on Student Research at the University of St. Thomas."
  ], 
  "acks": " We would like to thank F.A. San Lucas for technical assistance. ", 
  "authors": [
    " Lukas Cara", 
    " Medina Baitemirova", 
    " Franklin Duong", 
    " Maia Larios-Sanz", 
    " Albert Ribes-Zamora", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "proteomics", 
      "containing", 
      "sequences", 
      "searching", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-08-15T05:12:51Z"
}{
  "doi": "10.1093/bioinformatics/btu861", 
  "name": "scrm efficiently simulating long sequences using the approximated coalescent with recombination", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://scrm.github.io", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Genetics and population analysis scrm: efficiently simulating long sequences using the approximated coalescent with recombination", 
  "toolName": "scrm.github.io", 
  "abstract": "Motivation: Coalescent-based simulation software for genomic sequences allows the efficient in silico generation of short-and medium-sized genetic sequences. However, the simulation of gen-ome-size datasets as produced by next-generation sequencing is currently only possible using fairly crude approximations. Results: We present the sequential coalescent with recombination model (SCRM), a new method that efficiently and accurately approximates the coalescent with recombination, closing the gap between current approximations and the exact model. We present an efficient implementation and show that it can simulate genomic-scale datasets with an essentially correct linkage structure. Availability and implementation: The open source implementation scrm is freely available at https://scrm.github.io under the conditions of the GPLv3", 
  "summary": "scrm: efficiently simulating long sequences using the approximated coalescent with recombination\nResults: We present the sequential coalescent with recombination model (SCRM), a new method that efficiently and accurately approximates the coalescent with recombination, closing the gap between current approximations and the exact model.\nIn order to resolve this problem, McVean and Cardin (2005) introduced the sequentially Markov coalescence (SMC) model, a method that approximates the CWR by partially ignoring genetic linkage between simulated sites.\nWe here present an efficient implementation of this model, termed scrm, and show that by using an intermediate approximation level it allows the simulation of sequences of arbitrary length with an essentially correct linkage structure.", 
  "affiliations": [
    " University of Oxford", 
    " Department of Biology Trust Centre for Human Genetics Ludwig-Maximilians-Universit\u00e4t M\u00fc nchen "
  ], 
  "grants": [
    "In the settings of Figure 2 and using an exact window size of 300 kb, scrm simulates essentially correct\n\nFunding\nThis work was supported by the Deutsche Forschungsgemeinschaft [DFG ME 3134/3-2] and the Wellcome Trust [090532/Z/09/Z]."
  ], 
  "sourcelinks": [
    "https://scrm.github.io"
  ], 
  "acks": " ", 
  "authors": [
    " Paul R Staab", 
    " Sha Zhu", 
    " Dirk Metzler", 
    " Gerton Lunter", 
    " Wellcome "
  ], 
  "keyWords": [
    "approximated coalescent", 
    [
      "approximations", 
      "simulations", 
      "recombinations", 
      "coalescence", 
      "bioinformatics", 
      "sequencing", 
      "modeling", 
      "scrm"
    ]
  ], 
  "github_data": {
    "name": "scrm.github.io", 
    "contributors": [
      {
        "contributions": 48, 
        "html_url": "https://github.com/paulstaab"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/terhorst"
      }
    ], 
    "versions": [], 
    "created_at": "2014-05-31T15:31:57Z", 
    "updated_at": "2016-01-27T10:08:08Z", 
    "languages": [
      "Makefile", 
      "HTML", 
      "Ruby", 
      "JavaScript", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/paulstaab"
      }, 
      {
        "html_url": "https://github.com/shajoezhu"
      }, 
      {
        "html_url": "https://github.com/gerton"
      }, 
      {
        "html_url": "https://github.com/DirkMetzler"
      }
    ], 
    "owner": "https://github.com/scrm", 
    "homepage": "https://scrm.github.io"
  }, 
  "technologies": [], 
  "dateCreated": "2015-01-10T04:10:59Z"
}{
  "doi": "10.1093/bioinformatics/btu709", 
  "name": "SEABED Small molEcule activity scanner weB servicE baseD", 
  "links": [
    "http://www.bsc.es/SEABED/static/RigidDocking.html", 
    "http://www.bsc.es/SEABED/static/EnsembleDocking.html", 
    "http://www.bsc.es", 
    "http://www.bsc.es/SEABED/static", 
    "http://mmb.irbbarce", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Structural bioinformatics SEABED: Small molEcule activity scanner weB servicE baseD", 
  "toolName": "Structural bioinformatics SEABED: Small molEcule activity scanner weB servicE baseD", 
  "abstract": "Motivation: The SEABED web server integrates a variety of docking and QSAR techniques in a user-friendly environment. SEABED goes beyond the basic docking and QSAR web tools and implements extended functionalities like receptor preparation, library editing, flexible ensemble docking , hybrid docking/QSAR experiments or virtual screening on protein mutants. SEABED is not a monolithic workflow tool but Software as a Service platform.", 
  "summary": "For a rigid docking, user needs to first upload the target structure, upload a set of small molecules and select a binding site before launching the docking (see Fig. 1ad) and full description at http://www.bsc.es/SEABED/static/RigidDocking.html).\nFor rigid docking, users (a) upload a protein structure, (b) upload a set of small molecules, (c) select or predict binding site, (d) launch docking and (f) get small molecules ranked by docking score.\nFor MD ensemble docking the workflow is similar but users need to (g) upload a MD trajectory instead of a rigid PDB structure and then (h) choose a set of snapshots before (i) launch and (j) get docking results.", 
  "affiliations": [], 
  "grants": [
    "Funding\nSpanish Ministry of Science and Competitiveness (MINECO; Bio201232868; SEV-2011-00067); National Institute of Bioinformatics (INB) Marato de TV3, and the Catalan AGAUR (Grup Consolidat; MO)."
  ], 
  "acks": " We thank technical assistance of V\u00edctor Guallar (PELE), Josep Llu\u00eds Gelp\u00ed and Laia Cod\u00f3 (cMIP), Santiago Villalba, Manuel Rueda. We also thank Robert Soliva, Floriane Montanari and Luc\u00eda D\u00edaz for their valuable feedback. ", 
  "authors": [
    " Carlos Fenollosa", 
    " Marcel Ot\u00f3", 
    " Pau Andrio", 
    " Jorge Cort\u00e9", 
    " Modesto Orozco", 
    " J Ramon Go\u00f1", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "models", 
      "seabed", 
      "dockings", 
      "proteins", 
      "structures"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-28T04:40:37Z"
}{
  "doi": "10.1093/bioinformatics/btu693", 
  "name": "Seed a userfriendly tool for exploring and visualizing microbial community data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/danlbek/Seed", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Genetics and population analysis Seed: a user-friendly tool for exploring and visualizing microbial community data", 
  "toolName": "Seed", 
  "abstract": "In this article we present Simple Exploration of Ecological Data (Seed), a data exploration tool for microbial communities. Seed is written in R using the Shiny library. This provides access to powerful R-based functions and libraries through a simple user interface. Seed allows users to explore ecological datasets using principal coordinate analyses, scatter plots, bar plots, hierarchal clustering and heatmaps. Availability and implementation: Seed is open source and available at https://github.com/danlbek/Seed. Contact:", 
  "summary": "Seed: a user-friendly tool for exploring and visualizing microbial community data\nABSTRACT Summary: In this article we present Simple Exploration of Ecological Data (Seed), a data exploration tool for microbial communities.\nSimple Exploration of Ecological Data (Seed) fills a currently unmet need for a tool that allows researchers to quickly and easily visualize and explore the data that results from these pipelines.\nIn this article, we present Seed, a software package that focuses on data exploration and visualization of microbial community data derived from high-throughput sequencing.\nSeed is an open-source application that allows researchers to visually explore microbial community data.", 
  "affiliations": [
    " Department of Biological Sciences University of Idaho "
  ], 
  "grants": [
    "Funding: This work was supported by the National Institutes of Health (P20GM016454) and by the National Science Foundation (DBI0939454)."
  ], 
  "sourcelinks": [
    "https://github.com/danlbek/Seed"
  ], 
  "acks": " We thank Larry Forney, Roxana Hickey, Janet Williams and other users for helpful conversations, recommendations and bug reports and for the datasets used for the figures herein. ", 
  "authors": [
    " Daniel Beck", 
    " Christopher Dennis", 
    " James A Foster", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    "microbial community data", 
    "user tool", 
    [
      "communities", 
      "seed", 
      "tools", 
      "users"
    ]
  ], 
  "github_data": {
    "name": "seed", 
    "contributors": [
      {
        "contributions": 25, 
        "html_url": "https://github.com/unscriptable"
      }, 
      {
        "contributions": 20, 
        "html_url": "https://github.com/briancavalier"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/fabricematrat"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/scothis"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/gogamoga"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/cujojs/seed/zipball/0.3.0", 
        "tarball_url": "https://api.github.com/repos/cujojs/seed/tarball/0.3.0", 
        "name": "0.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/cujojs/seed/zipball/0.2.4", 
        "tarball_url": "https://api.github.com/repos/cujojs/seed/tarball/0.2.4", 
        "name": "0.2.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/cujojs/seed/zipball/0.2.3", 
        "tarball_url": "https://api.github.com/repos/cujojs/seed/tarball/0.2.3", 
        "name": "0.2.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/cujojs/seed/zipball/0.2.2", 
        "tarball_url": "https://api.github.com/repos/cujojs/seed/tarball/0.2.2", 
        "name": "0.2.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/cujojs/seed/zipball/0.2.1", 
        "tarball_url": "https://api.github.com/repos/cujojs/seed/tarball/0.2.1", 
        "name": "0.2.1"
      }
    ], 
    "created_at": "2012-06-12T12:13:19Z", 
    "updated_at": "2016-03-26T18:19:56Z", 
    "languages": [
      "JavaScript", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/briancavalier"
      }, 
      {
        "html_url": "https://github.com/tomelam"
      }, 
      {
        "html_url": "https://github.com/joshlong"
      }, 
      {
        "html_url": "https://github.com/unscriptable"
      }, 
      {
        "html_url": "https://github.com/tfzxyinhao"
      }, 
      {
        "html_url": "https://github.com/anoldasgupta"
      }
    ], 
    "owner": "https://github.com/cujojs", 
    "homepage": "http://cujojs.com"
  }, 
  "technologies": [
    "R", 
    "Seed"
  ], 
  "dateCreated": "2014-10-21T03:20:28Z"
}{
  "doi": "10.1093/bioinformatics/btu707", 
  "name": "Semantic Body Browser graphical exploration of an organism and spatially resolved expression data visualization", 
  "links": [
    "https://github.com", 
    "http://browser.openworm.org", 
    "http://sbb.cellfinder.org", 
    "http://cellfinder.org", 
    "http://www.w3.org/TR/rdfa-core", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://zygotebody.com"
  ], 
  "title": "Semantic Body Browser: graphical exploration of an organism and spatially resolved expression data visualization", 
  "toolName": "github.com", 
  "abstract": "Advancing technologies generate large amounts of molecular and phenotypic data on cells, tissues and organisms, leading to an ever-growing detail and complexity while information retrieval and analysis becomes increasingly time-consuming. The Semantic Body Browser is a web application for intuitively exploring the body of an organism from the organ to the subcellular level and visualising expression profiles by means of semantically annotated anatomical illustrations. It is used to comprehend biological and medical data related to the different body structures while relying on the strong pattern recognition capabilities of human users. Availability and implementation: The Semantic Body Browser is a JavaScript web application that is freely available at", 
  "summary": "Similarly, large tables of numbers are suitable for computational analysis, but require expert knowledge and time to be analysed manually.\nThe representation of data is facilitated by means of interactive, annotated anatomical illustrations through a user-friendly web application for fast access to upto-date computationally derived data.\nLittle to no computational or terminological background knowledge is required to operate the SBB.\nNat. Protoc., 8, 17651786.\nNucleic Acids Res., 41, D991D995.\nIEEE Comput.\nAcids Res., 42, D749D755.\nNat. Methods, 9, 357359.\nNucleic Acids Res., 37, D782D785.\nNucleic Acids Res., 41, D987D990.\nNucleic Acid Res., 42, D950D958.", 
  "affiliations": [
    " Berlin-Brandenburg Center for Regenerative Therapies Charit\u00e9 \u2013Universit\u00e4 tsmedizin Berlin "
  ], 
  "grants": [
    "Funding\nThis work was supported by the Deutsche Forschungsgemeinschaft, grant KU 851/3-1 to AK and partially supported by the Research Institute for Veterinary Science, Seoul National University."
  ], 
  "sourcelinks": [
    "http://www.w3.org/TR/rdfa-core", 
    "https://github.com"
  ], 
  "acks": " The authors thank all CellFinder team members for their excellent comments and support in implementing and validating the SBB. This work was supported by the Deutsche Forschungsgemeinschaft, grant KU 851/3-1 to AK and partially supported by the Research Institute for Veterinary Science, Seoul National University. Conflict of Interest: none declared. ", 
  "authors": [
    " Fritz Lekschas", 
    " Harald Stachelscheid", 
    " Stefanie Seltmann", 
    " Andreas Kurtz"
  ], 
  "keyWords": [
    "expression data", 
    [
      "ontologically", 
      "bioinformatics"
    ]
  ], 
  "github_data": {
    "name": "node-v0.x-archive", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/orangemocha"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/works", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/works", 
        "name": "works"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.7", 
        "name": "v0.12.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.6", 
        "name": "v0.12.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.5", 
        "name": "v0.12.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.4", 
        "name": "v0.12.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.3", 
        "name": "v0.12.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.2", 
        "name": "v0.12.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.1", 
        "name": "v0.12.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.0", 
        "name": "v0.12.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.16", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.16", 
        "name": "v0.11.16"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.15", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.15", 
        "name": "v0.11.15"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.14", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.14", 
        "name": "v0.11.14"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.13", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.13", 
        "name": "v0.11.13"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.12", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.12", 
        "name": "v0.11.12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.11", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.11", 
        "name": "v0.11.11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.10", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.10", 
        "name": "v0.11.10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.9", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.9", 
        "name": "v0.11.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.8", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.8", 
        "name": "v0.11.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.7", 
        "name": "v0.11.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.6", 
        "name": "v0.11.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.5", 
        "name": "v0.11.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.4", 
        "name": "v0.11.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.3", 
        "name": "v0.11.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.40", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.40", 
        "name": "v0.10.40"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.39", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.39", 
        "name": "v0.10.39"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.38", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.38", 
        "name": "v0.10.38"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.37", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.37", 
        "name": "v0.10.37"
      }
    ], 
    "created_at": "2009-05-27T16:29:46Z", 
    "updated_at": "2016-08-09T16:10:45Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/isaacs"
      }, 
      {
        "html_url": "https://github.com/koichik"
      }, 
      {
        "html_url": "https://github.com/igorzi"
      }, 
      {
        "html_url": "https://github.com/kunoichi-co"
      }, 
      {
        "html_url": "https://github.com/carloslemes"
      }, 
      {
        "html_url": "https://github.com/aboyon"
      }, 
      {
        "html_url": "https://github.com/arden"
      }, 
      {
        "html_url": "https://github.com/guyoun"
      }, 
      {
        "html_url": "https://github.com/vissul"
      }, 
      {
        "html_url": "https://github.com/bdtgzj"
      }, 
      {
        "html_url": "https://github.com/santigimeno"
      }, 
      {
        "html_url": "https://github.com/chrisbateskeegan"
      }, 
      {
        "html_url": "https://github.com/FlashSoft"
      }, 
      {
        "html_url": "https://github.com/charlesgan"
      }, 
      {
        "html_url": "https://github.com/BlaneCordes"
      }, 
      {
        "html_url": "https://github.com/mdgoody"
      }, 
      {
        "html_url": "https://github.com/gindis"
      }, 
      {
        "html_url": "https://github.com/chrisdickinson"
      }, 
      {
        "html_url": "https://github.com/jorgenio"
      }, 
      {
        "html_url": "https://github.com/joityui"
      }, 
      {
        "html_url": "https://github.com/wxnet2013"
      }, 
      {
        "html_url": "https://github.com/be5invis"
      }, 
      {
        "html_url": "https://github.com/ashleymcfarland"
      }, 
      {
        "html_url": "https://github.com/jimxl"
      }, 
      {
        "html_url": "https://github.com/bluefisher80"
      }, 
      {
        "html_url": "https://github.com/subhaze"
      }, 
      {
        "html_url": "https://github.com/yaoming6046"
      }, 
      {
        "html_url": "https://github.com/hirozhang"
      }, 
      {
        "html_url": "https://github.com/rhyw"
      }, 
      {
        "html_url": "https://github.com/rhaldkhein"
      }
    ], 
    "owner": "https://github.com/nodejs", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-25T03:54:31Z"
}{
  "doi": "10.1093/bioinformatics/btu561", 
  "name": "SecureMA protecting participant privacy in genetic association metaanalysis", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://github", 
    "http://ec.europa.eu/justice/data-protec", 
    "http://github.com/XieConnect/SecureMA", 
    "http://www"
  ], 
  "title": "Genetics and population analysis SecureMA: protecting participant privacy in genetic association meta-analysis", 
  "toolName": "SecureMA", 
  "abstract": "Motivation: Sharing genomic data is crucial to support scientific investigation such as genome-wide association studies. However, recent investigations suggest the privacy of the individual participants in these studies can be compromised, leading to serious concerns and consequences, such as overly restricted access to data. Results: We introduce a novel cryptographic strategy to securely perform meta-analysis for genetic association studies in large consortia. Our methodology is useful for supporting joint studies among dispar-ate data sites, where privacy or confidentiality is of concern. We validate our method using three multisite association studies. Our research shows that genetic associations can be analyzed efficiently and accurately across substudy sites, without leaking information on individual participants and site-level association summaries. Availability and implementation: Our software for secure meta-analysis of genetic association studies, SecureMA, is publicly available at http://github.com/XieConnect/SecureMA. Our customized secure computation framework is also publicly available at http://github. com", 
  "summary": "To address the privacy concerns on individual genomic information as well as site-level summary statistics, we engineered a practical protocol to securely perform meta-analysis for genotypephenotype association studies across substudy sites in large consortia (Fig. 1).\nThese include encrypting genomic sequences and supporting simple queries (Kantarcioglu et al., 2008), obfuscating raw (short) genome sequences and allowing for retrieval (Ayday et al., 2014), splitting regression analyses into local-site computations and center-level aggregation (Wolfson et al., 2010) and hosting participant-level genomic data using a cryptographic technique and facilitating genetic association studies (Kamm et al., 2013).", 
  "affiliations": [
    " Department of Computer Science University of Texas at Dallas ", 
    " Department of Electrical Engineering & Computer Science Vanderbilt University ", 
    " Department of Biomedical Informatics"
  ], 
  "grants": [
    "Funding: This work was supported by the National Institutes of Health (R01LM009989, U01HG006378) and the National Science Foundation (CCF0424422).", 
    "For instance, based on (Homer et al., 2008), the NIH and Wellcome Trust stopped sharing aggregate genomic data directly to the public (Zerhouni and Nabel, 2008)."
  ], 
  "sourcelinks": [
    "http://github", 
    "http://github.com/XieConnect/SecureMA"
  ], 
  "acks": " ", 
  "authors": [
    " Wei Xie", 
    " Murat Kantarcioglu", 
    " William S Bush", 
    " Dana Crawford", 
    " Joshua C Denny", 
    " Raymond Heatherly", 
    " Bradley A Malin", 
    " Jeffery Barrett"
  ], 
  "keyWords": [
    "genomic data", 
    [
      "genomics", 
      "security", 
      "computational", 
      "studies", 
      "analysis"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/leverdeterre/SecureMappingKit/master/LICENCE"
      }
    ], 
    "name": "SecureMappingKit", 
    "contributors": [
      {
        "contributions": 54, 
        "html_url": "https://github.com/leverdeterre"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/leverdeterre/SecureMappingKit/zipball/0.0.1b1", 
        "tarball_url": "https://api.github.com/repos/leverdeterre/SecureMappingKit/tarball/0.0.1b1", 
        "name": "0.0.1b1"
      }
    ], 
    "created_at": "2014-05-04T18:30:25Z", 
    "updated_at": "2016-06-20T22:06:44Z", 
    "languages": [
      "Objective-C", 
      "Ruby"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/leverdeterre"
      }, 
      {
        "html_url": "https://github.com/DmitrySeredinov"
      }, 
      {
        "html_url": "https://github.com/fbxcelsior"
      }, 
      {
        "html_url": "https://github.com/yanndupuy"
      }, 
      {
        "html_url": "https://github.com/capriele"
      }, 
      {
        "html_url": "https://github.com/mayulu"
      }
    ], 
    "owner": "https://github.com/leverdeterre", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-08-22T03:08:27Z"
}{
  "doi": "10.1093/bioinformatics/btu805", 
  "name": "Semiconductor sequencing how many flows do you need", 
  "links": [
    "http://www.R-project.org/.Ronaghi,M", 
    "http://cran.r-project.org/web/packages/ionflows/.Budczies,J", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://ioncommunity"
  ], 
  "title": "Semiconductor sequencing: how many flows do you need?", 
  "toolName": "Semiconductor sequencing: how many flows do you need?", 
  "abstract": "Motivation: Semiconductor sequencing directly translates chemically encoded information (A, C, G or T) into voltage signals that are detected by a semiconductor device. Changes of pH value and thereby of the electric potential in the reaction well are detected during strand synthesis from nucleotides provided in cyclic repeated flows for each type of nucleotide. To minimize time requirement and costs, it is necessary to know the number of flows that are required for complete coverage of the templates. Results: We calculate the number of required flows in a random sequence model and present exact expressions for cumulative distribution function, expected value and variance. Additionally, we provide an algorithm to calculate the number of required flows for a concrete list of amplicons using a BED file of genomic positions as input. We apply the algorithm to calculate the number of flows that are required to cover six amplicon panels that are used for targeted sequencing in cancer research. The upper bounds obtained for the number of flows allow to enhance the instrument throughput from two chips to three chips per day for four of these panels. Availability and implementation: The algorithm for calculation of the flows was implemented in R and is available as package ionflows from the CRAN repository.", 
  "summary": "Results: We calculate the number of required flows in a random sequence model and present exact expressions for cumulative distribution function, expected value and variance.\nWe apply the algorithm to calculate the number of flows that are required to cover six amplicon panels that are used for targeted sequencing in cancer research.\nWe construct a stochastic model to calculate the number of required flows to sequence a template of length n.\nAs an example, the number of required flows was calculated for six amplicon panels for targeted sequencing in cancer research.\nThe number of required flows for six amplicon panels for targeted sequencing in cancer research.", 
  "affiliations": [
    " Institute of Pathology Charit\u00e9 University Hospital "
  ], 
  "grants": [
    "Funding\nThis work was supported by the German Consortium for Translational Cancer Research (DKTK)."
  ], 
  "acks": " ", 
  "authors": [
    " Jan Budczies", 
    " Michael Bockmayr", 
    " Denise Treue", 
    " Frederick Klauschen", 
    " Carsten Denkert"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "panels", 
      "genomes", 
      "flows"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-12-06T02:51:31Z"
}{
  "doi": "10.1093/bioinformatics/btu855", 
  "name": "Selection of models for the analysis of riskfactor trees leveraging biological knowledge to mine large sets of risk factors with application to microbiome data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://dsgweb.wustl.edu/qunyuan/software/smartscan"
  ], 
  "title": "Selection of models for the analysis of risk-factor trees: leveraging biological knowledge to mine large sets of risk factors with application to microbiome data Downloaded from", 
  "toolName": "Selection of models for the analysis of risk-factor trees: leveraging biological knowledge to mine large sets of risk factors with application to microbiome data Downloaded from", 
  "abstract": "Motivation: Establishment of a statistical association between microbiome features and clinical outcomes is of growing interest because of the potential for yielding insights into biological mechanisms and pathogenesis. Extracting microbiome features that are relevant for a disease is challenging and existing variable selection methods are limited due to large number of risk factor variables from microbiome sequence data and their complex biological structure. Results: We propose a tree-based scanning method, Selection of Models for the Analysis of Risk factor Trees (referred to as SMART-scan), for identifying taxonomic groups that are associated with a disease or trait. SMART-scan is a model selection technique that uses a predefined tax-onomy to organize the large pool of possible predictors into optimized groups, and hierarchically searches and determines variable groups for association test. We investigate the statistical properties of SMART-scan through simulations, in comparison to a regular single-variable analysis and three commonly-used variable selection methods, stepwise regression, least absolute shrinkage and selection operator (LASSO) and classification and regression tree (CART). When there are taxo-nomic group effects in the data, SMART-scan can significantly increase power by using bacterial taxonomic information to split large numbers of variables into groups. Through an application to microbiome data from a vervet monkey diet experiment, we demonstrate that SMART-scan can identify important phenotype-associated taxonomic features missed by single-variable analysis, stepwise regression, LASSO and CART.", 
  "summary": "In this case, only modeling individual OTUs at the lowest taxonomic level may significantly lose power because of weaker effects of individual OTUs. Although existing univariate tests and variable selection methods (even treebased multivariate methods such as UniFrac) can be applied to different levels or subsets of the data to identify specific association, conducting such analyses of all possible levels and subsets of a large taxonomic tree creates an intractable multiple comparison problem, therefore, strategies leveraging additional biological information to reduce the model space and optimize the search procedure are desirable.", 
  "affiliations": [
    " The Jackson Laboratory for Genomic Medicine", 
    " Department of Pediatrics Medical College of Wisconsin ", 
    " Division of Statistical Genomics Washington University School of Medicine St. Louis "
  ], 
  "grants": [
    "Funding\nThe National Institute of Health (NIH) (1R01DK8925601 to I.B.B) and (5R01DK075681 to I.B.B)."
  ], 
  "acks": " We thank Kathie Mihindukulasuriya, Yanjiao Zhou and Erica Sodergren for technical assistance and access to the vervet monkey data. ", 
  "authors": [
    " Qunyuan Zhang", 
    " Haley Abel", 
    " Alan Wells", 
    " Petra Lenzini", 
    " Felicia Gomez", 
    " Michael A Province", 
    " Alan A Templeton", 
    " George M Weinstock", 
    " Nita H Salzman", 
    " Ingrid B Borecki", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "microbiomes", 
      "methods", 
      "variables", 
      "modelled", 
      "trees", 
      "grouping"
    ]
  ], 
  "sourcelinks": [
    "https://dsgweb.wustl.edu/qunyuan/software/smartscan"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2015-01-08T02:10:15Z"
}{
  "doi": "10.1093/bioinformatics/btu616", 
  "name": "Shinyphyloseq Web application for interactive microbiome analysis with provenance tracking", 
  "links": [
    "http://creativecommons.org/licenses/by/4.0", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://joey711.github.io"
  ], 
  "title": "Data and text mining Shiny-phyloseq: Web application for interactive microbiome analysis with provenance tracking", 
  "toolName": "joey711.github.io", 
  "abstract": "We have created a Shiny-based Web application, called Shiny-phyloseq, for dynamic interaction with microbiome data that runs on any modern Web browser and requires no programming, increasing the accessibility and decreasing the entrance requirement to using phyloseq and related R tools. Along with a data-and context-aware dynamic interface for exploring the effects of parameter and method choices, Shiny-phyloseq also records the complete user input and subsequent graphical results of a user's session, allowing the user to archive, share and reproduce the sequence of steps that created their result\u2014without writing any new code themselves. Availability and implementation: Shiny-phyloseq is implemented entirely in the R language. It can be hosted/launched by any system with R installed, including Windows, Mac OS and most Linux distributions. Information technology administrators can also host Shiny-phyloseq from a remote server, in which case users need only have a Web browser installed. Shiny-phyloseq is provided free of charge under a GPL-3 open-source license through GitHub at", 
  "summary": "Here we describe our release of `Shiny-phyloseq', a Web browser GUI that leverages phyloseq and other R resources for the analysis of microbiome census data--while also allowing the user to archive the complete code and data necessary to exactly reproduce their session results.\nThe current implementation of Shiny-phyloseq is dependent on many important updates to the phyloseq package, including (i) an interface to DESeq2 (Anders and Huber, 2010) for a negative Binomial method recommended by McMurdie and Holmes, 2014; (ii) a ggplot2-friendly data organizing function, psmelt; (iii) the inclusion of low-level C code from APE (Paradis et al., 2004) for $100 faster UniFrac (Hamady et al., 2009) distance calculations and tree plotting; (iv) faster network plot function with better default settings, plot_net; and (v) additional options for ordering heatmaps by covariates.", 
  "affiliations": [
    " Department of Statistics Stanford University "
  ], 
  "grants": [
    "Funding: This work was supported by National Institutes of Health [grant number R01-GM086884 to SH]."
  ], 
  "acks": " ", 
  "authors": [
    " Paul J Mcmurdie", 
    " Susan Holmes"
  ], 
  "keyWords": [
    [
      "interactively", 
      "phyloseq", 
      "shiny", 
      "panels", 
      "data", 
      "networks"
    ]
  ], 
  "sourcelinks": [
    "http://joey711.github.io"
  ], 
  "technologies": [
    "C", 
    "R"
  ], 
  "dateCreated": "2014-09-28T00:25:27Z"
}{
  "doi": "10.1093/bioinformatics/btu691", 
  "name": "Simple rapid and accurate genotypingbysequencing from aligned whole genomes with ArrayMaker", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/cw2014"
  ], 
  "title": "Simple, rapid and accurate genotyping-by-sequencing from aligned whole genomes with ArrayMaker", 
  "toolName": "cw2014", 
  "abstract": "Whole-genome sequencing has revolutionized the study of genetics. Genotyping-by-sequencing is now a viable method of genotyping, yet the bioinformatics involved can be daunting if not prohibitive for some laboratories. Here we present ArrayMaker, a user-friendly tool that extracts accurate single nucleotide polymorph-ism genotypes at pre-defined loci from whole-genome alignments and presents them in a standard genotyping format compatible with association analysis software and datasets genotyped on commercial array platforms. Using this tool, geneticists with only basic computing ability can genotype samples at any desired list of markers, facilitating genome-wide association analysis, fine mapping, candidate variant assessment, data sharing and compatibility of data sourced from multiple technologies. Availability and implementation: ArrayMaker is licensed under The MIT License and can be freely obtained at https://github.com/cw2014/ ArrayMaker/. The program is implemented in Perl and runs on Linux operating systems.", 
  "summary": "Here we present ArrayMaker, a user-friendly tool that extracts accurate single nucleotide polymorphism genotypes at pre-defined loci from whole-genome alignments and presents them in a standard genotyping format compatible with association analysis software and datasets genotyped on commercial array platforms.\n3.2 Concordance with commercial genotyping arrays To perform high-throughput validation of ArrayMaker SNP calls, genotypes from Illumina SNP arrays for the same samples that were sequenced were used as the `truth-dataset' (Table 1).\nThe tool can replicate existing and obsolete SNP array datasets, create GWAS-ready files from bespoke marker lists, and rapidly genotype candidate SNP loci in new samples without the need for complicated post-alignment bioinformatics.", 
  "affiliations": [
    " Faculty of Veterinary Science", 
    " School of Information Technologies University of Sydney "
  ], 
  "grants": [
    "ACKNOWLEDGEMENTS We are grateful for donation of data by Tosso Leeb, Stefan Rieder; Danika Bannasch, Zena Wolf (NIH NIDCR R01DE022532); Jonathan Early, Elizabeth Arnott, Paul\n\nMcGreevy (MLA RIRDC PRJ-007806); Tracey Chew; Annie Pan, Peter Williamson; TGEN.", 
    "Funding: This research was funded by APA and Val Street scholarships.", 
    "Donated data was funded in part by NIH and MLA (see Acknowledgements)."
  ], 
  "sourcelinks": [
    "https://github.com/cw2014"
  ], 
  "acks": " We are grateful for donation of data by Tosso Leeb, Stefan Rieder; Danika Bannasch, Zena Wolf (NIH NIDCR R01DE022532); Jonathan Early, Elizabeth Arnott, Paul McGreevy (MLA RIRDC PRJ-007806); Tracey Chew; Annie Pan, Peter Williamson; TGEN. ", 
  "authors": [
    " Cali E Willet", 
    " Bianca Haase", 
    " Michael A Charleston", 
    " Claire M Wade", 
    " Janet Kelso"
  ], 
  "keyWords": [
    "snp arrays", 
    [
      "sequencing", 
      "genotyping", 
      "arraymaker", 
      "alignments", 
      "bioinformatics"
    ]
  ], 
  "github_data": {
    "name": "pt_cw2014", 
    "versions": [], 
    "created_at": "2014-05-08T14:52:44Z", 
    "updated_at": "2014-05-08T14:52:44Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/izamarciniak123"
      }
    ], 
    "owner": "https://github.com/izamarciniak123", 
    "homepage": null
  }, 
  "technologies": [
    "Perl"
  ], 
  "dateCreated": "2014-10-22T05:07:31Z"
}{
  "doi": "10.1093/bioinformatics/btu856", 
  "name": "Shifted Hamming distance a fast and accurate SIMDfriendly filter to accelerate alignment verification in read mapping", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/CMU-SAFARI/SHD"
  ], 
  "title": "Shifted Hamming distance: a fast and accurate SIMD-friendly filter to accelerate alignment verification in read mapping", 
  "toolName": "SHD", 
  "abstract": "Motivation: Calculating the edit-distance (i.e. minimum number of insertions, deletions and substitutions) between short DNA sequences is the primary task performed by seed-and-extend based mappers, which compare billions of sequences. In practice, only sequence pairs with a small edit-distance provide useful scientific data. However, the majority of sequence pairs analyzed by seed-and-extend based mappers differ by significantly more errors than what is typically allowed. Such error-abundant sequence pairs needlessly waste resources and severely hinder the performance of read mappers. Therefore, it is crucial to develop a fast and accurate filter that can rapidly and efficiently detect error-abundant string pairs and remove them from consideration before more computationally expensive methods are used. Results: We present a simple and efficient algorithm, Shifted Hamming Distance (SHD), which accelerates the alignment verification procedure in read mapping, by quickly filtering out error-abundant sequence pairs using bit-parallel and SIMD-parallel operations. SHD only filters string pairs that contain more errors than a user-defined threshold, making it fully comprehensive. It also maintains high accuracy with moderate error threshold (up to 5% of the string length) while achieving a 3-fold speedup over the best previous algorithm (Gene Myers's bit-vector algorithm). SHD is compatible with all mappers that perform sequence alignment for verification. Availability and implementation: We provide an implementation of SHD in C with Intel SSE instructions at: https://github.com/CMU-SAFARI/SHD.", 
  "summary": "Results: We present a simple and efficient algorithm, Shifted Hamming Distance (SHD), which accelerates the alignment verification procedure in read mapping, by quickly filtering out error-abundant sequence pairs using bit-parallel and SIMD-parallel operations.\nIn this article, we present shifted hamming distance (SHD), a fast and accurate SIMD-friendly bit-vector filter to accelerate the local alignment (verification) procedure in read mapping.\nBy incrementally shifting the read in SHM, all basepairs between the read and the reference of a correct mapping (except the errors) are brought into alignment with at least one matching bp of the read and identified in one or more of the 2e  1 masks, as shown in Figure 1.", 
  "affiliations": [
    " Computer Science Department", 
    " Department of Electrical and Computer Engineering", 
    " Department of Computer Engineering Bilkent University ", 
    " Computational Biology Department Carnegie Mellon University "
  ], 
  "grants": [
    "Funding\nThis study is supported by NIH Grants (HG006004 to O. Mutlu and C. Alkan, and HG007104 to C. Kingsford) and a Marie Curie Career Integration Grant (PCIG-2011-303772) to C. Alkan under the Seventh Framework Programme."
  ], 
  "sourcelinks": [
    "https://github.com/CMU-SAFARI/SHD"
  ], 
  "acks": " ", 
  "authors": [
    " Hongyi Xin", 
    " John Greth", 
    " John Emmons", 
    " Gennady Pekhimenko", 
    " Carl Kingsford", 
    " Can Alkan", 
    " Onur Mutlu"
  ], 
  "keyWords": [
    [
      "reads", 
      "filtering", 
      "algorithms", 
      "errors"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/BKcore/Shdr/master/LICENSE"
      }
    ], 
    "name": "Shdr", 
    "contributors": [
      {
        "contributions": 3, 
        "html_url": "https://github.com/BKcore"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/cyrilf"
      }
    ], 
    "versions": [], 
    "created_at": "2013-02-17T17:35:24Z", 
    "updated_at": "2016-08-09T15:52:25Z", 
    "languages": [
      "CoffeeScript", 
      "ApacheConf", 
      "JavaScript", 
      "HTML", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/BKcore"
      }, 
      {
        "html_url": "https://github.com/fundon"
      }, 
      {
        "html_url": "https://github.com/phoenixiizero"
      }, 
      {
        "html_url": "https://github.com/medfreeman"
      }, 
      {
        "html_url": "https://github.com/e7dal"
      }, 
      {
        "html_url": "https://github.com/silviacarolina"
      }, 
      {
        "html_url": "https://github.com/bagobor"
      }, 
      {
        "html_url": "https://github.com/tonosaman"
      }, 
      {
        "html_url": "https://github.com/qiulvwei"
      }, 
      {
        "html_url": "https://github.com/melling"
      }, 
      {
        "html_url": "https://github.com/dpuyosa"
      }, 
      {
        "html_url": "https://github.com/mmitang"
      }, 
      {
        "html_url": "https://github.com/AmazedStream"
      }, 
      {
        "html_url": "https://github.com/mcanthony"
      }, 
      {
        "html_url": "https://github.com/ElectricJack"
      }, 
      {
        "html_url": "https://github.com/fmichaelw"
      }, 
      {
        "html_url": "https://github.com/anticafe"
      }, 
      {
        "html_url": "https://github.com/catafest"
      }, 
      {
        "html_url": "https://github.com/abhishekchakraborty"
      }, 
      {
        "html_url": "https://github.com/siriusye"
      }
    ], 
    "owner": "https://github.com/BKcore", 
    "homepage": "http://shdr.bkcore.com/"
  }, 
  "technologies": [], 
  "dateCreated": "2015-01-11T01:27:33Z"
}{
  "doi": "10.1093/bioinformatics/btu657", 
  "name": "SIST stressinduced structural transitions in superhelical DNA", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://bitbucket.org/benhamlab/sist"
  ], 
  "title": "Genome analysis SIST: stress-induced structural transitions in superhelical DNA", 
  "toolName": "Genome analysis SIST: stress-induced structural transitions in superhelical DNA", 
  "abstract": "Supercoiling imposes stress on a DNA molecule that can drive susceptible sequences into alternative non-B form structures. This phenomenon occurs frequently in vivo and has been implicated in biological processes, such as replication, transcription, recombin-ation and translocation. SIST is a software package that analyzes sequence dependent structural transitions in kilobase length superhelical DNA molecules. The numerical algorithms in SIST are based on a statistical mechanical model that calculates the equilibrium probability of transition for each base pair in the domain. They are extensions of the original stress-induced duplex destabilization (SIDD) method, which analyzes stress-driven DNA strand separation. SIST also includes algorithms to analyze B-Z transitions and cruciform extrusion. The SIST pipeline has an option to use the DZCBtrans algorithm , which analyzes the competition among these three transitions within a superhelical domain. Availability and implementation: The package and additional documentation are freely available at https://bitbucket.org/benhamlab/sist_ codes.", 
  "summary": "SIST: stress-induced structural transitions in superhelical DNA\nSIST is a software package that analyzes sequence-dependent structural transitions in kilobase length superhelical DNA molecules.\nSIST also includes algorithms to analyze B-Z transitions and cruciform extrusion.\nThe SIST pipeline has an option to use the DZCBtrans algorithm, which analyzes the competition among these three transitions within a superhelical domain.\nSeveral types of superhelical transitions have been shown to occur in vitro, including local strand separations, Z-DNA, cruciform extrusion and others.\nTransition probabilities calculated by SIST as functions of base pair position are shown for a sequence where regions susceptible to melting, Z-DNA and cruciform extrusion were inserted into the pbr322 plasmid.", 
  "affiliations": [
    " Department of Mathematics Davis University of California ", 
    " UC Davis Genome Center"
  ], 
  "grants": [
    "Funding: This research was supported by grant DBI 0850214 from the National Science Foundation."
  ], 
  "acks": " The authors thank Brittney Hoff for programming assistance on this project. ", 
  "authors": [
    " Dina Zhabinskaya", 
    " Sally Madden", 
    " Craig J Benham", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "algorithmic", 
      "forming", 
      "including", 
      "sequences", 
      "sist", 
      "transitions", 
      "superhelically"
    ]
  ], 
  "sourcelinks": [
    "https://bitbucket.org/benhamlab/sist"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-10-05T00:35:52Z"
}{
  "doi": "10.1093/bioinformatics/btu641", 
  "name": "Sigma Strainlevel inference of genomes from metagenomic analysis for biosurveillance", 
  "links": [
    "http://sourceforge.net/projects/pathoscope/.BLAST", 
    "http://sigma.omics", 
    "https://github.com/ngscomparison/NGS-Benchtop-Comparison.MEGAN", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://ab.inf.uni-tuebingen.de/software/megan/.Metagenomic", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://bitbucket.org/nsegata/metaphlan"
  ], 
  "title": "Genome analysis Sigma: Strain-level inference of genomes from metagenomic analysis for biosurveillance", 
  "toolName": "pathoscope", 
  "abstract": "Motivation: Metagenomic sequencing of clinical samples provides a promising technique for direct pathogen detection and characterization in biosurveillance. Taxonomic analysis at the strain level can be used to resolve serotypes of a pathogen in biosurveillance. Sigma was developed for strain-level identification and quantification of pathogens using their reference genomes based on metagenomic analysis. Results: Sigma provides not only accurate strain-level inferences, but also three unique capabilities: (i) Sigma quantifies the statistical uncertainty of its inferences, which includes hypothesis testing of identified genomes and confidence interval estimation of their relative abundances ; (ii) Sigma enables strain variant calling by assigning metage-nomic reads to their most likely reference genomes; and (iii) Sigma supports parallel computing for fast analysis of large datasets. The algorithm performance was evaluated using simulated mock communities and fecal samples with spike-in pathogen strains. Availability and Implementation: Sigma was implemented in C++ with source codes and binaries freely available at", 
  "summary": "Results: Sigma provides not only accurate strain-level inferences, but also three unique capabilities: (i) Sigma quantifies the statistical uncertainty of its inferences, which includes hypothesis testing of identified genomes and confidence interval estimation of their relative abundances; (ii) Sigma enables strain variant calling by assigning metagenomic reads to their most likely reference genomes; and (iii) Sigma supports parallel computing for fast analysis of large datasets.\nFinally, given the MLE of genome relative abundances, Sigma can calculate the probabilities of sampling a read from its mapped genomes, which allows assigning metagenomic reads to their most likely originating genomes for variant calling.", 
  "affiliations": [
    " Computer Science and Mathematics Division Oak Ridge National Laboratory Oak Ridge "
  ], 
  "grants": [
    "This work was supported by Laboratory Directed Research and Development (LDRD) funding from Oak Ridge National Laboratory.", 
    "The contribution of J.C. was sponsored by the Office of Advanced Scientific Computing Research."
  ], 
  "acks": " The authors would like to thank Richard Stouder, Loren Hauser, and Robert Cottingham for their support and assistance . This work was supported by ", 
  "authors": [
    " Tae-Hyuk Ahn", 
    " Juanjuan Chai", 
    " Chongle Pan"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "genomics", 
      "sigma", 
      "metagenomics", 
      "strains"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net/projects/pathoscope/.BLAST", 
    "http://ab.inf.uni-tuebingen.de/software/megan/.Metagenomic", 
    "https://github.com/ngscomparison/NGS-Benchtop-Comparison.MEGAN", 
    "http://sigma.omics"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-30T05:52:55Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/pathoscope/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/mani2012/", 
        "name": "Solaiappan Manimaran"
      }, 
      {
        "url": "https://sourceforge.net/u/jperezrogers/", 
        "name": "Joseph Perez-Rogers"
      }, 
      {
        "url": "https://sourceforge.net/u/hengwu/", 
        "name": "hengwu"
      }, 
      {
        "url": "https://sourceforge.net/u/pathoscope/", 
        "name": "Pathoscope"
      }, 
      {
        "url": "https://sourceforge.net/u/albyrd/", 
        "name": "Allyson Byrd"
      }, 
      {
        "url": "https://sourceforge.net/u/ecastron/", 
        "name": "Eduardo Castro Nallar"
      }, 
      {
        "url": "https://sourceforge.net/u/auppal/", 
        "name": "Ahsen Uppal"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu604", 
  "name": "SignalSpider probabilistic pattern discovery on multiple normalized ChIPSeq signal profiles", 
  "links": [
    "http://www.cs.toronto.edu/$wkc/SignalSpider", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genome analysis SignalSpider: probabilistic pattern discovery on multiple normalized ChIP-Seq signal profiles", 
  "toolName": "Genome analysis SignalSpider: probabilistic pattern discovery on multiple normalized ChIP-Seq signal profiles", 
  "abstract": "Motivation: Chromatin immunoprecipitation (ChIP) followed by high-throughput sequencing (ChIP-Seq) measures the genome-wide occupancy of transcription factors in vivo. Different combinations of DNA-binding protein occupancies may result in a gene being expressed in different tissues or at different developmental stages. To fully understand the functions of genes, it is essential to develop probabilistic models on multiple ChIP-Seq profiles to decipher the combinatorial regulatory mechanisms by multiple transcription factors. Results: In this work, we describe a probabilistic model (SignalSpider) to decipher the combinatorial binding events of multiple transcription factors. Comparing with similar existing methods, we found SignalSpider performs better in clustering promoter and enhancer regions. Notably, SignalSpider can learn higher-order combinatorial patterns from multiple ChIP-Seq profiles. We have applied SignalSpider on the normalized ChIP-Seq profiles from the ENCODE consortium and learned model instances. We observed different higher-order enrichment and depletion patterns across sets of proteins. Those clustering patterns are supported by Gene Ontology (GO) enrichment, evolutionary conservation and chromatin interaction enrichment, offering biological insights for further focused studies. We also proposed a specific enrichment map visualization method to reveal the genome-wide transcription factor combinatorial patterns from the models built, which extend our existing fine-scale knowledge on gene regulation to a genome-wide level. Availability and implementation: The matrix-algebra-optimized exe-cutables and source codes are available at the authors' websites:", 
  "summary": "However such approaches have two limitations, as (i) peak-calling ignores the contributions from weak bindings of TFs, and (ii) pair-wise analysis ignores the complex combinatorial binding pattern among the TFs. Here we propose a new approach to build fine-scale probabilistic models for directly analyzing multiple normalized ChIP-Seq signal profiles on all the promoter and enhancer regions quantitatively so that weak bindings can be taken into account (Cheng et al., 2012).\n; 10g; the number of profiles (M) was selected from f3; 4; 5; 10g to reflect the usual clique sizes in the following section; the number of ChIP-Seq signal components of each DNA-binding protein (N) was set to 2 to accommodate the binary clustering nature of ChromHMM and jMOSAiCS (although SignalSpider can handle 3 or more in this aspect).", 
  "affiliations": [
    " Terrence Donnelly Centre for Cellular and Biomolecular Research University of Toronto ", 
    " CEMSE Division King Abdullah University of Science and Technology ", 
    " Department of Computer Science"
  ], 
  "grants": [
    "Funding: Discovery Grant from Natural Sciences and Engineering Research Council, Canada (NSERC), grant number [327612-2009 RGPIN to Z.Z."
  ], 
  "acks": " The authors would like to thank the anonymous reviewers for their positive and constructive comments. ", 
  "authors": [
    " Ka-Chun Wong", 
    " Yue Li", 
    " Chengbin Peng", 
    " Zhaolei Zhang", 
    " "
  ], 
  "keyWords": [
    [
      "clustering", 
      "genes", 
      "proteins", 
      "modeling", 
      "bindings", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://www.cs.toronto.edu/$wkc/SignalSpider"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-06T04:48:38Z"
}{
  "doi": "10.1093/bioinformatics/btu779", 
  "name": "SNiPA an interactive genetic variantcentered annotation browser", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.snipa.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "SNiPA: an interactive, genetic variant-centered annotation browser", 
  "toolName": "SNiPA: an interactive, genetic variant-centered annotation browser", 
  "abstract": "Motivation: Linking genes and functional information to genetic variants identified by association studies remains difficult. Resources containing extensive genomic annotations are available but often not fully utilized due to heterogeneous data formats. To enhance their accessibility, we integrated many annotation datasets into a user-friendly webserver. Availability and implementation: http://www.snipa.org/", 
  "summary": "SNiPA includes a wide range of genome-level datasets contained in the Ensembl database (Flicek et al., 2014) as an established backbone of annotations for the human genome.\nSNiPA contains annotations for all bi-allelic variants in phase 3 version 5 of the 1000 genomes project (1000 Genomes Project Consortium et al., 2012) and provides pre-calculated LD-data for r2  0.1 for all super populations (African, American, South and East Asian, European).\nPlot' and `Linkage Disequilibrium Plot' (Diabetes Genetics Initiative of Broad Institute of Harvard et al., 2007) that combine publication-ready plotting of association results and LD values, respectively, with the interactive interface of the `Variant Browser'; (vi) `Proxy Search' and `Pairwise LD' that allow querying precalculated LD values augmented with variant annotations.", 
  "affiliations": [
    " Institute of Bioinformatics and Systems Biology Helmholtz Zentrum M\u00fc nchen \u2013 German Research Center for Environmental Health "
  ], 
  "grants": [
    "Funding\nThis work was supported by the Helmholtz Portfolio theme `Metabolic Dysfunction and common disease' and by the research project Greifswald Approach to Individualized Medicine (GANI_MED) (BMBF: 03IS2061A).", 
    "is supported by Biomedical Research Program funds at Weill Cornell Medical College in Qatar, a program funded by the Qatar Foundation."
  ], 
  "acks": " ", 
  "authors": [
    " Matthias Arnold", 
    " Johannes Raffler", 
    " Arne Pfeufer", 
    " Karsten Suhre", 
    " Gabi Kastenm\u00fc Ller"
  ], 
  "keyWords": [
    "variant annotation", 
    "genomic annotations", 
    [
      "associations", 
      "genomes", 
      "snipa", 
      "variants", 
      "annotating", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-28T15:35:07Z"
}{
  "doi": "10.1093/bioinformatics/btu800", 
  "name": "SMARTS reconstructing disease response networks from multiple individuals using time series gene expression data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "SMARTS: reconstructing disease response networks from multiple individuals using time series gene expression data", 
  "toolName": "SMARTS: reconstructing disease response networks from multiple individuals using time series gene expression data", 
  "abstract": "Motivation: Current methods for reconstructing dynamic regulatory networks are focused on mod-eling a single response network using model organisms or cell lines. Unlike these models or cell lines, humans differ in their background expression profiles due to age, genetics and life factors. In addition, there are often differences in start and end times for time series human data and in the rate of progress based on the specific individual. Thus, new methods are required to integrate time series data from multiple individuals when modeling and constructing disease response networks. Results: We developed Scalable Models for the Analysis of Regulation from Time Series (SMARTS), a method integrating static and time series data from multiple individuals to reconstruct condition-specific response networks in an unsupervised way. Using probabilistic graphical models, SMARTS iterates between reconstructing different regulatory networks and assigning individuals to these networks, taking into account varying individual start times and response rates. These models can be used to group different sets of patients and to identify transcription factors that differentiate the observed responses between these groups. We applied SMARTS to analyze human response to influenza and mouse brain development. In both cases, it was able to greatly improve baseline groupings while identifying key relevant TFs that differ between the groups. Several of these groupings and TFs are known to regulate the relevant processes while others represent novel hypotheses regarding immune response and development. Availability and implementation: Software and Supplementary information are available at", 
  "summary": "Results: We developed Scalable Models for the Analysis of Regulation from Time Series (SMARTS), a method integrating static and time series data from multiple individuals to reconstruct condition-specific response networks in an unsupervised way.\nSimilarly, a simplified version of SMARTS's regulatory network model that uses neither time series alignment nor networkbased gene selection (see Section 2.3) is also unable to correctly separate these two classes of patients.\nThe SMARTS framework integrates data from many individual gene expression time series with TFgene interaction data, allowing for novel forms of analysis, such as analyzing and classifying human disease time series or modeling the differentiation of tissues during embryonic development.", 
  "affiliations": [
    " Lane Center for Computational Biology"
  ], 
  "grants": [
    "Funding\nThe work was supported in part by National Institute of Health [grant number 1 U54 HL127624-01 to Z.B.J.", 
    "], by the National Science Foundation [grant number DBI- 1356505 to Z.B.J.]"
  ], 
  "acks": " ", 
  "authors": [
    " Aaron Wise", 
    " Ziv Bar-Joseph"
  ], 
  "keyWords": [
    [
      "smarts", 
      "responses", 
      "genes", 
      "modelling", 
      "times", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-12-06T02:51:31Z"
}{
  "doi": "10.1093/bioinformatics/btu777", 
  "name": "Singlecell transcriptional analysis to uncover regulatory circuits driving cell fate decisions in early mouse development", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Single-cell transcriptional analysis to uncover regulatory circuits driving cell fate decisions in early mouse development", 
  "toolName": "Single-cell transcriptional analysis to uncover regulatory circuits driving cell fate decisions in early mouse development", 
  "abstract": "Motivation: Transcriptional regulatory networks controlling cell fate decisions in mammalian em-bryonic development remain elusive despite a long time of research. The recent emergence of single cell RNA profiling technology raises hope for new discovery. Although experimental works have obtained intriguing insights into the mouse early development, a holistic and systematic view is still missing. Mathematical models of cell fates tend to be concept-based, not designed to learn from real data. To elucidate the regulatory mechanisms behind cell fate decisions, it is highly desirable to synthesize the data-driven and knowledge-driven modeling approaches. Results: We propose a novel method that integrates the structure of a cell lineage tree with tran-scriptional patterns from single-cell data. This method adopts probabilistic Boolean network (PBN) for network modeling, and genetic algorithm as search strategy. Guided by the 'directionality' of cell development along branches of the cell lineage tree, our method is able to accurately infer the regulatory circuits from single-cell gene expression data, in a holistic way. Applied on the single cell transcriptional data of mouse preimplantation development, our algorithm outperforms conventional methods of network inference. Given the network topology, our method can also identify the operational interactions in the gene regulatory network (GRN), corresponding to specific cell fate determination. This is one of the first attempts to infer GRNs from single-cell transcriptional data, incorporating dynamics of cell development along a cell lineage tree.", 
  "summary": "For example, in (Parikh et al., 2011) and (Hashimoto et al., 2012), cell lineage trees were incorporated into machine learning algorithms to infer gene networks and expression programs, respectively.\nINPUT: Single-cell gene expression data, population size N, number of selected individuals N0, mutation rate m, permutations per chromosome Z, and maximum iteration times MaxIt OUTPUT: A vector P, which consists of regulators of each gene in the network (i.e. P is the representation of a network topology)\nRobson's single-cell gene expression data, our method is able to identify the significant rules (i.e. rules with high probabilities) for each lineage formation, shown in Table 3 (see S4 of Supplementary material for more details).", 
  "affiliations": [
    " School of Electronics and Computer Science University of Southampton ", 
    " School of Computer Engineering Nanyang Technological University ", 
    " Genome Institute of Singapore"
  ], 
  "grants": [
    "Downloaded from http://bioinformatics.oxfordjournals.org/ at :: on August 8, 2016\n\nFunding\nThis work was supported by MOE AcRF Tier 1 Seed Grant on Complexity [RGC 2/13, M4011101.020], Ministry of Education Singapore."
  ], 
  "acks": " ", 
  "authors": [
    " Haifen Chen", 
    " Jing Guo", 
    " Shital K Mishra", 
    " Paul Robson", 
    " Mahesan Niranjan", 
    " Jie Zheng"
  ], 
  "keyWords": [
    [
      "methods", 
      "cells", 
      "genes", 
      "states", 
      "data", 
      "networks"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C", 
    "R", 
    "Java"
  ], 
  "dateCreated": "2014-11-22T02:58:16Z"
}{
  "doi": "10.1093/bioinformatics/btu804", 
  "name": "SNPlice variants that modulate Intron retention from RNAsequencing data", 
  "links": [
    "http://srv00.ibbe.cnr.it/ASPicDB/newresults.php?organism-human", 
    "http://samtools.sourceforge.net/mpileup-.shtml", 
    "https://cghub.ucsc", 
    "http://www.ncbi.nlm.nih.gov/IEB/Research/Acembly/av.cgi?dbhu", 
    "https://code.google.com/p-/snplice", 
    "http://www.ncbi.nlm.nih.gov", 
    "http://snp.gs", 
    "https://code.goo", 
    "http://f1000r.es/378", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "SNPlice: variants that modulate Intron retention from RNA-sequencing data", 
  "toolName": "samtools", 
  "abstract": "Rationale: The growing recognition of the importance of splicing, together with rapidly accumulating RNA-sequencing data, demand robust high-throughput approaches, which efficiently analyze experimentally derived whole-transcriptome splice profiles. Results: We have developed a computational approach, called SNPlice, for identifying cis-acting, splice-modulating variants from RNA-seq datasets. SNPlice mines RNA-seq datasets to find reads that span single-nucleotide variant (SNV) loci and nearby splice junctions, assessing the co-occurrence of variants and molecules that remain unspliced at nearby exon\u2013intron boundaries. Hence, SNPlice highlights variants preferentially occurring on intron-containing molecules, possibly resulting from altered splicing. To illustrate co-occurrence of variant nucleotide and exon\u2013intron boundary, allele-specific sequencing was used. SNPlice results are generally consistent with splice-prediction tools, but also indicate splice-modulating elements missed by other algorithms. SNPlice can be applied to identify variants that correlate with unexpected splicing events, and to measure the splice-modulating potential of canonical splice-site SNVs. Availability and implementation: SNPlice is freely available for download from https://code.goo gle.com/p/snplice/ as a self-contained binary package for 64-bit Linux computers and as python source-code.", 
  "summary": "The emerging recognition of the importance of splicing has stimulated efforts for modeling the splice-modulating potential of genetic variants through probabilistic predictions based on junction nucleotide composition, dependencies among neighboring bases, local optimality in the context of the gene structure, homology alignments, interaction with splicing factors and comparison with experimentally verified splice-modulating motifs (Brunak et al., 1991; Brendel and Kleffe, 1998; Dogan et al., 2007; Faber et al., 2011; Kamath et al., 2012; Pertea et al., 2001; Piva et al., 2012; Riva et al., 2012; Woolfe et al., 2010; Yeo and Burge, 2004).\nSplicePort estimated that the variant diminishes the acceptor strength, SpliceAid2 modeled that the change switches the binding site preference from the splice-regulator TIA-1 to PPT-binding protein PTB (Cavaloc et al., 1999; Caputi and Zahler, 2002; Jurica et al., 2002; Supplementary Fig. 5), and Skippy predicted gain of one new ESS (See Table 1).", 
  "affiliations": [
    " Department of Ophthalmology", 
    " Department of Biochemistry and Molecular & Cellular Biology School of Medicine Georgetown University "
  ], 
  "grants": [
    "Funding\nThis work is supported by MGPC, GWU; NIH National Center for Advancing Translational Sciences [UL1TR000075]; Clinical and Translational Science Institute at Children's National Medical Center [CTSI-CN to A.H.]; and the Georgetown University Dean's Pilot [GX4002-753 to N.G.]."
  ], 
  "acks": " ", 
  "authors": [
    " Prakriti Mudvari", 
    " Mercedeh Movassagh", 
    " Kamran Kowsari", 
    " Ali Seyfi", 
    " Maria Kokkinaki", 
    " Nathan J Edwards", 
    " Nady Golestaneh", 
    " Anelia Horvath"
  ], 
  "keyWords": [
    "snplice variants", 
    [
      "sequencing", 
      "splicing", 
      "reading", 
      "variant", 
      "genomics"
    ]
  ], 
  "sourcelinks": [
    "http://srv00.ibbe.cnr.it/ASPicDB/newresults.php?organism-human", 
    "http://samtools.sourceforge.net/mpileup-.shtml", 
    "https://code.google.com/p-/snplice", 
    "https://code.goo"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-07T01:08:27Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/samtools/", 
    "languages": [
      "Perl", 
      "C"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/jmarshall/", 
        "name": "John Marshall"
      }, 
      {
        "url": "https://sourceforge.net/u/dvoet/", 
        "name": "Douglas Voet"
      }, 
      {
        "url": "https://sourceforge.net/u/bhandsaker/", 
        "name": "Bob Handsaker"
      }, 
      {
        "url": "https://sourceforge.net/u/lh3lh3/", 
        "name": "lh3"
      }, 
      {
        "url": "https://sourceforge.net/u/petulda/", 
        "name": "Petr Danecek"
      }, 
      {
        "url": "https://sourceforge.net/u/brilliantred/", 
        "name": "Kathleen Tibbetts"
      }, 
      {
        "url": "https://sourceforge.net/u/tfenne/", 
        "name": "Tim Fennell"
      }, 
      {
        "url": "https://sourceforge.net/u/alecw/", 
        "name": "Alec Wysoker"
      }, 
      {
        "url": "https://sourceforge.net/u/ljostins/", 
        "name": "Luke Jostins"
      }, 
      {
        "url": "https://sourceforge.net/u/ruanjue/", 
        "name": "Jue Ruan"
      }
    ], 
    "Development Status": [
      {
        "status": "5 - Production/Stable"
      }
    ], 
    "license": [
      {
        "name": "BSD License"
      }, 
      {
        "name": "MIT License"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu603", 
  "name": "Snowball resampling combined with distancebased regression to discover transcriptional consequences of a driver mutation", 
  "links": [
    "http://www.icgc.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://cran.r-project.org/web/packages/DES", 
    "http://bioinfo.mc.vanderbilt.edu/DES"
  ], 
  "title": "Gene expression Snowball: resampling combined with distance-based regression to discover transcriptional consequences of a driver mutation", 
  "toolName": "Gene expression Snowball: resampling combined with distance-based regression to discover transcriptional consequences of a driver mutation", 
  "abstract": "Motivation: Large-scale cancer genomic studies, such as The Cancer Genome Atlas (TCGA), have profiled multidimensional genomic data, including mutation and expression profiles on a variety of cancer cell types, to uncover the molecular mechanism of cancerogenesis. More than a hundred driver mutations have been characterized that confer the advantage of cell growth. However, how driver mutations regulate the transcriptome to affect cellular functions remains largely unex-plored. Differential analysis of gene expression relative to a driver mutation on patient samples could provide us with new insights in understanding driver mutation dysregulation in tumor genome and developing personalized treatment strategies. Results: Here, we introduce the Snowball approach as a highly sensitive statistical analysis method to identify transcriptional signatures that are affected by a recurrent driver mutation. Snowball utilizes a resampling-based approach and combines a distance-based regression framework to assign a robust ranking index of genes based on their aggregated association with the presence of the mutation, and further selects the top significant genes for downstream data analyses or experiments. In our application of the Snowball approach to both synthesized and TCGA data, we demonstrated that it outper-forms the standard methods and provides more accurate inferences to the functional effects and transcriptional dysregulation of driver mutations. Availability and implementation: R package and source code are available from CRAN at http://cran.r-project.org/web/packages/DES", 
  "summary": "Firstly, a whole genome gene expression matrix combined with a known driver mutation measurement on a patient cohort is resampled on gene dimension to generated B number of matrices with a fixed number of d genes, each containing a specific gene Xi. Secondly, the resulting matrices are further resampled on the sample dimension to obtain an equal number of samples within each group.\nSnowball, Random forests and LIMMA approaches were compared when applied to our case study dataset (see the Methods section) for the identification of the top gene list whose expression profile exhibited significant association with the BRAF mutation status.", 
  "affiliations": [
    " Department of Biomedical Informatics", 
    " Department of Epidemiology and Biostatistics Case Western Reserve University "
  ], 
  "grants": [
    "Funding: National Institutes of Health Grants (R01LM011177, R03CA167695, P50CA095103, P50CA098131 and P30CA068485), Ingram Professorship Funds (to Z.Z.", 
    "), and supported in part by National Science Foundation Grant (DMS-0306202 to J.S.)."
  ], 
  "acks": " The authors would like to thank Drs. Ramkrishna Mitra and Xi Chen for their helpful discussion to improve the results. The authors thank the three anonymous reviewers whose comments greatly helped improve the quality of this work. Funding: National Institutes of Health Grants (R01LM011177, R03CA167695, P50CA095103, P50CA098131 and P30CA068485), Ingram Professorship Funds (to Z.Z.), and The Robert J. Kleberg, Jr. and Helen C. Kleberg Foundation (to Z.Z.), and supported in part by National Science Foundation Grant (DMS-0306202 to J.S.). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health and the National Science Foundation. Conflict of Interest: none declared. ", 
  "authors": [
    " Yaomin Xu", 
    " Xingyi Guo", 
    " Jiayang Sun", 
    " Zhongming Zhao"
  ], 
  "keyWords": [
    [
      "snowball", 
      "genes", 
      "mutational"
    ]
  ], 
  "sourcelinks": [
    "http://cran.r-project.org/web/packages/DES", 
    "http://bioinfo.mc.vanderbilt.edu/DES"
  ], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-09-06T04:48:38Z"
}{
  "doi": "10.1093/bioinformatics/btu655", 
  "name": "SNPsnap a Webbased tool for identification and annotation of matched SNPs", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.broadinstitute.org/mpg/snpsnap"
  ], 
  "title": "Genome analysis SNPsnap: a Web-based tool for identification and annotation of matched SNPs", 
  "toolName": "Genome analysis SNPsnap: a Web-based tool for identification and annotation of matched SNPs", 
  "abstract": "An important computational step following genome-wide association studies (GWAS) is to assess whether disease or trait-associated single-nucleotide polymorphisms (SNPs) enrich for particular biological annotations. SNP-based enrichment analysis needs to account for biases such as co-localization of GWAS signals to gene-dense and high linkage disequilibrium (LD) regions, and correlations of gene size, location and function. The SNPsnap Web server enables SNP-based enrichment analysis by providing matched sets of SNPs that can be used to calibrate background expectations. Specifically, SNPsnap efficiently identifies sets of randomly drawn SNPs that are matched to a set of query SNPs based on allele frequency, number of SNPs in LD, distance to nearest gene and gene density. Availability and implementation: SNPsnap server is available at", 
  "summary": "A typical first step following GWAS is to assess whether associated loci as a group implicate biological pathways (Wang et al., 2010), or whether associated single-nucleotide polymorphisms (SNPs) are enriched for annotations such as non-coding functional elements (Ward and Kellis, 2012) or missense variants (Lango Allen et al., 2010).\nThe SNPsnap Web server identifies randomly selected SNPs with similar genetic properties as a set of query (associated) SNPs. Random SNPs are matched based on minor allele frequency, number of SNPs in LD (LD buddies), distance to nearest gene and number of nearby genes (gene density).", 
  "affiliations": [], 
  "grants": [
    "Funding: T.H.P."
  ], 
  "acks": " The authors thank The Genetic Investigation of ANthropometric Traits (GIANT) Consortium height working group for useful discussions. ", 
  "authors": [
    " Tune H Pers", 
    " Pascal Timshel", 
    " Joel N Hirschhorn", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    [
      "associations", 
      "genetically", 
      "snpsnap", 
      "matching"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-15T02:56:07Z"
}{
  "doi": "10.1093/bioinformatics/btu755", 
  "name": "SNVPPILP refined SNV calling for tumor data using perfect phylogenies and ILP", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://sourceforge.net/apps"
  ], 
  "title": "SNV-PPILP: refined SNV calling for tumor data using perfect phylogenies and ILP", 
  "toolName": "https://sourceforge", 
  "abstract": "Motivation: Recent studies sequenced tumor samples from the same progenitor at different development stages and showed that by taking into account the phylogeny of this development, single-nucleotide variant (SNV) calling can be improved. Accurate SNV calls can better reveal early-stage tumors, identify mechanisms of cancer progression or help in drug targeting. Results: We present SNV-PPILP, a fast and easy to use tool for refining GATK's Unified Genotyper SNV calls, for multiple samples assumed to form a phylogeny. We tested SNV-PPILP on simulated data, with a varying number of samples, SNVs, read coverage and violations of the perfect phylogeny assumption. We always match or improve the accuracy of GATK, with a significant improvement on low read coverage.", 
  "summary": "We tested SNV-PPILP on simulated data, with a varying number of samples, SNVs, read coverage and violations of the perfect phylogeny assumption.\n(2013), by proposing a tool for refining the SNV calling of GATK's Unified Genotyper (McKenna et al., 2010), the state-of-the-art SNV multi-sample caller for next-generation sequencing data.\nIn this note, we present SNV-PPILP (SNV calling with Perfect Phylogenies and Integer Linear Programming), a tool for refining GATK's Unified Genotyper SNV calls for multiple samples.\nThe F measure of GATK's Unified Genotyper's calls (column A), SNV-PPILP's calls (column B) and of the method of Salari et al.", 
  "affiliations": [
    " Department of Computer Science Helsinki Institute for Information Technology HIIT University of Helsinki "
  ], 
  "grants": [
    "Funding\nThis work was supported by Academy of Finland [250345] (CoECGR) and [274977 to A.I.T.]."
  ], 
  "acks": " We thank the anonymous referees for very helpful comments. This work was supported by Academy of Finland (CoECGR) and [274977 to A.I.T.]. Conflict of Interest: none declared. ", 
  "authors": [
    " Karen E Van Rens", 
    " Veli M\u00e4 Kinen", 
    " Alexandru I Tomescu"
  ], 
  "keyWords": [
    [
      "columns", 
      "mutational", 
      "samples", 
      "snvs", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "https://sourceforge.net/apps"
  ], 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-11-15T04:10:48Z"
}{
  "doi": "10.1093/bioinformatics/btu494", 
  "name": "STAMP statistical analysis of taxonomic and functional profiles", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://kiwi.cs.dal.ca/Software/STAMP"
  ], 
  "title": "Genome analysis STAMP: statistical analysis of taxonomic and functional profiles", 
  "toolName": "Genome analysis STAMP: statistical analysis of taxonomic and functional profiles", 
  "abstract": "STAMP is a graphical software package that provides statistical hypothesis tests and exploratory plots for analysing taxo-nomic and functional profiles. It supports tests for comparing pairs of samples or samples organized into two or more treatment groups. Effect sizes and confidence intervals are provided to allow critical assessment of the biological relevancy of test results. A user-friendly graphical interface permits easy exploration of statistical results and generation of publication-quality plots. Availability and implementation: STAMP is licensed under the GNU GPL. Python source code and binaries are available from our website at", 
  "summary": "Extended error bar plots (e.g. Fig. 1c) provide a single figure indicating statistically significant features along with the P-values, effect sizes and confidence intervals.\nHere we use STAMP to examine the taxonomic profiles of 44 CBM communities sampled from drilled cores, shallow (51000 mbs) and deep (1000 mbs) core cuttings, and produced waters (Supplementary Methods; An et al., 2013).\nHere we use STAMP to compare COG profiles (Supplementary Methods) of the non-photosynthetic Melainabacteria with the Oxyphotobacteria, the class name proposed by Soo et al.", 
  "affiliations": [
    " Dalhousie University", 
    " School of Chemistry and Molecular Biosciences Australian Centre for Ecogenomics The University of Queensland "
  ], 
  "grants": [
    "Funding: D.H.P.", 
    "are supported by a Discovery Outstanding Researcher Award (DORA) and Queen Elizabeth II Fellowship from the\n\nAustralian Research Council, grants DP120103498 and DP1093175, respectively."
  ], 
  "acks": " ", 
  "authors": [
    " Donovan H Parks", 
    " Gene W Tyson", 
    " Philip Hugenholtz", 
    " Robert G Beiko", 
    " John Hancock"
  ], 
  "keyWords": [
    "stamp statistical", 
    [
      "plots", 
      "testing", 
      "statistically", 
      "profiling", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://kiwi.cs.dal.ca/Software/STAMP"
  ], 
  "technologies": [], 
  "dateCreated": "2014-07-25T04:43:31Z"
}{
  "doi": "10.1093/bioinformatics/btu818", 
  "name": "Sputnik ad hoc distributed computation", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://github.com"
  ], 
  "title": "Sputnik: ad hoc distributed computation", 
  "toolName": "github.com", 
  "abstract": "Motivation: In bioinformatic applications, computationally demanding algorithms are often paral-lelized to speed up computation. Nevertheless, setting up computational environments for distributed computation is often tedious. Aim of this project were the lightweight ad hoc set up and fault-tolerant computation requiring only a Java runtime, no administrator rights, while utilizing all CPU cores most effectively. Results: The Sputnik framework provides ad hoc distributed computation on the Java Virtual Machine which uses all supplied CPU cores fully. It provides a graphical user interface for deployment setup and a web user interface displaying the current status of current computation jobs. Neither a permanent setup nor administrator privileges are required. We demonstrate the utility of our approach on feature selection of microarray data.", 
  "summary": "Also unlike the JPPF framework (www.jppf.org) Sputnik achieves a full utilization of all available CPU cores during job execution through its batch-wise task distribution to the worker.\nDistributed computation via Sputnik is fault-tolerant with respect to severe failures at the worker nodes because in that case the Sputnik server will reassign tasks of the crashed worker node to other worker nodes.\nSputnik schedules the tasks of a computation job such that the CPU utilization of the workers is maximized.\nThe scheduling strategy of Sputnik tries to have 2n task at a worker that performs n computations in parallel.", 
  "affiliations": [
    " Core Unit Medical Systems Biology"
  ], 
  "grants": [
    "Funding\nThe research leading to these results has received funding from the European Community's Seventh Framework Programme [FP7/20072013] under grant 55 agreement n602783 (to H.A.K."
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://github.com"
  ], 
  "acks": " ", 
  "authors": [
    " Gunnar V\u00f6 Lkel", 
    " Ludwig Lausser", 
    " Florian Schmid", 
    " Johann M Kraus", 
    " Hans A Kestler"
  ], 
  "keyWords": [
    [
      "tasks", 
      "computationally", 
      "parallelization", 
      "workers", 
      "jppf", 
      "runtimes", 
      "sputnik", 
      "data"
    ]
  ], 
  "github_data": {
    "name": "node-v0.x-archive", 
    "contributors": [
      {
        "contributions": 1, 
        "html_url": "https://github.com/orangemocha"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/works", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/works", 
        "name": "works"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.7", 
        "name": "v0.12.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.6", 
        "name": "v0.12.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.5", 
        "name": "v0.12.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.4", 
        "name": "v0.12.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.3", 
        "name": "v0.12.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.2", 
        "name": "v0.12.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.1", 
        "name": "v0.12.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.12.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.12.0", 
        "name": "v0.12.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.16", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.16", 
        "name": "v0.11.16"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.15", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.15", 
        "name": "v0.11.15"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.14", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.14", 
        "name": "v0.11.14"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.13", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.13", 
        "name": "v0.11.13"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.12", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.12", 
        "name": "v0.11.12"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.11", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.11", 
        "name": "v0.11.11"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.10", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.10", 
        "name": "v0.11.10"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.9", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.9", 
        "name": "v0.11.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.8", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.8", 
        "name": "v0.11.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.7", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.7", 
        "name": "v0.11.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.6", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.6", 
        "name": "v0.11.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.5", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.5", 
        "name": "v0.11.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.4", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.4", 
        "name": "v0.11.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.3", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.3", 
        "name": "v0.11.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.40", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.40", 
        "name": "v0.10.40"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.39", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.39", 
        "name": "v0.10.39"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.38", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.38", 
        "name": "v0.10.38"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/zipball/v0.10.37", 
        "tarball_url": "https://api.github.com/repos/nodejs/node-v0.x-archive/tarball/v0.10.37", 
        "name": "v0.10.37"
      }
    ], 
    "created_at": "2009-05-27T16:29:46Z", 
    "updated_at": "2016-08-09T16:10:45Z", 
    "languages": [], 
    "subscribers": [
      {
        "html_url": "https://github.com/isaacs"
      }, 
      {
        "html_url": "https://github.com/koichik"
      }, 
      {
        "html_url": "https://github.com/igorzi"
      }, 
      {
        "html_url": "https://github.com/kunoichi-co"
      }, 
      {
        "html_url": "https://github.com/carloslemes"
      }, 
      {
        "html_url": "https://github.com/aboyon"
      }, 
      {
        "html_url": "https://github.com/arden"
      }, 
      {
        "html_url": "https://github.com/guyoun"
      }, 
      {
        "html_url": "https://github.com/vissul"
      }, 
      {
        "html_url": "https://github.com/bdtgzj"
      }, 
      {
        "html_url": "https://github.com/santigimeno"
      }, 
      {
        "html_url": "https://github.com/chrisbateskeegan"
      }, 
      {
        "html_url": "https://github.com/FlashSoft"
      }, 
      {
        "html_url": "https://github.com/charlesgan"
      }, 
      {
        "html_url": "https://github.com/BlaneCordes"
      }, 
      {
        "html_url": "https://github.com/mdgoody"
      }, 
      {
        "html_url": "https://github.com/gindis"
      }, 
      {
        "html_url": "https://github.com/chrisdickinson"
      }, 
      {
        "html_url": "https://github.com/jorgenio"
      }, 
      {
        "html_url": "https://github.com/joityui"
      }, 
      {
        "html_url": "https://github.com/wxnet2013"
      }, 
      {
        "html_url": "https://github.com/be5invis"
      }, 
      {
        "html_url": "https://github.com/ashleymcfarland"
      }, 
      {
        "html_url": "https://github.com/jimxl"
      }, 
      {
        "html_url": "https://github.com/bluefisher80"
      }, 
      {
        "html_url": "https://github.com/subhaze"
      }, 
      {
        "html_url": "https://github.com/yaoming6046"
      }, 
      {
        "html_url": "https://github.com/hirozhang"
      }, 
      {
        "html_url": "https://github.com/rhyw"
      }, 
      {
        "html_url": "https://github.com/rhaldkhein"
      }
    ], 
    "owner": "https://github.com/nodejs", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-12-14T01:33:54Z"
}{
  "doi": "10.1093/bioinformatics/btu756", 
  "name": "SplitMEM a graphical algorithm for pangenome analysis with suffix skips", 
  "links": [
    "http://splitmem.sourceforge.net", 
    "http://splitmem.sourceforge.net.1.2", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://hallirmm.ccsd.cnrs.fr/lirmm-00950983.Chikhi,R"
  ], 
  "title": "Genome analysis SplitMEM: a graphical algorithm for pan-genome analysis with suffix skips", 
  "toolName": "splitmem", 
  "abstract": "Motivation: Genomics is expanding from a single reference per species paradigm into a more comprehensive pan-genome approach that analyzes multiple individuals together. A compressed de Bruijn graph is a sophisticated data structure for representing the genomes of entire populations. It robustly encodes shared segments, simple single-nucleotide polymorphisms and complex structural variations far beyond what can be represented in a collection of linear sequences alone. Results: We explore deep topological relationships between suffix trees and compressed de Bruijn graphs and introduce an algorithm, splitMEM, that directly constructs the compressed de Bruijn graph in time and space linear to the total number of genomes for a given maximum genome size. We introduce suffix skips to traverse several suffix links simultaneously and use them to efficiently decompose maximal exact matches into graph nodes. We demonstrate the utility of splitMEM by analyzing the nine-strain pan-genome of Bacillus anthracis and up to 62 strains of Escherichia coli, revealing their core-genome properties. Availability and implementation: Source code and documentation available open-source http://splitmem.sourceforge.net.", 
  "summary": "Results: We explore deep topological relationships between suffix trees and compressed de Bruijn graphs and introduce an algorithm, splitMEM, that directly constructs the compressed de Bruijn graph in time and space linear to the total number of genomes for a given maximum genome size.\n(2014) presents an algorithm for identifying the nodes and edges in a compressed de Bruijn graph in linear time from the suffix tree of a sequence, although this also requires exhaustively evaluating each k-mer in the sequence.\nThe basis of our algorithm is deriving the set of compressed de Bruijn graph nodes from the set of MEM!k nodes in the suffix tree, i.e. internal nodes that represent MEMs of length !", 
  "affiliations": [
    " Simons Center for Quantitative Biology Cold Spring Harbor Laboratory Cold Spring Harbor "
  ], 
  "grants": [
    "Funding: This project was supported in part by National Institutes of Health award [R01-HG006677] and National Science Foundation awards [DBI-126383 and DBI-1350041 to M.C.S.]."
  ], 
  "acks": " The authors would like to acknowledge Steven Skiena, Art Delcher, Adam Phillippy, Cole Trapnell, Mihai Pop and Steven Salzberg for helpful discussions leading to this work. ", 
  "authors": [
    " Shoshana Marcus", 
    " Hayan Lee", 
    " Michael C Schatz"
  ], 
  "keyWords": [
    [
      "genomics", 
      "algorithmic", 
      "graphs", 
      "mems", 
      "sequencing", 
      "nodes", 
      "suffixes"
    ]
  ], 
  "sourcelinks": [
    "http://splitmem.sourceforge.net", 
    "http://splitmem.sourceforge.net.1.2", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-15T04:10:48Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/splitmem/", 
    "languages": [
      "C++"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/hayanlee/", 
        "name": "Hayan Lee"
      }, 
      {
        "url": "https://sourceforge.net/u/smarcus/", 
        "name": "Shoshana Marcus"
      }, 
      {
        "url": "https://sourceforge.net/u/mcschatz/", 
        "name": "Michael Schatz"
      }
    ], 
    "Development Status": [
      {
        "status": "5 - Production/Stable"
      }
    ], 
    "license": [
      {
        "name": "Apache License V2.0"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu674", 
  "name": "Statistical significance of variables driving systematic variation in highdimensional data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Gene expression Statistical significance of variables driving systematic variation in high-dimensional data", 
  "toolName": "Gene expression Statistical significance of variables driving systematic variation in high-dimensional data", 
  "abstract": "Motivation: There are a number of well-established methods such as principal component analysis (PCA) for automatically capturing systematic variation due to latent variables in large-scale genomic data. PCA and related methods may directly provide a quantitative characterization of a complex biological variable that is otherwise difficult to precisely define or model. An unsolved problem in this context is how to systematically identify the genomic variables that are drivers of systematic variation captured by PCA. Principal components (PCs) (and other estimates of systematic variation) are directly constructed from the genomic variables themselves, making measures of statistical significance artificially inflated when using conventional methods due to over-fitting. Results: We introduce a new approach called the jackstraw that allows one to accurately identify genomic variables that are statistically significantly associated with any subset or linear combination of PCs. The proposed method can greatly simplify complex significance testing problems encountered in genomics and can be used to identify the genomic variables significantly associated with latent variables. Using simulation, we demonstrate that our method attains accurate measures of statistical significance over a range of relevant scenarios. We consider yeast cell-cycle gene expression data, and show that the proposed method can be used to straightforwardly identify genes that are cell-cycle regulated with an accurate measure of statistical significance. We also analyze gene expression data from post-trauma patients, allowing the gene expression data to provide a molecularly driven phenotype. Using our method, we find a greater enrichment for inflammatory-related gene sets compared to the original analysis that uses a clinically defined, although likely imprecise, phenotype. The proposed method provides a useful bridge between large-scale quan-tifications of systematic variation and gene-level significance analyses. Availability and implementation: An R software package, called", 
  "summary": "The hypothesis test is now simply whether gene i is associated with r^=2 latent variables estimated by the top two PCs. We applied the proposed method (with s = 100 and B=2  m) to test this hypothesis and identified a large number of genes associated with yeast cell-cycle regulation.\nThe generalized proposed method allows us to compute statistical significance measures of associations with a subset of PCs. When testing for associations with the first PC while adjusting for the second PC, 1666 genes were called statistically significant at FDR 1%, with the estimated proportion of null variables ^ 0=34:4%.", 
  "affiliations": [
    " Lewis-Sigler Institute for Integrative Genomics", 
    " Department of Molecular Biology Princeton University "
  ], 
  "grants": [
    "ACKNOWLEDGEMENTS\nThis research was supported in part by NIH grant HG002913 and Office of Naval Research grant N00014-12-1-0764."
  ], 
  "acks": " This research was supported in part by NIH grant HG002913 and Office of Naval Research grant N00014-12-1-0764. of interest: none declared. ", 
  "authors": [
    " Neo Christopher Chung", 
    " John D Storey", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "methods", 
      "variables", 
      "genes", 
      "analysis", 
      "signaling", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-10-22T05:07:31Z"
}{
  "doi": "10.1093/bioinformatics/btu552", 
  "name": "subSeq Determining Appropriate Sequencing Depth Through Efficient Read Subsampling", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses", 
    "http://github.com/StoreyLab/subSeq"
  ], 
  "title": "Gene expression subSeq: Determining Appropriate Sequencing Depth Through Efficient Read Subsampling", 
  "toolName": "subSeq", 
  "abstract": "Motivation: Next-generation sequencing experiments, such as RNA-Seq, play an increasingly important role in biological research. One complication is that the power and accuracy of such experiments depend substantially on the number of reads sequenced, so it is important and challenging to determine the optimal read depth for an experiment or to verify whether one has adequate depth in an existing experiment. Results: By randomly sampling lower depths from a sequencing experiment and determining where the saturation of power and accuracy occurs, one can determine what the most useful depth should be for future experiments, and furthermore, confirm whether an existing experiment had sufficient depth to justify its conclusions. We introduce the subSeq R package, which uses a novel efficient approach to perform this subsampling and to calculate informative metrics at each depth. Availability and Implementation: The subSeq R package is available at", 
  "summary": "For instance, in RNA-Seq greater read depth is known to increase the power of differential expression testing and the accuracy of expression estimates (Liu et al., 2013; Tarazona et al., 2011).\nStudies have used random subsampling to propose guidelines for future experiments (Black et al., 2014; Liu et al., 2014), to perform a survey of different RNA-Seq analysis methods at varying read depths (Labaj et al., 2011; Liu et al., 2013; Rapaport et al., 2013), or to demonstrate that they had achieved adequate read depth (Daines et al., 2011; Toung et al., 2011; Wang et al., 2011).", 
  "affiliations": [
    " Lewis-Sigler Institute for Integrative Genomics"
  ], 
  "grants": [
    "Funding: This work was supported in part by NIH (R01 HG002913)."
  ], 
  "sourcelinks": [
    "http://github.com/StoreyLab/subSeq"
  ], 
  "acks": " ", 
  "authors": [
    " David G Robinson", 
    " John D Storey"
  ], 
  "keyWords": [
    [
      "depths", 
      "reads", 
      "experiments", 
      "subsamplings", 
      "bioinformatics"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/StoreyLab/subSeq/master/LICENSE"
      }
    ], 
    "name": "subSeq", 
    "contributors": [
      {
        "contributions": 19, 
        "html_url": "https://github.com/dgrtwo"
      }, 
      {
        "contributions": 16, 
        "html_url": "https://github.com/rileysg"
      }, 
      {
        "contributions": 8, 
        "html_url": "https://github.com/ajbass"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/mikelove"
      }
    ], 
    "versions": [], 
    "created_at": "2014-07-26T22:46:54Z", 
    "updated_at": "2016-06-22T11:29:48Z", 
    "languages": [
      "R"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/dgrtwo"
      }, 
      {
        "html_url": "https://github.com/jdstorey"
      }, 
      {
        "html_url": "https://github.com/ajbass"
      }, 
      {
        "html_url": "https://github.com/ginolhac"
      }
    ], 
    "owner": "https://github.com/StoreyLab", 
    "homepage": null
  }, 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-09-05T05:34:33Z"
}{
  "doi": "10.1093/bioinformatics/btu622", 
  "name": "switchBox an R package for kTop Scoring Pairs classifier development", 
  "links": [
    "http://hal.archives-ouvertes.fr/docs/00/78/48/69/PDF/Article.pdf", 
    "http://www.bioconductor.org", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Gene expression switchBox: an R package for k\u2013Top Scoring Pairs classifier development", 
  "toolName": "Gene expression switchBox: an R package for k\u2013Top Scoring Pairs classifier development", 
  "abstract": "k\u2013Top Scoring Pairs (kTSP) is a classification method for prediction from high-throughput data based on a set of the paired measurements. Each of the two possible orderings of a pair of measurements (e.g. a reversal in the expression of two genes) is associated with one of two classes. The kTSP prediction rule is the aggregation of voting among such individual two-feature decision rules based on order switching. kTSP, like its predecessor, Top Scoring Pair (TSP), is a parameter-free classifier relying only on ranking of a small subset of features, rendering it robust to noise and potentially easy to interpret in biological terms. In contrast to TSP, kTSP has comparable accuracy to standard genomics classification techniques, including Support Vector Machines and Prediction Analysis for Microarrays. Here, we describe 'switchBox', an R package for kTSP-based prediction. Availability: The 'switchBox' package is freely available from Bioconductor: http://www.bioconductor.org.", 
  "summary": "The first and simplest of these rank-based methods, the Top Scoring Pair (TSP) classifier, in which the decision rule is entirely determined by the ordering of two features (i.e. the relative expression of two genes), was introduced in Geman et al.\nBelow we briefly show how to train a kTSP classifier for breast cancer recurrence within 5 years using gene expression data from Marchionni et al.\n4 CONCLUSION We introduced `switchBox', an R package for kTSP classifier with a robust procedure for pair selection as previously described in Afsari et al.", 
  "affiliations": [
    " Department of Applied Mathematics and Statistics Johns Hopkins University ", 
    " Department of Oncology School of Medicine Sidney Kimmel Comprehensive Cancer Center Johns Hopkins University "
  ], 
  "grants": [
    "by the NIH-NCRR [UL1 RR 025005].", 
    "was supported by the National Institutes of Health (NIH-)NCI [P30 CA006973]; L.M.", 
    "Funding: L.M.", 
    "by NIH-NCI [K25 CA141053]."
  ], 
  "acks": " ", 
  "authors": [
    " Bahman Afsari", 
    " Elana J Fertig", 
    " Donald Geman", 
    " Luigi Marchionni", 
    " Janet Kelso"
  ], 
  "keyWords": [
    "pairs classifier", 
    [
      "based", 
      "classification", 
      "ktsp", 
      "paired", 
      "cancers", 
      "bioinformatics", 
      "basing"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-09-28T00:25:27Z"
}{
  "doi": "10.1093/bioinformatics/btu578", 
  "name": "String graph construction using incremental hashing", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.cs.tau.ac.il"
  ], 
  "title": "Sequence analysis String graph construction using incremental hashing", 
  "toolName": "Sequence analysis String graph construction using incremental hashing", 
  "abstract": "Motivation: New sequencing technologies generate larger amount of short reads data at decreasing cost. De novo sequence assembly is the problem of combining these reads back to the original genome sequence, without relying on a reference genome. This presents algo-rithmic and computational challenges, especially for long and repetitive genome sequences. Most existing approaches to the assembly problem operate in the framework of de Bruijn graphs. Yet, a number of recent works use the paradigm of string graph, using a variety of methods for storing and processing suffixes and prefixes, like suffix arrays, the Burrows\u2013Wheeler transform or the FM index. Our work is motivated by a search for new approaches to constructing the string graph, using alternative yet simple data structures and algorithmic concepts. Results: We introduce a novel hash-based method for constructing the string graph. We use incremental hashing, and specifically a modification of the Karp\u2013Rabin fingerprint, and Bloom filters. Using these probabilistic methods might create false-positive and false-negative edges during the algorithm's execution, but these are all detected and corrected. The advantages of the proposed approach over existing methods are its simplicity and the incorporation of established probabilistic techniques in the context of de novo genome sequen-cing. Our preliminary implementation is favorably comparable with the first string graph construction of Simpson and Durbin (2010) (but not with subsequent improvements). Further research and optimizations will hopefully enable the algorithm to be incorporated, with noticeable performance improvement, in state-of-the-art string graph-based assemblers. Availability and implementation: A beta version of all source code used in this work can be downloaded from", 
  "summary": "This was achieved by first producing a relevant subset of all overlaps between read pairs (using matches between smaller strings as a filter), and then outputting the set of irreducible edges by applying a traversal algorithm on a graph representing the sorted set of candidate overlaps.\n(2) Overlaps phase: using the index generated in the previous step, the algorithm finds, for every read s, the set of hash values of prefixes of reads ri that overlap a suffix of s (see Fig. 4).\nWithout loss of generality, the hash value of the overlapping part appears in the root of the ol-prefixes tree of read r.", 
  "affiliations": [
    " School of Computer Science Tel-Aviv University "
  ], 
  "grants": [
    "Funding: This research was partially funded by an Agilent Technologies University Relations grant, by the Deutsch Institute and by the Safra Center for Bioinformatics at TelAviv University."
  ], 
  "acks": " The authors thank Richard Durbin, Jared Simpson and the anonymous referees for helpful suggestions. ", 
  "authors": [
    " Ilan Ben-Bassat", 
    " Benny Chor"
  ], 
  "keyWords": [
    [
      "assembling", 
      "algorithmic", 
      "overlapping", 
      "reads", 
      "edges", 
      "false", 
      "strings", 
      "hashing"
    ]
  ], 
  "sourcelinks": [
    "http://www.cs.tau.ac.il"
  ], 
  "technologies": [], 
  "dateCreated": "2014-09-03T06:17:49Z"
}{
  "doi": "10.1093/bioinformatics/btu550", 
  "name": "SUBAcon a consensus algorithm for unifying the subcellular localization data of the Arabidopsis proteome", 
  "links": [
    "http://atted.jp", 
    "http://suba.plantenergy.uwa.edu.au", 
    "http://suba.plantenergy.uwa.edu.au/flatfile", 
    "http://suba.plantenergy", 
    "http://suba.plantenergy.uwa.edu.au/SUBAcon.html", 
    "http://suba.plantenergy.uwa.edu.au/ASURE", 
    "http://suba.plantenergy.uwa.edu.au//SUBAcon.html", 
    "http://suba.plantenergy.uwa", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Data and text mining SUBAcon: a consensus algorithm for unifying the subcellular localization data of the Arabidopsis proteome", 
  "toolName": "Data and text mining SUBAcon: a consensus algorithm for unifying the subcellular localization data of the Arabidopsis proteome", 
  "abstract": "Motivation: Knowing the subcellular location of proteins is critical for understanding their function and developing accurate networks representing eukaryotic biological processes. Many computational tools have been developed to predict proteome-wide subcellular location, and abundant experimental data from green fluorescent protein (GFP) tagging or mass spectrometry (MS) are available in the model plant, Arabidopsis. None of these approaches is error-free, and thus, results are often contradictory. Results: To help unify these multiple data sources, we have developed the SUBcellular Arabidopsis consensus (SUBAcon) algorithm, a naive Bayes classifier that integrates 22 computational prediction algorithms, experimental GFP and MS localizations, protein\u2013protein interaction and co-expression data to derive a consensus call and probability. SUBAcon classifies protein location in Arabidopsis more accurately than single predictors. Availability: SUBAcon is a useful tool for recovering proteome-wide subcellular locations of Arabidopsis proteins and is displayed in the SUBA3 database (http://suba.plantenergy.uwa.edu.au). The source code and input data is available through the SUBA3 server (http://suba.plantenergy.uwa.edu.au//SUBAcon.html) and the Arabidopsis SUbproteome REference (ASURE) training set can be ac-cessed using the ASURE web portal (http://suba.", 
  "summary": "Many computational tools have been developed to predict proteome-wide subcellular location, and abundant experimental data from green fluorescent protein (GFP) tagging or mass spectrometry (MS) are available in the model plant, Arabidopsis.\nHere, we fully describe the development of the Bayesian classifier SUBAcon that uses aggregated computational predictor outputs, direct MS, GFP localizations and now also indirect PPI and co-expression data to produce consensus calls for subcellular locations of Arabidopsis proteins.\nSUBAcon integrates 22 published predictors and four experimental data types to generate a probability distribution of subcellular location for each Arabidopsis protein.", 
  "affiliations": [
    " ARC Centre of Excellence in Plant Energy Biology The University of Western Australia ", 
    " Centre of Excellence in Computational Systems Biology The University of Western Australia "
  ], 
  "grants": [
    "]; and the Government of Western Australia through funding for the WA Centre of Excellence for Computational Systems Biology.", 
    "Funding: This work was supported by the Australian Research Council [CE0561495, CE140100008 to A.H.M."
  ], 
  "acks": " ", 
  "authors": [
    " Cornelia M Hooper", 
    " Sandra K Tanz", 
    " Ian R Castleden", 
    " Michael A Vacher", 
    " Ian D Small", 
    " A Harvey Millar"
  ], 
  "keyWords": [
    [
      "proteomics", 
      "subacon", 
      "data", 
      "locations", 
      "predictions"
    ]
  ], 
  "sourcelinks": [
    "http://suba.plantenergy.uwa.edu.au/SUBAcon.html", 
    "http://suba.plantenergy.uwa", 
    "http://suba.plantenergy.uwa.edu.au", 
    "http://suba.plantenergy", 
    "http://suba.plantenergy.uwa.edu.au//SUBAcon.html"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-23T04:14:10Z"
}{
  "doi": "10.1093/bioinformatics/btu667", 
  "name": "Tabhu tools for antibody humanization", 
  "links": [
    "http://www.biocomputing.it/tabhu", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses"
  ], 
  "title": "Structural bioinformatics Tabhu: tools for antibody humanization", 
  "toolName": "Structural bioinformatics Tabhu: tools for antibody humanization", 
  "abstract": "Antibodies are rapidly becoming essential tools in the clinical practice, given their ability to recognize their cognate antigens with high specificity and affinity, and a high yield at reasonable costs in model animals. Unfortunately, when administered to human patients, xenogeneic antibodies can elicit unwanted and dangerous immuno-genic responses. Antibody humanization methods are designed to produce molecules with a better safety profile still maintaining their ability to bind the antigen. This can be accomplished by grafting the non-human regions determining the antigen specificity into a suitable human template. Unfortunately, this procedure may results in a partial or complete loss of affinity of the grafted molecule that can be restored by back-mutating some of the residues of human origin to the corresponding murine ones. This trial-and-error procedure is hard and involves expensive and time-consuming experiments. Here we present tools for antibody humanization (Tabhu) a web server for antibody humanization. Tabhu includes tools for human template selection, grafting, back-mutation evaluation, antibody modelling and structural analysis, helping the user in all the critical steps of the humanization experiment protocol.", 
  "summary": "Taking advantage of our antibody structure prediction tools (Chailyan et al., 2011; Marcatili et al., 2008), upon user request, Tabhu builds the three-dimensional models of the mouse and humanized antibodies, runs the procheck and EDTSurf tools (Laskowski et al., 1996; Xu and Zhang, 2009) and alerts the user if the introduction of a back-mutation generates clashes or cavities, that the user can ignore or use as a guide to remove or introduce additional back-mutations.", 
  "affiliations": [
    " Department of Physics Sapienza University ", 
    " Center for Biological Sequence Analysis Department of Systems Biology Technical University of Denmark "
  ], 
  "grants": [
    "Funding: KAUST Award No."
  ], 
  "acks": " The authors are grateful to Daniel D'Andrea and all other members of the \" Sapienza \" Biocomputing unit for useful discussion and for testing the server. ", 
  "authors": [
    " Pier Paolo Olimpieri", 
    " Paolo Marcatili", 
    " Anna Tramontano", 
    " Istituto Pasteur", 
    " \u2013 Fondazione", 
    " Cenci Bolognetti", 
    " Rome", 
    " Italy Associate", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "antibody humanization", 
    [
      "antibodies", 
      "humanness", 
      "structures", 
      "tabhu", 
      "sequences"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-11T03:33:58Z"
}{
  "doi": "10.1093/bioinformatics/btu428", 
  "name": "TAPAS tools to assist the targeted protein quantification of human alternative splice variants", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://davinci.crg.es/tapas"
  ], 
  "title": "TAPAS: tools to assist the targeted protein quantification of human alternative splice variants", 
  "toolName": "TAPAS: tools to assist the targeted protein quantification of human alternative splice variants", 
  "abstract": "Motivation: In proteomes of higher eukaryotes, many alternative splice variants can only be detected by their shared peptides. This makes it highly challenging to use peptide-centric mass spectrometry to distinguish and to quantify protein isoforms resulting from alternative splicing events. Results: We have developed two complementary algorithms based on linear mathematical models to efficiently compute a minimal set of shared and unique peptides needed to quantify a set of isoforms and splice variants. Further, we developed a statistical method to estimate the splice variant abundances based on stable isotope labeled peptide quantities. The algorithms and databases are integrated in a web-based tool, and we have experimentally tested the limits of our quantification method using spiked proteins and cell extracts. Availability and implementation: The TAPAS server is available at", 
  "summary": "Results: We have developed two complementary algorithms based on linear mathematical models to efficiently compute a minimal set of shared and unique peptides needed to quantify a set of isoforms and splice variants.\nExperimental design: TAPAS generates lists of unique and shared peptides that can be used in SIL-based targeted proteomics approaches to quantify selected alternative spliced variants.\nAbsolute quantification of splice variants: TAPAS estimates the amounts of alternative splice variants by considering the abundance of peptides measured from SIL-based targeted MS\nTo estimate the protein abundances of selected splicing variants, TAPAS requires the observed abundance of peptides, identification of peptidases and splice variant sequences as inputs.", 
  "affiliations": [], 
  "grants": [
    "Funding: The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement Nr.", 
    "The CRG/UPF Proteomics Unit is part of the \"Plataforma de Recursos Biomoleculares y Bioinformaticos\" (Instituto de Salud Carlos III), supported by grant PT13/0001."
  ], 
  "acks": " ", 
  "authors": [
    " Jae-Seong Yang", 
    " Eduard Sabid", 
    " Luis Serrano", 
    " Christina Kiel"
  ], 
  "keyWords": [
    [
      "splicing", 
      "tapas", 
      "based", 
      "peptides", 
      "proteomics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-05T02:02:58Z"
}{
  "doi": "10.1093/bioinformatics/btu675", 
  "name": "SYSBIONS nested sampling for systems biology", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.theosysbio.bio.ic.ac.uk/resources/sysbions", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Systems biology SYSBIONS: nested sampling for systems biology", 
  "toolName": "Systems biology SYSBIONS: nested sampling for systems biology", 
  "abstract": "Motivation: Model selection is a fundamental part of the scientific process in systems biology. Given a set of competing hypotheses, we routinely wish to choose the one that best explains the observed data. In the Bayesian framework, models are compared via Bayes factors (the ratio of evidences), where a model's evidence is the support given to the model by the data. A parallel interest is inferring the distribution of the parameters that define a model. Nested sampling is a method for the computation of a model's evidence and the generation of samples from the posterior parameter distribution. Results: We present a C-based, GPU-accelerated implementation of nested sampling that is designed for biological applications. The algorithm follows a standard routine with optional extensions and additional features. We provide a number of methods for sampling from the prior subject to a likelihood constraint. Availability and implementation: The software SYSBIONS is available from http://www.theosysbio.bio.ic.ac.uk/resources/sysbions/", 
  "summary": "Nested sampling is a method for the computation of a model's evidence and the generation of samples from the posterior parameter distribution.\nNested sampling is a Bayesian method for evidence estimation and parameter inference for systems where a likelihood function can be defined (Skilling, 2006).\nA summary file of input and output information is created, documenting the number of live points, number of iterations, tolerance, sampling method and parameter ranges, followed by the evidence with standard deviation, the prior-to-posterior information gain and the means of all parameters and their standard deviations.\n(2014) Bayesian model comparison and parameter inference in systems biology using nested sampling.", 
  "affiliations": [
    " Centre for Integrative Systems Biology and Bioinformatics Department of Life Sciences Imperial College London "
  ], 
  "grants": [
    "Funding: This work was supported through a BBSRC PhD studentship to R.J. and an HFSP grant [RGP0061/2011] to P.K."
  ], 
  "acks": " We gratefully acknowledge the help and support of the Theoretical Systems Biology Group at Imperial College London. Funding: This work was supported through a BBSRC PhD studentship to R.J. and an HFSP grant to P.K. and M.P.H.S. Conflict of interest: none declared. ", 
  "authors": [
    " Rob Johnson", 
    " Paul Kirk", 
    " Michael P H Stumpf", 
    " Janet Kelso"
  ], 
  "keyWords": [
    [
      "methods", 
      "models", 
      "sampling", 
      "bioinformatics", 
      "bayesian", 
      "biological"
    ]
  ], 
  "sourcelinks": [
    "http://www.theosysbio.bio.ic.ac.uk/resources/sysbions"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-10-18T02:34:38Z"
}{
  "doi": "10.1093/bioinformatics/btu847", 
  "name": "The anisotropic network model web server at 2015 ANM 20", 
  "links": [
    "http://crd-legacy.lbl.gov/osni", 
    "http://anm.csb.pitt.edu", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://enm.lobos.nih.gov", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "The anisotropic network model web server at 2015 (ANM 2.0)", 
  "toolName": "The anisotropic network model web server at 2015 (ANM 2.0)", 
  "abstract": "The anisotropic network model (ANM) is one of the simplest yet powerful tools for exploring protein dynamics. Its main utility is to predict and visualize the collective motions of large complexes and assemblies near their equilibrium structures. The ANM server, introduced by us in 2006 helped making this tool more accessible to non-sophisticated users. We now provide a new version (ANM 2.0), which allows inclusion of nucleic acids and ligands in the network model and thus enables the investigation of the collective motions of protein\u2013DNA/RNA and \u2013ligand systems. The new version offers the flexibility of defining the system nodes and the interaction types and cutoffs. It also includes extensive improvements in hardware, software and graphical interfaces. Availability and implementation: ANM 2.0 is available at http://anm.csb.", 
  "summary": "Among ENMs, the anisotropic network model (ANM) and similar residue-based spring-and-node models, introduced 15 years ago (Atilgan et al., 2001; Doruker et al., 2000; Hinsen, 1998; Sanejouand and Tama, 2001), have found wide applications in molecular and structural biology due to their simplicity, yet proved successful for predicting the directions of large-scale functional motions in accord with experimental observations (Bahar et al., 2011; Bakan et al., 2009).", 
  "affiliations": [
    " Department of Computational and System Biology University of Pittsburgh "
  ], 
  "grants": [
    "We also added the core C and Matlab code for inclusion of nucleic acids and ligand atoms into the source distribution packages available at: http://anm.csb.pitt.edu/ anmdocs/source.html\nFunding\nThis work was sponsored by NIH Awards 1R01GM099738 and 5R01GM086238."
  ], 
  "acks": " ", 
  "authors": [
    " Eran Eyal", 
    " Gengkon Lum", 
    " Ivet Bahar"
  ], 
  "keyWords": [
    [
      "models", 
      "structures", 
      "dynamics", 
      "proteins"
    ]
  ], 
  "sourcelinks": [
    "http://anm.csb.pitt.edu"
  ], 
  "technologies": [
    "C", 
    "Perl"
  ], 
  "dateCreated": "2015-01-08T02:10:15Z"
}{
  "doi": "10.1093/bioinformatics/btu601", 
  "name": "The GOBLET training portal a global repository of bioinformatics training materials courses and trainers", 
  "links": [
    "http://mygoblet.org/training-portal", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Databases and ontologies The GOBLET training portal: a global repository of bioinformatics training materials, courses and trainers", 
  "toolName": "Databases and ontologies The GOBLET training portal: a global repository of bioinformatics training materials, courses and trainers", 
  "abstract": "Rapid technological advances have led to an explosion of biomedical data in recent years. The pace of change has inspired new collaborative approaches for sharing materials and resources to help train life scientists both in the use of cutting-edge bioinformatics tools and databases and in how to analyse and interpret large datasets. A prototype platform for sharing such training resources was recently created by the Bioinformatics Training Network (BTN). Building on this work, we have created a centralized portal for sharing training materials and courses, including a catalogue of trainers and course organizers , and an announcement service for training events. For course organizers, the portal provides opportunities to promote their training events; for trainers, the portal offers an environment for sharing materials , for gaining visibility for their work and promoting their skills; for trainees, it offers a convenient one-stop shop for finding suitable training resources and identifying relevant training events and activities locally and worldwide. Availability and implementation:", 
  "summary": "The portal inherits much of the functionality of the prototype BTN website (Schneider et al., 2012), extending its features to accommodate the diverse needs of global communities of life scientists: enhancements include (i) the addition of features such as the definition of fields for describing materials, to make them more discoverable, and (ii) the possibility to add course pages, linked to their associated materials, so that the portal is both a repository and a record of what is to be, and what was, taught at a given time, rather than just a bag of disconnected contents.\n3 CONCLUSIONS The GOBLET training portal is a pioneering global initiative to federate information relevant to bioinformatics, biocomputing, biocuration and computational biology trainers, courses and materials.", 
  "affiliations": [
    " Academis", 
    " SIB Swiss Institute of Bioinformatics", 
    " The Genome Analysis Centre", 
    " The University of Manchester", 
    " The Roslin Institute", 
    " CSIRO, Bioinformatics Core", 
    " Ontario Institute for Cancer Research", 
    " CSC -IT Center for Science Ltd", 
    " The Sainsbury Laboratory Norwich Research Park ", 
    " Department of Physics Sapienza University ", 
    " ELIXIR Wellcome Trust Genome Campus ", 
    " Instituto Gulbenkian de Ci^ encia", 
    " The Swedish University for Agricultural Sciences", 
    " European Molecular Biology Laboratory", 
    " Whitehead Institute for Biomedical Research MIT ", 
    " The Nowgen Centre", 
    " The University of New South Wales"
  ], 
  "grants": [
    "the AllBio consortium recently saw the advantage of exploiting the portal, rather than creating yet another training resource that would be unsupported when project funds cease.", 
    "Fudning: We are grateful to BBSRC strategic core funding from The Genome Analysis Centre (TGAC) for funding this Open Access article."
  ], 
  "acks": " The authors gratefully acknowledge the support of GOBLET's member organizations. ", 
  "authors": [
    " Manuel Corpas", 
    " Rafael C Jimenez", 
    " Erik Bongcam-Rudloff", 
    " Aidan Budd", 
    " Michelle D Brazas", 
    " Pedro L Fernandes", 
    " Bruno Gaeta", 
    " Celia Van Gelder", 
    " Eija Korpelainen", 
    " Fran Lewitter", 
    " Annette Mcgrath", 
    " Daniel Maclean", 
    " Patricia M Palagi", 
    " Kristian Rother", 
    " Jan Taylor", 
    " Allegra Via", 
    " Mick Watson", 
    " Maria Victoria Schneider", 
    " Teresa K Attwood"
  ], 
  "keyWords": [
    "materials courses", 
    [
      "course", 
      "training", 
      "portal", 
      "researchers", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-05T05:34:33Z"
}{
  "doi": "10.1093/bioinformatics/btu613", 
  "name": "The Ensembl REST API Ensembl Data for Any Language", 
  "links": [
    "http://wasabiapp.org", 
    "http://creativecommons.org/licenses/by/4.0", 
    "https://github.com", 
    "http://rest.ensembl.org/sequence/id", 
    "http://rest.ensembl.org", 
    "http://rest.ensembl.org/overlap/id/ENSG00000", 
    "http://github.com/Ensembl", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Databases and ontologies The Ensembl REST API: Ensembl Data for Any Language", 
  "toolName": "Ensembl", 
  "abstract": "Motivation: We present a Web service to access Ensembl data using Representational State Transfer (REST). The Ensembl REST server enables the easy retrieval of a wide range of Ensembl data by most programming languages, using standard formats such as JSON and FASTA while minimizing client work. We also introduce bindings to the popular Ensembl Variant Effect Predictor tool permitting large-scale programmatic variant analysis independent of any specific programming language. Availability and implementation: The Ensembl REST API can be accessed at http://rest.ensembl.org and source code is freely available under an Apache 2.0 license from", 
  "summary": "The Ensembl REST server enables the easy retrieval of a wide range of Ensembl data by most programming languages, using standard formats such as JSON and FASTA while minimizing client work.\nRNACentral (Bateman et al., 2011) displays non-coding gene models alongside Ensembl annotation in Genoverse (http:// genoverse.org/), a HTML5 genome browser, using data from our REST API.\nThe Ensembl REST API can be used to query the Ensembl data resources and tools from a variety of programming languages and enables flexible programmatic access previously only supported by our Perl API.", 
  "affiliations": [
    " European Bioinformatics Institute European Molecular Biology Laboratory Wellcome Trust Genome Campus ", 
    " Wellcome Trust Sanger Institute Wellcome Trust Genome Campus "
  ], 
  "grants": [
    "Funding: This work was supported by The Wellcome Trust (WT095908) and the European Molecular Biology Laboratory."
  ], 
  "sourcelinks": [
    "http://github.com/Ensembl", 
    "http://rest.ensembl.org", 
    "http://rest.ensembl.org/overlap/id/ENSG00000", 
    "https://github.com"
  ], 
  "acks": " The authors thank Glenn Proctor and Monika Byrne for their work on earlier REST API versions, Daniel Zerbino for his help in reviewing this paper and the Ensembl team at EMBL-EBI. ", 
  "authors": [
    " Andrew Yates", 
    " Kathryn Beal", 
    " Stephen Keenan", 
    " William Mclaren", 
    " Miguel Pignatelli", 
    " Graham R S Ritchie", 
    " Magali Ruffier", 
    " Kieron Taylor", 
    " Alessandro Vullo", 
    " Paul Flicek", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "genomics", 
      "http", 
      "ensembl", 
      "clients", 
      "rest", 
      "bioinformatics", 
      "data"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "Apache License 2.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/Ensembl/ensembl/release/85/LICENSE"
      }
    ], 
    "name": "ensembl", 
    "contributors": [
      {
        "contributions": 1017, 
        "html_url": "https://github.com/magaliruffier"
      }, 
      {
        "contributions": 960, 
        "html_url": "https://github.com/andrewyatz"
      }, 
      {
        "contributions": 338, 
        "html_url": "https://github.com/nerdstrike"
      }, 
      {
        "contributions": 248, 
        "html_url": "https://github.com/avullo"
      }, 
      {
        "contributions": 173, 
        "html_url": "https://github.com/ens-st3"
      }, 
      {
        "contributions": 121, 
        "html_url": "https://github.com/jgrg"
      }, 
      {
        "contributions": 116, 
        "html_url": "https://github.com/arnaudxk"
      }, 
      {
        "contributions": 107, 
        "html_url": "https://github.com/swingingsimian"
      }, 
      {
        "contributions": 71, 
        "html_url": "https://github.com/ens-ba1"
      }, 
      {
        "contributions": 63, 
        "html_url": "https://github.com/soupdragon"
      }, 
      {
        "contributions": 59, 
        "html_url": "https://github.com/danstaines"
      }, 
      {
        "contributions": 55, 
        "html_url": "https://github.com/khowe"
      }, 
      {
        "contributions": 54, 
        "html_url": "https://github.com/willmclaren"
      }, 
      {
        "contributions": 52, 
        "html_url": "https://github.com/ens-lgil"
      }, 
      {
        "contributions": 52, 
        "html_url": "https://github.com/mjg17"
      }, 
      {
        "contributions": 50, 
        "html_url": "https://github.com/kulesha-ebi"
      }, 
      {
        "contributions": 35, 
        "html_url": "https://github.com/lairdm"
      }, 
      {
        "contributions": 34, 
        "html_url": "https://github.com/RhodaK"
      }, 
      {
        "contributions": 33, 
        "html_url": "https://github.com/james-monkeyshines"
      }, 
      {
        "contributions": 33, 
        "html_url": "https://github.com/at7"
      }, 
      {
        "contributions": 24, 
        "html_url": "https://github.com/amonida"
      }, 
      {
        "contributions": 20, 
        "html_url": "https://github.com/skeenan"
      }, 
      {
        "contributions": 19, 
        "html_url": "https://github.com/ens-ds23"
      }, 
      {
        "contributions": 18, 
        "html_url": "https://github.com/jherrero"
      }, 
      {
        "contributions": 17, 
        "html_url": "https://github.com/fcunningham"
      }, 
      {
        "contributions": 16, 
        "html_url": "https://github.com/nicklangridge"
      }, 
      {
        "contributions": 15, 
        "html_url": "https://github.com/ens-admin"
      }, 
      {
        "contributions": 15, 
        "html_url": "https://github.com/epaule"
      }, 
      {
        "contributions": 14, 
        "html_url": "https://github.com/thomasmaurel"
      }, 
      {
        "contributions": 11, 
        "html_url": "https://github.com/ens-ap5"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/61-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/61-1", 
        "name": "cvs/release/vega/61-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/59-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/59-1", 
        "name": "cvs/release/vega/59-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/55-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/55-1", 
        "name": "cvs/release/vega/55-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/49-5", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/49-5", 
        "name": "cvs/release/vega/49-5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/49-3", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/49-3", 
        "name": "cvs/release/vega/49-3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/49-2", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/49-2", 
        "name": "cvs/release/vega/49-2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/49-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/49-1", 
        "name": "cvs/release/vega/49-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/46-4", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/46-4", 
        "name": "cvs/release/vega/46-4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/46-3", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/46-3", 
        "name": "cvs/release/vega/46-3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/46-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/46-1", 
        "name": "cvs/release/vega/46-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/44", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/44", 
        "name": "cvs/release/vega/44"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/44-3", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/44-3", 
        "name": "cvs/release/vega/44-3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/44-2", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/44-2", 
        "name": "cvs/release/vega/44-2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/43", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/43", 
        "name": "cvs/release/vega/43"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/41", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/41", 
        "name": "cvs/release/vega/41"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/41-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/41-1", 
        "name": "cvs/release/vega/41-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/40-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/40-1", 
        "name": "cvs/release/vega/40-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/39", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/39", 
        "name": "cvs/release/vega/39"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/38-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/38-1", 
        "name": "cvs/release/vega/38-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/37-2", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/37-2", 
        "name": "cvs/release/vega/37-2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/37-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/37-1", 
        "name": "cvs/release/vega/37-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/35-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/35-1", 
        "name": "cvs/release/vega/35-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/31", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/31", 
        "name": "cvs/release/vega/31"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/31-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/31-1", 
        "name": "cvs/release/vega/31-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/30-3", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/30-3", 
        "name": "cvs/release/vega/30-3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/30-2", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/30-2", 
        "name": "cvs/release/vega/30-2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/30-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/30-1", 
        "name": "cvs/release/vega/30-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/28-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/28-1", 
        "name": "cvs/release/vega/28-1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/26-2", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/26-2", 
        "name": "cvs/release/vega/26-2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/Ensembl/ensembl/zipball/cvs/release/vega/26-1", 
        "tarball_url": "https://api.github.com/repos/Ensembl/ensembl/tarball/cvs/release/vega/26-1", 
        "name": "cvs/release/vega/26-1"
      }
    ], 
    "created_at": "2013-12-17T15:11:49Z", 
    "updated_at": "2016-08-08T04:39:23Z", 
    "languages": [
      "HTML", 
      "Shell", 
      "CSS", 
      "PLpgSQL", 
      "Perl"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/dzerbino"
      }, 
      {
        "html_url": "https://github.com/andrewyatz"
      }, 
      {
        "html_url": "https://github.com/thomasmaurel"
      }, 
      {
        "html_url": "https://github.com/nerdstrike"
      }, 
      {
        "html_url": "https://github.com/avullo"
      }, 
      {
        "html_url": "https://github.com/magaliruffier"
      }, 
      {
        "html_url": "https://github.com/danstaines"
      }, 
      {
        "html_url": "https://github.com/james-monkeyshines"
      }, 
      {
        "html_url": "https://github.com/mcast"
      }, 
      {
        "html_url": "https://github.com/ens-carlos"
      }, 
      {
        "html_url": "https://github.com/reece"
      }, 
      {
        "html_url": "https://github.com/yanlinlin82"
      }, 
      {
        "html_url": "https://github.com/RLuisier"
      }, 
      {
        "html_url": "https://github.com/willmclaren"
      }, 
      {
        "html_url": "https://github.com/at7"
      }, 
      {
        "html_url": "https://github.com/fcunningham"
      }, 
      {
        "html_url": "https://github.com/sarahhunt"
      }, 
      {
        "html_url": "https://github.com/lairdm"
      }, 
      {
        "html_url": "https://github.com/gawbul"
      }, 
      {
        "html_url": "https://github.com/binlu1981"
      }, 
      {
        "html_url": "https://github.com/premanand17"
      }
    ], 
    "owner": "https://github.com/Ensembl", 
    "homepage": "http://www.ensembl.org"
  }, 
  "technologies": [
    "Python", 
    " REST ", 
    "Ruby", 
    " SOAP ", 
    "Perl"
  ], 
  "dateCreated": "2014-09-19T04:50:41Z"
}{
  "doi": "10.1093/bioinformatics/btu789", 
  "name": "The chemical component dictionary complete descriptions of constituent molecules in experimentally determined 3D macromolecules in the Protein Data Bank", 
  "links": [
    "http://mmcif", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://wwpdb.org"
  ], 
  "title": "The chemical component dictionary: complete descriptions of constituent molecules in experimentally determined 3D macromolecules in the Protein Data Bank", 
  "toolName": "The chemical component dictionary: complete descriptions of constituent molecules in experimentally determined 3D macromolecules in the Protein Data Bank", 
  "abstract": "The Chemical Component Dictionary (CCD) is a chemical reference data resource that describes all residue and small molecule components found in Protein Data Bank (PDB) entries. The CCD contains detailed chemical descriptions for standard and modified amino acids/ nucleotides, small molecule ligands and solvent molecules. Each chemical definition includes descriptions of chemical properties such as stereochemical assignments, chemical descriptors, systematic chemical names and idealized coordinates. The content, preparation, validation and distribution of this CCD chemical reference dataset are described. Availability and implementation: The CCD is updated regularly in conjunction with the scheduled weekly release of new PDB structure data. The CCD and amino acid variant reference datasets are hosted in the public PDB ftp repository at ftp", 
  "summary": "(a) An example CCD definition for 4-hydroxyproline, including molecular-level information: accession details (e.g. identifiers and status information), molecular names, molecular type, formula and formula weight; (b) An abbreviated example CCD definition for 4-hydroxyproline illustrating the atom-level features including atom names, stereochemical and aromatic molecular identifiers, experimental model coordinates, computed ideal coordinates; (c) An abbreviated example CCD definition for 4-hydroxyproline illustrating the bond-level features including bonding atom names, bond type, stereochemical and aromatic molecular identifiers; (d) An example CCD definition for 4-hydroxyproline illustrating SMILES and InChI descriptors, systematic chemical name identifiers and the programs and versions used to compute each name and descriptor\nIn cases where coordinate data are required to define the chemical structure of a component, the perception of covalent bond types from the 3D experimental coordinates is based on a consensus of results obtained from OpenBabel (O'Boyle et al., 2011) and OEChemTK (Stahl and Mauser, 2005) libraries.", 
  "affiliations": [
    " Protein Data Bank in Europe European Molecular Biology Laboratory European Bioinformatics Institute (EMBL-EBI) Wellcome Trust Genome Campus ", 
    " Department of Chemistry and Chemical Biology and Center for Integrative Proteomics Research RCSB PDB The State University of New Jersey "
  ], 
  "grants": [
    "Funding\nRCSB PDB is supported by NSF [DBI-1338415], NIH, DOE; PDBe by EMBL-EBI, Wellcome Trust [088944], BBSRC [BB/J007471/1, BB/K016970/ 1, BB/K020013/1, BB/M013146/1], NIGMS [1RO1 GM079429-01A1], EU [284209] and MRC [MR/L007835/1]; PDBj by JST-NBDC and BMRB by NLM P41 LM05799."
  ], 
  "acks": " We thank Kim Henrick (formerly PDBe) and Dimitris Dimitropoulos (formerly PDBe and RCSB PDB) for their efforts in nomenclature standardization and software tool development that contributed significantly to the development of CCD. We thank Helen Berman (RCSB PDB) for her leadership , long-standing support for developing data standards and assistance in the preparation of this manuscript. InChI: the worldwide chemical structure identifier standard. J Cheminform., 5, 7. Remediation of the protein data bank archive. Nucleic Acids Res., 36, D426\u2013D433. Hill,E.A. (1900) On a system of indexing chemical literature; adopted by the classification division of the U.S. patent office. J. Am. Chem. Soc., 22, 478\u2013494. Ihlenfeldt,W.D. et al. (1992) CACTVS: a chemistry algorithm develop- ment environment. In: Machida,K. and Nishioka,T. (eds.) Daijuukagakutouronkai Dainijuukai Kouzoukasseisoukan Shinpojiumu Kouenyoushishuu. Kyoto University Press, Kyoto, pp. 102\u2013105. IUPAC Commission on Macromolecular Nomenclature. (1979) Stereochemical definitions and notations relating to polymers. Pure Appl. Chem., 51, 1101\u20131121. Kinjo,A.R. et al. (2012) Protein data bank Japan (PDBj): maintaining a structural data archive and resource description framework format. Nucleic Acids Res., 40, D453\u2013D460. Markley,J.L. et al. (2008) BioMagResBank (BMRB) as a partner in the worldwide protein data bank (wwPDB): new policies affecting biomolecular NMR depositions. J. Biomol. NMR, 40, 153\u2013155. O'Boyle,N.M. et al. (2011) Open babel: an open chemical toolbox. J. Cheminform., 3, 33. OpenEye Scientific Software Inc. (2007) OpenEye OEChem, version 1.5. Sadowski,J. and Gasteiger,J. (1994) Comparison of automatic three-dimensional model builders using 639 X-ray structures. J. Chem. Inj. Comput. Sci., 34, 1000\u20131008. Stahl,M. and Mauser,H. (2005) Database clustering with a combination of fingerprint and maximum common substructure methods. J. Chem. Inf. Model, 45, 449\u2013462. Weininger,D. (1988) SMILES 1: introduction and encoding rules. J. Chem. Inf. Comput. Sci., 28, 31. Westbrook,J. et al. (2005) 3.6.2 the protein data bank exchange data dictionary. In: Hall,S.R. and McMahon,B. (eds.) International Tables for Crystallography. Springer, Dordrecht, The Netherlands, pp. 195\u2013198. Young,J.Y. et al. (2013) Chemical annotation of small and peptide-like molecules at the protein data bank. Database, 2013, bat079. ", 
  "authors": [
    " John D Westbrook", 
    " Chenghua Shao", 
    " Zukang Feng", 
    " Marina Zhuravleva", 
    " Sameer Velankar", 
    " Jasmine Young"
  ], 
  "keyWords": [
    "chemical component", 
    [
      "data", 
      "atomic", 
      "components"
    ]
  ], 
  "sourcelinks": [
    "http://mmcif"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-04T02:51:43Z"
}{
  "doi": "10.1093/bioinformatics/btu649", 
  "name": "The RNA shapes studio", 
  "links": [
    "http://bibiserv.cebitec.uni-biele", 
    "http://bibiserv.cebitec.unibielefeld.de/rnashapesstudio", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Sequence analysis The RNA shapes studio", 
  "toolName": "Sequence analysis The RNA shapes studio", 
  "abstract": "Motivation: Abstract shape analysis, first proposed in 2004, allows one to extract several relevant structures from the folding space of an RNA sequence, preferable to focusing in a single structure of minimal free energy. We report recent extensions to this approach. Results: We have rebuilt the original RNASHAPES as a repository of components that allows us to integrate several established tools for RNA structure analysis: RNASHAPES, RNAALISHAPES and PKNOTSRG, including its recent extension PKISS. As a spin-off, we obtain heretofore unavailable functionality: e. g. with PKISS, we can now perform abstract shape analysis for structures holding pseudoknots up to the complexity of kissing hairpin motifs. The new tool PALIKISS can predict kissing hairpin motifs from aligned sequences. Along with the integration, the functionality of the tools was also extended in manifold ways. Availability and implementation: As before, the tool is available on the Bielefeld Bioinformatics server at", 
  "summary": "1.1 Integration of tools for RNA abstract shape analysis The framework of algebraic dynamic programming (ADP) allows us to express dynamic programming algorithms for sequence analysis on a high level of abstraction.\nRelying on the recent BELLMAN'S GAP system (Sauthoff et al., 2013), which implements the ADP framework, we have built a repository of components that allows us to integrate several established tools for RNA structure analysis: RNASHAPES, RNAALISHAPES and PKNOTSRG, including its recent extension PKISS.\nThe graphical motif description tool LOCOMOTIF (Reeder et al., 2007) now uses modules from the RNA shapes studio.", 
  "affiliations": [
    " Practical Computer Science Faculty of Technology Bielefeld University "
  ], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " Stefan Janssen", 
    " Robert Giegerich", 
    " John Hancock"
  ], 
  "keyWords": [
    "rna shapes", 
    [
      "shape", 
      "structures", 
      "analysis", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu745", 
  "name": "Taxatortk precise taxonomic assignment of metagenomes by fast approximation of evolutionary neighborhoods", 
  "links": [
    "http://algbio.cs.uni-duesseldorf.de/software/.2", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://algbio.cs.uni-duesseldorf.de/software", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Taxator-tk: precise taxonomic assignment of metagenomes by fast approximation of evolutionary neighborhoods", 
  "toolName": "Taxator-tk: precise taxonomic assignment of metagenomes by fast approximation of evolutionary neighborhoods", 
  "abstract": "Motivation: Metagenomics characterizes microbial communities by random shotgun sequencing of DNA isolated directly from an environment of interest. An essential step in computational meta-genome analysis is taxonomic sequence assignment, which allows identifying the sequenced community members and reconstructing taxonomic bins with sequence data for the individual taxa. For the massive datasets generated by next-generation sequencing technologies, this cannot be performed with de-novo phylogenetic inference methods. We describe an algorithm and the accompanying software, taxator-tk, which performs taxonomic sequence assignment by fast approximate determination of evolutionary neighbors from sequence similarities. Results: Taxator-tk was precise in its taxonomic assignment across all ranks and taxa for a range of evolutionary distances and for short as well as for long sequences. In addition to the taxonomic binning of metagenomes, it is well suited for profiling microbial communities from metagenome samples because it identifies bacterial, archaeal and eukaryotic community members without being affected by varying primer binding strengths, as in marker gene amplification, or copy number variations of marker genes across different taxa. Taxator-tk has an efficient, parallelized implementation that allows the assignment of 6 Gb of sequence data per day on a standard multi-processor system with 10 CPU cores and microbial RefSeq as the genomic reference data. Availability and implementation: Taxator-tk source and binary program files are publicly available at", 
  "summary": "Taxator-tk determines a subset of homologs, which represent the approximate evolutionary neighbors for a query sequence, by a linear number of pairwise sequence comparisons with regard to the number of considered homologs and then assigns a taxon ID using a reference taxonomy based on the taxonomic IDs of these neighbors.\nTo target draft genome reconstructions, the data assigned to individual taxonomic bins by taxator-tk can be used as training data for complementary approaches, such as composition-based methods, or as independent information in combination with recently proposed clustering methods using the abundance of genes or contigs across multiple samples.", 
  "affiliations": [], 
  "grants": [
    "Funding\nThe authors gratefully acknowledge funding by the Max-Planck society and Heinrich Heine University Du sseldorf."
  ], 
  "acks": " Computational support and infrastructure were provided by the 'Centre for Information and Media Technology' (ZIM) at the University of D\u00fc sseldorf (Germany). The authors gratefully acknowledge funding by the Max-Planck society and Heinrich Heine University D\u00fc sseldorf. Conflict of Interest: none declared. ", 
  "authors": [
    " J Dr\u00f6 Ge", 
    " I Gregor", 
    " A C Mchardy"
  ], 
  "keyWords": [
    "taxonomic assignment", 
    [
      "genomics", 
      "binning", 
      "assignments", 
      "metagenomics", 
      "sequencing", 
      "bins"
    ]
  ], 
  "sourcelinks": [
    "http://algbio.cs.uni-duesseldorf.de/software/.2", 
    "http://algbio.cs.uni-duesseldorf.de/software"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-12T04:44:24Z"
}{
  "doi": "10.1093/bioinformatics/btu831", 
  "name": "The QDREC web server determining doseresponse characteristics of complex macroparasites in phenotypic drug screens", 
  "links": [
    "http://tintin.sfsu.edu/projects/phenotypicAssays.html", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://haddock4.sfsu.edu"
  ], 
  "title": "The QDREC web server: determining dose\u2013response characteristics of complex macroparasites in phenotypic drug screens", 
  "toolName": "The QDREC web server: determining dose\u2013response characteristics of complex macroparasites in phenotypic drug screens", 
  "abstract": "Neglected tropical diseases (NTDs) caused by helminths constitute some of the most common infections of the world's poorest people. The etiological agents are complex and recalci-trant to standard techniques of molecular biology. Drug screening against helminths has often been phenotypic and typically involves manual description of drug effect and efficacy. A key challenge is to develop automated, quantitative approaches to drug screening against helminth diseases. The quantal dose\u2013response calculator (QDREC) constitutes a significant step in this direction. It can be used to automatically determine quantitative dose\u2013response characteristics and half-maximal effective concentration (EC 50) values using image-based readouts from phenotypic screens, thereby allowing rigorous comparisons of the efficacies of drug compounds. QDREC has been developed and validated in the context of drug screening for schistosomiasis, one of the most important NTDs. However, it is equally applicable to general phenotypic screening involving helminths and other complex parasites. Availability and implementation: QDREC is publically available at:", 
  "summary": "QDREC has been developed and validated in the context of drug screening for schistosomiasis, one of the most important NTDs. However, it is equally applicable to general phenotypic screening involving helminths and other complex parasites.\nIn this area, recent progress has been made in parasite segmentation (Asarnow and Singh, 2013; Singh et al., 2009), parasite tracking from video recordings (Saha and Singh, 2012) and quantitative definitions and analysis of helminth phenotypes (Lee et al., 2012; Singh et al., 2009), including hit detection in highthroughput screens (Marcellino et al., 2012; Paveley et al., 2012).\nQuantal doseresponse calculator (QDREC) is a web server for automatically determining quantal time and doseresponse characteristics from image-based, phenotypic drug screens.", 
  "affiliations": [
    " Center for Discovery and Innovation in Parasitic Diseases University of California ", 
    " Department of Computer Science San Francisco State University ", 
    " Department of Pathology"
  ], 
  "grants": [
    "Funding\nThis work was supported in part by the National Institutes of Health [grants 1R01AI089896, R21AI107390] and by the National Science Foundation [grant IIS-0644418]."
  ], 
  "acks": " ", 
  "authors": [
    " Daniel Asarnow", 
    " Liliana Rojo-Arreola", 
    " Brian M Suzuki", 
    " Conor R Caffrey", 
    " Rahul Singh", 
    " "
  ], 
  "keyWords": [
    "parasite segmentation", 
    "phenotypic drug", 
    [
      "classifications", 
      "qdrec", 
      "drugs", 
      "parasitic", 
      "phenotypes", 
      "imaging", 
      "based", 
      "segmenting"
    ]
  ], 
  "sourcelinks": [
    "http://tintin.sfsu.edu/projects/phenotypicAssays.html"
  ], 
  "technologies": [], 
  "dateCreated": "2014-12-25T04:16:00Z"
}{
  "doi": "10.1093/bioinformatics/btu579", 
  "name": "The Naked Mole Rat Genome Resource facilitating analyses of cancer and longevityrelated adaptations", 
  "links": [
    "http://www.ncbi.nlm.nih.gov/books/NBK", 
    "http://creativecommons.org/licenses/by/4.0", 
    "https://github.com/maglab/naked-mole-rat-portal", 
    "http://www.naked-mole-rat.org).2", 
    "http://www.naked-mole-rat.org", 
    "http://www.nakedmole-rat.org", 
    "http://www.naked-mole-rat.org/annotations", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.genomesize.com/result_species.php", 
    "http://www.naked-mole-rat.org/static/downloads/naked"
  ], 
  "title": "BIOINFORMATICS DISCOVERY NOTE The Naked Mole Rat Genome Resource: facilitating analyses of cancer and longevity-related adaptations", 
  "toolName": "naked-mole-rat-portal", 
  "abstract": "Motivation: The naked mole rat (Heterocephalus glaber) is an exceptionally long-lived and cancer-resistant rodent native to East Africa. Although its genome was previously sequenced, here we report a new assembly sequenced by us with substantially higher N50 values for scaffolds and contigs. Results: We analyzed the annotation of this new improved assembly and identified candidate genomic adaptations which may have contributed to the evolution of the naked mole rat's extraordinary traits, including in regions of p53, and the hyaluronan receptors CD44 and HMMR (RHAMM). Furthermore, we developed a freely available web portal, the Naked Mole Rat Genome Resource (http://www.naked-mole-rat.org), featuring the data and results of our analysis, to assist researchers interested in the genome and genes of the naked mole rat, and also to facilitate further studies on this fascinating species. Availability and implementation: The Naked Mole Rat Genome Resource is freely available online at http://www.naked-mole-rat.org. This resource is open source and the source code is available at https://github.com/maglab/naked-mole-rat-portal.", 
  "summary": "Genes that have been associated with longevity are cross-linked with the GenAge database (http://www.naked-mole-rat.org/annotations/ results/genage/) (Tacutu et al., 2013).\nMoreover, an additional $23-fold coverage assembly of the NMR genome generated by The Genome Analysis Centre (TGAC) based on two Illumina paired-end sequencing runs is available for download (http://www.naked-mole-rat.org/static/downloads/naked_ mole_rat_contigs.zip).\nFunctionally enriched DAVID (v6.7) clusters (using human/mouse orthologs and a background of the 12 837 best-match transcripts; otherwise default parameters were used) with an enrichment score 41.3, corresponding to P50.05 (Huang et al., 2009), for the top 5% of NMR genes by Ka/Ks included cytokine activity, signal peptide and defense response (Table 2).", 
  "affiliations": [
    " Integrative Genomics of Ageing Group Institute of Integrative Biology University of Liverpool ", 
    " Department of Biology University of Rochester ", 
    " Broad Institute of MIT and Harvard", 
    " Department of Genetics Harvard Medical School "
  ], 
  "grants": [
    "The TADII is in green, the PRD in yellow and PXXP motifs are boxed\n\ngrants from the National Human Genome Research Institute (NHGRI).", 
    "is supported by a Wellcome Trust grant (WT094386MA) to J.P.M.", 
    "and a US National Institutes of Health grant to V.G.", 
    "Funding: This work was partly funded by a Marie Curie International Reintegration Grant within EC-FP7 to J.P.M., Senior Scholar grants from the Ellison Medical Foundation to V.G."
  ], 
  "sourcelinks": [
    "http://www.naked-mole-rat.org/static/downloads/naked", 
    "https://github.com/maglab/naked-mole-rat-portal", 
    "http://www.naked-mole-rat.org).2", 
    "http://www.naked-mole-rat.org", 
    "http://www.nakedmole-rat.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.genomesize.com/result_species.php"
  ], 
  "acks": " We would like to thank the Genomics Platform of the Broad Institute for sequencing the naked mole rat genome. Further thanks to the NCBI for the annotation of the genome and to Susan Hiatt for her assistance. We are also thankful to TGAC and the BBSRC for the generation of additional genomic data. ", 
  "authors": [
    " Michael Keane", 
    " Thomas Craig", 
    " Jessica Alf", 
    " \u20ac Oldi", 
    " Aaron M Berlin", 
    " Jeremy Johnson", 
    " Andrei Seluanov", 
    " Vera Gorbunova", 
    " Federica Di Palma", 
    " Kerstin Lindblad-Toh", 
    " George M Church", 
    " Jo ~ Ao", 
    " Pedro De Magalh", 
    " ~ Aes", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "sequencing", 
      "genomics", 
      "assemblies"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v2.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/maglab/naked-mole-rat-portal/master/LICENSE"
      }
    ], 
    "name": "naked-mole-rat-portal", 
    "contributors": [
      {
        "contributions": 22, 
        "html_url": "https://github.com/tjomasc"
      }
    ], 
    "versions": [], 
    "created_at": "2014-05-23T11:00:53Z", 
    "updated_at": "2015-01-07T02:17:37Z", 
    "languages": [
      "C", 
      "Shell", 
      "Java", 
      "Python", 
      "JavaScript", 
      "C++", 
      "Perl", 
      "TeX", 
      "Lua", 
      "PHP", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/tjomasc"
      }, 
      {
        "html_url": "https://github.com/gawbul"
      }
    ], 
    "owner": "https://github.com/maglab", 
    "homepage": null
  }, 
  "technologies": [
    "Assembly"
  ], 
  "dateCreated": "2014-08-30T00:28:26Z"
}{
  "doi": "10.1093/bioinformatics/btu773", 
  "name": "The Victor C library for protein representation and advanced manipulation", 
  "links": [
    "http://protein.bio.unipd.it/victor", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://github.com/BioComputingUP/Victor.calculate", 
    "http://protein.bio"
  ], 
  "title": "The Victor C11 library for protein representation and advanced manipulation", 
  "toolName": "The Victor C11 library for protein representation and advanced manipulation", 
  "abstract": "Motivation: Protein sequence and structure representation and manipulation require dedicated software libraries to support methods of increasing complexity. Here, we describe the VIrtual Constrution TOol for pRoteins (Victor) C\u00fe\u00fe library, an open source platform dedicated to enabling inexperienced users to develop advanced tools and gathering contributions from the community. The provided application examples cover statistical energy potentials, profile\u2013profile sequence alignments and ab initio loop modeling. Victor was used over the last 15 years in several publications and optimized for efficiency. It is provided as a GitHub repository with source files and unit tests, plus extensive online documentation, including a Wiki with help files and tutorials, examples and Doxygen documentation. Availability and implementation: The C\u00fe\u00fe library and online documentation, distributed under a GPL license are available from URL: http://protein.bio.unipd.it/victor/.", 
  "summary": "The critical assessment of techniques for protein structure prediction (CASP) series of experiments (Moult et al., 2014) demonstrates that structure prediction is increasingly becoming an engineering problem, where sophisticated methods have to be combined into extensive pipelines to provide state-of-the-art results (Khoury et al., 2014).\nVictor is both an efficiently designed C library, able to manipulate protein structures with minimal computing time, and a collection of advanced components for protein sequence and structure manipulation.\nThe Align directory provides basic sequence alignment algorithms (Tosatto et al., 2006) augmented with secondary structure element (Fontana et al., 2005).", 
  "affiliations": [
    " Department of Information Engineering University of Padua ", 
    " Department of Biomedical Sciences University of Padua "
  ], 
  "grants": [
    "Funding\nThis project was funded by FIRB Futuro in Ricerca grant RBFR08ZSXY and University of Padua grant CPDR123473 to S.T."
  ], 
  "acks": " To the Francesco Lovo, Enrico Negri and several students for contributing to the Victor project over the years as well as to members of the BioComputing UP lab for insightful discussions. This project was funded by FIRB Futuro in Ricerca grant RBFR08ZSXY and University of Padua grant CPDR123473 to S.T. of interest: none declared. ", 
  "authors": [
    " Layla Hirsh", 
    " Damiano Piovesan", 
    " Manuel Giollo", 
    " Carlo Ferrari", 
    " Silvio C E Tosatto"
  ], 
  "keyWords": [
    [
      "proteins", 
      "extensively", 
      "libraries", 
      "bioinformatics", 
      "victor", 
      "structural"
    ]
  ], 
  "sourcelinks": [
    "https://github.com/BioComputingUP/Victor.calculate"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-21T01:18:58Z"
}{
  "doi": "10.1093/bioinformatics/btu390", 
  "name": "The Scramble conversion tool", 
  "links": [
    "https://github.com/lomereiter/sambamba", 
    "https://www.ebi.ac.uk/ena/about/cram", 
    "http://creativecommons.org/licenses/by/3.0", 
    "http://res.illumina.com/documents/products/whitepa", 
    "https://github.com/samtools/.ACKNOWLEDGEMENTThe", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://picard.sourceforge.net"
  ], 
  "title": "Sequence analysis The Scramble conversion tool", 
  "toolName": "sambamba", 
  "abstract": "Motivation: The reference CRAM file format implementation is in Java. We present 'Scramble': a new C implementation of SAM, BAM and CRAM file I/O. Results: The C implementation of for CRAM is 1.5\u20131.7\u00c2 slower than BAM at decoding but 1.8\u20132.6\u00c2 faster at encoding. We see file size savings of 34\u201355%. Availability and implementation: Source code is available at http:// sourceforge.net/projects/staden/files/io_lib/ under the BSD software licence.", 
  "summary": "Motivation: The reference CRAM file format implementation is in Java.\nWe present `Scramble': a new C implementation of SAM, BAM and CRAM file I/O.\nThe CRAM format (Fritz et al., 2011) is a practical implementation of reference-based compression and is a viable alternative to the earlier BAM format (Li et al., 2009).\nWe will not cover the CRAM file format here except to note that CRAM internally separates data by type before compressing with Zlib (Deutsch and Gailly, 1996).\nScramble has full multi-threading support for both reading and writing of BAM and CRAM file formats.", 
  "affiliations": [
    " DNA Pipelines Wellcome Trust Sanger Institute "
  ], 
  "grants": [
    "Funding: Wellcome Trust (098051)."
  ], 
  "sourcelinks": [
    "https://github.com/lomereiter/sambamba", 
    "https://www.ebi.ac.uk/ena/about/cram", 
    "http://picard.sourceforge.net", 
    "https://github.com/samtools/.ACKNOWLEDGEMENTThe"
  ], 
  "acks": " ", 
  "authors": [
    " James K Bonfield", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    "cram file format implementation", 
    [
      "files", 
      "samtools", 
      "bioinformatics", 
      "implementations", 
      "formats", 
      "data"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v2.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/lomereiter/sambamba/master/LICENSE"
      }
    ], 
    "name": "sambamba", 
    "contributors": [
      {
        "contributions": 633, 
        "html_url": "https://github.com/lomereiter"
      }, 
      {
        "contributions": 37, 
        "html_url": "https://github.com/pjotrp"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/csw"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/sambrightman"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/chapmanb"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/rernst"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.6.3", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.6.3", 
        "name": "v0.6.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.6.2", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.6.2", 
        "name": "v0.6.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.6.1", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.6.1", 
        "name": "v0.6.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.6.0", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.6.0", 
        "name": "v0.6.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.9", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.9", 
        "name": "v0.5.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.8", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.8", 
        "name": "v0.5.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.7", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.7", 
        "name": "v0.5.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.6", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.6", 
        "name": "v0.5.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.5", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.5", 
        "name": "v0.5.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.4", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.4", 
        "name": "v0.5.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.3", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.3", 
        "name": "v0.5.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.2", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.2", 
        "name": "v0.5.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.1", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.1", 
        "name": "v0.5.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.0", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.0", 
        "name": "v0.5.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.5.0-alpha", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.5.0-alpha", 
        "name": "v0.5.0-alpha"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.4.7", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.4.7", 
        "name": "v0.4.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.4.6", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.4.6", 
        "name": "v0.4.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.4.5", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.4.5", 
        "name": "v0.4.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.4.4", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.4.4", 
        "name": "v0.4.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.4.3", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.4.3", 
        "name": "v0.4.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.4.2", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.4.2", 
        "name": "v0.4.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.4.1", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.4.1", 
        "name": "v0.4.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.4.0", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.4.0", 
        "name": "v0.4.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.3.3", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.3.3", 
        "name": "v0.3.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.3.2", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.3.2", 
        "name": "v0.3.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.3.1", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.3.1", 
        "name": "v0.3.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.3.0", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.3.0", 
        "name": "v0.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.2.9", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.2.9", 
        "name": "v0.2.9"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.2.8", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.2.8", 
        "name": "v0.2.8"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lomereiter/sambamba/zipball/v0.2.7", 
        "tarball_url": "https://api.github.com/repos/lomereiter/sambamba/tarball/v0.2.7", 
        "name": "v0.2.7"
      }
    ], 
    "created_at": "2012-04-28T13:46:53Z", 
    "updated_at": "2016-08-06T14:27:49Z", 
    "languages": [
      "Python", 
      "Makefile", 
      "Shell", 
      "Ruby", 
      "D"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/lomereiter"
      }, 
      {
        "html_url": "https://github.com/pjotrp"
      }, 
      {
        "html_url": "https://github.com/cjfields"
      }, 
      {
        "html_url": "https://github.com/arq5x"
      }, 
      {
        "html_url": "https://github.com/svm-zhang"
      }, 
      {
        "html_url": "https://github.com/gatoravi"
      }, 
      {
        "html_url": "https://github.com/yanlinlin82"
      }, 
      {
        "html_url": "https://github.com/jdeligt"
      }, 
      {
        "html_url": "https://github.com/snewhouse"
      }, 
      {
        "html_url": "https://github.com/melferink"
      }, 
      {
        "html_url": "https://github.com/jcchai"
      }, 
      {
        "html_url": "https://github.com/isthisthat"
      }, 
      {
        "html_url": "https://github.com/rernst"
      }, 
      {
        "html_url": "https://github.com/mschilli87"
      }, 
      {
        "html_url": "https://github.com/asandra"
      }, 
      {
        "html_url": "https://github.com/junehawk"
      }, 
      {
        "html_url": "https://github.com/biorelated"
      }, 
      {
        "html_url": "https://github.com/SamBuckberry"
      }, 
      {
        "html_url": "https://github.com/af8"
      }, 
      {
        "html_url": "https://github.com/thaze"
      }, 
      {
        "html_url": "https://github.com/wyleung"
      }, 
      {
        "html_url": "https://github.com/jvdp11"
      }, 
      {
        "html_url": "https://github.com/plijnzaad"
      }
    ], 
    "owner": "https://github.com/lomereiter", 
    "homepage": "lomereiter.github.io/sambamba"
  }, 
  "technologies": [], 
  "dateCreated": "2014-06-16T20:35:17Z"
}{
  "doi": "10.1093/bioinformatics/btu528", 
  "name": "Thresher an improved algorithm for peak height thresholding of microbial community profiles", 
  "links": [
    "http://verenastarke.wordpress.com.ACKNOWLEDGEMENTS", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://verenastarke.wordpress.com"
  ], 
  "title": "Data and text mining Thresher: an improved algorithm for peak height thresholding of microbial community profiles", 
  "toolName": "Data and text mining Thresher: an improved algorithm for peak height thresholding of microbial community profiles", 
  "abstract": "Motivation: This article presents Thresher, an improved technique for finding peak height thresholds for automated rRNA intergenic spacer analysis (ARISA) profiles. We argue that thresholds must be sample dependent, taking community richness into account. In most previous fragment analyses, a common threshold is applied to all samples simultaneously , ignoring richness variations among samples and thereby compromising cross-sample comparison. Our technique solves this problem, and at the same time provides a robust method for outlier rejection, selecting for removal any replicate pairs that are not valid replicates. Results: Thresholds are calculated individually for each replicate in a pair, and separately for each sample. The thresholds are selected to be the ones that minimize the dissimilarity between the replicates after thresholding. If a choice of threshold results in the two replicates in a pair failing a quantitative test of similarity, either that threshold or that sample must be rejected. We compare thresholded ARISA results with sequencing results, and demonstrate that the Thresher algorithm out-performs conventional thresholding techniques. Availability and Implementation: The software is implemented in R, and the code is available at", 
  "summary": "Figure 2 shows two examples of the 101  101 dissimilarity matrix for two endolithic samples, displaying dissimilarity as a function of the thresholds for each replicate in the pair as color contour maps (Fig. 2a) and the corresponding standardized ARISA profiles for both replicates (Fig. 2b).\nWe tested Dismin as chosen by Thresher for each replicate and each sample, and compared it with results for single thresholds of 0.1 (least stringent allowing more peaks), 0.5 and 0.9% (most stringent) that were applied across the whole dataset.\nComparison of BrayCurtis dissimilarity between replicates in a pair for all samples except the rejected outlier (sample 3) using three different common thresholds and separate thresholds.", 
  "affiliations": [
    " Geophysical Laboratory Carnegie Institution of Washington "
  ], 
  "grants": [
    "Funding: NASA ASTEP grants NNX09AB74G and NNX12AP77G, the CIW-NASA Astrobiology Institute (NNA09DA81A) and the W. M. Keck Foundation (June 29, 2007)."
  ], 
  "acks": " ", 
  "authors": [
    " Verena Starke", 
    " Andrew Steele"
  ], 
  "keyWords": [
    [
      "thresholding", 
      "replicates", 
      "arisa", 
      "dissimilarities", 
      "samples", 
      "communities"
    ]
  ], 
  "sourcelinks": [
    "http://verenastarke.wordpress.com", 
    "http://verenastarke.wordpress.com.ACKNOWLEDGEMENTS"
  ], 
  "technologies": [], 
  "dateCreated": "2014-08-06T05:13:41Z"
}{
  "doi": "10.1093/bioinformatics/btu721", 
  "name": "TIPP taxonomic identification and phylogenetic profiling", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.cs.utexas.edu/users/phylo/software"
  ], 
  "title": "Sequence analysis TIPP: taxonomic identification and phylogenetic profiling", 
  "toolName": "Sequence analysis TIPP: taxonomic identification and phylogenetic profiling", 
  "abstract": "Motivation: Abundance profiling (also called 'phylogenetic profiling') is a crucial step in understanding the diversity of a metagenomic sample, and one of the basic techniques used for this is taxonomic identification of the metagenomic reads. Results: We present taxon identification and phylogenetic profiling (TIPP), a new marker-based taxon identification and abundance profiling method. TIPP combines SAT\\'e-enabled phylogenetic placement a phylogenetic placement method, with statistical techniques to control the classification precision and recall, and results in improved abundance profiles. TIPP is highly accurate even in the presence of high indel errors and novel genomes, and matches or improves on previous approaches, including NBC, mOTU, PhymmBL, MetaPhyler and MetaPhlAn. Availability and implementation: Software and supplementary materials are available at", 
  "summary": "that rely on various machine learning techniques (Support Vector Machines in PhyloPythia and PhyloPythiaS (McHardy et al., 2007; Patil et al., 2011), Interpolated Markov Models in Phymm (Brady and Salzberg, 2011), Bayesian models in NBC [Rosen et al., 2011), or neural networks (Abe et al., 2006)] to classify sequences based on their DNA composition (usually based on the frequency of short k-mers), (b) more sophisticated analyses of similarity search results [e.g. using lowest common ancestor aggregation in Megan (Huson et al., 2007), or classifiers built from similarity search results as done in MetaPhyler (Liu et al., 2010, 2011), MetaPhlAn (Segata et al., 2012) and mOTU (Sunagawa et al., 2013) or protein profiles in Carma (Gerlach and Stoye, 2011)], and (c) combinations of multiple approaches (e.g., composition and similarity based approaches in PhymmBL (Brady and Salzberg, 2009)).", 
  "affiliations": [
    " Center for Bioinformatics and Computational Biology University of Maryland at College Park ", 
    " Department of Computer Science The University of Texas at Austin "
  ], 
  "grants": [
    "; and National Institutes of Health grant R01-AI-100947 to M.P.", 
    "Funding: This research was partially supported by a Guggenheim Foundation Fellowship to T.W.", 
    "; the iPlant Collaborative U.S. National Science Foundation grant DBI-1265383 (via TACC) to N.N.", 
    "; National Science Foundation grants DBI-1062335, DBI-1461364 and DEB 0733029 to T.W."
  ], 
  "acks": " The authors thank Sean Eddy for his help with the derivations of the probability that a query sequence is generated by one HMM in a set of HMMs. The authors also thank the Huttenhower lab for sharing the synthetic data with us for the taxonomic profiling experiment. ", 
  "authors": [
    " Nam-Phuong Nguyen", 
    " Siavash Mirarab", 
    " Bo Liu", 
    " Mihai Pop", 
    " Tandy Warnow", 
    " "
  ], 
  "keyWords": [
    [
      "datasets", 
      "methods", 
      "metaphlan", 
      "reads", 
      "profiling", 
      "sequencing", 
      "tipp"
    ]
  ], 
  "sourcelinks": [
    "http://www.cs.utexas.edu/users/phylo/software"
  ], 
  "technologies": [], 
  "dateCreated": "2014-10-31T00:44:56Z"
}{
  "doi": "10.1093/bioinformatics/btv026", 
  "name": "Topologyfunction conservation in proteinprotein interaction networks", 
  "links": [
    "http://bio-nets.doc.ic.ac.uk/goCCA.zip", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by-nc/4.0/),which"
  ], 
  "title": "Topology-function conservation in protein\u2013protein interaction networks", 
  "toolName": "Topology-function conservation in protein\u2013protein interaction networks", 
  "abstract": "Motivation: Proteins underlay the functioning of a cell and the wiring of proteins in protein\u2013protein interaction network (PIN) relates to their biological functions. Proteins with similar wiring in the PIN (topology around them) have been shown to have similar functions. This property has been successfully exploited for predicting protein functions. Topological similarity is also used to guide network alignment algorithms that find similarly wired proteins between PINs of different species; these similarities are used to transfer annotation across PINs, e.g. from model organisms to human. To refine these functional predictions and annotation transfers, we need to gain insight into the variability of the topology-function relationships. For example, a function may be significantly associated with specific topologies, while another function may be weakly associated with several different topologies. Also, the topology-function relationships may differ between different species. Results: To improve our understanding of topology-function relationships and of their conservation among species, we develop a statistical framework that is built upon canonical correlation analysis. Using the graphlet degrees to represent the wiring around proteins in PINs and gene ontology (GO) annotations to describe their functions, our framework: (i) characterizes statistically significant topology-function relationships in a given species, and (ii) uncovers the functions that have conserved topology in PINs of different species, which we term topologically orthologous functions. We apply our framework to PINs of yeast and human, identifying seven biological process and two cellular component GO terms to be topologically orthologous for the two organisms.", 
  "summary": "Using the graphlet degrees to represent the wiring around proteins in PINs and gene ontology (GO) annotations to describe their functions, our framework: (i) characterizes statistically significant topology-function relationships in a given species, and (ii) uncovers the functions that have conserved topology in PINs of different species, which we term topologically orthologous functions.\nAlthough the association matrix can be used for predicting the GO term annotations of the proteins from their wiring patterns in PINs, our results show that not all topology-function relationships are conserved across species, which is likely to negatively impact the quality of predictions.", 
  "affiliations": [
    " California Institute of Telecommunications and Technology (Calit2) University of California Irvine ", 
    " Department of Computing Imperial College London "
  ], 
  "grants": [
    "Funding\nThis work is supported by the European Research Council (ERC) Starting Independent Researcher Grant [278212], National Science Foundation (NSF) Cyber-Enabled Discovery and Innovation (CDI) [OIA-1028394], ARRS project [J1-5454], the Serbian Ministry of Education and Science Project [III44006] and the intramural programme of US National Library of Medicine."
  ], 
  "acks": " We would like to thank Prof. Carter T. Butts for his comments and suggestions on developing the methodology. This work is supported by the European Research Council (ERC) Starting Independent Researcher Grant, National Science Foundation (NSF) Cyber-Enabled Discovery and Innovation (CDI) , ARRS pro- ject, the Serbian Ministry of Education and Science Project and the intramural programme of US National Library of Medicine. Conflict of Interest: none declared. ", 
  "authors": [
    " Darren Davis", 
    " Nebil Yaverog", 
    " \u02d8 Lu", 
    " No\u00eb L Malod-Dognin", 
    " Aleksandar Stojmirovic", 
    " Nata\u0161 ", 
    " Pr\u017e Ulj"
  ], 
  "keyWords": [
    [
      "proteins", 
      "orbits", 
      "networks", 
      "patterns", 
      "functioning", 
      "topologically"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-22T03:50:58Z"
}{
  "doi": "10.1093/bioinformatics/btu411", 
  "name": "TPpred2 improving the prediction of mitochondrial targeting peptide cleavage sites by exploiting sequence motifs", 
  "links": [
    "http://tppred2.biocomp.unibo.it", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://tppred2.bio"
  ], 
  "title": "Sequence analysis TPpred2: improving the prediction of mitochondrial targeting peptide cleavage sites by exploiting sequence motifs", 
  "toolName": "Sequence analysis TPpred2: improving the prediction of mitochondrial targeting peptide cleavage sites by exploiting sequence motifs", 
  "abstract": "Targeting peptides are N-terminal sorting signals in proteins that promote their translocation to mitochondria through the interaction with different protein machineries. We recently developed TPpred, a machine learning-based method scoring among the best ones available to predict the presence of a targeting peptide into a protein sequence and its cleavage site. Here we introduce TPpred2 that improves TPpred performances in the task of identifying the cleavage site of the targeting peptides. TPpred2 is now available as a web interface and as a stand-alone version for users who can freely download and adopt it for processing large volumes of sequences. Availability and implementaion: TPpred2 is available both as web server and stand-alone version at http://tppred2.", 
  "summary": "Advance Access publication June 27, 2014\nCastrense Savojardo1, Pier Luigi Martelli1,*, Piero Fariselli1,2 and Rita Casadio1\n1Biocomputing Group, University of Bologna, CIRI-Health Science and Technology/Department of Biology, 40126 Bologna and 2Department of Computer Science and Engineering, University of Bologna, 40127 Bologna, Italy\nReceived on April 22, 2014; revised on June 19, 2014; accepted on June 23, 2014\nThe new predictor combines TPpred, our previous GRHCRF-based method for the prediction of mitochondrial and plastidic targeting peptides (Fariselli et al., 2009; Indio et al., 2013), with an additional refinement step--SVMbased--for cleavage site identification.\nPublished by Oxford University Press.\nC.Savojardo et al.\nNat. Protoc., 2, 953971.\nFariselli,P.\nNat. Rev.", 
  "affiliations": [
    " Department of Computer Science and Engineering University of Bologna ", 
    " Biocomputing Group Department of Biology University of Bologna CIRI-Health Science and Technology "
  ], 
  "grants": [
    "Funding: PRIN 2010-2011 project 20108XYHJS (to P.L.M.)"
  ], 
  "acks": " ", 
  "authors": [
    " Castrense Savojardo", 
    " Pier Luigi Martelli", 
    " Piero Fariselli", 
    " Rita Casadio", 
    " John Hancock"
  ], 
  "keyWords": [
    "cleavage sites", 
    [
      "proteins", 
      "site", 
      "tppred", 
      "mitochondrial", 
      "motifs", 
      "sequences"
    ]
  ], 
  "sourcelinks": [
    "http://tppred2.bio"
  ], 
  "technologies": [], 
  "dateCreated": "2014-06-29T00:12:32Z"
}{
  "doi": "10.1093/bioinformatics/btu786", 
  "name": "Tissueaware data integration approach for the inference of pathway interactions in metazoan organisms", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://pathwaynet.princeton.edu.2", 
    "https://bitbucket.org/libsleipnir/lib", 
    "http://pathwaynet.princeton.edu"
  ], 
  "title": "Data and text mining Tissue-aware data integration approach for the inference of pathway interactions in metazoan organisms", 
  "toolName": "Data and text mining Tissue-aware data integration approach for the inference of pathway interactions in metazoan organisms", 
  "abstract": "Motivation: Leveraging the large compendium of genomic data to predict biomedical pathways and specific mechanisms of protein interactions genome-wide in metazoan organisms has been challenging. In contrast to unicellular organisms, biological and technical variation originating from diverse tissues and cell-lineages is often the largest source of variation in metazoan data com-pendia. Therefore, a new computational strategy accounting for the tissue heterogeneity in the functional genomic data is needed to accurately translate the vast amount of human genomic data into specific interaction-level hypotheses. Results: We developed an integrated, scalable strategy for inferring multiple human gene interaction types that takes advantage of data from diverse tissue and cell-lineage origins. Our approach specifically predicts both the presence of a functional association and also the most likely interaction type among human genes or its protein products on a whole-genome scale. We demonstrate that directly incorporating tissue contextual information improves the accuracy of our predictions , and further, that such genome-wide results can be used to significantly refine regulatory interactions from primary experimental datasets (e.g. ChIP-Seq, mass spectrometry). Availability and implementation: An interactive website hosting all of our interaction predictions is publically available at http://pathwaynet.princeton.edu. Software was implemented using the open-source Sleipnir library, which is available for download at https://bitbucket.org/libsleipnir/lib", 
  "summary": "We then utilize a Support Vector Machine (SVM) (Noble, 2006) to generate initial predictions of multiple gene interaction types independently in the context of each tissue (i.e. utilizing the tissue-specific gold standard) by integrating $1600 heterogeneous human experimental datasets [e.g. mRNA expression, transcription factor (TF) and kinase motifs and post-translational modifications (PTMs)].\nFinally, our predictions significantly outperform predictions made without incorporating tissue contextual information (i.e. tissueunaware predictions) and the more general coupling of `functional association' (Huttenhower et al., 2009), thus highlighting the importance of interaction type specific integration of multiple data sources (full interaction type performance comparison for all networks shown in Supplementary Fig. S5).", 
  "affiliations": [
    " Lewis-Sigler Institute for Integrative Genomics Princeton University "
  ], 
  "grants": [
    "Motif information for TFs were obtained from JASPAR (Sandelin et al., 2004), DNaseI profiling (Neph et al., 2012a,b), FastCompare (Elemento and Tavazoie, 2005), CISBP (Ray et al., 2013) and TRANSFAC (Matys et al., 2003), miRNA from MSigDB mir database (Subramanian et al., 2005) and EBI MicroCosm database (Griffiths-Jones et al., 2008) and protein kinases from PhosphoMotif (Amanchy et al., 2007).", 
    "Funding\nThis work was supported in part by R01s GM071966 and HG005998 and by P50 GM071508.", 
    "(2003) TRANSFACVR : transcriptional regulation, from patterns to profiles."
  ], 
  "acks": " The authors acknowledge John Wiggins for technical support. ", 
  "authors": [
    " Christopher Y Park", 
    " Arjun Krishnan", 
    " Qian Zhu", 
    " Aaron K Wong", 
    " Young-Suk Lee", 
    " Olga G Troyanskaya"
  ], 
  "keyWords": [
    [
      "genes", 
      "interactions", 
      "tissues", 
      "humans", 
      "predictions", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "https://bitbucket.org/libsleipnir/lib"
  ], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-11-28T15:35:07Z"
}{
  "doi": "10.1093/bioinformatics/btu356", 
  "name": "Toward better understanding of artifacts in variant calling from highcoverage samples", 
  "links": [
    "https://github.com/lh3/varcmp", 
    "http://bit.ly/eelabdb", 
    "http://clinvar.com", 
    "http://bit.ly/1g8XqRt", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://bit.ly/mdust-LC"
  ], 
  "title": "Toward better understanding of artifacts in variant calling from high-coverage samples", 
  "toolName": "varcmp", 
  "abstract": "Motivation: Whole-genome high-coverage sequencing has been widely used for personal and cancer genomics as well as in various research areas. However, in the lack of an unbiased whole-genome truth set, the global error rate of variant calls and the leading causal artifacts still remain unclear even given the great efforts in the evaluation of variant calling methods. Results: We made 10 single nucleotide polymorphism and INDEL call sets with two read mappers and five variant callers, both on a haploid human genome and a diploid genome at a similar coverage. By investigating false heterozygous calls in the haploid genome, we identified the erroneous realignment in low-complexity regions and the incomplete reference genome with respect to the sample as the two major sources of errors, which press for continued improvements in these two areas. We estimated that the error rate of raw genotype calls is as high as 1 in 10\u201315 kb, but the error rate of post-filtered calls is reduced to 1 in 100\u2013200 kb without significant compromise on the sensitivity. Availability and implementation: BWA-MEM alignment and raw variant calls are available at http://bit.ly/1g8XqRt scripts and miscellaneous data at https://github.com/lh3/varcmp.", 
  "summary": "The simplest approach to the evaluation of variant calling is to simulate variants and reads from a reference genome (Li et al., 2008).\nA better simulation is to take the reads sequenced from one sample with a finished genome, map them to another finished genome, call variants and then compare the calls to the differences found by genome-to-genome alignment (Li et al., 2008).\nAlthough the CHM1hTERT cell line is supposed to be haploid, we may still see heterozygous variant calls potentially because: (i) the cell line is not truly haploid; (ii) there are somatic mutations in the cell line; (iii) there are library construction and sequencing errors (Robasky et al., 2014), which ought to be considered by the calling algorithms; and (iv) mapping or variant calling algorithms have flaws.", 
  "affiliations": [], 
  "grants": [
    "Funding: NHGRI U54HG003037; NIH GM100233.", 
    "ACKNOWLEDGEMENTS\nThe authors are grateful to Richard Wilson and his team who sequenced the CHM1 cell line and granted us the permission to use the data in this study."
  ], 
  "sourcelinks": [
    "https://github.com/lh3/varcmp", 
    "http://bit.ly/1g8XqRt"
  ], 
  "acks": " The authors are grateful to Richard Wilson and his team who sequenced the CHM1 cell line and granted us the permission to use the data in this study. The authors also thank the 1000 Genomes Project analysis group for the helpful comments and discussions, and Mike Lin and the anonymous reviewers whose comments have helped us to improve the manuscript. ", 
  "authors": [], 
  "keyWords": [
    "genomes sequenced", 
    [
      "genomics", 
      "filtering", 
      "sequencers", 
      "reads", 
      "errors", 
      "variants"
    ]
  ], 
  "github_data": {
    "name": "varcmp", 
    "contributors": [
      {
        "contributions": 40, 
        "html_url": "https://github.com/lh3"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/lh3/varcmp/zipball/v1", 
        "tarball_url": "https://api.github.com/repos/lh3/varcmp/tarball/v1", 
        "name": "v1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/lh3/varcmp/zipball/rev1", 
        "tarball_url": "https://api.github.com/repos/lh3/varcmp/tarball/rev1", 
        "name": "rev1"
      }
    ], 
    "created_at": "2014-04-07T17:28:24Z", 
    "updated_at": "2016-02-05T14:44:27Z", 
    "languages": [
      "TeX", 
      "JavaScript", 
      "Gnuplot", 
      "Perl"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/lh3"
      }, 
      {
        "html_url": "https://github.com/gatoravi"
      }, 
      {
        "html_url": "https://github.com/ashwinkalbhor"
      }, 
      {
        "html_url": "https://github.com/snewhouse"
      }
    ], 
    "owner": "https://github.com/lh3", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-06-29T00:12:32Z"
}{
  "doi": "10.1093/bioinformatics/btu513", 
  "name": "Trowel a fast and accurate error correction module for Illumina sequencing reads", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://trowel-ec.sourceforge.net"
  ], 
  "title": "Genome analysis Trowel: a fast and accurate error correction module for Illumina sequencing reads", 
  "toolName": "trowel-ec", 
  "abstract": "Motivation: The ability to accurately read the order of nucleotides in DNA and RNA is fundamental for modern biology. Errors in next-generation sequencing can lead to many artifacts, from erroneous genome assemblies to mistaken inferences about RNA editing. Uneven coverage in datasets also contributes to false corrections. Result: We introduce Trowel, a massively parallelized and highly efficient error correction module for Illumina read data. Trowel both corrects erroneous base calls and boosts base qualities based on the k-mer spectrum. With high-quality k-mers and relevant base information , Trowel achieves high accuracy for different short read sequen-cing applications.The latency in the data path has been significantly reduced because of efficient data access and data structures. In performance evaluations, Trowel was highly competitive with other tools regardless of coverage, genome size read length and fragment size. Availability and implementation: Trowel is written in C+ + and is provided under the General Public License v3.0 (GPLv3). It is available at http://trowel", 
  "summary": "Result: We introduce Trowel, a massively parallelized and highly efficient error correction module for Illumina read data.\nWith high-quality k-mers and relevant base information, Trowel achieves high accuracy for different short read sequencing applications.The latency in the data path has been significantly reduced because of efficient data access and data structures.\nThe most widely applied error correction methods rely on k-mer spectrum-based algorithms, following the spectral alignment (SA) approach (Pevzner et al., 2001).\nTrowel has a better performance than the other tools with genome assemblies on high-coverage datasets, while on low-coverage datasets, the alignment-based tool Coral outperformed all other k-merbased methods including Trowel.", 
  "affiliations": [
    " Department of Molecular Biology Max Planck Institute for Developmental Biology "
  ], 
  "grants": [
    "Funding: This work was supported by the Max Planck Society."
  ], 
  "acks": " The authors thank Christa Lanz for sequencing and Xi Wang for making the A. thaliana dataset available. ", 
  "authors": [
    " Eun-Cheon Lim", 
    " Jonas M ", 
    " J \u20ac Org Hagmann", 
    " Stefan R Henz", 
    " Sang-Tae Kim", 
    " Detlef Weigel"
  ], 
  "keyWords": [
    "sequencing reads", 
    "error correction", 
    [
      "errors", 
      "based", 
      "read", 
      "trowel", 
      "supplementary", 
      "datasets", 
      "sequences", 
      "corrections"
    ]
  ], 
  "sourcelinks": [
    "http://trowel-ec.sourceforge.net"
  ], 
  "technologies": [
    "C++"
  ], 
  "dateCreated": "2014-07-30T00:14:11Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/trowel-ec/", 
    "languages": [
      "C++"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/abysslover/", 
        "name": "Euncheon Lim"
      }
    ], 
    "Development Status": [
      {
        "status": "4 - Beta"
      }
    ], 
    "license": [
      {
        "name": "GNU General Public License version 3.0 (GPLv3)"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu523", 
  "name": "Tracking global changes induced in the CD4 Tcell receptor repertoire by immunization with a complex antigen using short stretches of CDR3 protein sequence", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Tracking global changes induced in the CD4 T-cell receptor repertoire by immunization with a complex antigen using short stretches of CDR3 protein sequence", 
  "toolName": "Tracking global changes induced in the CD4 T-cell receptor repertoire by immunization with a complex antigen using short stretches of CDR3 protein sequence", 
  "abstract": "Motivation: The clonal theory of adaptive immunity proposes that immunological responses are encoded by increases in the frequency of lymphocytes carrying antigen-specific receptors. In this study, we measure the frequency of different T-cell receptors (TcR) in CD4 + T cell populations of mice immunized with a complex antigen, killed Mycobacterium tuberculosis, using high throughput parallel sequen-cing of the TcR chain. Our initial hypothesis that immunization would induce repertoire convergence proved to be incorrect, and therefore an alternative approach was developed that allows accurate stratifi-cation of TcR repertoires and provides novel insights into the nature of CD4 + T-cell receptor recognition. Results: To track the changes induced by immunization within this heterogeneous repertoire, the sequence data were classified by counting the frequency of different clusters of short (3 or 4) continuous stretches of amino acids within the antigen binding complementarity determining region 3 (CDR3) repertoire of different mice. Both un-supervised (hierarchical clustering) and supervised (support vector machine) analyses of these different distributions of sequence clusters differentiated between immunized and unimmunized mice with 100% efficiency. The CD4 + TcR repertoires of mice 5 and 14 days post-immunization were clearly different from that of unimmunized mice but were not distinguishable from each other. However, the repertoires of mice 60 days postimmunization were distinct both from naive mice and the day 5/14 animals. Our results reinforce the remarkable diversity of the TcR repertoire, resulting in many diverse private TcRs contributing to the T-cell response even in genetically identical mice responding to the same antigen. However, specific motifs defined by short stretches of amino acids within the CDR3 region may determine TcR specificity and define a new approach to TcR sequence classification. Availability and implementation: The analysis was implemented in R and Python, and source code can be found in Supplementary Data.", 
  "summary": "Results: To track the changes induced by immunization within this heterogeneous repertoire, the sequence data were classified by counting the frequency of different clusters of short (3 or 4) continuous stretches of amino acids within the antigen binding complementarity determining region 3 (CDR3) repertoire of different mice.\nIn this study, we develop an approach based on the well-studied bag-of-words (BOW) (Csurka et al., 2004; Joachims, 1998; Lowe, 1999) algorithm to categorize and classify sets of TcR sequences from immunized and unimmunized mice at different times postimmunization.", 
  "affiliations": [
    " UCL CoMPLEX", 
    " Weizmann Institute of Science", 
    " UCL Division of Infection and Immunity"
  ], 
  "grants": [
    "Funding: Engineering and Physical Sciences Research Council UK, the Medical Research Council UK, Microsoft Research, and Weizmann UK."
  ], 
  "acks": " ", 
  "authors": [
    " Niclas Thomas", 
    " Katharine Best", 
    " Mattia Cinelli", 
    " Shlomit Reich-Zeliger", 
    " Hilah Gal", 
    " Eric Shifrut", 
    " Asaf Madi", 
    " Nir Friedman", 
    " John Shawe-Taylor", 
    " Benny Chain"
  ], 
  "keyWords": [
    "receptor repertoire", 
    "mice immunized", 
    [
      "sequencing", 
      "repertoires", 
      "antigenic", 
      "immunizations", 
      "receptors"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-06T05:13:41Z"
}{
  "doi": "10.1093/bioinformatics/btu733", 
  "name": "TreeEFM calculating elementary flux modes using linear optimization in a treebased algorithm", 
  "links": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "TreeEFM: calculating elementary flux modes using linear optimization in a tree-based algorithm", 
  "toolName": "TreeEFM: calculating elementary flux modes using linear optimization in a tree-based algorithm", 
  "abstract": "Motivation: Elementary flux modes (EFMs) analysis constitutes a fundamental tool in systems biology. However, the efficient calculation of EFMs in genome-scale metabolic networks (GSMNs) is still a challenge. We present a novel algorithm that uses a linear programming-based tree search and efficiently enumerates a subset of EFMs in GSMNs. Results: Our approach is compared with the EFMEvolver approach, demonstrating a significant improvement in computation time. We also validate the usefulness of our new approach by studying the acetate overflow metabolism in the Escherichia coli bacteria. To do so, we computed 1 million EFMs for each energetic amino acid and then analysed the relevance of each energetic amino acid based on gene/protein expression data and the obtained EFMs. We found good agreement between previous experiments and the conclusions reached using EFMs. Finally, we also analysed the performance of our approach when applied to large GSMNs. Availability and implementation: The stand-alone software TreeEFM is implemented in C\u00fe\u00fe and interacts with the open-source linear solver COIN-OR Linear program Solver (CLP).", 
  "summary": "Here we analyse the relevance of eight energetic amino acids in the overflow metabolism using our approach to compute a large set of EFMs. We then apply the methodology presented in Rezola et al.\nIn this section, we first study the computational performance of the TreeEFM approach when calculating the aforementioned 1 million EFMs. Then, we analyse the obtained results after projecting the experimental data in Valgepea et al.\nThe usefulness of the TreeEFM approach is validated with the successful enumeration of 1 million EFMs in the genome-scale metabolic reconstruction of E.coli (Feist et al., 2007).", 
  "affiliations": [
    " CEIT and TECNUN University of Navarra ", 
    " Mathematical Sciences Brunel University ", 
    " Computer Engineering Department School of Computer Science University of Murcia "
  ], 
  "grants": [
    "Funding\nThe work of Jon Pey and Luis Tobalina was supported by the Basque Government."
  ], 
  "acks": " The authors thank Kaspar Valgepea for providing us with the gene/protein expression data in the acetate overflow metabolism, as well as Juliane Gebauer and Christoph Kaleta for kindly providing us with the data in Table 1. The authors thank the anonymous referees for their valuable comments and suggestions , which improved the manuscript significantly. The work of Jon Pey and Luis Tobalina was supported by the Basque Government. In addition, this work was partially supported by the Fundaci\u00f3 n S\u00e9neca (Agencia Regional de Ciencia y Tecnolog\u00eda, Regi\u00f3 n de Murcia); the Spanish MEC and European Commission FEDER; and the Minister of Economy and Competitiveness of Spain. Conflict of interest: none declared. ", 
  "authors": [
    " Jon Pey", 
    " Juan A Villar", 
    " Luis Tobalina", 
    " Alberto Rezola", 
    " Jos\u00e9 Manuel Garc\u00eda", 
    " John E Beasley", 
    " Francisco J Planes"
  ], 
  "keyWords": [
    [
      "reactions", 
      "nodes", 
      "metabolisms", 
      "efms"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "C"
  ], 
  "dateCreated": "2014-11-08T01:28:35Z"
}{
  "doi": "10.1093/bioinformatics/btu607", 
  "name": "UNDO a Bioconductor R package for unsupervised deconvolution of mixed gene expressions in tumor samples", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://bioconductor.org/packages"
  ], 
  "title": "Data and text mining UNDO: a Bioconductor R package for unsupervised deconvolution of mixed gene expressions in tumor samples", 
  "toolName": "Data and text mining UNDO: a Bioconductor R package for unsupervised deconvolution of mixed gene expressions in tumor samples", 
  "abstract": "We develop a novel unsupervised deconvolution method, within a well-grounded mathematical framework, to dissect mixed gene expressions in heterogeneous tumor samples. We implement an R package, UNsupervised DecOnvolution (UNDO), that can be used to automatically detect cell-specific marker genes (MGs) located on the scatter radii of mixed gene expressions, estimate cellular proportions in each sample and deconvolute mixed expressions into cell-specific expression profiles. We demonstrate the performance of UNDO over a wide range of tumor\u2013stroma mixing proportions, validate UNDO on various biologically mixed benchmark gene expression datasets and further estimate tumor purity in TCGA/CPTAC datasets. The highly accurate deconvolution results obtained suggest not only the existence of cell-specific MGs but also UNDO's ability to detect them blindly and correctly. Although the principal application here involves microarray gene expressions, our methodology can be readily applied to other types of quantitative molecular profiling data. Availability and implementation: UNDO is available at http://biocon-ductor.org/packages.", 
  "summary": "We implement an R package, UNsupervised DecOnvolution (UNDO), that can be used to automatically detect cell-specific marker genes (MGs) located on the scatter radii of mixed gene expressions, estimate cellular proportions in each sample and deconvolute mixed expressions into cellspecific expression profiles.\nUNDO method again accurately estimated the cell mixing proportions with E1 = 0.0778 (rp = 0.99) and cell-specific expression profiles with average rp = 0.99 and rc = 0.98 between the deconvoluted expression profile and the measured expression profile in the pure cell lines (Fig. 2).", 
  "affiliations": [
    " Departments of Pathology and Oncology Johns Hopkins University ", 
    " Department of Surgery Memorial Sloan-Kettering Cancer Center ", 
    " Department of Electrical and Computer Engineering Virginia Polytechnic Institute and State University ", 
    " Lombardi Comprehensive Cancer Center Georgetown University ", 
    " Department of Molecular Carcinogenesis The University of Texas MD Anderson Cancer Center "
  ], 
  "grants": [
    "Funding: This work was supported by National Institutes of Health [Grant numbers NS29525, CA149147, CA160036, HL111362], in part."
  ], 
  "acks": " ", 
  "authors": [
    " Niya Wang", 
    " Ting Gong", 
    " Robert Clarke", 
    " Lulu Chen", 
    " Ie-Ming Shih", 
    " Zhen Zhang", 
    " Douglas A Levine", 
    " Jianhua Xuan", 
    " Yue Wang"
  ], 
  "keyWords": [
    "gene expressions", 
    [
      "data", 
      "cells", 
      "genes", 
      "undo", 
      "expression", 
      "tumors"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-09-12T02:13:15Z"
}{
  "doi": "10.1093/bioinformatics/btv024", 
  "name": "Ultrafast SNP analysis using the BurrowsWheeler transform of shortread data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://picard.sourceforge.net"
  ], 
  "title": "Ultrafast SNP analysis using the Burrows\u2013Wheeler transform of short-read data", 
  "toolName": "picard", 
  "abstract": "Motivation: Sequence-variation analysis is conventionally performed on mapping results that are highly redundant and occasionally contain undesirable heuristic biases. A straightforward approach to single-nucleotide polymorphism (SNP) analysis, using the Burrows\u2013Wheeler transform (BWT) of short-read data, is proposed. Results: The BWT makes it possible to simultaneously process collections of read fragments of the same sequences; accordingly, SNPs were found from the BWT much faster than from the mapping results. It took only a few minutes to find SNPs from the BWT (with a supplementary data, fragment depth of coverage [FDC]) using a desktop workstation in the case of human exome or transcrip-tome sequencing data and 20 min using a dual-CPU server in the case of human genome sequenc-ing data. The SNPs found with the proposed method almost agreed with those found by a time-consuming state-of-the-art tool, except for the cases in which the use of fragments of reads led to sensitivity loss or sequencing depth was not sufficient. These exceptions were predictable in advance on the basis of minimum length for uniqueness (MLU) and FDC defined on the reference genome. Moreover, BWT and FDC were computed in less time than it took to get the mapping results , provided that the data were large enough. Availability and implementation: A proof-of-concept binary code for a Linux platform is available on request to the corresponding", 
  "summary": "For any DNA sequence, w, the number of occurrences of w in the reference genome and that in the short-read data, denoted by NG w and NRw, are immediately computed from TG and TR, respectively, by using rank functions (Gonza lez et al., 2005) [a.k.a. LF mappings (Ferragina and Manzini, 2000)].\n(a) The FDCs at x in the positive and negative directions, dx  and dx , are defined as the number of occurrences in the short-read data of genomic fragments on the positive and negative strands starting at x with length `x  and `x , which are chosen to be larger than MLU: `6x   k6x   a for a small positive constant a.", 
  "affiliations": [
    " Biosystems Research Department Central Research Laboratory Hitachi, Ltd "
  ], 
  "grants": [], 
  "acks": " ", 
  "authors": [
    " Kouichi Kimura", 
    " Asako Koike"
  ], 
  "keyWords": [
    [
      "genomic", 
      "reads", 
      "sequencers", 
      "methods", 
      "data"
    ]
  ], 
  "sourcelinks": [
    "http://picard.sourceforge.net"
  ], 
  "technologies": [
    "C", 
    "R"
  ], 
  "dateCreated": "2015-01-22T03:50:58Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/picard/", 
    "languages": [
      "Java"
    ], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/jrobinso/", 
        "name": "James Robinson"
      }, 
      {
        "url": "https://sourceforge.net/u/geoffjentry/", 
        "name": "Jeff Gentry"
      }, 
      {
        "url": "https://sourceforge.net/u/gbggrant/", 
        "name": "gbggrant"
      }, 
      {
        "url": "https://sourceforge.net/u/bhandsaker/", 
        "name": "Bob Handsaker"
      }, 
      {
        "url": "https://sourceforge.net/u/jacobbroad/", 
        "name": "Jacob"
      }, 
      {
        "url": "https://sourceforge.net/u/droazen/", 
        "name": "droazen"
      }, 
      {
        "url": "https://sourceforge.net/u/delangel1/", 
        "name": "Guillermo del Angel"
      }, 
      {
        "url": "https://sourceforge.net/u/jmthibault/", 
        "name": "Joel Thibault"
      }, 
      {
        "url": "https://sourceforge.net/u/tfenne/", 
        "name": "Tim Fennell"
      }, 
      {
        "url": "https://sourceforge.net/u/yfarjoun/", 
        "name": "Yossi"
      }, 
      {
        "url": "https://sourceforge.net/u/alecw/", 
        "name": "Alec Wysoker"
      }, 
      {
        "url": "https://sourceforge.net/u/nilshomer/", 
        "name": "Nils Homer"
      }, 
      {
        "url": "https://sourceforge.net/u/brilliantred/", 
        "name": "Kathleen Tibbetts"
      }, 
      {
        "url": "https://sourceforge.net/u/ami-gatk/", 
        "name": "Ami Levy Moonshine"
      }, 
      {
        "url": "https://sourceforge.net/u/vdauwera/", 
        "name": "Geraldine A Van der Auwera"
      }, 
      {
        "url": "https://sourceforge.net/u/vadimzalunin/", 
        "name": "Vadim Zalunin"
      }, 
      {
        "url": "https://sourceforge.net/u/bradtaylor/", 
        "name": "Brad Taylor"
      }, 
      {
        "url": "https://sourceforge.net/u/jacarey/", 
        "name": "jcarey"
      }, 
      {
        "url": "https://sourceforge.net/u/eitanbanks/", 
        "name": "Eric Banks"
      }
    ], 
    "Development Status": [
      {
        "status": "5 - Production/Stable"
      }
    ], 
    "license": [
      {
        "name": "MIT License"
      }, 
      {
        "name": "Apache License V2.0"
      }
    ]
  }
}{
  "doi": "10.1093/bioinformatics/btu730", 
  "name": "Ulysses accurate detection of lowfrequency structural variations in large insertsize sequencing libraries", 
  "links": [
    "http://www.lcqb.upmc.fr/ulysses", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.lcqb.upmc.fr", 
    "http://www.isoft.fr/bio/biopack_en.htm"
  ], 
  "title": "Ulysses: accurate detection of low-frequency structural variations in large insert-size sequencing libraries", 
  "toolName": "Ulysses: accurate detection of low-frequency structural variations in large insert-size sequencing libraries", 
  "abstract": "Motivation: The detection of structural variations (SVs) in short-range Paired-End (PE) libraries remains challenging because SV breakpoints can involve large dispersed repeated sequences, or carry inherent complexity, hardly resolvable with classical PE sequencing data. In contrast, large insert-size sequencing libraries (Mate-Pair libraries) provide higher physical coverage of the gen-ome and give access to repeat-containing regions. They can thus theoretically overcome previous limitations as they are becoming routinely accessible. Nevertheless, broad insert size distributions and high rates of chimerical sequences are usually associated to this type of libraries, which makes the accurate annotation of SV challenging. Results: Here, we present Ulysses, a tool that achieves drastically higher detection accuracy than existing tools, both on simulated and real mate-pair sequencing datasets from the 1000 Human Genome project. Ulysses achieves high specificity over the complete spectrum of variants by assessing , in a principled manner, the statistical significance of each possible variant (duplications, deletions, translocations, insertions and inversions) against an explicit model for the generation of experimental noise. This statistical model proves particularly useful for the detection of low frequency variants. SV detection performed on a large insert Mate-Pair library from a breast cancer sample revealed a high level of somatic duplications in the tumor and, to a lesser extent, in the blood sample as well. Altogether, these results show that Ulysses is a valuable tool for the characterization of somatic mosaicism in human tissues and in cancer genomes. Availability and implementation: Ulysses is available at", 
  "summary": "Benchmarks on real MP sequencing datasets from the 1000 Human Genome project, on MP simulated datasets as well as on a breast cancer tumor MP library showed that Ulysses outperforms three commonly used detection tools [Breakdancer (Chen et al., 2009), GASVpro (Sindi et al., 2012) and Delly (Rausch et al., 2012)] for all types of SV and notably for low frequency structural variants in MP libraries.\nResults are presented only for Ulysses, Delly and BreakDancer (SV detection could not finish in reasonable time with GASVpro on MP datasets, even with sequences split by chromosome, see Section 2).", 
  "affiliations": [], 
  "grants": [
    "Funding\nThis work was supported by the Agence Nationale pour la Recherche grant 2010 BLAN1606 and by an ATIP grant from the Centre National de la\n\nRecherche Scientifique (CNRS)."
  ], 
  "acks": " The authors thank our colleagues from LCQB for fruitful discussions and Jean-Philippe Meyniel from ISoft for invaluable tips and advices for pipeline developments in AMADEA. This work was supported by the Agence Nationale pour la Recherche grant 2010 BLAN1606 and by an ATIP grant from the Centre National de la Recherche Scientifique (CNRS). H.R. was partly supported by a Japan Society for the Promotion of Science (JSPS) fellowship. Conflict of interest: none declared. ", 
  "authors": [
    " Alexandre Gillet-Markowska", 
    " Hugues Richard", 
    " Gilles Fischer", 
    " Ingrid Lafontaine", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "coverages", 
      "genomics", 
      "detectable", 
      "ulysses", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-08T01:28:35Z"
}{
  "doi": "10.1093/bioinformatics/btu739", 
  "name": "UniRef clusters a comprehensive and scalable alternative for improving sequence similarity searches", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.uniprot.org/blast", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches", 
  "toolName": "UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches", 
  "abstract": "Motivation: UniRef databases provide full-scale clustering of UniProtKB sequences and are utilized for a broad range of applications, particularly similarity-based functional annotation. Non-redundancy and intra-cluster homogeneity in UniRef were recently improved by adding a sequence length overlap threshold. Our hypothesis is that these improvements would enhance the speed and sensitivity of similarity searches and improve the consistency of annotation within clusters. Results: Intra-cluster molecular function consistency was examined by analysis of Gene Ontology terms. Results show that UniRef clusters bring together proteins of identical molecular function in more than 97% of the clusters, implying that clusters are useful for annotation and can also be used to detect annotation inconsistencies. To examine coverage in similarity results, BLASTP searches against UniRef50 followed by expansion of the hit lists with cluster members demonstrated advantages compared with searches against UniProtKB sequences; the searches are concise ($7 times shorter hit list before expansion), faster ($6 times) and more sensitive in detection of remote similarities (>96% recall at e-value <0.0001). Our results support the use of UniRef clusters as a comprehensive and scalable alternative to native sequence databases for similarity searches and reinforces its reliability for use in functional annotation. Availability and implementation: Web access and file download from UniProt website at", 
  "summary": "Considering GO molecular function annotations in UniProtKB are predominantly assigned using sequence similarity-based methods (e.g. InterPro), we also tested whether our results are biased by a data circularity problem and replicated our consistency analysis on UniRef clusters containing members from 12 model organisms with comprehensive and reliable GO annotation: nine are part of the GO Reference set (Reference Genome Group of the Gene Ontology Consortium, 2009), including Arabidopsis thaliana, Caenorhabditis elegans, Danio rerio, Drosophila melanogaster, Gallus, Homo sapiens, Mus musculus, Rattus norvegicus and Saccharomyces cerevisiae; the remaining three are Bos Taurus, Canis familiaris and Sus scrofa.", 
  "affiliations": [
    " Center for Bioinformatics and Computational Biology and Protein Information Resource University of Delaware "
  ], 
  "grants": [
    "Funding\nThis project is supported by the UniProt grant U41HG006104 from the National Institutes of Health."
  ], 
  "acks": " The authors are grateful to our UniProt Consortium collaborators at the Swiss Institute of Bioinformatics and European Bioinformatics Institute for their fruitful discussions. Authors would like to thank Drs Darren A. Natale, Shawn Polson, and John S. Garavelli for reviewing the manuscript prior to submission. This project is supported by the UniProt grant U41HG006104 from the National Institutes of Health. Conflict of interest: none declared. ", 
  "authors": [
    " Baris E Suzek", 
    " Yuqi Wang", 
    " Hongzhan Huang", 
    " Peter B Mcgarvey", 
    " Cathy H Wu", 
    " Uniprot Consortium"
  ], 
  "keyWords": [
    [
      "clustering", 
      "proteins", 
      "searching", 
      "uniref", 
      "databases", 
      "sequencing"
    ]
  ], 
  "sourcelinks": [
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-15T04:10:48Z"
}{
  "doi": "10.1093/bioinformatics/btu843", 
  "name": "UProC tools for ultrafast protein domain classification", 
  "links": [
    "http://uproc.gobics.de/downloads", 
    "http://creativecommons.org/licenses/by/4.0", 
    "https://github", 
    "http://uproc", 
    "http://hmpdacc.org", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "UProC: tools for ultra-fast protein domain classification", 
  "toolName": "github", 
  "abstract": "Motivation: With rapidly increasing volumes of biological sequence data the functional analysis of new sequences in terms of similarities to known protein families challenges classical bioinformatics. Results: The ultrafast protein classification (UProC) toolbox implements a novel algorithm ('Mosaic Matching') for large-scale sequence analysis. UProC is by three orders of magnitude faster than profile-based methods and in a metagenome simulation study achieved up to 80% higher sensitivity on unassembled 100 bp reads. Availability and implementation: UProC is available as an open-source software at https://github. com/gobics/uproc. Precompiled databases (Pfam) are linked on the UProC homepage: http://uproc. gobics.de/.", 
  "summary": "The Pfam (Finn et al., 2010) database of protein families in combination with the HMMER (Eddy, 1998) profile hidden Markov models is widely used for functional annotation of genomic and metagenomic sequences.\nTo compare UProC with state-of-the-art profile methods for protein domain detection we also measured the classification performance of HMMER and RPS-BLAST (Marchler-Bauer et al., 2002) on the same short read test data.\nThe computationally more expensive profile methods that provide state-of-the-art classification performance on full-length protein sequences might not be optimal for this kind of short read data.", 
  "affiliations": [], 
  "grants": [
    "Funding\nThis work was partially funded by the German Research Foundation (DFG) (grant number ME 3138)."
  ], 
  "sourcelinks": [
    "https://github", 
    "http://uproc.gobics.de/downloads", 
    "http://uproc"
  ], 
  "acks": " Many thanks to Robin Martinjak, Heiner Klingenberg and Rasmus Steinkamp for technical support and Thomas Lingner for reading and discussing the manuscript. ", 
  "authors": [], 
  "keyWords": [
    [
      "uproc", 
      "bioinformatics", 
      "proteins", 
      "positional", 
      "words", 
      "databases", 
      "sequencing", 
      "reading"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "BSD 3-clause \"New\" or \"Revised\" License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/michael/github/master/LICENSE"
      }
    ], 
    "name": "github", 
    "contributors": [
      {
        "contributions": 88, 
        "html_url": "https://github.com/AurelioDeRosa"
      }, 
      {
        "contributions": 44, 
        "html_url": "https://github.com/clayreimann"
      }, 
      {
        "contributions": 41, 
        "html_url": "https://github.com/ingalls"
      }, 
      {
        "contributions": 38, 
        "html_url": "https://github.com/aendrew"
      }, 
      {
        "contributions": 31, 
        "html_url": "https://github.com/mattpass"
      }, 
      {
        "contributions": 12, 
        "html_url": "https://github.com/darvin"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/iamdanfox"
      }, 
      {
        "contributions": 7, 
        "html_url": "https://github.com/coderaiser"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/ctalau"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/STRd6"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/captn3m0"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/jlord"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/knsh14"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/mtscout6"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/maxogden"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/randalpinto"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/raphink"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/tricknotes"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/ele828"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/kpdecker"
      }, 
      {
        "contributions": 2, 
        "html_url": "https://github.com/tristen"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/PeterDaveHello"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/arosenberg01"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/BernhardBezdek"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/mscdex"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/cassioscabral"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/dafortune"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/klcodanr"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/divergentdave"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/incrop"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.3.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.3.0", 
        "name": "v2.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.2.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.2.0", 
        "name": "v2.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.1.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.1.0", 
        "name": "v2.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v2.0.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v2.0.0", 
        "name": "v2.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.3.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.3.0", 
        "name": "v1.3.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.2.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.2.1", 
        "name": "v1.2.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.2.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.2.0", 
        "name": "v1.2.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.1.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.1.0", 
        "name": "v1.1.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v1.0.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v1.0.0", 
        "name": "v1.0.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.2", 
        "name": "v0.11.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.1", 
        "name": "v0.11.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.11.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.11.0", 
        "name": "v0.11.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.7", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.7", 
        "name": "v0.10.7"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.6", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.6", 
        "name": "v0.10.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.5", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.5", 
        "name": "v0.10.5"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.4", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.4", 
        "name": "v0.10.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.3", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.3", 
        "name": "v0.10.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.2", 
        "name": "v0.10.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.1", 
        "name": "v0.10.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.10.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.10.0", 
        "name": "v0.10.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.9.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.9.2", 
        "name": "v0.9.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.9.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.9.0", 
        "name": "v0.9.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.8.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.8.1", 
        "name": "v0.8.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.8.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.8.0", 
        "name": "v0.8.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.7.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.7.0", 
        "name": "v0.7.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.6.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.6.0", 
        "name": "v0.6.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.2", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.2", 
        "name": "v0.5.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.1", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.1", 
        "name": "v0.5.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.5.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.5.0", 
        "name": "v0.5.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/michael/github/zipball/v0.4.0", 
        "tarball_url": "https://api.github.com/repos/michael/github/tarball/v0.4.0", 
        "name": "v0.4.0"
      }
    ], 
    "created_at": "2012-03-06T18:08:53Z", 
    "updated_at": "2016-08-09T15:55:59Z", 
    "languages": [
      "Shell", 
      "JavaScript", 
      "HTML"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/GamesDev"
      }, 
      {
        "html_url": "https://github.com/fokusferit"
      }, 
      {
        "html_url": "https://github.com/rocel"
      }, 
      {
        "html_url": "https://github.com/thinkxl"
      }, 
      {
        "html_url": "https://github.com/schlagobers"
      }, 
      {
        "html_url": "https://github.com/jasonyandell"
      }, 
      {
        "html_url": "https://github.com/t0n1"
      }, 
      {
        "html_url": "https://github.com/timrchavez"
      }, 
      {
        "html_url": "https://github.com/apsrose"
      }, 
      {
        "html_url": "https://github.com/Bediako"
      }, 
      {
        "html_url": "https://github.com/kooyeed"
      }, 
      {
        "html_url": "https://github.com/PivotLogix"
      }, 
      {
        "html_url": "https://github.com/Timothee"
      }, 
      {
        "html_url": "https://github.com/esimionato"
      }, 
      {
        "html_url": "https://github.com/alixcan"
      }, 
      {
        "html_url": "https://github.com/FrediBach"
      }, 
      {
        "html_url": "https://github.com/antiface"
      }, 
      {
        "html_url": "https://github.com/ghostx2013"
      }, 
      {
        "html_url": "https://github.com/dnordstrom"
      }, 
      {
        "html_url": "https://github.com/bernardoantunes"
      }, 
      {
        "html_url": "https://github.com/petrosh"
      }, 
      {
        "html_url": "https://github.com/cloudtrends"
      }, 
      {
        "html_url": "https://github.com/cgkio"
      }, 
      {
        "html_url": "https://github.com/greenflag"
      }, 
      {
        "html_url": "https://github.com/loopByte"
      }, 
      {
        "html_url": "https://github.com/hasithaAlex"
      }, 
      {
        "html_url": "https://github.com/malei0311"
      }, 
      {
        "html_url": "https://github.com/majicmike"
      }, 
      {
        "html_url": "https://github.com/wuwenvogue"
      }, 
      {
        "html_url": "https://github.com/ingalls"
      }
    ], 
    "owner": "https://github.com/michael", 
    "homepage": ""
  }, 
  "technologies": [], 
  "dateCreated": "2014-12-25T04:16:00Z"
}{
  "doi": "10.1093/bioinformatics/btu706", 
  "name": "Using 2k  2 bubble searches to find single nucleotide polymorphisms in kmer graphs", 
  "links": [
    "https://github.com/danmaclean/2kplus2", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Using 2k 1 2 bubble searches to find single nucleotide polymorphisms in k-mer graphs", 
  "toolName": "2kplus2", 
  "abstract": "Motivation: Single nucleotide polymorphism (SNP) discovery is an important preliminary for understanding genetic variation. With current sequencing methods, we can sample genomes comprehensively. SNPs are found by aligning sequence reads against longer assembled references. De Bruijn graphs are efficient data structures that can deal with the vast amount of data from modern technologies. Recent work has shown that the topology of these graphs captures enough information to allow the detection and characterization of genetic variants, offering an alternative to alignment-based methods. Such methods rely on depth-first walks of the graph to identify closing bifurcations. These methods are conservative or generate many false-positive results, particularly when traversing highly interconnected (complex) regions of the graph or in regions of very high coverage. Results: We devised an algorithm that calls SNPs in converted De Bruijn graphs by enumerating 2k \u00fe 2 cycles. We evaluated the accuracy of predicted SNPs by comparison with SNP lists from alignment-based methods. We tested accuracy of the SNP calling using sequence data from 16 ecotypes of Arabidopsis thaliana and found that accuracy was high. We found that SNP calling was even across the genome and genomic feature types. Using sequence-based attributes of the graph to train a decision tree allowed us to increase accuracy of SNP calls further. Together these results indicate that our algorithm is capable of finding SNPs accurately in complex sub-graphs and potentially comprehensively from whole genome graphs.", 
  "summary": "Each bubble in a graph was classified as a Real SNP or a false positive according to its presence in the SNP list and were divided into a training set and testing set.\nTwo-third of the dataset is used for training the classifier and one third for testing.\nTo train, we used a decision tree found in the freely available WEKA package (Hall, 2009) and kept the default parameters of the classifier.\nClassification accuracy of SNPs called using the Decision Tree algorithm to classify bubbles found in 2k  2 searches on graphs made from sequence data of 13 ecotypes of A.thaliana.\nSci., 6393, 147158.\nSci. USA, 98, 97489753.", 
  "affiliations": [
    " The Sainsbury Laboratory Norwich Research Park "
  ], 
  "grants": [
    "was funded by a BBSRC (Biotechnology and Biological Sciences Research Council) TRDF2 grant (ref: BB/I023798/1) to D.M.", 
    "Funding\nR.Y."
  ], 
  "sourcelinks": [
    "https://github.com/danmaclean/2kplus2"
  ], 
  "acks": " The authors wish to thank Dr Mario Caccamo, Dr Richard Leggett, Ricardo Ramirez-Gonzalez and Dr Graham Etherington for assistance during this project. ", 
  "authors": [
    " Reda Younsi", 
    " Dan Maclean"
  ], 
  "keyWords": [
    [
      "snps", 
      "genomics", 
      "bubbles", 
      "data", 
      "graphs"
    ]
  ], 
  "github_data": {
    "name": "2kplus2", 
    "contributors": [
      {
        "contributions": 140, 
        "html_url": "https://github.com/redayounsi"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/danmaclean"
      }
    ], 
    "versions": [], 
    "created_at": "2013-01-23T14:13:22Z", 
    "updated_at": "2016-04-16T22:39:22Z", 
    "languages": [
      "C", 
      "Shell", 
      "C++"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/danmaclean"
      }
    ], 
    "owner": "https://github.com/redayounsi", 
    "homepage": null
  }, 
  "technologies": [], 
  "dateCreated": "2014-10-25T03:54:31Z"
}{
  "doi": "10.1093/bioinformatics/btu828", 
  "name": "VarSim a highfidelity simulation and validation framework for highthroughput genome sequencing with cancer applications", 
  "links": [
    "http://bioinform.github.io/varsim", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "VarSim: a high-fidelity simulation and validation framework for high-throughput genome sequencing with cancer applications", 
  "toolName": "varsim", 
  "abstract": "VarSim is a framework for assessing alignment and variant calling accuracy in high-throughput genome sequencing through simulation or real data. In contrast to simulating a random mutation spectrum, it synthesizes diploid genomes with germline and somatic mutations based on a realistic model. This model leverages information such as previously reported mutations to make the synthetic genomes biologically relevant. VarSim simulates and validates a wide range of variants, including single nucleotide variants, small indels and large structural variants. It is an automated, comprehensive compute framework supporting parallel computation and multiple read simulators. Furthermore, we developed a novel map data structure to validate read alignments, a strategy to compare variants binned in size ranges and a lightweight, interactive, graphical report to visualize validation results with detailed statistics. Thus far, it is the most comprehensive validation tool for secondary analysis in next generation sequencing. Availability and implementation: Code in Java and Python along with instructions to download the reads and variants is at", 
  "summary": "Summary: VarSim is a framework for assessing alignment and variant calling accuracy in highthroughput genome sequencing through simulation or real data.\nFor generating a perturbed genome, VarSim samples small variants and SVs from existing databases (e.g. dbSNP, DGV) and/or a provided VCF file.\nWe demonstrated VarSim's completeness in both simulation and validation by simulating NA12878's personal genome with small variants from genome in a bottle (GiaB) high-confidence regions (Zook et al., 2014), and with SVs from 1000 genomes (Mills et al., 2011) and DGV (MacDonald et al., 2014).\nVarSim is the most comprehensive pipeline for simulation and validation of secondary analysis, covering both small variants and SVs on a diploid genome.", 
  "affiliations": [
    " Program in Computational Biology and Bioinformatics Yale University ", 
    " Department of Health Sciences Research Mayo Clinics ", 
    " Department of Bioinformatics Bina Technologies "
  ], 
  "grants": [
    "were supported by National Institute of Health grants [1R01HG006018] and [1R01GM109836].", 
    "Funding\nJ.C.M."
  ], 
  "sourcelinks": [
    "http://bioinform.github.io/varsim"
  ], 
  "acks": " We would like to thank Aparna Chhibber, Christopher Yau and Li Tai Fang for their valuable comments and advice. ", 
  "authors": [
    " John C Mu", 
    " Marghoob Mohiyuddin", 
    " Jian Li", 
    " Narges Bani Asadi", 
    " Mark B Gerstein", 
    " Alexej Abyzov", 
    " Wing H Wong", 
    " Hugo Y K Lam"
  ], 
  "keyWords": [
    "genome sequencing", 
    "tools simulate", 
    [
      "simulating", 
      "varsim", 
      "tool", 
      "genomes", 
      "sequences", 
      "variants", 
      "reading"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "BSD 2-clause \"Simplified\" License"
      }, 
      {
        "link": "https://raw.githubusercontent.com/bioinform/varsim/master/LICENSE.txt"
      }
    ], 
    "name": "varsim", 
    "contributors": [
      {
        "contributions": 238, 
        "html_url": "https://github.com/johnmu"
      }, 
      {
        "contributions": 112, 
        "html_url": "https://github.com/marghoob"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/bayolau"
      }
    ], 
    "versions": [
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.6.3", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.6.3", 
        "name": "v0.6.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.6.2", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.6.2", 
        "name": "v0.6.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.6.1", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.6.1", 
        "name": "v0.6.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.6", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.6", 
        "name": "v0.6"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.5.4", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.5.4", 
        "name": "v0.5.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.5.3", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.5.3", 
        "name": "v0.5.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.5.2", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.5.2", 
        "name": "v0.5.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.5.1", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.5.1", 
        "name": "v0.5.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.5.0", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.5.0", 
        "name": "v0.5.0"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.4.1", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.4.1", 
        "name": "v0.4.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.4", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.4", 
        "name": "v0.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.3.4", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.3.4", 
        "name": "v0.3.4"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.3.3", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.3.3", 
        "name": "v0.3.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.3.2", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.3.2", 
        "name": "v0.3.2"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.3.1", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.3.1", 
        "name": "v0.3.1"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/v0.3", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/v0.3", 
        "name": "v0.3"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/0.2-beta", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/0.2-beta", 
        "name": "0.2-beta"
      }, 
      {
        "zipball_url": "https://api.github.com/repos/bioinform/varsim/zipball/0.1", 
        "tarball_url": "https://api.github.com/repos/bioinform/varsim/tarball/0.1", 
        "name": "0.1"
      }
    ], 
    "created_at": "2014-05-12T22:59:15Z", 
    "updated_at": "2016-08-08T17:48:22Z", 
    "languages": [
      "Python", 
      "Shell", 
      "HTML", 
      "Java"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/johnmu"
      }, 
      {
        "html_url": "https://github.com/marghoob"
      }, 
      {
        "html_url": "https://github.com/hugolam"
      }, 
      {
        "html_url": "https://github.com/bayolau"
      }, 
      {
        "html_url": "https://github.com/AndrewUzilov"
      }, 
      {
        "html_url": "https://github.com/yanickpouliot"
      }, 
      {
        "html_url": "https://github.com/gdgib-bina"
      }
    ], 
    "owner": "https://github.com/bioinform", 
    "homepage": "http://bioinform.github.io/varsim/"
  }, 
  "technologies": [], 
  "dateCreated": "2014-12-19T10:40:49Z"
}{
  "doi": "10.1093/bioinformatics/btu650", 
  "name": "VCF2Networks applying genotype networks to singlenucleotide variants data", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "https://bitbucket.org/dalloliogm/vcf2networks"
  ], 
  "title": "Genetics and population analysis VCF2Networks: applying genotype networks to single-nucleotide variants data", 
  "toolName": "Genetics and population analysis VCF2Networks: applying genotype networks to single-nucleotide variants data", 
  "abstract": "A wealth of large-scale genome sequencing projects opens the doors to new approaches to study the relationship between genotype and phenotype. One such opportunity is the possibility to apply genotype networks analysis to population genetics data. Genotype networks are a representation of the set of genotypes associated with a single phenotype, and they allow one to estimate properties such as the robustness of the phenotype to mutations, and the ability of its associated genotypes to evolve new adaptations. So far, though, genotype networks analysis has rarely been applied to population genetics data. To help fill this gap, here we present VCF2Networks, a tool to determine and study genotype network structure from single-nucleotide variant data.", 
  "summary": "In these cases, genotype networks were used to predict the robustness of a phenotype to mutations, and the potential of the underlying genotypes to evolve new and innovative traits.\nFor example, the number of nodes and the average node degree can be interpreted as a measure of a phenotype's robustness (Iban~ ezMarcelo and Alarco n, 2014), while the diameters of some networks can serve as proxy for innovative potentials (Ciliberti et al., 2007).\nAs the number of samples is lower than in 1000 genomes file, we use a network size of only five SNVs. Regions showing high robustness (high average degree) and high evolvability (high average path length and diameter) may be important in the evolution of the cancer phenotype.", 
  "affiliations": [
    " Institute of Evolutionary Biology and Environmental Studies/Swiss Institute of Bioinformatics University of Zurich ", 
    " Department of Experimental and Health Sciences Institut de Biologia Evolutiva (CSIC-Universitat Pompeu Fabra) "
  ], 
  "grants": [
    "Funding: This study has been possible thanks the grant BFU201343726-P awarded by Ministerio de Economia y Competitividad (Spain) and with the support of Secretaria d'Universitats i Recerca del Departament d'Economia i Coneixement de la Generalitat de Catalunya (GRC 2014 SGR 866)."
  ], 
  "acks": " The authors thank Tiago Carvalho, Brandon Invergo and Christian P erez-Llamas for feedback. ", 
  "authors": [
    " Giovanni Marco Dall 'olio", 
    " Ali R Vahdati", 
    " Jaume Bertranpetit", 
    " Andreas Wagner", 
    " Hafid Laayouni"
  ], 
  "keyWords": [
    "genotype networks", 
    [
      "network_", 
      "genotypes", 
      "data", 
      "phenotypes", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    "Python"
  ], 
  "dateCreated": "2014-10-05T00:35:52Z"
}{
  "doi": "10.1093/bioinformatics/btu758", 
  "name": "Unsupervised discovery of information structure in biomedical documents", 
  "links": [
    "http://svn.ask.it.usyd.edu.au/trac/candc", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://www.cs.utexas.edu/users/dml/Software/graclus.html", 
    "http://www.biomedcentral.com/1471-2105/12/69.Guo,Y"
  ], 
  "title": "Data and text mining Unsupervised discovery of information structure in biomedical documents", 
  "toolName": "Data and text mining Unsupervised discovery of information structure in biomedical documents", 
  "abstract": "Motivation: Information structure (IS) analysis is a text mining technique, which classifies text in biomedical articles into categories that capture different types of information, such as objectives, methods, results and conclusions of research. It is a highly useful technique that can support a range of Biomedical Text Mining tasks and can help readers of biomedical literature find information of interest faster, accelerating the highly time-consuming process of literature review. Several approaches to IS analysis have been presented in the past, with promising results in real-world bio-medical tasks. However, all existing approaches, even weakly supervised ones, require several hundreds of hand-annotated training sentences specific to the domain in question. Because bio-medicine is subject to considerable domain variation, such annotations are expensive to obtain. This makes the application of IS analysis across biomedical domains difficult. In this article, we investigate an unsupervised approach to IS analysis and evaluate the performance of several un-supervised methods on a large corpus of biomedical abstracts collected from PubMed. Results: Our best unsupervised algorithm (multilevel-weighted graph clustering algorithm) performs very well on the task, obtaining over 0.70 F scores for most IS categories when applied to well-known IS schemes. This level of performance is close to that of lightly supervised IS methods and has proven sufficient to aid a range of practical tasks. Thus, using an unsupervised approach, IS could be applied to support a wide range of tasks across sub-domains of biomedicine. We also demonstrate that unsupervised learning brings novel insights into IS of biomedical literature and discovers information categories that are not present in any of the existing IS schemes. Availability and Implementation: The annotated corpus and software are available at", 
  "summary": "In this article, we investigate an unsupervised approach to IS analysis and evaluate the performance of several unsupervised methods on a large corpus of biomedical abstracts collected from PubMed. Results: Our best unsupervised algorithm (multilevel-weighted graph clustering algorithm) performs very well on the task, obtaining over 0.70 F scores for most IS categories when applied to well-known IS schemes.\nWe experiment with a large corpus of biomedical abstracts annotated according to two different IS schemes: section names (SN) and AZs. We apply to this corpus two canonical clustering algorithms-- spherical k-means (Zhong, 2005) and Expectation MaximizationGaussian Mixture Model (EM-GMM) (Dempster et al., 1977)--as well as the-state- of-the-art multilevel-weighted graph clustering algorithm (Dhillon et al., 2007), which is an efficient approximation of the very popular spectral clustering algorithm.", 
  "affiliations": [
    " Computer Laboratory University of Cambridge ", 
    " Institute of Environmental Medicine Karolinska Institutet "
  ], 
  "grants": [
    "Funding\nThis work was supported by the Royal Society (UK), the Swedish Research Council, FAS (Sweden) and an Engineering and Physical Sciences Research Council (EPSRC) doctoral training grant (to D.K.)."
  ], 
  "acks": " ", 
  "authors": [
    " Douwe Kiela", 
    " Yufan Guo", 
    " Ulla Stenius", 
    " Anna Korhonen"
  ], 
  "keyWords": [
    [
      "clustering", 
      "information", 
      "classes"
    ]
  ], 
  "sourcelinks": [
    "http://www.cs.utexas.edu/users/dml/Software/graclus.html"
  ], 
  "technologies": [
    "MATLAB"
  ], 
  "dateCreated": "2014-11-20T06:56:29Z"
}{
  "doi": "10.1093/bioinformatics/btu606", 
  "name": "Using population data for assessing nextgeneration sequencing performance", 
  "links": [
    "http://www.novo", 
    "http://bioinformatics.oxfordjournals.org", 
    "https://genome.ucsc.edu/).Genotyping", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Sequence analysis Using population data for assessing next-generation sequencing performance", 
  "toolName": "Sequence analysis Using population data for assessing next-generation sequencing performance", 
  "abstract": "Motivation: During the past 4 years, whole-exome sequencing has become a standard tool for finding rare variants causing Mendelian disorders. In that time, there has also been a proliferation of both sequencing platforms and approaches to analyse their output. This requires approaches to assess the performance of different methods. Traditionally, criteria such as comparison with microarray data or a number of known polymorphic sites have been used. Here we expand such approaches, developing a maximum likelihood framework and using it to estimate the sensitivity and specificity of whole-exome sequencing data. Results: Using whole-exome sequencing data for a panel of 19 individuals, we show that estimated sensitivity and specificity are similar to those calculated using microarray data as a reference. We explore the effect of frequency misspecification arising from using an inappropriately selected population and find that, although the estimates are affected, the rankings across procedures remain the same. Availability and implementation: An implementation using Perl and R can be found at busso.ncl.ac.uk (Username: igm101; Password: Z1z1nts).", 
  "summary": "The approach we presented here estimates two parameters, the sensitivity and the specificity of variant calls in a next-generation sequencing experiment, by comparing the observed variants with population allele frequency data in a maximum likelihood framework.\nAs both sensitivity and specificity are influenced by various experimental factors, including sample preparation, the sequencing itself and the bioinformatic pipelines used to analyse data, the procedure could be used to assess the performance of a sequencing experiment globally and could complement other commonly used approaches, such as the assessment of base call quality or of coverage metrics.", 
  "affiliations": [
    " The Wellcome Trust Sanger Institute Wellcome Trust Genome Campus ", 
    " Oxford Gene Technology"
  ], 
  "grants": [
    "Funding: This work was supported by the British Heart Foundation [Grant reference: FS/10/008/28146]; S.A.T.", 
    "were funded by the Wellcome Trust [Grant Number: WT098051]."
  ], 
  "acks": " ", 
  "authors": [
    " Darren T Houniet", 
    " Thahira J Rahman", 
    " Saeed Al Turki", 
    " Matthew E Hurles", 
    " Yaobo Xu", 
    " Judith Goodship", 
    " Bernard Keavney", 
    " Mauro Santibanez Koref"
  ], 
  "keyWords": [
    "sequence analysis", 
    [
      "sequencing", 
      "sensitivity", 
      "specificities", 
      "variants", 
      "bioinformatics"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-09-19T04:50:41Z"
}{
  "doi": "10.1093/bioinformatics/btu567", 
  "name": "Visualization of protein sequence features using JavaScript and SVG with pVizjs", 
  "links": [
    "http://backbonejs.org", 
    "http://research-pub.gene.com/pviz/app/proteomics-3d3", 
    "http://d3js.org", 
    "http://jquery.com", 
    "https://github.com/Genentech/pviz", 
    "http://getboot", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://research-pub.gene"
  ], 
  "title": "Sequence analysis Visualization of protein sequence features using JavaScript and SVG with pViz.js", 
  "toolName": "pviz", 
  "abstract": "pViz.js is a visualization library for displaying protein sequence features in a Web browser. By simply providing a sequence and the locations of its features, this lightweight, yet versatile, JavaScript library renders an interactive view of the protein features. Interactive exploration of protein sequence features over the Web is a common need in Bioinformatics. Although many Web sites have developed viewers to display these features, their implementations are usually focused on data from a specific source or use case. Some of these viewers can be adapted to fit other use cases but are not designed to be reusable. pViz makes it easy to display features as boxes aligned to a protein sequence with zooming functionality but also includes predefined renderings for secondary structure and post-translational modifications. The library is designed to further customize this view. We demonstrate such applications of pViz using two examples: a proteomic data visualization tool with an embedded viewer for displaying features on protein structure, and a tool to visualize the results of the variant_effect_predictor tool from Ensembl. Availability and implementation: pViz.js is a JavaScript library, available on github at https://github.com/Genentech/pviz. This site includes examples and functional applications, installation instructions and usage documentation. A Readme file, which explains how to use pViz with examples, is available as Supplementary Material A.", 
  "summary": "ABSTRACT Summary: pViz.js is a visualization library for displaying protein sequence features in a Web browser.\nWe demonstrate such applications of pViz using two examples: a proteomic data visualization tool with an embedded viewer for displaying features on protein structure, and a tool to visualize the results of the variant_effect_predictor tool from Ensembl.\nTools for customizing the data sources, user interactions or styling of protein features are limited for developers of Web-based visualizations.\npViz.js (pViz) makes it easy to display protein sequence features on a Web page.", 
  "affiliations": [
    " Department of Bioinformatics and Computational Biology Genentech Inc South San Francisco "
  ], 
  "grants": [], 
  "sourcelinks": [
    "https://github.com/Genentech/pviz"
  ], 
  "acks": " The authors thank Melanie Huntley, Donald Kirkpatrick, Tobias Maile, William Young and Gerard Manning for their feedback during the development of pViz and helpful comments on the manuscript. ", 
  "authors": [
    " Kiran Mukhyala", 
    " Alexandre Masselot", 
    " John Hancock"
  ], 
  "keyWords": [
    [
      "interactivity", 
      "features", 
      "proteins", 
      "pviz", 
      "libraries", 
      "bioinformatics", 
      "data"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "Other"
      }, 
      {
        "link": "https://raw.githubusercontent.com/Genentech/pviz/master/LICENSE.txt"
      }
    ], 
    "name": "pviz", 
    "contributors": [
      {
        "contributions": 14, 
        "html_url": "https://github.com/mierzwid"
      }, 
      {
        "contributions": 5, 
        "html_url": "https://github.com/alexmasselot"
      }, 
      {
        "contributions": 3, 
        "html_url": "https://github.com/lfrohman"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/jensreeder"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/mukhyala"
      }, 
      {
        "contributions": 1, 
        "html_url": "https://github.com/watonn"
      }
    ], 
    "versions": [], 
    "created_at": "2013-08-20T20:27:28Z", 
    "updated_at": "2016-07-11T09:22:07Z", 
    "languages": [
      "JavaScript", 
      "HTML", 
      "CSS"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/watonn"
      }, 
      {
        "html_url": "https://github.com/alexmasselot"
      }, 
      {
        "html_url": "https://github.com/mierzwid"
      }, 
      {
        "html_url": "https://github.com/mukhyala"
      }, 
      {
        "html_url": "https://github.com/jmrinaldi"
      }, 
      {
        "html_url": "https://github.com/connollyst"
      }, 
      {
        "html_url": "https://github.com/lfrohman"
      }, 
      {
        "html_url": "https://github.com/jgross"
      }, 
      {
        "html_url": "https://github.com/leonqli"
      }, 
      {
        "html_url": "https://github.com/omarwagih"
      }, 
      {
        "html_url": "https://github.com/tweep"
      }, 
      {
        "html_url": "https://github.com/stachnim"
      }
    ], 
    "owner": "https://github.com/Genentech", 
    "homepage": null
  }, 
  "technologies": [
    "JavaScript"
  ], 
  "dateCreated": "2014-08-22T03:08:27Z"
}{
  "doi": "10.1093/bioinformatics/btu587", 
  "name": "VirVarSeq a lowfrequency virus variant detection pipeline for Illumina sequencing using adaptive basecalling accuracy filtering", 
  "links": [
    "http://sourceforge.net/projects/virtools/?source=directory", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "Genetics and population analysis VirVarSeq: a low-frequency virus variant detection pipeline for Illumina sequencing using adaptive base-calling accuracy filtering", 
  "toolName": "virtools", 
  "abstract": "Motivation: In virology, massively parallel sequencing (MPS) opens many opportunities for studying viral quasi-species, e.g. in HIV-1-and HCV-infected patients. This is essential for understanding pathways to resistance, which can substantially improve treatment. Although MPS platforms allow in-depth characterization of sequence variation, their measurements still involve substantial technical noise. For Illumina sequencing, single base substitutions are the main error source and impede powerful assessment of low-frequency mutations. Fortunately, base calls are complemented with quality scores (Qs) that are useful for differentiating errors from the real low-frequency mutations. Results: A variant calling tool, Q-cpileup, is proposed, which exploits the Qs of nucleotides in a filtering strategy to increase specificity. The tool is imbedded in an open-source pipeline, VirVarSeq, which allows variant calling starting from fastq files. Using both plasmid mixtures and clinical samples, we show that Q-cpileup is able to reduce the number of false-positive findings. The filtering strategy is adaptive and provides an optimized threshold for individual samples in each sequencing run. Additionally, linkage information is kept between single-nucleotide polymorphisms as variants are called at the codon level. This enables virologists to have an immediate biological interpretation of the reported variants with respect to their antiviral drug responses. A comparison with existing SNP caller tools reveals that calling variants at the codon level with Q-cpileup results in an outstanding sensitivity while maintaining a good specificity for variants with frequencies down to 0.5%.", 
  "summary": "In this article, we present an innovative approach for variant calling at the codon level, named Q-cpileup, that reduces the number of false-positive findings by exploiting the Qs of the nucleotides generated by sequencing.\nThese raw data were only trimmed, which is partially based on Qs. Without Q-cpileup, the comparison of the samples sequenced on both lanes reveals that (i) the number of reported variants differs up to 16 variants, (ii) deviations of the frequencies go up to 6% and (iii) even for the variants with frequency 41%, some clear deviations between the two lanes can be observed.", 
  "affiliations": [
    " Applied Mathematics, Informatics and Statistics Ghent University "
  ], 
  "grants": [
    "Funding: Part of this research was supported by Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT) who provided a Baekeland mandatory to BV [BM100679], by IAP research network grant [no."
  ], 
  "acks": " The authors wish to thank the scientists at Janssen Pharmaceutica who collected and produced the data as well as Tobias Verbeke and Joris Meys who gave the necessary IT support. Herwig Van Marck is kindly acknowledged for the insightful discussions. The authors are grateful to Prof McLachlan and his team for providing the original Fortran code and for giving support to write the R-wrapper. Thanks to Osvaldo, developer of ShoRAH, who was always very helpful when problems were encountered while running ShoRAH. ", 
  "authors": [
    " Bie M P Verbist", 
    " Kim Thys", 
    " Joke Reumers", 
    " Yves Wetzels", 
    " Koen Van Der Borght", 
    " Willem Talloen", 
    " Jeroen Aerssens", 
    " Lieven Clement", 
    " Olivier Thas"
  ], 
  "keyWords": [
    [
      "runs", 
      "frequencies", 
      "codoner", 
      "running", 
      "sequencing", 
      "variants"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net/projects/virtools/?source=directory"
  ], 
  "technologies": [
    "Fortran"
  ], 
  "dateCreated": "2014-09-02T00:17:48Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/virtools/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/jokereumers/", 
        "name": "Joke Reumers"
      }, 
      {
        "url": "https://sourceforge.net/u/yveswetzels/", 
        "name": "Yves Wetzels"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btu585", 
  "name": "Visual workflows for 13Cmetabolic flux analysis", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://www"
  ], 
  "title": "Visual workflows for 13 C-metabolic flux analysis", 
  "toolName": "Visual workflows for 13 C-metabolic flux analysis", 
  "abstract": "Motivation: The precise quantification of intracellular metabolic flow rates is of fundamental importance in bio(techno)logy and medical research. The gold standard in the field is metabolic flux analysis (MFA) with 13 C-labeling experiments. 13 C-MFA workflows orchestrate several, mainly human-in-the-loop, software applications, integrating them with plenty of heterogeneous information. In practice, this had posed a major practical barrier for evaluating, interpreting and understanding isotopic data from carbon labeling experiments. Results: Graphical modeling, interactive model exploration and visual data analysis are the key to overcome this limitation. We have developed a first-of-its-kind graphical tool suite providing scientists with an integrated software framework for all aspects of 13 C-MFA. Almost 30 modules (plug-ins) have been implemented for the Omix visualization software. Several advanced graphical workflows and ergonomic user interfaces support major domain-specific modeling and proofreading tasks. With that, the graphical suite is a productivity enhancing tool and an original educational training instrument supporting the adoption of 13 C-MFA applications in all life science fields. Availability: The Omix Light Edition is freely available at http://www. omix", 
  "summary": "A lot of information must be compiled on the metabolic network structure, carbon atom mappings (CAMs) for reactions, measurement configurations, flux constraints, input substrate compositions, etc.\nIt is neither easy nor straightforward to build up the complex configuration for 13C-MFA, to check its correctness and consistency, to trigger simulation and data evaluation runs or to automatically visualize the final flux maps in the metabolic network context.\nBased on the CAM network model with specified input substrate labeling, constraints and, optionally, measurements, the forward simulation of a CLE represents the central computational module of any 13C-MFA workflow.", 
  "affiliations": [
    " Institute of Bio-and Geosciences Forschungszentrum J \u20ac ulich GmbH "
  ], 
  "grants": [
    "Funding: This work is partially supported by the European Commission project SysInBio (212766)."
  ], 
  "acks": " ", 
  "authors": [
    " Katharina N \u20ac Oh", 
    " Peter Droste", 
    " Wolfgang Wiechert"
  ], 
  "keyWords": [
    "c flux analysis", 
    [
      "visualization", 
      "fluxes", 
      "networks", 
      "modeling"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-09T01:49:44Z"
}{
  "doi": "10.1093/bioinformatics/btu700", 
  "name": "Waggawagga comparative visualization of coiledcoil predictions and detection of stable single helices SAH domains", 
  "links": [
    "http://jquery.com", 
    "http://www.gnuplot.info", 
    "http://waggawagga.motorprotein.de", 
    "http://www", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://fancybox.net"
  ], 
  "title": "Waggawagga: comparative visualization of coiled-coil predictions and detection of stable single a-helices (SAH domains)", 
  "toolName": "Waggawagga: comparative visualization of coiled-coil predictions and detection of stable single a-helices (SAH domains)", 
  "abstract": "Waggawagga is a web-based tool for the comparative visualization of coiled-coil predictions and the detection of stable single a-helices (SAH domains). Overview schemes show the predicted coiled-coil regions found in the query sequence and provide sliders, which can be used to select segments for detailed helical wheel and helical net views. A window-based score has been developed to predict SAH domains. Export to several bitmap and vector graphics formats is supported. Availability and implementation: http://waggawagga.", 
  "summary": "Summary: Waggawagga is a web-based tool for the comparative visualization of coiled-coil predictions and the detection of stable single a-helices (SAH domains).\nVisualization of the coiled-coil predictions in helical wheel schemes provides the possibility to fast and easily identify potential hydrophobic seams in `a' and `d' and oppositely charged residues in `e' and `g' positions, respectively.\nThere are other proteins that are mispredicted as coiled coils although forming monomeric a-helical structures, such as stathmin, which, however, are not enriched in E, Q, K, and R residues and do not form stable structures in solution (Honnappa et al., 2006).", 
  "affiliations": [
    " Introduction", 
    " Department of NMR-based Structural Biology Max-Planck-Institute for Biophysical Chemistry "
  ], 
  "grants": [], 
  "acks": " We would like to thank Christian Griesinger for his continuous generous support. Conflict of Interest: none declared. ", 
  "authors": [
    " Dominic Simm", 
    " Klas Hatje", 
    " Martin Kollmar", 
    " ", 
    " "
  ], 
  "keyWords": [
    "coiled predictions", 
    [
      "coils", 
      "interactions", 
      "predicting", 
      "proteins", 
      "sahs", 
      "domains"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-23T00:08:46Z"
}{
  "doi": "10.1093/bioinformatics/btu754", 
  "name": "ViQuaS an improved reconstruction pipeline for viral quasispecies spectra generated by nextgeneration sequencing", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://sourceforge.net/projects/viquas"
  ], 
  "title": "Genetics and population analysis ViQuaS: an improved reconstruction pipeline for viral quasispecies spectra generated by next-generation sequencing", 
  "toolName": "viquas", 
  "abstract": "Motivation: The combined effect of a high replication rate and the low fidelity of the viral polymer-ase in most RNA viruses and some DNA viruses results in the formation of a viral quasispecies. Uncovering information about quasispecies populations significantly benefits the study of disease progression, antiviral drug design, vaccine design and viral pathogenesis. We present a new analysis pipeline called ViQuaS for viral quasispecies spectrum reconstruction using short next-generation sequencing reads. ViQuaS is based on a novel reference-assisted de novo assembly algorithm for constructing local haplotypes. A significantly extended version of an existing global strain reconstruction algorithm is also used. Results: Benchmarking results showed that ViQuaS outperformed three other previously published methods named ShoRAH, QuRe and PredictHaplo, with improvements of at least 3.1\u201353.9% in recall , 0\u201312.1% in precision and 0\u201338.2% in F-score in terms of strain sequence assembly and improvements of at least 0.006\u20130.143 in KL-divergence and 0.001\u20130.035 in root mean-squared error in terms of strain frequency estimation, over the next-best algorithm under various simulation settings. We also applied ViQuaS on a real read set derived from an in vitro human immunodeficiency virus (HIV)-1 population, two independent datasets of foot-and-mouth-disease virus derived from the same biological sample and a real HIV-1 dataset and demonstrated better results than other methods available. Availability and implementation: http://sourceforge.net/projects/viquas/", 
  "summary": "Even though the correctness of the reconstructed strains cannot be evaluated due to the unavailability of exact nucleotide content of the strains in the real FMDV samples (ERR180978 and ERR180979) and real HIV-1 dataset V11909 (Wang et al., 2007), the consistency of results produced by ViQuaS on real FMDV samples and the higher degree of biological validity of the strains reconstructed from real HIV-1 data by ViQuaS indicate the applicability of ViQuaS on real sequence data and better performance over ShoRAH, QuRe and PredictHaplo.", 
  "affiliations": [
    " Department of Mechanical Engineering Optimisation and Pattern Recognition Research Group Melbourne School of Engineering The University of Melbourne ", 
    " Portland House Research and Advisors Ltd", 
    " Yourgene Bioscience", 
    " Biodiversity Research Center Academia Sinica "
  ], 
  "grants": [
    "Funding\nThis work was partially supported by Australian Research Council [grant number LP140100670] and by MIFRS and MIRS scholarships of The University of Melbourne (to D.J.)."
  ], 
  "acks": " We wish to acknowledge the contributions of Rene Warren for providing valuable feedback on SSAKE algorithm and Osvaldo Zagordi, Irina Astrovskaya and Mattia Prosperi for their support during the comparison study. This work was partially supported by Australian Research Council ", 
  "authors": [
    " Duleepa Jayasundara", 
    " I Saeed", 
    " Suhinthan Maheswararajah", 
    " B C Chang", 
    " S.-L Tang", 
    " Saman K Halgamuge"
  ], 
  "keyWords": [
    [
      "methods", 
      "strains", 
      "reads", 
      "sequencing", 
      "reconstructible", 
      "viquas"
    ]
  ], 
  "sourcelinks": [
    "http://sourceforge.net/projects/viquas"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-15T04:10:48Z", 
  "sourceforge_data": {
    "url": "https://sourceforge.net/p/viquas/", 
    "languages": [], 
    "developers": [
      {
        "url": "https://sourceforge.net/u/duleepalj/", 
        "name": "Duleepa Jayasundara"
      }
    ], 
    "Development Status": [], 
    "license": []
  }
}{
  "doi": "10.1093/bioinformatics/btv027", 
  "name": "WALTZDB a benchmark database of amyloidogenic hexapeptides", 
  "links": [
    "http://waltzdb.switchlab.org", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "WALTZ-DB: a benchmark database of amyloidogenic hexapeptides", 
  "toolName": "WALTZ-DB: a benchmark database of amyloidogenic hexapeptides", 
  "abstract": "Accurate prediction of amyloid-forming amino acid sequences remains an important challenge. We here present an online database that provides open access to the largest set of experimentally characterized amyloid forming hexapeptides. To this end, we expanded our previous set of 280 hexapeptides used to develop the Waltz algorithm with 89 peptides from literature review and by systematic experimental characterisation of the aggregation of 720 hexapeptides by transmission electron microscopy, dye binding and Fourier transform infrared spectroscopy. This brings the total number of experimentally characterized hexapeptides in the WALTZ-DB database to 1089, of which 244 are annotated as positive for amyloid formation. Availability and implementation: The WALTZ-DB database is freely available without any registration requirement at http://waltzdb.", 
  "summary": "Additional hexapeptide properties such as WALTZ, TANGO (Fernandez-Escamilla et al., 2004) and PASTA scores (Trovato et al., 2007), hydrophobicity, Chou-Fasman values for helix and strand propensity (Chou and Fasman, 1974) were calculated and amyloid structural class (Eisenberg et al., 2009) prediction and predicted atomic structure models were obtained through comparative modeling using the FoldX force field (Schymkowitz et al., 2005) on the publically available structures of amyloid cores (Eisenberg et al., 2005; Morris et al., 2013).", 
  "affiliations": [
    " School of Life Sciences University of Sussex "
  ], 
  "grants": [
    "Funding\nThe Switch Laboratory was supported by grants from VIB, University of Leuven, the Funds for Scientific Research Flanders (FWO G.0509.13), the Flanders Institute for Science and Technology (IWT) and the Federal Office for Scientific Affairs of Belgium (Belspo IUAP P7/16).", 
    "held a personal grant from the Fonds Wetenschappelijk Onderzoek Vlaanderen (FWOTM443)."
  ], 
  "acks": " ", 
  "authors": [
    " Jacinte Beerten", 
    " Joost Van Durme", 
    " Rodrigo Gallardo", 
    " Emidio Capriotti", 
    " Louise Serpell", 
    " Frederic Rousseau", 
    " Joost Schymkowitz"
  ], 
  "keyWords": [
    [
      "amyloid", 
      "hexapeptides", 
      "proteins", 
      "peptides", 
      "databases", 
      "structures"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2015-01-20T01:59:41Z"
}{
  "doi": "10.1093/bioinformatics/btu718", 
  "name": "WebPSN a web server for highthroughput investigation of structural communication in biomacromolecules", 
  "links": [
    "http://www.rcsb.org", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://webpsn.hpc"
  ], 
  "title": "WebPSN: a web server for high-throughput investigation of structural communication in biomacromolecules", 
  "toolName": "WebPSN: a web server for high-throughput investigation of structural communication in biomacromolecules", 
  "abstract": "We developed a mixed Protein Structure Network (PSN) and Elastic Network Model-Normal Mode Analysis (ENM-NMA)-based strategy (i.e. PSN-ENM) to investigate structural communication in biomacromolecules. The approach starts from a Protein Structure Graph and searches for all shortest communication pathways between user-specified residues. The graph is computed on a single preferably high-resolution structure. Information on system's dynamics is supplied by ENM-NMA. The PSN\u2013ENM methodology is made of multiple steps both in the setup and analysis stages, which may discourage inexperienced users. To facilitate its usage, we implemented WebPSN, a freely available web server that allows the user to easily setup the calculation, perform post-processing analyses and both visualize and download numerical and 3D representations of the output. Speed and accuracy make this server suitable to investigate structural communication, including alloster-ism, in large sets of bio-macromolecular systems. Availability and implementation: The WebPSN server is freely available at http://webpsn.hpc. unimore.it.", 
  "summary": "An interactive table lists all pathways with a number of associated indices, such as: (i) path length, (ii) mean square distance fluctuations between all node pairs in a path (p) (MSDFp), which is a measure of path stiffness (see Supplementary Methods; Raimondi et al., 2013); (iii) hub content; (iv) fraction of user-defined relevant amino acid nodes; (v) correlation score, i.e. the ratio between number of correlated residues and path length (the latter excludes the two extremities); (vi) average similarity score (according to the clusterization method) between the path and each path in the cluster; and (vii) number of the cluster the path belongs to (notice that such number increases with decreasing cluster population).", 
  "affiliations": [
    " Department of Life Sciences University of Modena and Reggio Emilia "
  ], 
  "grants": [
    "Funding\nThis study was supported by an Airc-Italy grant [IG10740] and a Telethon-Italy grant [S00068TELC to F.F.]."
  ], 
  "acks": " ", 
  "authors": [
    " Michele Seeber", 
    " Angelo Felline", 
    " Francesco Raimondi", 
    " Simona Mariani", 
    " Francesca Fanelli"
  ], 
  "keyWords": [
    [
      "clusterization", 
      "paths", 
      "methods", 
      "server", 
      "raimondi", 
      "nodes", 
      "structures", 
      "linked"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-10-30T05:01:31Z"
}{
  "doi": "10.1093/bioinformatics/btu571", 
  "name": "VSEAMS a pipeline for variant set enrichment analysis using summary GWAS data identifies IKZF3 BATF and ESRRA as key transcription factors in type 1 diabetes", 
  "links": [
    "http://view.ncbi.nlm.nih.gov/dbgap", 
    "http://creativecommons.org/licenses/by/4.0", 
    "http://github.com/ollyburren/macd).3.2", 
    "http://www", 
    "http://github.com/ollyburren/vseams", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://immunobase.org"
  ], 
  "title": "Genetics and population analysis", 
  "toolName": "vseams", 
  "abstract": "Motivation: Genome-wide association studies (GWAS) have identified many loci implicated in disease susceptibility. Integration of GWAS summary statistics (P-values) and functional genomic datasets should help to elucidate mechanisms. Results: We extended a non-parametric SNP set enrichment method to test for enrichment of GWAS signals in functionally defined loci to a situation where only GWAS P-values are available. The approach is implemented in VSEAMS, a freely available software pipeline. We use VSEAMS to identify enrichment of type 1 diabetes (T1D) GWAS associations near genes that are targets for the transcription factors IKZF3, BATF and ESRRA. IKZF3 lies in a known T1D susceptibility region, while BATF and ESRRA overlap other immune disease susceptibility regions, validating our approach and suggesting novel avenues of research for T1D. Availability and implementation: VSEAMS is available for download", 
  "summary": "Genes perturbed by 3 of 59 transcription factors in knock-down experiments (Cusanovich et al., 2014) were enriched for association with T1D (Fig. 4): IKZF3 (P = 1.1  104; n=1798), BATF (P = 4.4  104; n=210) and ESRRA (P = 8.0  104; n=614), where n is the number of genes in each set.\nThe original GSEA method accounts for this correlation by permuting phenotypes and repeating the entire gene expression analysis multiple times (Subramanian et al., 2005), an approach we also took in a previous variant set enrichment analysis (Heinig et al., 2010).", 
  "affiliations": [
    " Department of Medical Genetics JDRF/Wellcome Trust Diabetes and Inflammation Laboratory NIHR Cambridge Biomedical Research Centre Cambridge Institute for Medical Research University of Cambridge Wellcome Trust/MRC Building ", 
    " MRC Biostatistics Unit Cambridge Institute of Public Health "
  ], 
  "grants": [
    "23 2014, pages 33423348 doi:10.1093/bioinformatics/btu571\n\nGenetics and population analysis\n\nAdvance Access publication August 27, 2014\n\nVSEAMS: a pipeline for variant set enrichment analysis using\nsummary GWAS data identifies IKZF3, BATF and ESRRA as key\ntranscription factors in type 1 diabetes\nOliver S. Burren1, Hui Guo1 and Chris Wallace1,2,*\n1Department of Medical Genetics, JDRF/Wellcome Trust Diabetes and Inflammation Laboratory, NIHR Cambridge Biomedical Research Centre, Cambridge Institute for Medical Research, University of Cambridge, Wellcome Trust/MRC Building, Cambridge Biomedical Campus, Cambridge, CB2 0XY, UK and 2MRC Biostatistics Unit, Cambridge Institute of Public Health, Forvie Site, Robinson Way, Cambridge Biomedical Campus, Cambridge, CB2 0SR, UK\nAssociate Editor: Jeffrey Barrett\n\nABSTRACT Motivation: Genome-wide association studies (GWAS) have identified many loci implicated in disease susceptibility.", 
    "Nimgaonkar's group at the University of Pittsburgh as part of a multi-institutional collaborative research project with J. Smoller and P. Sklar (Massachusetts General Hospital (MH 63420).The authors gratefully acknowledge the Genetics of Kidneys in Diabetes (GoKinD) study obtained from the Genetic Association Information Network (GAIN) database found at http://view.ncbi.nlm.nih.gov/dbgap/ through dbGaP accession number phs000018.v1.p1\nFunding: This work was funded by the JDRF (9-2011-253), the Wellcome Trust (091157) and the National Institute for Health Research Cambridge Biomedical Research Centre.", 
    "Funding for the project was provided by the Wellcome Trust under award (076113).", 
    "The JDRF/Wellcome Trust Diabetes and Inflammation Laboratory receives funding from Hoffmann La Roche and Eli Lilly and Company.", 
    "This study uses resources provided by the Type 1 Diabetes Genetics Consortium, a collaborative clinical study sponsored by the National Institute of Diabetes and Digestive and Kidney Diseases, National Institute of Allergy and Infectious Diseases, National Human Genome Research Institute, National Institute of Child Health and Human Development and JDRF and supported by (U01 DK062418).", 
    "The research leading to these results has received funding from the European Unions seventh Framework Programme (FP7/2007-2013) under grant agreement no."
  ], 
  "sourcelinks": [
    "http://github.com/ollyburren/vseams", 
    "http://github.com/ollyburren/macd).3.2"
  ], 
  "acks": " The authors thank John Todd for his help in conceiving the study, interpreting the results and comments on the manuscript. The authors thank Vin Everett and Wojciech Giel for computing support, as well as other members of the Diabetes and Inflammation Laboratory for assistance throughout. The authors acknowledge Darren Cusanovich for facilitating early access to knockdown experimental data. This study uses resources provided by the Type 1 Diabetes Genetics Consortium, a collaborative clinical study sponsored by the National Institute of Diabetes and Digestive and Kidney Diseases, National Institute of Allergy and Infectious Diseases, National Human Genome Research Institute, National Institute of Child Health and Human Development ", 
  "authors": [
    " Oliver S Burren", 
    " Hui Guo", 
    " Chris Wallace", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    [
      "vseams", 
      "testing", 
      "genetics", 
      "diseases"
    ]
  ], 
  "github_data": {
    "license": [
      {
        "name": "GNU General Public License v2.0"
      }, 
      {
        "link": "https://raw.githubusercontent.com/ollyburren/vseams/master/LICENSE"
      }
    ], 
    "name": "vseams", 
    "contributors": [
      {
        "contributions": 10, 
        "html_url": "https://github.com/ollyburren"
      }
    ], 
    "versions": [], 
    "created_at": "2014-02-14T09:15:11Z", 
    "updated_at": "2014-09-09T16:19:54Z", 
    "languages": [
      "R", 
      "Perl"
    ], 
    "subscribers": [
      {
        "html_url": "https://github.com/ollyburren"
      }
    ], 
    "owner": "https://github.com/ollyburren", 
    "homepage": null
  }, 
  "technologies": [
    "R"
  ], 
  "dateCreated": "2014-08-29T00:22:23Z"
}{
  "doi": "10.1093/bioinformatics/btu737", 
  "name": "What handedness and angles between helices has the studied threehelical protein domain", 
  "links": [
    "http://bioinfo", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "What handedness and angles between helices has the studied three-helical protein domain?", 
  "toolName": "What handedness and angles between helices has the studied three-helical protein domain?", 
  "abstract": "We have created a new server FoldHandedness. Using this server it is possible: (i) to define the regions of helices from two issues (from the PDB file and using the last version of the DSSP program), (ii) to determine the handedness for any chosen three helices and (iii) to calculate the angle and sign between the chosen pairs of the helices for large proteins and complexes of proteins with DNA or RNA.", 
  "summary": "Using this server it is possible: (i) to define the regions of helices from two issues (from the PDB file and using the last version of the DSSP program), (ii) to determine the handedness for any chosen three helices and (iii) to calculate the angle and sign between the chosen pairs of the helices for large proteins and complexes of proteins with DNA or RNA.\nLeft-handed three-helical domains have the fastest folding rates in comparison with right-handed threehelical domains among eight proteins with known experimental folding rates (Glyakina et al., 2013).", 
  "affiliations": [
    " Institute of Protein Research"
  ], 
  "grants": [
    "This study was supported by the Russian Science Foundation grant No 14-14-00536, and by the Russian Academy of Sciences (Molecular and Cell Biology program (grant 01201353567) to I.V.S."
  ], 
  "acks": " ", 
  "authors": [
    " Leonid B Pereyaslavets", 
    " Anna V Glyakina", 
    " Nikita V Dovidchenko", 
    " Igor V Sokolovskiy", 
    " Oxana V Galzitskaya"
  ], 
  "keyWords": [
    [
      "domains", 
      "proteins", 
      "folding", 
      "helix", 
      "structures"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-11-12T04:44:24Z"
}{
  "doi": "10.1093/bioinformatics/btu636", 
  "name": "WhopGenome highspeed access to wholegenome variation and sequence data in R", 
  "links": [
    "http://bioinformatics.oxfordjournals.org", 
    "http://cran.r-project.org/web/packages", 
    "http://*To"
  ], 
  "title": "Genome analysis WhopGenome: high-speed access to whole-genome variation and sequence data in R", 
  "toolName": "Genome analysis WhopGenome: high-speed access to whole-genome variation and sequence data in R", 
  "abstract": "The statistical programming language R has become a de facto standard for the analysis of many types of biological data, and is well suited for the rapid development of new algorithms. However, variant call data from population-scale resequencing projects are typically too large to be read and processed efficiently with R's built-in I/O capabilities. WhopGenome can efficiently read whole-genome variation data stored in the widely used variant call format (VCF) file format into several R data types. VCF files can be accessed either on local hard drives or on remote servers. WhopGenome can associate variants with annotations such as those available from the UCSC genome browser, and can accelerate the reading process by filtering loci according to user-defined criteria. WhopGenome can also read other Tabix-indexed files and create indices to allow fast selective access to FASTA-formatted sequence files. Availability and implementation: The WhopGenome R package is available on CRAN at http://cran.r-project.org/web/packages/ WhopGenome/. A Bioconductor package has been submitted. Contact:", 
  "summary": "WhopGenome can efficiently read wholegenome variation data stored in the widely used variant call format (VCF) file format into several R data types.\nAlthough several R packages are capable of reading VCF files [VariantAnnotation (Obenchain et al., 2014), seqminer (http://\nHere, we present WhopGenome, an R package for fast, straightforward and flexible processing of genomic variation data in VCF format.\nThe same selective access functionality exists also for FASTA files through WhopGenome's interface to FaIdx (the indexing solution included in samtools) (1000 Genome Project Data Processing Subgroup, 2009).\nTo link genomic variation to pedigree data, WhopGenome includes support for .PED files (a simple text-based table format used, e.g. by PLINK).", 
  "affiliations": [
    " Cluster of Excellence on Plant Sciences CEPLAS", 
    " Institute for Computer Science Heinrich Heine University "
  ], 
  "grants": [
    "Funding: This work was supported by the German Research Foundation [DFG grants EXC 1028 and CRC 680 to M.J.L.]."
  ], 
  "acks": " ", 
  "authors": [
    " Ulrich Wittelsb", 
    " Bastian Pfeifer", 
    " Martin J Lercher", 
    " Alfonso Valencia"
  ], 
  "keyWords": [
    [
      "files", 
      "genomics", 
      "whopgenome", 
      "formatted", 
      "bioinformatics", 
      "reading", 
      "data"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [
    " SQL ", 
    "R", 
    "C++"
  ], 
  "dateCreated": "2014-10-02T06:07:10Z"
}{
  "doi": "10.1093/bioinformatics/btu761", 
  "name": "XenoSite server a webavailable site of metabolism prediction tool", 
  "links": [
    "http://www.pylonsproject.org", 
    "http://openbabel.org", 
    "http://openmo", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://eyesopen.com", 
    "http://swami.wustl.edu/xenosite"
  ], 
  "title": "XenoSite server: a web-available site of metabolism prediction tool", 
  "toolName": "XenoSite server: a web-available site of metabolism prediction tool", 
  "abstract": "Cytochrome P450 enzymes (P450s) are metabolic enzymes that process the majority of FDA-approved, small-molecule drugs. Understanding how these enzymes modify molecule structure is key to the development of safe, effective drugs. XenoSite server is an online implementation of the XenoSite, a recently published computational model for P450 metabolism. XenoSite predicts which atomic sites of a molecule\u2014sites of metabolism (SOMs)\u2014are modified by P450s. XenoSite server accepts input in common chemical file formats including SDF and SMILES and provides tools for visualizing the likelihood that each atomic site is a site of metabolism for a variety of important P450s, as well as a flat file download of SOM predictions. Availability and implementation: XenoSite server is available at", 
  "summary": "XenoSite predicts which atomic sites of a molecule--sites of metabolism (SOMs)--are modified by P450s.\nXenoSite server accepts input in common chemical file formats including SDF and SMILES and provides tools for visualizing the likelihood that each atomic site is a site of metabolism for a variety of important P450s, as well as a flat file download of SOM predictions.\nIn this article, we describe a publicly available, web-based tool for predicting P450-mediated xenobiotic metabolism of user-submitted molecules, based on the XenoSite Cytochrome P450 metabolism prediction models (http://swami.wustl.edu/xenosite).", 
  "affiliations": [
    " Department of Pathology and Immunology Washington University School of Medicine ", 
    " Division of Biology and Biological Sciences"
  ], 
  "grants": [
    "Funding\nThis study was funded by the Pathology and Immunology Department of Washington University in Saint Louis."
  ], 
  "acks": " The authors would like to extend special thanks to Brooke Hauser for her help with website graphics and layout. Figure 1 and molecule images on the web server are created with OEDepict, version 1.7.4.5, OpenEye Scientific Software, Inc., Santa Fe, NM, www.eyesopen.com, 2014. M.K.M. wrote the manuscript and implemented XenoSite server, T.B.H. implemented the molecule visualization tools and S.J.S. provided project oversight and substantive editing. This study was funded by the Pathology and Immunology Department of Washington University in Saint Louis. Conflict of Interest: none declared. ", 
  "authors": [
    " Matthew K Matlock", 
    " Tyler B Hughes", 
    " S Joshua Swamidass"
  ], 
  "keyWords": [
    "metabolism prediction", 
    [
      "computational", 
      "xenosite", 
      "predictions", 
      "metabolized", 
      "molecules", 
      "soms", 
      "columns"
    ]
  ], 
  "sourcelinks": [
    "http://eyesopen.com"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-20T06:56:29Z"
}{
  "doi": "10.1093/bioinformatics/btu508", 
  "name": "Walking the interactome for candidate prioritization in exome sequencing studies of Mendelian diseases", 
  "links": [
    "http://compbio.charite.de", 
    "http://bioinformatics.oxfordjournals.org", 
    "http://compbio", 
    "http://evs.gs.washington.edu/EVS", 
    "http://creativecommons.org/licenses/by/4.0"
  ], 
  "title": "Genetics and population analysis Walking the interactome for candidate prioritization in exome sequencing studies of Mendelian diseases", 
  "toolName": "Genetics and population analysis Walking the interactome for candidate prioritization in exome sequencing studies of Mendelian diseases", 
  "abstract": "Motivation: Whole-exome sequencing (WES) has opened up previously unheard of possibilities for identifying novel disease genes in Mendelian disorders, only about half of which have been elucidated to date. However, interpretation of WES data remains challenging. Results: Here, we analyze protein\u2013protein association (PPA) networks to identify candidate genes in the vicinity of genes previously implicated in a disease. The analysis, using a random-walk with restart (RWR) method, is adapted to the setting of WES by developing a composite variant-gene relevance score based on the rarity, location and predicted pathogenicity of variants and the RWR evaluation of genes harboring the variants. Benchmarking using known disease variants from 88 disease-gene families reveals that the correct gene is ranked among the top 10 candidates in !50% of cases, a figure which we confirmed using a prospective study of disease genes identified in 2012 and PPA data produced before that date. We implement our method in a freely available Web server, ExomeWalker, that displays a ranked list of candidates together with information on PPAs, frequency and predicted pathogenicity of the variants to allow quick and effective searches for candidates that are likely to reward closer investigation. Availability and implementation:", 
  "summary": "The tools combine filtering steps that exclude common variants and retain only variants that are predicted likely pathogenic using tools such as MutationTaster (Schwarz et al., 2010), and then exploit sequences from multiple unrelated individuals with the sought-after disease to search for genes mutated in most or all of the individuals, as well as linkage or pedigree analysis (Coutant et al., 2012; Li et al., 2012; Santoni et al., 2014; Sifrim et al., 2012; Yandell et al., 2011; Zhang et al., 2013).\nTo validate our methodology, we developed a simulation strategy based on adding known disease-causing mutations from the Human Gene Mutation Database (HGMD) into either one of 1092 unaffected wholeexome files in variant call format (VCF) from the 1000 Genomes Project (1000 Genomes Project Consortium et al., 2012) or 144 in-house exomes.", 
  "affiliations": [
    " Genome Informatics Department Institute of Human Genetics University Hospital Essen University of Duisburg-Essen ", 
    " McKusick-Nathans Institute of Genetic Medicine John Hopkins University School of Medicine ", 
    " Institute for Medical Genetics and Human Genetics", 
    " Mouse Informatics Group The Wellcome Trust Sanger Institute Wellcome Trust Genome Campus "
  ], 
  "grants": [
    "These predictions were extracted from dbNSFP (Liu et al., 2011).", 
    "Funding: This work was supported by grants of the Bundesministerium fu r Bildung und Forschung (BMBF project number 0313911), by the European Community's Seventh Framework Programme (Grant Agreement 602300; SYBIL) and by core infrastructure funding from the Wellcome Trust.", 
    "(2011) dbNSFP: a lightweight database of human nonsynonymous SNPs and their functional predictions."
  ], 
  "acks": " ", 
  "authors": [
    " Damian Smedley", 
    " Sebastian K \u20ac Ohler", 
    " Johanna Christina Czeschik", 
    " Joanna Amberger", 
    " Carol Bocchini", 
    " Ada Hamosh", 
    " Julian Veldboer", 
    " Tomasz Zemojtel", 
    " Peter N Robinson", 
    " Jeffrey Barrett"
  ], 
  "keyWords": [
    "disease genes", 
    [
      "sequencing", 
      "genomics", 
      "variants", 
      "genetically", 
      "diseases"
    ]
  ], 
  "sourcelinks": [], 
  "technologies": [], 
  "dateCreated": "2014-07-31T03:29:43Z"
}{
  "doi": "10.1093/bioinformatics/btu757", 
  "name": "WemIQ an accurate and robust isoform quantification method for RNAseq data", 
  "links": [
    "http://www-rcf.usc.edu", 
    "http://bioinformatics.oxfordjournals.org"
  ], 
  "title": "WemIQ: an accurate and robust isoform quantification method for RNA-seq data", 
  "toolName": "WemIQ: an accurate and robust isoform quantification method for RNA-seq data", 
  "abstract": "Motivation: The deconvolution of isoform expression from RNA-seq remains challenging because of non-uniform read sampling and subtle differences among isoforms. Results: We present a weighted-log-likelihood expectation maximization method on isoform quan-tification (WemIQ). WemIQ integrates an effective bias removal with a weighted expectation maxi-mization (EM) algorithm to distribute reads among isoforms efficiently. The weight represents the oversampling or undersampling of sequence reads and is estimated through a generalized Poisson model without any presumption on the bias sources and formats. WemIQ significantly improves the quantification of isoform and gene expression as well as the derived exon inclusion rates. It provides robust expression estimates across different laboratories and protocols, which is valuable for the integrative analysis of RNA-seq. For the recent single-cell RNA-seq data, WemIQ also provides the opportunity to distinguish bias heterogeneity from true biological heterogeneity and uncovers smaller cell-to-cell expression variability. Availability and implementation: WemIQ can be downloaded from", 
  "summary": "Our WemIQ removes the bias heterogeneity in RNA-seq when deconvoluting isoform-level expression through a weighted EM algorithm, as shown in the flow chart in Supplementary Figure S1.\nBased on the expression estimates from WemIQ, Cufflinks and RSEM, the CV across 18 single-cell RNA-seq data is calculated and their empirical cumulative distribution functions are shown for A: 991 highly expressed genes; and B: 907 highly expressed isoforms\nIn summary, we propose WemIQ to quantify gene expression from the RNA-seq data with the transcript isoform resolution.", 
  "affiliations": [
    " Molecular and Computational Biology Department of Biological Sciences University of Southern California ", 
    " Department of Electrical Engineering Ming Hsieh University of Southern California "
  ], 
  "grants": [
    "Funding\nThis work was supported by the National Institutes of Health [R01GM097230]."
  ], 
  "acks": " ", 
  "authors": [
    " Jing Zhang", 
    " C.-C Jay Kuo", 
    " Liang Chen"
  ], 
  "keyWords": [
    [
      "_wemiq", 
      "genes", 
      "reads", 
      "biases", 
      "isoforms", 
      "wemiq"
    ]
  ], 
  "sourcelinks": [
    "http://www-rcf.usc.edu"
  ], 
  "technologies": [], 
  "dateCreated": "2014-11-19T01:15:08Z"
}